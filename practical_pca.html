
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Análisis de Componentes Principales &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=530fe47d" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/custom-checkpoint.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'practical_pca';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/custom.js?v=14184634"></script>
    <script src="_static/.ipynb_checkpoints/custom-checkpoint.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Evaluación de modelos" href="model_evaluation.html" />
    <link rel="prev" title="Redes Neuronales y Deep Learning" href="ann_model.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fotolihki.jpg" class="logo__image only-light" alt="Machine Learning - Home"/>
    <script>document.write(`<img src="_static/fotolihki.jpg" class="logo__image only-dark" alt="Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Profesor: Dr. Lihki Rubio
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="supervised_intro.html">Aprendizaje supervisado</a></li>
<li class="toctree-l1"><a class="reference internal" href="knn_model.html"><span class="math notranslate nohighlight">\(k\)</span>-Vecinos más cercanos</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_model.html">Modelos lineales</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes_model.html">Clasificadores Naive Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="decisiontree_model.html">Árboles de decisión</a></li>

<li class="toctree-l1"><a class="reference internal" href="svm_model.html">Máquinas de vectores de soporte</a></li>
<li class="toctree-l1"><a class="reference internal" href="ann_model.html">Redes Neuronales y Deep Learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Análisis de Componentes Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">Evaluación de modelos</a></li>
<li class="toctree-l1"><a class="reference internal" href="chains_pipelines.html">Cadenas de Algoritmos y Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/practical_pca.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Análisis de Componentes Principales</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concepto-basico">Concepto básico</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autovalores-autovectores-y-componentes-principales">Autovalores, Autovectores y Componentes Principales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proceso-de-derivacion-de-componentes-principales-y-propiedades">Proceso de Derivación de Componentes Principales y Propiedades</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#el-problema-de-los-autovalores-de-la-matriz-de-varianza-covarianza-muestral-y-los-componentes-principales">El problema de los autovalores de la matriz de varianza-covarianza muestral y los componentes principales</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autovalores-y-autovectores-de-una-matriz-simetrica">Autovalores y autovectores de una matriz simétrica</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estandarizacion-y-matriz-de-correlacion-muestral">Estandarización y matriz de correlación muestral</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reduccion-de-dimension-y-perdida-de-informacion">Reducción de dimensión y pérdida de información</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">Implementación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-al-analisis-de-componentes-principales">Introducción al análisis de componentes principales</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-para-reduccion-de-dimension">PCA para reducción de dimensión</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-para-visualizacion-digitos-escritos-a-mano">PCA para visualización: Dígitos escritos a mano</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-del-numero-de-componentes">Selección del número de componentes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-como-filtro-de-ruido">PCA como filtro de ruido</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-eigenfaces">Ejemplo Eigenfaces</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="analisis-de-componentes-principales">
<h1>Análisis de Componentes Principales<a class="headerlink" href="#analisis-de-componentes-principales" title="Link to this heading">#</a></h1>
<section id="analisis">
<h2>Análisis<a class="headerlink" href="#analisis" title="Link to this heading">#</a></h2>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>El <em><strong>Análisis de Componentes Principales (PCA)</strong></em> es un método para consolidar las variables mutuamente correlacionadas de datos observados multidimensionales en nuevas variables mediante <em><strong>combinaciones lineales de las variables originales con una pérdida mínima de información en los datos observados</strong></em>.</p></li>
<li><p><em><strong>PCA permite extraer información relevante de los datos</strong></em> al fusionar múltiples variables que caracterizan a los individuos. También se puede utilizar como técnica para <em><strong>comprender visualmente las estructuras de datos</strong></em> al consolidar datos de alta dimensión en un número menor de variables, <em><strong>realizando una reducción de dimensión y proyectando los resultados en una línea unidimensional, plano bidimensional o espacio tridimensional</strong></em>.</p></li>
<li><p>En este capítulo se discuten los <em><strong>conceptos básicos y el propósito de PCA en el contexto de la linealidad, así como su aplicación en la descompresión de imágenes y la descomposición de valores singulares de matrices de datos</strong></em>. También se aborda el <em><strong>PCA no lineal utilizando el método del kernel</strong></em> para la extracción de información de datos multidimensionales con estructuras no lineales complejas.</p></li>
</ul>
</div>
</section>
<section id="concepto-basico">
<h2>Concepto básico<a class="headerlink" href="#concepto-basico" title="Link to this heading">#</a></h2>
<figure class="align-center" id="pca-3projections-fig">
<a class="reference internal image-reference" href="_images/pca_3projections.png"><img alt="_images/pca_3projections.png" src="_images/pca_3projections.png" style="width: 420.0px; height: 297.59999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 68 </span><span class="caption-text">Proyección en tres ejes diferentes, (a), (b) y (c), y la dispersión de los datos. Fuente <span id="id1">[<a class="reference internal" href="biblio.html#id20" title="S. Konishi. Introduction to Multivariate Analysis: Linear and Nonlinear Modeling. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis, 2014. ISBN 9781466567283. URL: https://books.google.com.co/books?id=fcuuAwAAQBAJ.">Konishi, 2014</a>]</span>.</span><a class="headerlink" href="#pca-3projections-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>La <a class="reference internal" href="#pca-3projections-fig"><span class="std std-numref">Fig. 68</span></a> muestra un gráfico de las puntuaciones de los exámenes de matemáticas (<span class="math notranslate nohighlight">\(x_{1}\)</span>) e inglés (<span class="math notranslate nohighlight">\(x_{2}\)</span>) de 25 estudiantes. <em><strong>Proyectando estos datos bidimensionales en un solo eje mediante la sustitución de las puntuaciones de ambas materias en la siguiente ecuación, se transforman en datos unidimensionales</strong></em>.</p>
<div class="math notranslate nohighlight">
\[
  y=w_{1}x_{1}+w_{2}x_{2}.
  \]</div>
</li>
</ul>
<ul class="simple">
<li><p>La <a class="reference internal" href="#pca-3projections-fig"><span class="std std-numref">Fig. 68</span></a> muestra la <em><strong>proyección en tres ejes diferentes, (a), (b) y (c)</strong></em>. La <em><strong>dispersión de los datos</strong></em> (como medida de la varianza) para cada eje se indica mediante una <em><strong>flecha de doble cabeza</strong></em> (<span class="math notranslate nohighlight">\(\boldsymbol{\leftarrow-\rightarrow}\)</span>).</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Esta figura muestra que la varianza es mayor cuando se proyecta en el eje (b) que en el eje (a), y mayor en el eje (c) que en el eje (b). <em><strong>El eje con mayor varianza muestra con mayor claridad la separación entre los datos</strong></em>. Surge la pregunta inmediata de <em><strong>cómo encontrar mejor el eje de proyección que produce la máxima varianza</strong></em>.</p></li>
<li><p>En <em><strong>PCA</strong></em>, utilizamos una <em><strong>secuencia de ejes de proyección con este propósito</strong></em>.</p>
<ul>
<li><p>Primero encontramos el <em><strong>eje de proyección, conocido como el primer componente principal, que maximiza la varianza global</strong></em>.</p></li>
<li><p>Luego encontramos el <em><strong>eje de proyección que maximiza la varianza bajo la restricción de ortogonalidad al primer componente principal</strong></em>. Este eje se conoce como el <em><strong>segundo componente principal</strong></em>.</p></li>
<li><p>La <em><strong>ortogonalidad entre las componentes principales en PCA</strong></em> garantiza una <em><strong>representación óptima de la variabilidad de los datos</strong></em>, lo que <em><strong>facilita la interpretación</strong></em> y el análisis de los mismos.</p></li>
</ul>
</li>
</ul>
</div>
<ul class="simple">
<li><p>Apliquemos lo mencionado en la anterior observación a datos bidimensionales. Denotamos los <span class="math notranslate nohighlight">\(n\)</span> <em><strong>datos observados bidimensionales</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}=(x_{1}, x_{2})^{T}\)</span> como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-bidimentional-data-pca">
<span class="eqno">(69)<a class="headerlink" href="#equation-bidimentional-data-pca" title="Link to this equation">#</a></span>\[\begin{split}
\boldsymbol{x}_{1}=\begin{pmatrix}x_{11}\\x_{12}\end{pmatrix},~\boldsymbol{x}_{2}=\begin{pmatrix}x_{21}\\x_{22}\end{pmatrix},\dots,\boldsymbol{x}_{n}=\begin{pmatrix}x_{n1}\\x_{n2}\end{pmatrix}.
\end{split}\]</div>
<ul>
<li><p>Estos <span class="math notranslate nohighlight">\(n\)</span> <em><strong>datos bidimensionales se proyectan sobre</strong></em> <span class="math notranslate nohighlight">\(y=w_{1}x_{1}+w_{2}x_{2}\)</span>, y luego se expresan como</p>
<div class="math notranslate nohighlight">
\[
    y_{i}=w_{1}x_{i1}+w_{2}x_{i2}=\boldsymbol{w}^{T}\boldsymbol{x}_{i},~i=1,2,\dots,n,
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\boldsymbol{w}=(w_{1}, w_{2})^{T}\)</span> representa un <em><strong>vector de coeficientes</strong></em>.</p>
</li>
</ul>
<figure class="align-center" id="id4">
<img alt="_images/Principal20Analysis20principal.gif" src="_images/Principal20Analysis20principal.gif" />
<figcaption>
<p><span class="caption-number">Fig. 69 </span><span class="caption-text">Simulación de <em><strong>múltiples ejes de proyección</strong></em> <span class="math notranslate nohighlight">\(y=w_{1}x_{1}+w_{2}x_{2}\)</span> en el PCA.</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="dropdown admonition">
<p class="admonition-title">Datos proyectados repetidos?</p>
<ul class="simple">
<li><p>Es posible que los <em><strong>datos proyectados en las componentes principales sean repetidos en ciertas circunstancias</strong></em>, especialmente cuando los datos originales tienen una <em><strong>alta dimensionalidad</strong></em> o cuando hay <em><strong>baja variabilidad en algunas dimensiones</strong></em>.</p></li>
<li><p>Esto es importante tener en cuenta al interpretar los resultados de PCA y al realizar análisis subsiguientes. Cuando esto ocurre, puedes considerar las siguientes estrategias:</p>
<ul>
<li><p><em>Normalización de los datos</em></p></li>
<li><p><em>Reducción de multicolinealidad</em></p></li>
<li><p>Usar <em>Análisis de Factores</em></p></li>
</ul>
</li>
</ul>
</div>
<ul>
<li><p>La media de los datos <span class="math notranslate nohighlight">\(y_{1}, y_{2}, \dots, y_{n}\)</span> que se <em><strong>proyectan sobre el eje de proyección</strong></em> es</p>
<div class="math notranslate nohighlight">
\[
    \overline{y}=\frac{1}{n}\sum_{i=1}^{n}y_{i}=\frac{1}{n}\sum_{i=1}^{n}(w_{1}x_{i1}+w_{2}x_{i2})=w_{1}\overline{x}_{1}+w_{2}\overline{x}_{2}=\boldsymbol{w}\overline{\boldsymbol{x}},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\overline{\boldsymbol{x}}=(\overline{x}_{1}, \overline{x}_{2})^{T}\)</span> es el vector de medias muestrales que <em><strong>contiene como sus componentes principales la media muestral</strong></em> <span class="math notranslate nohighlight">\(\overline{x}_{j}=n^{-1}\sum_{i=1}^{n}x_{ij}~(j=1,2)\)</span> de cada variable.</p>
</li>
</ul>
<ul>
<li><p>La <em><strong>varianza</strong></em> puede ser expresada como</p>
<div class="math notranslate nohighlight" id="equation-variance-quad-error">
<span class="eqno">(70)<a class="headerlink" href="#equation-variance-quad-error" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    s_{y}^{2} &amp;= \frac{1}{n}\sum_{i=1}^{n}(y_{i}-\overline{y})^{2}\\
    &amp;= \frac{1}{n}\sum_{i=1}^{n}\left\{w_{1}(x_{i1}-\overline{x}_{1})+w_{2}(x_{i2}-\overline{x}_{2})\right\}^{2}\\
    &amp;= w_{1}^{2}\frac{1}{n}\sum_{i=1}^{n}(x_{i1}-\overline{x}_{1})^{2}+2w_{1}w_{2}\frac{1}{n}\sum_{i=1}^{n}(x_{i1}-\overline{x}_{1})(x_{i2}-\overline{x}_{2})+w_{2}^{2}\frac{1}{n}\sum_{i=1}^{n}(x_{i2}-\overline{x}_{2})^{2}\\[1mm]
    &amp;= w_{1}^{2}s_{11}+2w_{1}w_{2}s_{12}+w_{2}^{2}s_{22}\\[4mm]
    &amp;= \boldsymbol{w}^{T}S\boldsymbol{w},
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(S\)</span> es la <em><strong>matriz de covarianza muestral</strong></em>, definida por (<code class="docutils literal notranslate"><span class="pre">verifíquelo</span></code>)</p>
<div class="math notranslate nohighlight" id="equation-sample-covariance-matrix">
<span class="eqno">(71)<a class="headerlink" href="#equation-sample-covariance-matrix" title="Link to this equation">#</a></span>\[\begin{split}
    S=\begin{pmatrix}s_{11} &amp; s_{12}\\s_{21} &amp; s_{22}\end{pmatrix},~s_{jk}=\frac{1}{n}\sum_{i=1}^{n}(x_{ij}-\overline{x}_{j})(x_{ik}-\overline{x}_{k}),~j,k=1,2. 
    \end{split}\]</div>
</li>
</ul>
<div class="admonition-maximizacion-de-la-varianza admonition">
<p class="admonition-title">Maximización de la varianza</p>
<ul class="simple">
<li><p>El problema de encontrar el vector de coeficientes <span class="math notranslate nohighlight">\(\boldsymbol{w}=(w_{1}, w_{2})^{T}\)</span>, <em><strong>que corresponde a la máxima varianza</strong></em> para los <span class="math notranslate nohighlight">\(n\)</span> datos bidimensionales proyectados sobre <span class="math notranslate nohighlight">\(y=w_{1}x_{1}+w_{2}x_{2}\)</span>, se convierte en el <em><strong>problema de maximización de la varianza</strong></em> <span class="math notranslate nohighlight">\(S_{y}=\boldsymbol{w}^{T}S\boldsymbol{w}\)</span> en la Ecuación <a class="reference internal" href="#equation-variance-quad-error">(70)</a> <em><strong>bajo la restricción</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{w}^{T}\boldsymbol{w}=1\)</span>. Esta restricción se aplica porque <span class="math notranslate nohighlight">\(\|\boldsymbol{w}\|\)</span> <em><strong>sería infinitamente grande sin ella y la varianza divergiría</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>El <em><strong>problema de maximización de la varianza bajo esta restricción se puede resolver mediante el método de los multiplicadores de Lagrange</strong></em>, encontrando el punto estacionario (donde la derivada se hace 0) de la función Lagrangiana.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(\boldsymbol{w}, \lambda)=\boldsymbol{w}^{T}S\boldsymbol{w}+\lambda(1-\boldsymbol{w}^{T}\boldsymbol{w})
\]</div>
<ul>
<li><p>Dado que <span class="math notranslate nohighlight">\(\partial(\boldsymbol{w}^{T}S\boldsymbol{w})/\partial\boldsymbol{w}=2S\boldsymbol{w}\)</span> (ver Ecuación <a class="reference internal" href="linear_model.html#equation-quadratic-form-dedrivate">(13)</a>) y <span class="math notranslate nohighlight">\(\partial(\boldsymbol{w}^{T}\boldsymbol{w})/\partial\boldsymbol{w}=2\boldsymbol{w}\)</span>, entonces <span class="math notranslate nohighlight">\(\partial L(\boldsymbol{w}, \lambda)/\partial\boldsymbol{w}=0\)</span> sii</p>
<div class="math notranslate nohighlight" id="equation-eigen-valuesvector-problem">
<span class="eqno">(72)<a class="headerlink" href="#equation-eigen-valuesvector-problem" title="Link to this equation">#</a></span>\[
  S\boldsymbol{w}=\lambda\boldsymbol{w}.
  \]</div>
</li>
</ul>
<ul>
<li><p>Esta solución es el <em><strong>vector propio</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{w}_{1}=(w_{11}, w_{12})^{T}\)</span> <em><strong>correspondiente al valor propio máximo</strong></em> <span class="math notranslate nohighlight">\(\lambda_{1}\)</span> obtenido al <em><strong>resolver la ecuación característica para la matriz de varianza-covarianza</strong></em> de la muestra <span class="math notranslate nohighlight">\(S\)</span>. Por consiguiente, <em><strong>el primer componente principal</strong></em> <span class="math notranslate nohighlight">\(y_{1}\)</span> está dado por</p>
<div class="math notranslate nohighlight">
\[
  y_{1}=w_{11}x_{1}+w_{12}x_{2}=\boldsymbol{w}_{1}^{T}\boldsymbol{x}.
  \]</div>
</li>
</ul>
<ul>
<li><p>La varianza del <em><strong>primer componente principal</strong></em> <span class="math notranslate nohighlight">\(y_{1}\)</span> se alcanza con <span class="math notranslate nohighlight">\(\boldsymbol{w}=\boldsymbol{w}_{1}\)</span>, el cual maximiza la varianza de la Ecuación <a class="reference internal" href="#equation-variance-quad-error">(70)</a>, y es, por lo tanto,</p>
<div class="math notranslate nohighlight" id="equation-variance-firstpc">
<span class="eqno">(73)<a class="headerlink" href="#equation-variance-firstpc" title="Link to this equation">#</a></span>\[
  s_{y_{1}}^{2}=\boldsymbol{w}_{1}^{T}S\boldsymbol{w}_{1}=\lambda_{1}.
  \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Nótese que, la <a class="reference internal" href="#equation-variance-firstpc">(73)</a> es obtenida gracias a la relación entre autovalor-autovector <span class="math notranslate nohighlight">\(S\boldsymbol{w}_{1}=\lambda_{1}\boldsymbol{w}_{1}\)</span> de la matriz simétrica <span class="math notranslate nohighlight">\(S\)</span>, y que el vector <span class="math notranslate nohighlight">\(\boldsymbol{w}_{1}\)</span> es normalizado a longitud 1 (<code class="docutils literal notranslate"><span class="pre">verifíquelo</span></code>).</p></li>
</ul>
<ul>
<li><p>El <em><strong>segundo componente principal</strong></em>, al proyectar los datos bidimensionales en <span class="math notranslate nohighlight">\(y=w_{1}x_{1}+w_{2}x_{2}\)</span> bajo la <em><strong>restricción de normalización</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{w}\boldsymbol{w}^{T}=1\)</span> junto con la <em><strong>ortogonalidad al primer componente principal</strong></em>, se define como el vector de coeficientes que <em><strong>maximiza la varianza</strong></em> <span class="math notranslate nohighlight">\(S_{y_{2}}^{2}=\boldsymbol{w}^{T}S\boldsymbol{w}\)</span> con <span class="math notranslate nohighlight">\(\boldsymbol{w}_{1}^{T}\boldsymbol{w}=0\)</span>.</p></li>
<li><p>La solución, <em><strong>de la misma manera que para el primer componente principal</strong></em>, se obtiene como el punto estacionario respecto a <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> para la función Lagrangiana</p>
<div class="math notranslate nohighlight">
\[
  L(\boldsymbol{w}, \lambda, \gamma)=\boldsymbol{w}^{T}S\boldsymbol{w}+\lambda(1-\boldsymbol{w}^{T}\boldsymbol{w})+\gamma\boldsymbol{w}_{1}^{T}\boldsymbol{w},
  \]</div>
<p>donde <span class="math notranslate nohighlight">\(\lambda, \gamma\)</span> son <em><strong>multiplicadores de Lagrange</strong></em>.</p>
</li>
</ul>
<ul>
<li><p>Cuando <em><strong>diferenciamos parcialmente</strong></em> la ecuación anterior respecto a <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> y establecemos el <em><strong>resultado igual a 0</strong></em> (<span class="math notranslate nohighlight">\(\partial_{\boldsymbol{w}}L=\boldsymbol{0}\)</span>), obtenemos la ecuación</p>
<div class="math notranslate nohighlight" id="equation-system-2nd-pc">
<span class="eqno">(74)<a class="headerlink" href="#equation-system-2nd-pc" title="Link to this equation">#</a></span>\[
  \frac{\partial L(\boldsymbol{w},\lambda, \gamma)}{\partial\boldsymbol{w}}=0\Leftrightarrow 2S\boldsymbol{w}-2\lambda w+\gamma\boldsymbol{w}_{1}=\boldsymbol{0}.
  \]</div>
</li>
</ul>
<ul>
<li><p>Si luego <em><strong>multiplicamos a la izquierda por el vector propio</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{w}_{1}^{T}\)</span> correspondiente al valor propio máximo <span class="math notranslate nohighlight">\(\lambda_{1}\)</span>, obtenemos</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{align*}
  2\boldsymbol{w}_{1}^{T}S\boldsymbol{w}-2\lambda\textcolor{red}{\boldsymbol{w}_{1}^{T}\boldsymbol{w}}+\gamma\textcolor{red}{\boldsymbol{w}_{1}^{T}\boldsymbol{w}_{1}} &amp;= 0\Leftrightarrow\begin{cases}\boldsymbol{w}_{1}^{T}\boldsymbol{w}&amp;=0\\\boldsymbol{w}_{1}^{T}\boldsymbol{w}_{1}&amp;=1\end{cases}\\
  2\textcolor{red}{\boldsymbol{w}_{1}^{T}S}\boldsymbol{w}+\gamma &amp;= 0\Leftrightarrow (S\boldsymbol{w}_{1}=\lambda_{1}\boldsymbol{w}_{1}\Leftrightarrow\boldsymbol{w}_{1}^{T}S=\lambda_{1}\boldsymbol{w}_{1}^{T})\\[3mm] 
  2\textcolor{red}{\lambda\boldsymbol{w}_{1}^{T}}\boldsymbol{w}+\gamma &amp;= 0\Leftrightarrow\\[3mm]
  \gamma &amp;=0
  \end{align*}
  \end{split}\]</div>
</li>
</ul>
<ul>
<li><p>Entonces la Ecuación <a class="reference internal" href="#equation-system-2nd-pc">(74)</a> se convierte en</p>
<div class="math notranslate nohighlight" id="equation-eigenvalue-problema-system-2nd-pc">
<span class="eqno">(75)<a class="headerlink" href="#equation-eigenvalue-problema-system-2nd-pc" title="Link to this equation">#</a></span>\[\begin{split}
  2Sw-2\lambda\boldsymbol{w}+\gamma\boldsymbol{w}_{1}=2S\boldsymbol{w}-2\lambda\boldsymbol{2}=0\Leftrightarrow S\boldsymbol{w}=\lambda\boldsymbol{w},~\text{donde}~\begin{cases}\boldsymbol{w}_{1}^{T}\boldsymbol{w}&amp;=0\\\boldsymbol{w}_{1}^{T}\boldsymbol{w}_{1}&amp;=1\end{cases}
  \end{split}\]</div>
</li>
</ul>
<ul>
<li><p>La solución del problema de <em><strong>autovalores y autovectores</strong></em> asociado a la Ecuación <a class="reference internal" href="#equation-eigenvalue-problema-system-2nd-pc">(75)</a> entrega el par autovector/autovalor <span class="math notranslate nohighlight">\((\lambda_{2}, \boldsymbol{w}_{2})\)</span>, donde <span class="math notranslate nohighlight">\(\boldsymbol{w}_{2}=(w_{21}, w_{22})^{T}\)</span>.</p></li>
<li><p>El <em><strong>segundo componente principal</strong></em> puede ser dado por la siguiente ecuación</p>
<div class="math notranslate nohighlight">
\[
  y_{2}=w_{21}x_{1}+w_{22}x_{2}=\boldsymbol{w}_{2}^{T}\boldsymbol{x}
  \]</div>
</li>
</ul>
<ul class="simple">
<li><p>De la relación entre el <em><strong>autovalor y el autovector de la matriz simétrica</strong></em> <span class="math notranslate nohighlight">\(S\)</span>, la varianza del segundo componente principal <span class="math notranslate nohighlight">\(y_{2}\)</span> se convierte en <span class="math notranslate nohighlight">\(s_{y_{2}}^{2}=\boldsymbol{w}_{2}S\boldsymbol{w}_{2}=\lambda_{2}\)</span> de la misma manera que en la Ecuación <a class="reference internal" href="#equation-variance-firstpc">(73)</a>.</p></li>
<li><p>El <em><strong>PCA basado en los datos bidimensionales</strong></em> en Ecuación <a class="reference internal" href="#equation-bidimentional-data-pca">(69)</a> es esencialmente un <em><strong>problema de encontrar los autovalores y autovectores de la matriz de varianza-covarianza muestral</strong></em> <span class="math notranslate nohighlight">\(S\)</span>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Dado que <em><strong>cada componente principal <code class="docutils literal notranslate"><span class="pre">captura</span> <span class="pre">una</span> <span class="pre">parte</span> <span class="pre">única</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">variabilidad</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">datos</span></code></strong></em>, <em><strong>cada una contiene información diferente sobre la estructura de los datos originales</strong></em>. Esto significa que, <em>mientras más componentes principales tengamos, más información detallada obtenemos sobre la distribución de los datos en el espacio de características</em>.</p></li>
</ul>
</div>
</section>
<section id="autovalores-autovectores-y-componentes-principales">
<h2>Autovalores, Autovectores y Componentes Principales<a class="headerlink" href="#autovalores-autovectores-y-componentes-principales" title="Link to this heading">#</a></h2>
<ul>
<li><p>Denotamos los <em><strong>autovalores de la matriz de varianza-covarianza muestral</strong></em> <span class="math notranslate nohighlight">\(S\)</span> en orden descendente según su magnitud como <span class="math notranslate nohighlight">\(\lambda_{1}\geq\lambda_{2}\geq0\)</span>, y los <em><strong>correspondientes autovectores</strong></em> mutuamente <em><strong>ortogonales normalizados a longitud 1</strong></em> como <span class="math notranslate nohighlight">\(\boldsymbol{w}_{1}=(w_{11}, w_{12})^{T}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{w}_{2}=(w_{21}, w_{22})^{T}\)</span>.</p></li>
<li><p>Los <em><strong>primeros y segundos componentes principales</strong></em> y sus respectivas <em><strong>varianzas</strong></em> se dan entonces de la siguiente manera</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{align*}
  y_{1} &amp;= w_{11}x_{1}+w_{12}x_{2},\quad\text{var}(y_{1})=\lambda_{1}\\
  y_{2} &amp;= w_{21}x_{1}+w_{22}x_{2},\quad\text{var}(y_{2})=\lambda_{2}
  \end{align*}
  \end{split}\]</div>
</li>
</ul>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="_images/first_second_pca_bidimentional.png"><img alt="_images/first_second_pca_bidimentional.png" src="_images/first_second_pca_bidimentional.png" style="width: 496.29999999999995px; height: 386.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 70 </span><span class="caption-text"><em><strong>Valores propios</strong></em>, <em><strong>primeros y segundos componentes principales</strong></em>. Fuente <span id="id2">[<a class="reference internal" href="biblio.html#id20" title="S. Konishi. Introduction to Multivariate Analysis: Linear and Nonlinear Modeling. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis, 2014. ISBN 9781466567283. URL: https://books.google.com.co/books?id=fcuuAwAAQBAJ.">Konishi, 2014</a>]</span>.</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>El método de los <em><strong>multiplicadores de Lagrange</strong></em>, como se utiliza en esta sección, proporciona una herramienta para <em><strong>encontrar los puntos estacionarios de funciones multivariables de valores reales</strong></em> bajo <em><strong>restricciones de igualdad y desigualdad en las variables</strong></em>.</p></li>
</ul>
</section>
<section id="proceso-de-derivacion-de-componentes-principales-y-propiedades">
<h2>Proceso de Derivación de Componentes Principales y Propiedades<a class="headerlink" href="#proceso-de-derivacion-de-componentes-principales-y-propiedades" title="Link to this heading">#</a></h2>
<ul>
<li><p>En general, denotamos como <span class="math notranslate nohighlight">\(\boldsymbol{x}=(x_{1}, x_{2},\dots,x_{p})^{T}\)</span> las <span class="math notranslate nohighlight">\(p\)</span> <em><strong>variables representando las características individuales</strong></em>. Basados en los datos observados <span class="math notranslate nohighlight">\(\boldsymbol{x}_{1}, \boldsymbol{x}_{2},\dots, \boldsymbol{x}_{n}\)</span>; <span class="math notranslate nohighlight">\(p\)</span>-dimensionales, para las <span class="math notranslate nohighlight">\(p\)</span> variables, obtenemos la <em><strong>matriz de varianza-covarianza muestral</strong></em></p>
<div class="math notranslate nohighlight" id="equation-covariance-variance-nobs-pdimentionals">
<span class="eqno">(76)<a class="headerlink" href="#equation-covariance-variance-nobs-pdimentionals" title="Link to this equation">#</a></span>\[
  S=(s_{jk})=\frac{1}{n}\sum_{i=1}^{n}(\boldsymbol{x}_{i}-\overline{\boldsymbol{x}})(\boldsymbol{x}_{i}-\boldsymbol{x})^{T}
  \]</div>
<p>donde <span class="math notranslate nohighlight">\(\overline{\boldsymbol{x}}\)</span> es el vector de medias muestrales, <span class="math notranslate nohighlight">\(p\)</span>-dimensional, y <span class="math notranslate nohighlight">\(s_{jk}=\sum_{i=1}^{n}(x_{ij}-\overline{x}_{j})(x_{ik}-\overline{x}_{k})/n\)</span> (<code class="docutils literal notranslate"><span class="pre">escriba</span> <span class="pre">cada</span> <span class="pre">componente</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(S\)</span> <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">forma</span> <span class="pre">matricial</span></code>).</p>
</li>
</ul>
<ul>
<li><p>Siguiendo el concepto básico de la <em><strong>derivación de componentes principales</strong></em>, como se describe para <em><strong>datos bidimensionales</strong></em> en la sección anterior, primero <em><strong>proyectamos los</strong></em> <span class="math notranslate nohighlight">\(n\)</span> <em><strong>datos observados</strong></em>, <span class="math notranslate nohighlight">\(p\)</span><em><strong>-dimensionales sobre el eje de proyección</strong></em></p>
<div class="math notranslate nohighlight" id="equation-pdimensional-projection-pca">
<span class="eqno">(77)<a class="headerlink" href="#equation-pdimensional-projection-pca" title="Link to this equation">#</a></span>\[
  y=w_{1}x_{1}+w_{2}x_{2}+\cdots+w_{p}x_{p}=\boldsymbol{w}^{T}\boldsymbol{x},
  \]</div>
<p>y obtenemos los <em><strong>datos unidimensionales</strong></em> <span class="math notranslate nohighlight">\(y_{i}=\boldsymbol{w}^{T}\boldsymbol{x}_{i}~(i=1,2,\dots,n;~\boldsymbol{x}_{i}~p\text{-dimensional})\)</span>.</p>
</li>
</ul>
<figure class="align-center">
<img alt="_images/dim_red.jpeg" src="_images/dim_red.jpeg" />
</figure>
<ul>
<li><p>La <em><strong>media de los datos proyectados</strong></em>, representada por <span class="math notranslate nohighlight">\(\overline{y}=n^{-1}\sum_{i=1}^{n}\boldsymbol{w}^{T}\boldsymbol{x}_{i}=\boldsymbol{w}^{T}\overline{\boldsymbol{x}}\)</span> (<code class="docutils literal notranslate"><span class="pre">escriba</span> <span class="pre">el</span> <span class="pre">vector</span> <span class="pre">de</span> <span class="pre">componentes</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(\overline{\boldsymbol{x}}\)</span>) permite expresar la varianza como:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{align*}
  s_{y}^{2} &amp;= \frac{1}{n}\sum_{i=1}^{n}(y_{i}-\overline{y})^{2}\\
  &amp;= \frac{1}{n}\sum_{i=1}^{n}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}-\boldsymbol{w}^{T}\overline{\boldsymbol{x}})^{2}\\
  &amp;= \frac{1}{n}\sum_{i=1}^{n}(\boldsymbol{w}^{T}(\boldsymbol{x}_{i}-\overline{\boldsymbol{x}}))(\boldsymbol{w}^{T}(\boldsymbol{x}_{i}-\overline{\boldsymbol{x}}))^{T}\\
  &amp;= \boldsymbol{w}^{T}\frac{1}{n}\sum_{i=1}^{n}(\boldsymbol{x}_{i}-\overline{\boldsymbol{x}})(\boldsymbol{x}_{i}-\overline{\boldsymbol{x}})^{T}\boldsymbol{w}\\
  &amp;= \boldsymbol{w}^{T}S\boldsymbol{w}.
  \end{align*}
  \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>De manera <em><strong>similar a lo aplicado en dos dimensiones</strong></em>, el <em><strong>vector de coeficientes que maximiza la varianza de los datos proyectados</strong></em> puede expresarse como el <em><strong>autovector</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{w}_{1}\)</span> <em><strong>correspondiente al mayor autovalor</strong></em> <span class="math notranslate nohighlight">\(\lambda_{1}\)</span> de la <em><strong>matriz de varianza-covarianza muestral</strong></em> <span class="math notranslate nohighlight">\(S\)</span>.</p></li>
<li><p>El eje de proyección <span class="math notranslate nohighlight">\(y_{1}=\boldsymbol{w}_{1}^{T}\boldsymbol{x}\)</span> con <span class="math notranslate nohighlight">\(\boldsymbol{w}_{1}\)</span> como vector de coeficientes es entonces el <em><strong>primer componente principal</strong></em>. De la misma manera, nuevamente, la <em><strong>varianza en este eje de proyección es el mayor autovalor</strong></em> <span class="math notranslate nohighlight">\(\lambda_{1}\)</span></p></li>
<li><p>El <em><strong>segundo componente principal</strong></em> es el eje que, cumpliendo con la <em><strong>ortogonalidad al primer componente principal, maximiza la varianza de los datos proyectados</strong></em> <span class="math notranslate nohighlight">\(p\)</span>-dimensionales, y, por lo tanto, es el <em><strong>eje de proyección generado por el autovector</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{w}_{2}\)</span>, que corresponde al <em><strong>segundo mayor autovalor</strong></em> <span class="math notranslate nohighlight">\(\lambda_{2}\)</span> de la matriz <span class="math notranslate nohighlight">\(S\)</span></p></li>
</ul>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Continuando de la misma manera, el <em><strong>tercer componente principal</strong></em> se define como el eje que, <em><strong>cumpliendo con la ortogonalidad al primer y segundo componente principal, maximiza la varianza de los datos proyectados</strong></em> <span class="math notranslate nohighlight">\(p\)</span><em><strong>-dimensionales</strong></em>. Repitiendo sucesivamente este proceso, podemos en principio derivar <span class="math notranslate nohighlight">\(p\)</span> <em><strong>componentes principales para las combinaciones lineales de las variables originales</strong></em>.</p></li>
<li><p>La <em><strong>invertibilidad de la matriz de covarianza en PCA se garantiza</strong></em> debido a la necesidad de que <em><strong>las variables originales sean linealmente independientes</strong></em>, lo que conduce a una <em><strong>matriz de covarianza no singular</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Como resultado, <em><strong>PCA se convierte así en el problema de los autovalores de la matriz de varianza-covarianza muestral</strong></em> <span class="math notranslate nohighlight">\(S\)</span>, como se describe a continuación.</p></li>
</ul>
<section id="el-problema-de-los-autovalores-de-la-matriz-de-varianza-covarianza-muestral-y-los-componentes-principales">
<h3>El problema de los autovalores de la matriz de varianza-covarianza muestral y los componentes principales<a class="headerlink" href="#el-problema-de-los-autovalores-de-la-matriz-de-varianza-covarianza-muestral-y-los-componentes-principales" title="Link to this heading">#</a></h3>
<ul>
<li><p>Denotemos <span class="math notranslate nohighlight">\(S\)</span> como una <em><strong>matriz de varianza-covarianza muestral</strong></em> basada en <span class="math notranslate nohighlight">\(n\)</span> <em><strong>datos observados</strong></em> <span class="math notranslate nohighlight">\(p\)</span>-dimensionales. Como se observa en la Ecuación <a class="reference internal" href="#equation-covariance-variance-nobs-pdimentionals">(76)</a>, es una <em><strong>matriz simétrica de orden</strong></em> <span class="math notranslate nohighlight">\(p\)</span>. Luego, denotamos los <span class="math notranslate nohighlight">\(p\)</span> autovalores</p>
<div class="math notranslate nohighlight">
\[
  \lambda_{1}\geq\lambda_{2}\geq\cdots\lambda_{i}\geq\cdots\geq\lambda_{p}\geq0,
  \]</div>
<p>dados como la <em><strong>solución a la ecuación característica</strong></em> de <span class="math notranslate nohighlight">\(S,~|S-\lambda I_{p}|=0\)</span>.</p>
</li>
</ul>
<ul>
<li><p>Además, <em><strong>denotamos los autovectores</strong></em> <span class="math notranslate nohighlight">\(p\)</span>-dimensionales <em><strong>normalizados a longitud 1</strong></em> correspondientes a estos autovalores como</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \boldsymbol{w}_{1}=
  \begin{pmatrix}
  w_{11}\\
  w_{12}\\
  \vdots\\
  w_{1p}
  \end{pmatrix},~
  \boldsymbol{w}_{2}=
  \begin{pmatrix}
  w_{21}\\
  w_{22}\\
  \vdots\\
  w_{2p}
  \end{pmatrix},\dots,
  \boldsymbol{w}_{p}=
  \begin{pmatrix}
  w_{p1}\\
  w_{p2}\\
  \vdots\\
  w_{pp}
  \end{pmatrix}
  \end{split}\]</div>
</li>
</ul>
<ul>
<li><p>Para estos autovectores, la <em><strong>normalización</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{w}_{i}^{T}\boldsymbol{w}_{i}=1\)</span> para longitud del vector 1 y la <em><strong>ortogonalidad</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{w}_{i}^{T}\boldsymbol{w}_{j}=0\)</span> (para <span class="math notranslate nohighlight">\(i\neq j\)</span>) quedan así establecidas.</p></li>
<li><p>Los <span class="math notranslate nohighlight">\(p\)</span> <em><strong>componentes principales</strong></em> y su <em><strong>varianza expresada</strong></em> en términos de la <em><strong>combinación lineal de las variables originales</strong></em> pueden ser proporcionados en orden de la siguiente manera</p>
<div class="math notranslate nohighlight" id="equation-systemof-variance-principal-components">
<span class="eqno">(78)<a class="headerlink" href="#equation-systemof-variance-principal-components" title="Link to this equation">#</a></span>\[\begin{split}
  \begin{align*}
  y_{1} &amp;= w_{11}x_{1}+w_{12}x_{2}+\cdots+w_{1p}x_{p}=\boldsymbol{w}_{1}^{T}\boldsymbol{x},\quad\text{var}(y_{1})=\lambda_{1},\\
  y_{2} &amp;= w_{21}x_{1}+w_{22}x_{2}+\cdots+w_{2p}x_{p}=\boldsymbol{w}_{2}^{T}\boldsymbol{x},\quad\text{var}(y_{2})=\lambda_{2},\\
  &amp;\vdots\\
  y_{p} &amp;= w_{p1}x_{1}+w_{p2}x_{2}+\cdots+w_{pp}x_{p}=\boldsymbol{w}_{p}^{T}\boldsymbol{x},\quad\text{var}(y_{p})=\lambda_{p}.
  \end{align*}
  \end{split}\]</div>
</li>
</ul>
<div class="admonition-reduccion-de-dimension admonition">
<p class="admonition-title">Reducción de dimensión</p>
<ul>
<li><p>Al aplicar <span class="math notranslate nohighlight">\(PCA\)</span>, es posible <em><strong>reducir la dimensionalidad de los</strong></em> <span class="math notranslate nohighlight">\(n\)</span> <em><strong>datos observados</strong></em> <span class="math notranslate nohighlight">\(p\)</span>-dimensionales</p>
<div class="math notranslate nohighlight">
\[
    \{\boldsymbol{x}_{i}=(x_{i1}, x_{i2}, \dots, x_{ip})^{T}:~i=1,2,\dots, n\}
    \]</div>
<p>para las <span class="math notranslate nohighlight">\(p\)</span> <em><strong>variables originales a un número menor</strong></em>, utilizando solo los <em><strong>primeros componentes principales</strong></em>; por ejemplo, a datos bidimensionales</p>
<div class="math notranslate nohighlight">
\[
    \{(y_{i1}, y_{i2}):~i=1,2,\dots,n\}
    \]</div>
<p><em><strong>utilizando solo el primero y el segundo componente principal</strong></em>, donde <span class="math notranslate nohighlight">\(y_{i1}=\boldsymbol{w}_{1}^{T}\boldsymbol{x}_{i}\)</span>, <span class="math notranslate nohighlight">\(y_{i2}=\boldsymbol{w}_{2}^{T}\boldsymbol{x}_{i}\)</span></p>
</li>
<li><p>Al <em><strong>proyectar</strong></em> así el conjunto de datos <em><strong>del espacio de dimensiones superior sobre un plano bidimensional</strong></em>, podemos <em><strong>aprender visualmente la estructura de los datos</strong></em>. Al encontrar el significado de las nuevas variables combinadas como combinaciones lineales de las variables originales, además, podemos <em><strong>extraer información útil</strong></em>.</p></li>
</ul>
</div>
<ul>
<li><p>El significado de los componentes principales puede entenderse en términos de la <em><strong>magnitud y el signo de los coeficientes</strong></em> <span class="math notranslate nohighlight">\(w_{ij}\)</span> de cada variable. Además, la <em><strong>correlación entre los componentes principales y las variables</strong></em> como indicador cuantitativo es muy útil para <em><strong>identificar las variables que influyen en los componentes principales</strong></em>. La correlación entre el <span class="math notranslate nohighlight">\(i\)</span><em><strong>-ésimo componente principal</strong></em> <span class="math notranslate nohighlight">\(y_{i}\)</span> <em><strong>y la variable</strong></em> <span class="math notranslate nohighlight">\(x_{j}\)</span> está dada por</p>
<div class="math notranslate nohighlight">
\[
  r_{y_{i}, x_{j}}=\frac{\text{cov}(y_{i}, x_{j})}{\sqrt{\text{var}(y_{i})}\sqrt{\text{var}(x_{j})}}=
  \frac{\lambda_{i}w_{ij}}{\sqrt{\lambda_{i}}\sqrt{s_{jj}}}=\frac{\sqrt{\lambda_{i}}w_{ij}}{\sqrt{s_{jj}}}
  \]</div>
<p>donde <span class="math notranslate nohighlight">\(\lambda_{i}\)</span> es la varianza del <span class="math notranslate nohighlight">\(i\)</span><em><strong>-ésimo componente principal</strong></em>, <span class="math notranslate nohighlight">\(w_{ij}\)</span> es el coeficiente de la variable <span class="math notranslate nohighlight">\(x_{j}\)</span> para el <span class="math notranslate nohighlight">\(i\)</span><em><strong>-ésimo componente principal</strong></em>, y <span class="math notranslate nohighlight">\(s_{jj}\)</span> es la varianza de la variable <span class="math notranslate nohighlight">\(x_{j}\)</span>.</p>
</li>
</ul>
<div class="admonition-observacion-reduccion-de-dimensionalidad admonition">
<p class="admonition-title">Observación (Reducción de dimensionalidad)</p>
<ul class="simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">¿Cuántas</span> <span class="pre">componentes</span> <span class="pre">principales</span> <span class="pre">son</span> <span class="pre">necesarias?</span></code></strong></em>: Trazar un <em><strong>gráfico de la varianza explicada acumulativa en función del número de componentes</strong></em> y <em><strong>seleccionar el número de componentes que capturan una cantidad significativa de la varianza total</strong></em> (por ejemplo, el <em><strong><code class="docutils literal notranslate"><span class="pre">70%</span> <span class="pre">o</span> <span class="pre">el</span> <span class="pre">80%</span></code></strong></em>).</p></li>
<li><p>El <em><strong>70% o 80% de varianza explicada es un umbral comúnmente utilizado en PCA</strong></em> para determinar el número de componentes principales que se deben retener en un modelo. Al <em><strong>retener el 70% o 80% de la varianza</strong></em>, se <em><strong>conserva la mayor parte de la información importante en los datos originales</strong></em>, y generalmente, conservar este porcentaje implica una <em><strong>reducción relevante en la dimensionalidad de los datos</strong></em>.</p></li>
</ul>
</div>
<figure class="align-center">
<img alt="_images/pca2_pca3.png" src="_images/pca2_pca3.png" />
</figure>
</section>
</section>
<section id="autovalores-y-autovectores-de-una-matriz-simetrica">
<h2>Autovalores y autovectores de una matriz simétrica<a class="headerlink" href="#autovalores-y-autovectores-de-una-matriz-simetrica" title="Link to this heading">#</a></h2>
<ul>
<li><p>La matriz de <em><strong>varianza-covarianza muestral</strong></em> <span class="math notranslate nohighlight">\(S\)</span> dada por la Ecuación <a class="reference internal" href="#equation-covariance-variance-nobs-pdimentionals">(76)</a> es una <em><strong>matriz simétrica de orden</strong></em> <span class="math notranslate nohighlight">\(p\)</span>, con la siguiente <em><strong>relación entre sus autovalores y autovectores</strong></em></p>
<div class="math notranslate nohighlight" id="equation-eigenvalue-eigenvector-relation">
<span class="eqno">(79)<a class="headerlink" href="#equation-eigenvalue-eigenvector-relation" title="Link to this equation">#</a></span>\[
   S\boldsymbol{w}_{i}=\lambda_{i}\boldsymbol{w}_{i},~\boldsymbol{w}_{i}^{T}\boldsymbol{w}_{i}=1,~\boldsymbol{w}_{i}^{T}\boldsymbol{w}_{j}=0~(i\neq j)
   \]</div>
<p>para <span class="math notranslate nohighlight">\(i,j=1,2,\dots,p\)</span>.</p>
</li>
</ul>
<ul>
<li><p>Denotamos como <span class="math notranslate nohighlight">\(W\)</span> la <em><strong>matriz de orden</strong></em> <span class="math notranslate nohighlight">\(p\)</span> que tiene como <em><strong>columnas los</strong></em> <span class="math notranslate nohighlight">\(p\)</span> <em><strong>autovectores</strong></em>, y como <span class="math notranslate nohighlight">\(\Lambda\)</span> la matriz de orden <span class="math notranslate nohighlight">\(p\)</span> que tiene los <em><strong>autovalores como sus elementos diagonales</strong></em>, es decir,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  W=(\boldsymbol{w}_{1}, \boldsymbol{w}_{2}, \dots, \boldsymbol{w}_{p}),~ \Lambda =
  \begin{pmatrix}
  \lambda_{1} &amp; 0 &amp; \cdots &amp; 0\\
  0 &amp; \lambda_{2} &amp; \cdots &amp; 0\\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
  0 &amp; 0 &amp; \vdots &amp; \lambda_{p}
  \end{pmatrix}
  \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>La <em><strong>relación entre los autovalores y autovectores de la matriz de varianza-covarianza muestral</strong></em> <span class="math notranslate nohighlight">\(S\)</span> dada por Ecuación <a class="reference internal" href="#equation-eigenvalue-eigenvector-relation">(79)</a> puede expresarse de la siguiente manera:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(SW=W\Lambda,\quad W^{T}W=I_{p}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W^{T}SW=\Lambda\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(S=W\Lambda W^{T}=\lambda_{1}\boldsymbol{w}_{1}\boldsymbol{w}_{1}^{T}+\lambda_{2}\boldsymbol{w}_{2}\boldsymbol{w}_{2}^{T}+\cdots+\lambda_{p}\boldsymbol{w}_{p}\boldsymbol{w}_{p}^{T}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{tr}(S)=\text{tr}(W\Lambda W^{T})=\text{tr}(\Lambda)=\lambda_{1}+\lambda_{2}+\cdots+\lambda_{p}\)</span></p></li>
</ol>
</li>
</ul>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La Ecuación (2) muestra que <em><strong>la matriz simétrica</strong></em> <span class="math notranslate nohighlight">\(S\)</span> <em><strong>puede ser diagonalizada por la matriz ortogonal</strong></em> <span class="math notranslate nohighlight">\(W\)</span>, y la Ecuación (3) es conocida como la <em><strong>descomposición espectral de la matriz simétrica</strong></em> <span class="math notranslate nohighlight">\(S\)</span>.</p></li>
<li><p>La Ecuación (4) muestra que la suma, <span class="math notranslate nohighlight">\(\text{tr}(S)=s_{11}+s_{22}+\cdots+s_{pp}\)</span>, de las <em><strong>varianzas de las variables originales</strong></em> <span class="math notranslate nohighlight">\(x_{1}, x_{2}, \dots, x_{p}\)</span> es <em><strong>igual a la suma</strong></em>, <span class="math notranslate nohighlight">\(\text{tr}(\Lambda)=\lambda_{1}+\lambda_{2}+\cdots+\lambda_{p}\)</span>, <em><strong>de las varianzas de los</strong></em> <span class="math notranslate nohighlight">\(p\)</span> <em><strong>componentes principales</strong></em> construidos.</p></li>
</ul>
</div>
</section>
<section id="estandarizacion-y-matriz-de-correlacion-muestral">
<h2>Estandarización y matriz de correlación muestral<a class="headerlink" href="#estandarizacion-y-matriz-de-correlacion-muestral" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Supongamos que, además de las <em><strong>puntuaciones en matemáticas, inglés, ciencias y japonés</strong></em>, agregamos como <em><strong>quinta variable la proporción de respuestas correctas dadas para 100 problemas de cálculo</strong></em> que miden la habilidad para calcular y <em><strong>realizamos PCA basado en los datos resultantes de cinco dimensiones</strong></em>.</p></li>
<li><p>En este caso, <em><strong>las unidades de medida difieren sustancialmente entre la proporción de respuestas correctas</strong></em> y las <em><strong>puntuaciones de las pruebas de materias</strong></em>, por lo que <em><strong>es necesario estandarizar los datos observados</strong></em>.</p></li>
</ul>
</div>
<ul>
<li><p>Para los <span class="math notranslate nohighlight">\(n\)</span> datos observados <span class="math notranslate nohighlight">\(p\)</span> dimensionales</p>
<div class="math notranslate nohighlight" id="equation-vector-pvariables-ndimensionals">
<span class="eqno">(80)<a class="headerlink" href="#equation-vector-pvariables-ndimensionals" title="Link to this equation">#</a></span>\[
  \boldsymbol{x}_{i}=(x_{i1}, x_{i2}, \dots, x_{ip})^{T},\quad i=1, 2, \dots, n,
  \]</div>
<p>Primero obtenemos el <em><strong>vector de media muestral</strong></em> <span class="math notranslate nohighlight">\(\overline{\boldsymbol{x}}=(\overline{x}_{1}, \overline{x}_{2}, \dots, \overline{x}_{p})^{T}\)</span> y la <em><strong>matriz de varianza-covarianza muestral</strong></em> <span class="math notranslate nohighlight">\(S=(s_{jk})\)</span>. Estandarizamos los datos <span class="math notranslate nohighlight">\(p\)</span>-dimensionales en Ecuación <a class="reference internal" href="#equation-vector-pvariables-ndimensionals">(80)</a> de manera que</p>
<div class="math notranslate nohighlight">
\[
  \boldsymbol{z}_{i}=(z_{i1}, z_{i2}, \dots, z_{ip})^{T},\quad z_{ij}=\frac{x_{ij}-\overline{x}_{j}}{\sqrt{s_{jj}}},\quad j=1, 2, \dots, p.
  \]</div>
</li>
</ul>
<ul>
<li><p>La <strong>varianza muestral</strong> (<span class="math notranslate nohighlight">\(s_{jj}^{\star}\)</span>) y la <em><strong>covarianza muestral</strong></em> (<span class="math notranslate nohighlight">\(s_{jk}^{\star}\)</span>) basadas en estos <em><strong>datos</strong></em> <span class="math notranslate nohighlight">\(p\)</span><em><strong>-dimensionales estandarizados</strong></em> se dan entonces por</p>
<div class="math notranslate nohighlight">
\[
  s_{jk}^{\star}=\frac{1}{n}\sum_{i=1}^{n}z_{ij}z_{ik}=\frac{1}{n}\sum_{i=1}^{n}\frac{(x_{ij}-\overline{x}_{j})(x_{ik}-\overline{x}_{k})}{\sqrt{s_{jj}}\sqrt{s_{kk}}}=\frac{s_{jk}}{\sqrt{s_{jj}}\sqrt{s_{kk}}}\equiv r_{jk}
  \]</div>
<p>para <span class="math notranslate nohighlight">\(j, k=1,2,\dots,p\)</span>.</p>
</li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Nótese que en la <em><strong>matriz de varianza-covarianza muestral basada en los datos estandarizados</strong></em>, todos los <em><strong>elementos diagonales</strong></em> <span class="math notranslate nohighlight">\(r_{jj}\)</span> son 1, y obtenemos la matriz <span class="math notranslate nohighlight">\(R\)</span> con <span class="math notranslate nohighlight">\(r_{jk}\)</span> como los elementos no diagonales, los <em><strong>coeficientes de correlación muestral entre las variables</strong></em> <span class="math notranslate nohighlight">\(j\)</span>-ésima y <span class="math notranslate nohighlight">\(k\)</span>-ésima (<em>matriz de correlación muestral</em>).</p></li>
<li><p><em><strong>PCA</strong></em> comenzando <em><strong>con datos multidimensionales estandarizados</strong></em> se convierte así en un problema de <em><strong>encontrar los autovalores y autovectores de la matriz de correlación muestral</strong></em> <span class="math notranslate nohighlight">\(R\)</span>, lo cual se realiza mediante el <em><strong>mismo proceso de derivación de componentes principales que el de PCA comenzando con una matriz de varianza-covarianza muestral</strong></em>.</p></li>
</ul>
</div>
</section>
<section id="reduccion-de-dimension-y-perdida-de-informacion">
<h2>Reducción de dimensión y pérdida de información<a class="headerlink" href="#reduccion-de-dimension-y-perdida-de-informacion" title="Link to this heading">#</a></h2>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>En <span class="math notranslate nohighlight">\(PCA\)</span>, <em><strong>la varianza proporciona una medida de información</strong></em>, y, por lo tanto, <em><strong>la pérdida de información puede ser estimada cuantitativamente a partir de los tamaños relativos de las varianzas de los componentes principales</strong></em>.</p></li>
<li><p>Como se muestra en Ecuación <a class="reference internal" href="#equation-systemof-variance-principal-components">(78)</a>, la <em><strong>varianza del componente principal está dada por el autovalor de la matriz de varianza-covarianza muestral</strong></em>.</p></li>
<li><p>En resumen, podemos utilizar <span class="math notranslate nohighlight">\(\lambda_{1}/(\lambda_{1}+\lambda_{2}+\cdots+\lambda_{p})\)</span> para evaluar <em><strong>qué proporción de la información contenida en las</strong></em> <span class="math notranslate nohighlight">\(p\)</span> <em><strong>variables originales está presente en el primer componente principal</strong></em> <span class="math notranslate nohighlight">\(y_{1}\)</span>.</p></li>
</ul>
</div>
<ul>
<li><p>En general, la siguiente ecuación se utiliza como <em><strong>medida de la información presente en el</strong></em> <span class="math notranslate nohighlight">\(i\)</span>-<em><strong>ésimo componente principal</strong></em> <span class="math notranslate nohighlight">\(y_{i}\)</span></p>
<div class="math notranslate nohighlight">
\[
  \frac{\lambda_{i}}{\lambda_{1}+\lambda_{2}+\cdots+\lambda_{p}}.
  \]</div>
</li>
</ul>
<ul class="simple">
<li><p>De manera similar, el <em><strong>porcentaje de varianza explicada por los primeros</strong></em> <span class="math notranslate nohighlight">\(k\)</span> <em><strong>componentes principales</strong></em> se calcula mediante:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\lambda_{1}+\lambda_{2}+\cdots+\lambda_{k}}{\lambda_{1}+\lambda_{2}+\cdots+\lambda_{k}+\cdots+\lambda_{p}}.
\]</div>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>En conclusión, la <em><strong>medida cuantitativa de la información contenida en los componentes principales</strong></em> que realmente se utilizan se da por la <em><strong>relación de la suma de las varianzas de esos componentes principales a la suma de las varianzas de todos los componentes principales</strong></em>, lo que sirve como <em><strong>medida de la pérdida de información</strong></em>.</p></li>
</ul>
</div>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 5 </span> (Relación entre estructura de correlación y pérdida de información)</p>
<section class="example-content" id="proof-content">
<ul>
<li><p>Supongamos que tenemos la <em><strong>matriz de correlación muestral</strong></em> basada en datos observados bidimensionales para las variables <span class="math notranslate nohighlight">\(x_{1}, x_{2}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    R=\begin{pmatrix}1 &amp; r\\r &amp; 1\end{pmatrix}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(r (&gt; 0)\)</span> es el <em><strong>coeficiente de correlación muestral</strong></em> entre dos variables <span class="math notranslate nohighlight">\(x_{1}\)</span> y <span class="math notranslate nohighlight">\(x_{2}\)</span>.</p>
</li>
<li><p>La <em><strong>ecuación característica de la matriz de correlación de muestral</strong></em> <span class="math notranslate nohighlight">\(R\)</span> es</p>
<div class="math notranslate nohighlight">
\[
    |R-\lambda I_{2}|=(1-\lambda)^{2}-r^{2}=\lambda^{2}-2\lambda+1-r^{2}=0.
    \]</div>
</li>
<li><p>Sus soluciones son <span class="math notranslate nohighlight">\(\lambda_{1}=1+r\)</span> y <span class="math notranslate nohighlight">\(\lambda_{2}=1 - r\)</span>, y los <em><strong>correspondientes autovectores</strong></em> son entonces <span class="math notranslate nohighlight">\(\boldsymbol{w}_{1} = (1/\sqrt{2}, 1/\sqrt{2})^{T}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{w}_{2} = (-1/\sqrt{2}, 1/\sqrt{2})^{T}\)</span>, respectivamente.</p></li>
<li><p>Los <em><strong>componentes principales primero y segundo</strong></em> obtenidos a partir de la matriz de correlación de la muestra vienen dadas, por tanto, por</p>
<div class="math notranslate nohighlight">
\[
    y_{1}=\frac{1}{\sqrt{2}}x_{1}+\frac{1}{\sqrt{2}}x_{2},\quad y_{2}=-\frac{1}{\sqrt{2}}x_{1}+\frac{1}{\sqrt{2}}x_{2}
    \]</div>
</li>
<li><p>Las <em><strong>contribuciones del primer y segundo componentes principales son</strong></em>, respectivamente,</p>
<div class="math notranslate nohighlight">
\[
    \frac{\lambda_{1}}{\lambda_{1}+\lambda_{2}}=\frac{1+r}{2},\quad\frac{\lambda_{2}}{\lambda_{1}+\lambda_{2}}=\frac{1-r}{2}.
    \]</div>
</li>
</ul>
</section>
</div><ul class="simple">
<li><p>Como se muestra en la Figura <a class="reference internal" href="#exercise-two-pca-rgiven-fig"><span class="std std-numref">Fig. 71</span></a>, la contribución del primer componente principal, y, por lo tanto, la <em><strong>proporción que puede ser explicada únicamente por el primer componente principal, aumenta con la correlación creciente entre las dos variables</strong></em>.</p></li>
</ul>
<figure class="align-center" id="exercise-two-pca-rgiven-fig">
<a class="reference internal image-reference" href="_images/exercise_two_pca_rgiven.png"><img alt="_images/exercise_two_pca_rgiven.png" src="_images/exercise_two_pca_rgiven.png" style="width: 494.9px; height: 366.09999999999997px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 71 </span><span class="caption-text">Componentes principales basados en la <em><strong>matriz de correlación muestral y sus contribuciones</strong></em>. Fuente <span id="id3">[<a class="reference internal" href="biblio.html#id20" title="S. Konishi. Introduction to Multivariate Analysis: Linear and Nonlinear Modeling. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis, 2014. ISBN 9781466567283. URL: https://books.google.com.co/books?id=fcuuAwAAQBAJ.">Konishi, 2014</a>]</span>.</span><a class="headerlink" href="#exercise-two-pca-rgiven-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="implementacion">
<h2>Implementación<a class="headerlink" href="#implementacion" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Hasta este momento, hemos analizado en detalle <em><strong>estimadores de aprendizaje supervisado</strong></em>: aquellos estimadores que <em><strong>predicen etiquetas basadas en datos de entrenamiento etiquetados</strong></em>. Ahora, empezamos a examinar varios <em><strong>estimadores no supervisados</strong></em>, los cuales pueden resaltar aspectos interesantes de los datos sin hacer referencia a etiquetas conocidas.</p></li>
<li><p>En esta sección, exploramos lo que posiblemente es uno de los <em><strong>algoritmos no supervisados más ampliamente utilizados</strong></em>: el <em><strong>análisis de componentes principales (PCA)</strong></em>. PCA es fundamentalmente un <em><strong>algoritmo de reducción de dimensionalidad</strong></em>, pero también puede ser útil como <em><strong>herramienta de visualización, para filtrado de ruido, extracción y construcción de características, y mucho más</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span><span class="p">;</span> <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="introduccion-al-analisis-de-componentes-principales">
<h2>Introducción al análisis de componentes principales<a class="headerlink" href="#introduccion-al-analisis-de-componentes-principales" title="Link to this heading">#</a></h2>
<p>El <code class="docutils literal notranslate"><span class="pre">análisis</span> <span class="pre">de</span> <span class="pre">componentes</span> <span class="pre">principales</span> <span class="pre">(PCA)</span></code> es una <code class="docutils literal notranslate"><span class="pre">técnica</span> <span class="pre">no</span> <span class="pre">supervisada,</span> <span class="pre">rápida</span> <span class="pre">y</span> <span class="pre">adaptable</span></code>, diseñada para <code class="docutils literal notranslate"><span class="pre">reducir</span> <span class="pre">la</span> <span class="pre">dimensionalidad</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>. La forma más sencilla de comprender su funcionamiento es a través del <code class="docutils literal notranslate"><span class="pre">examen</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">bidimensional</span></code>. Tomemos en consideración la <code class="docutils literal notranslate"><span class="pre">recopilación</span> <span class="pre">posterior</span> <span class="pre">de</span> <span class="pre">200</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">datos</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f62369137b0b356ecc7ee5527e08f4878eabed3c6fc456b64035aace67950fac.png" src="_images/f62369137b0b356ecc7ee5527e08f4878eabed3c6fc456b64035aace67950fac.png" />
</div>
</div>
<ul class="simple">
<li><p>Visualmente, es evidente que <code class="docutils literal notranslate"><span class="pre">existe</span> <span class="pre">una</span> <span class="pre">conexión</span> <span class="pre">casi</span> <span class="pre">lineal</span> <span class="pre">entre</span> <span class="pre">las</span> <span class="pre">variables</span></code> <span class="math notranslate nohighlight">\(x\)</span> e <span class="math notranslate nohighlight">\(y\)</span>. En lugar de intentar predecir los valores de <span class="math notranslate nohighlight">\(y\)</span> basándose en los valores de <span class="math notranslate nohighlight">\(x\)</span>, el objetivo del <code class="docutils literal notranslate"><span class="pre">aprendizaje</span> <span class="pre">no</span> <span class="pre">supervisado</span></code> aquí es <code class="docutils literal notranslate"><span class="pre">comprender</span> <span class="pre">la</span> <span class="pre">conexión</span> <span class="pre">entre</span> <span class="pre">los</span> <span class="pre">valores</span></code> de <span class="math notranslate nohighlight">\(x\)</span> e <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">análisis</span> <span class="pre">de</span> <span class="pre">componentes</span> <span class="pre">principales</span> <span class="pre">(PCA)</span> <span class="pre">cuantifica</span> <span class="pre">esta</span> <span class="pre">conexión</span></code> identificando los <code class="docutils literal notranslate"><span class="pre">ejes</span> <span class="pre">principales</span></code> dentro de los datos y <code class="docutils literal notranslate"><span class="pre">utilizándolos</span> <span class="pre">para</span> <span class="pre">representar</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span></code>. Para realizar esto utilizando el estimador <code class="docutils literal notranslate"><span class="pre">PCA</span></code> de <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>, siga estos pasos:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>PCA(n_components=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">PCA</label><div class="sk-toggleable__content"><pre>PCA(n_components=2)</pre></div></div></div></div></div></div></div>
</div>
<ul class="simple">
<li><p>El proceso de ajuste <code class="docutils literal notranslate"><span class="pre">extrae</span> <span class="pre">información</span> <span class="pre">crucial</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>, principalmente los <code class="docutils literal notranslate"><span class="pre">&quot;componentes&quot;</span></code> y la <code class="docutils literal notranslate"><span class="pre">&quot;varianza</span> <span class="pre">explicada&quot;</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[-0.94446029 -0.32862557]
 [-0.32862557  0.94446029]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.7625315 0.0184779]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Para comprender el significado de estos valores, podemos <code class="docutils literal notranslate"><span class="pre">representarlos</span> <span class="pre">visualmente</span> <span class="pre">como</span> <span class="pre">vectores</span> <span class="pre">situados</span> <span class="pre">sobre</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrada</span></code>. En esta representación, los <code class="docutils literal notranslate"><span class="pre">&quot;componentes&quot;</span> <span class="pre">determinan</span> <span class="pre">la</span> <span class="pre">dirección</span> <span class="pre">del</span> <span class="pre">vector</span></code>, mientras que la <code class="docutils literal notranslate"><span class="pre">&quot;varianza</span> <span class="pre">explicada&quot;</span> <span class="pre">determina</span> <span class="pre">el</span> <span class="pre">cuadrado</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">longitud</span> <span class="pre">del</span> <span class="pre">vector</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">draw_vector</span><span class="p">(</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span>
                    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">shrinkA</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shrinkB</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">arrowprops</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">length</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">vector</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
    <span class="n">draw_vector</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="n">pca</span><span class="o">.</span><span class="n">mean_</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8e6b8e831da825bea5756226e63abe9d9c24922da550846c184c55b217c6b044.png" src="_images/8e6b8e831da825bea5756226e63abe9d9c24922da550846c184c55b217c6b044.png" />
</div>
</div>
<div class="proof observation admonition" id="observation-1">
<p class="admonition-title"><span class="caption-number">Observation 23 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>Estos vectores simbolizan las <code class="docutils literal notranslate"><span class="pre">orientaciones</span> <span class="pre">primarias</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>, y <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">longitud</span> <span class="pre">del</span> <span class="pre">vector</span></code> indica la <code class="docutils literal notranslate"><span class="pre">importancia</span> <span class="pre">de</span> <span class="pre">esa</span> <span class="pre">orientación</span> <span class="pre">para</span> <span class="pre">explicar</span> <span class="pre">la</span> <span class="pre">distribución</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>. Esta longitud <code class="docutils literal notranslate"><span class="pre">cuantifica</span> <span class="pre">la</span> <span class="pre">varianza</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">cuando</span> <span class="pre">se</span> <span class="pre">proyectan</span> <span class="pre">sobre</span> <span class="pre">esa</span> <span class="pre">orientación</span></code>. La proyección de los puntos de datos sobre estas orientaciones principales se denomina <code class="docutils literal notranslate"><span class="pre">&quot;componentes</span> <span class="pre">principales&quot;</span></code> de los datos.</p></li>
</ul>
</section>
</div><section id="pca-para-reduccion-de-dimension">
<h3>PCA para reducción de dimensión<a class="headerlink" href="#pca-para-reduccion-de-dimension" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>El <code class="docutils literal notranslate"><span class="pre">PCA</span></code> puede emplearse para <code class="docutils literal notranslate"><span class="pre">reducir</span> <span class="pre">la</span> <span class="pre">dimensionalidad</span> <span class="pre">anulando</span> <span class="pre">uno</span> <span class="pre">o</span> <span class="pre">varios</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">componentes</span> <span class="pre">principales</span> <span class="pre">más</span> <span class="pre">pequeños</span></code>. De este modo se obtiene una <code class="docutils literal notranslate"><span class="pre">representación</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">menor</span> <span class="pre">dimensión,</span> <span class="pre">conservando</span> <span class="pre">al</span> <span class="pre">mismo</span> <span class="pre">tiempo</span> <span class="pre">la</span> <span class="pre">mayor</span> <span class="pre">varianza</span> <span class="pre">posible</span></code> de los datos. A continuación se muestra un ejemplo de utilización de <code class="docutils literal notranslate"><span class="pre">PCA</span> <span class="pre">com</span></code> transformación para la reducción de la dimensionalidad`</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;original shape:   &quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;transformed shape:&quot;</span><span class="p">,</span> <span class="n">X_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>original shape:    (200, 2)
transformed shape: (200, 1)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Los <code class="docutils literal notranslate"><span class="pre">datos</span> <span class="pre">se</span> <span class="pre">han</span> <span class="pre">transformado</span> <span class="pre">y</span> <span class="pre">ahora</span> <span class="pre">existen</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">sola</span> <span class="pre">dimensión</span></code>. Para <code class="docutils literal notranslate"><span class="pre">comprender</span> <span class="pre">las</span> <span class="pre">implicaciones</span> <span class="pre">de</span> <span class="pre">esta</span> <span class="pre">reducción</span></code> de la dimensionalidad, podemos invertir el proceso de transformación en estos datos reducidos y luego <code class="docutils literal notranslate"><span class="pre">compararlos</span> <span class="pre">con</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">iniciales</span> <span class="pre">trazándolos</span> <span class="pre">juntos</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_pca</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f8f27c86d865d5bac672ad63d77340465a82df8fb9fb21c442678cb623925203.png" src="_images/f8f27c86d865d5bac672ad63d77340465a82df8fb9fb21c442678cb623925203.png" />
</div>
</div>
<ul class="simple">
<li><p>Los <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">más</span> <span class="pre">claros</span> <span class="pre">en</span> <span class="pre">azul</span></code> representan los datos iniciales, mientras que los <code class="docutils literal notranslate"><span class="pre">más</span> <span class="pre">oscuros</span> <span class="pre">en</span> <span class="pre">salmon</span></code> representan la <code class="docutils literal notranslate"><span class="pre">versión</span> <span class="pre">proyectada</span></code>. Esta comparación visual <code class="docutils literal notranslate"><span class="pre">aclara</span> <span class="pre">el</span> <span class="pre">concepto</span> <span class="pre">que</span> <span class="pre">subyace</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">reducción</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">dimensionalidad</span> <span class="pre">de</span> <span class="pre">PCA</span></code>: consiste en <code class="docutils literal notranslate"><span class="pre">eliminar</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">a</span> <span class="pre">lo</span> <span class="pre">largo</span> <span class="pre">del</span> <span class="pre">eje</span> <span class="pre">o</span> <span class="pre">ejes</span> <span class="pre">principales</span> <span class="pre">menos</span> <span class="pre">significativos,</span> <span class="pre">conservando</span> <span class="pre">únicamente</span> <span class="pre">el</span> <span class="pre">componente</span> <span class="pre">o</span> <span class="pre">componentes</span> <span class="pre">con</span> <span class="pre">mayor</span> <span class="pre">varianza</span></code>.</p></li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">grado</span> <span class="pre">de</span> <span class="pre">reducción</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">varianza</span></code> (relacionado con la dispersión de los puntos alrededor de la línea en la figura) indica aproximadamente el <code class="docutils literal notranslate"><span class="pre">grado</span> <span class="pre">de</span> <span class="pre">&quot;información&quot;</span> <span class="pre">descartada</span> <span class="pre">en</span> <span class="pre">esta</span> <span class="pre">reducción</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">dimensionalidad</span></code>. A pesar de que <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">dimensionalidad</span> <span class="pre">se</span> <span class="pre">ha</span> <span class="pre">reducido</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">mitad</span></code>, este nuevo conjunto de datos es, en varios aspectos, <code class="docutils literal notranslate"><span class="pre">suficientemente</span> <span class="pre">eficaz</span> <span class="pre">para</span> <span class="pre">captar</span> <span class="pre">las</span> <span class="pre">conexiones</span> <span class="pre">primarias</span> <span class="pre">entre</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">datos</span></code>. Las relaciones fundamentales entre los puntos de datos se conservan en gran medida.</p></li>
</ul>
</section>
<section id="pca-para-visualizacion-digitos-escritos-a-mano">
<h3>PCA para visualización: Dígitos escritos a mano<a class="headerlink" href="#pca-para-visualizacion-digitos-escritos-a-mano" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>El valor de <code class="docutils literal notranslate"><span class="pre">reducir</span> <span class="pre">la</span> <span class="pre">dimensionalidad</span> <span class="pre">puede</span> <span class="pre">no</span> <span class="pre">ser</span> <span class="pre">del</span> <span class="pre">todo</span> <span class="pre">evidente</span> <span class="pre">en</span> <span class="pre">sólo</span> <span class="pre">dos</span> <span class="pre">dimensiones</span></code>, pero se hace mucho más evidente cuando se trata de <code class="docutils literal notranslate"><span class="pre">datos</span> <span class="pre">de</span> <span class="pre">alta</span> <span class="pre">dimensionalidad</span></code>. Para ilustrarlo, examinaremos la aplicación de <code class="docutils literal notranslate"><span class="pre">PCA</span></code> al conjunto de datos de <code class="docutils literal notranslate"><span class="pre">dígitos</span> <span class="pre">escritos</span> <span class="pre">a</span> <span class="pre">mano</span></code> (imágenes <span class="math notranslate nohighlight">\(8\times8\)</span> (64-dimensional) de píxeles enteros en el rango <span class="math notranslate nohighlight">\(0,\dots,16\)</span>.).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">distinctipy</span> <span class="kn">import</span> <span class="n">distinctipy</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1797, 64)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3dece298d7a78ccf636cc03fbe55ebc847b68226088c93ba9070b894c9514e72.png" src="_images/3dece298d7a78ccf636cc03fbe55ebc847b68226088c93ba9070b894c9514e72.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.],
       [ 0.,  0.,  0., 11., 16.,  9.,  0.,  0.],
       [ 0.,  0.,  3., 15., 16.,  6.,  0.,  0.],
       [ 0.,  7., 15., 16., 16.,  2.,  0.,  0.],
       [ 0.,  0.,  1., 16., 16.,  3.,  0.,  0.],
       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],
       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],
       [ 0.,  0.,  0., 11., 16., 10.,  0.,  0.]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pixel_0_0</th>
      <th>pixel_0_1</th>
      <th>pixel_0_2</th>
      <th>pixel_0_3</th>
      <th>pixel_0_4</th>
      <th>pixel_0_5</th>
      <th>pixel_0_6</th>
      <th>pixel_0_7</th>
      <th>pixel_1_0</th>
      <th>pixel_1_1</th>
      <th>...</th>
      <th>pixel_6_6</th>
      <th>pixel_6_7</th>
      <th>pixel_7_0</th>
      <th>pixel_7_1</th>
      <th>pixel_7_2</th>
      <th>pixel_7_3</th>
      <th>pixel_7_4</th>
      <th>pixel_7_5</th>
      <th>pixel_7_6</th>
      <th>pixel_7_7</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>13.0</td>
      <td>9.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>6.0</td>
      <td>13.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>12.0</td>
      <td>13.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.0</td>
      <td>16.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>15.0</td>
      <td>12.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>11.0</td>
      <td>16.0</td>
      <td>9.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>15.0</td>
      <td>13.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>8.0</td>
      <td>...</td>
      <td>9.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>13.0</td>
      <td>13.0</td>
      <td>9.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>11.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>16.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 64 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pixel_0_0</th>
      <th>pixel_0_1</th>
      <th>pixel_0_2</th>
      <th>pixel_0_3</th>
      <th>pixel_0_4</th>
      <th>pixel_0_5</th>
      <th>pixel_0_6</th>
      <th>pixel_0_7</th>
      <th>pixel_1_0</th>
      <th>pixel_1_1</th>
      <th>...</th>
      <th>pixel_6_6</th>
      <th>pixel_6_7</th>
      <th>pixel_7_0</th>
      <th>pixel_7_1</th>
      <th>pixel_7_2</th>
      <th>pixel_7_3</th>
      <th>pixel_7_4</th>
      <th>pixel_7_5</th>
      <th>pixel_7_6</th>
      <th>pixel_7_7</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1797.0</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>...</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
      <td>1797.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.0</td>
      <td>0.303840</td>
      <td>5.204786</td>
      <td>11.835838</td>
      <td>11.848080</td>
      <td>5.781859</td>
      <td>1.362270</td>
      <td>0.129661</td>
      <td>0.005565</td>
      <td>1.993879</td>
      <td>...</td>
      <td>3.725097</td>
      <td>0.206455</td>
      <td>0.000556</td>
      <td>0.279354</td>
      <td>5.557596</td>
      <td>12.089037</td>
      <td>11.809126</td>
      <td>6.764051</td>
      <td>2.067891</td>
      <td>0.364496</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.0</td>
      <td>0.907192</td>
      <td>4.754826</td>
      <td>4.248842</td>
      <td>4.287388</td>
      <td>5.666418</td>
      <td>3.325775</td>
      <td>1.037383</td>
      <td>0.094222</td>
      <td>3.196160</td>
      <td>...</td>
      <td>4.919406</td>
      <td>0.984401</td>
      <td>0.023590</td>
      <td>0.934302</td>
      <td>5.103019</td>
      <td>4.374694</td>
      <td>4.933947</td>
      <td>5.900623</td>
      <td>4.090548</td>
      <td>1.860122</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.0</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>10.000000</td>
      <td>10.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>11.000000</td>
      <td>10.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.0</td>
      <td>0.000000</td>
      <td>4.000000</td>
      <td>13.000000</td>
      <td>13.000000</td>
      <td>4.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>4.000000</td>
      <td>13.000000</td>
      <td>14.000000</td>
      <td>6.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.0</td>
      <td>0.000000</td>
      <td>9.000000</td>
      <td>15.000000</td>
      <td>15.000000</td>
      <td>11.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>...</td>
      <td>7.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>10.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
      <td>12.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>0.0</td>
      <td>8.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
      <td>15.000000</td>
      <td>2.000000</td>
      <td>16.000000</td>
      <td>...</td>
      <td>16.000000</td>
      <td>13.000000</td>
      <td>1.000000</td>
      <td>9.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
      <td>16.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 64 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">color_bars</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="s1">&#39;cyan&#39;</span><span class="p">,</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;pink&#39;</span><span class="p">,</span> <span class="s1">&#39;brown&#39;</span><span class="p">,</span> <span class="s1">&#39;yellow&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">distinctipy</span><span class="o">.</span><span class="n">get_colors</span><span class="p">(</span><span class="mi">10</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9e5922b55e810a005bfadb9ad32ac1bf006f4e08523b0b0e54c8c335bd7d2e73.png" src="_images/9e5922b55e810a005bfadb9ad32ac1bf006f4e08523b0b0e54c8c335bd7d2e73.png" />
</div>
</div>
<ul class="simple">
<li><p>Para <code class="docutils literal notranslate"><span class="pre">comprender</span> <span class="pre">mejor</span> <span class="pre">las</span> <span class="pre">conexiones</span> <span class="pre">entre</span> <span class="pre">estos</span> <span class="pre">puntos</span></code>, podemos utilizar el <code class="docutils literal notranslate"><span class="pre">PCA</span> <span class="pre">para</span> <span class="pre">condensarlos</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">dimensiones</span> <span class="pre">más</span> <span class="pre">manejable</span></code>, por ejemplo, <code class="docutils literal notranslate"><span class="pre">dos</span> <span class="pre">dimensiones</span> <span class="pre">(proyectar</span> <span class="pre">de</span> <span class="pre">64</span> <span class="pre">a</span> <span class="pre">2</span> <span class="pre">dimensiones)</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">projected</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">projected</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1797, 64)
(1797, 2)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>El <code class="docutils literal notranslate"><span class="pre">trazado</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">dos</span> <span class="pre">componentes</span> <span class="pre">principales</span> <span class="pre">iniciales</span></code> de cada punto nos permite <code class="docutils literal notranslate"><span class="pre">extraer</span> <span class="pre">información</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">projected</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">projected</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">c</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">));</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;PC 1&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;PC 2&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/513434debffc235f293e38099ccee4b4d0bf1b1173b44eedce27bd7cc919cedb.png" src="_images/513434debffc235f293e38099ccee4b4d0bf1b1173b44eedce27bd7cc919cedb.png" />
</div>
</div>
<div class="proof observation admonition" id="observation-2">
<p class="admonition-title"><span class="caption-number">Observation 24 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>Consideremos la <code class="docutils literal notranslate"><span class="pre">importancia</span> <span class="pre">de</span> <span class="pre">estos</span> <span class="pre">componentes</span></code>: el conjunto completo de datos forma una <code class="docutils literal notranslate"><span class="pre">nube</span> <span class="pre">de</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">64</span> <span class="pre">dimensiones</span></code>, y estos puntos concretos representan la <code class="docutils literal notranslate"><span class="pre">proyección</span> <span class="pre">de</span> <span class="pre">cada</span> <span class="pre">punto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">a</span> <span class="pre">lo</span> <span class="pre">largo</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">direcciones</span> <span class="pre">con</span> <span class="pre">mayor</span> <span class="pre">varianza</span></code>.</p></li>
<li><p>Fundamentalmente, hemos<code class="docutils literal notranslate"><span class="pre">identificado</span> <span class="pre">el</span> <span class="pre">estiramiento</span> <span class="pre">y</span> <span class="pre">la</span> <span class="pre">rotación</span> <span class="pre">más</span> <span class="pre">eficaces</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">64</span> <span class="pre">dimensiones</span></code>, lo que nos permite <code class="docutils literal notranslate"><span class="pre">observar</span> <span class="pre">la</span> <span class="pre">disposición</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">dígitos</span> <span class="pre">en</span> <span class="pre">dos</span> <span class="pre">dimensiones</span></code>. Y lo que es más importante, esto se ha logrado <code class="docutils literal notranslate"><span class="pre">de</span> <span class="pre">forma</span> <span class="pre">no</span> <span class="pre">supervisada</span></code>, sin utilizar ninguna etiqueta.</p></li>
</ul>
</section>
</div><ul class="simple">
<li><p>El PCA puede conceptualizarse como la selección de <code class="docutils literal notranslate"><span class="pre">funciones</span> <span class="pre">base</span> <span class="pre">óptimas</span></code>. La <code class="docutils literal notranslate"><span class="pre">combinación</span> <span class="pre">de</span> <span class="pre">sólo</span> <span class="pre">un</span> <span class="pre">pequeño</span> <span class="pre">subconjunto</span> <span class="pre">de</span> <span class="pre">estas</span> <span class="pre">funciones</span> <span class="pre">basta</span> <span class="pre">para</span> <span class="pre">reconstruir</span> <span class="pre">eficazmente</span> <span class="pre">la</span> <span class="pre">mayoría</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">elementos</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span></code>. Los componentes principales, que sirven de representación condensada de los datos en dimensiones inferiores, son esencialmente los <code class="docutils literal notranslate"><span class="pre">coeficientes</span> <span class="pre">que</span> <span class="pre">escalan</span> <span class="pre">cada</span> <span class="pre">elemento</span> <span class="pre">de</span> <span class="pre">esta</span> <span class="pre">serie</span></code>. <code class="docutils literal notranslate"><span class="pre">PCA</span></code> nos permite <code class="docutils literal notranslate"><span class="pre">recuperar</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">más</span> <span class="pre">destacadas</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">imagen</span> <span class="pre">de</span> <span class="pre">entrada</span></code> ¡con sólo una media más ocho componentes! La cantidad de cada píxel en cada componente es el corolario de la <code class="docutils literal notranslate"><span class="pre">orientación</span> <span class="pre">del</span> <span class="pre">vector</span> <span class="pre">en</span> <span class="pre">nuestro</span> <span class="pre">ejemplo</span> <span class="pre">bidimensional</span></code>.</p></li>
</ul>
</section>
<section id="seleccion-del-numero-de-componentes">
<h3>Selección del número de componentes<a class="headerlink" href="#seleccion-del-numero-de-componentes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Un aspecto esencial de la aplicación práctica del <code class="docutils literal notranslate"><span class="pre">PCA</span></code> consiste en <code class="docutils literal notranslate"><span class="pre">determinar</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">óptimo</span> <span class="pre">de</span> <span class="pre">componentes</span> <span class="pre">necesarios</span> <span class="pre">para</span> <span class="pre">caracterizar</span> <span class="pre">los</span> <span class="pre">datos</span></code>. Esto puede lograrse analizando la <code class="docutils literal notranslate"><span class="pre">proporción</span> <span class="pre">de</span> <span class="pre">varianza</span> <span class="pre">explicada</span> <span class="pre">acumulativa</span> <span class="pre">en</span> <span class="pre">relación</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">componentes</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of components&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cumulative explained variance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/568de104340568699b1f1444e37d5be022f23287f52b962761c8e8092ed233f0.png" src="_images/568de104340568699b1f1444e37d5be022f23287f52b962761c8e8092ed233f0.png" />
</div>
</div>
<ul class="simple">
<li><p>La curva presentada proporciona una <code class="docutils literal notranslate"><span class="pre">medida</span> <span class="pre">de</span> <span class="pre">qué</span> <span class="pre">parte</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">varianza</span> <span class="pre">total</span></code>, que abarca <code class="docutils literal notranslate"><span class="pre">64</span> <span class="pre">dimensiones</span></code>, queda englobada por los <span class="math notranslate nohighlight">\(N\)</span> <code class="docutils literal notranslate"><span class="pre">componentes</span> <span class="pre">iniciales</span></code>. Por ejemplo, en el <code class="docutils literal notranslate"><span class="pre">caso</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">dígitos,</span> <span class="pre">los</span> <span class="pre">10</span> <span class="pre">componentes</span> <span class="pre">iniciales</span> <span class="pre">representan</span> <span class="pre">aproximadamente</span> <span class="pre">el</span> <span class="pre">75\%</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">varianza</span></code>, mientras que se necesitan unos <code class="docutils literal notranslate"><span class="pre">50</span> <span class="pre">componentes</span> <span class="pre">para</span> <span class="pre">describir</span> <span class="pre">casi</span> <span class="pre">el</span> <span class="pre">100\%</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">varianza</span></code>.</p></li>
<li><p>De ello se desprende que nuestra <code class="docutils literal notranslate"><span class="pre">proyección</span> <span class="pre">bidimensional</span> <span class="pre">conlleva</span> <span class="pre">una</span> <span class="pre">pérdida</span> <span class="pre">significativa</span> <span class="pre">de</span> <span class="pre">información</span></code>, como demuestra la varianza explicada. Para <code class="docutils literal notranslate"><span class="pre">conservar</span> <span class="pre">aproximadamente</span> <span class="pre">el</span> <span class="pre">90\%</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">varianza,</span> <span class="pre">serían</span> <span class="pre">necesarios</span> <span class="pre">unos</span> <span class="pre">20</span> <span class="pre">componentes</span></code>. El análisis de este gráfico para un conjunto de datos con dimensiones elevadas ayuda a <code class="docutils literal notranslate"><span class="pre">comprender</span> <span class="pre">el</span> <span class="pre">grado</span> <span class="pre">de</span> <span class="pre">redundancia</span> <span class="pre">presente</span> <span class="pre">en</span> <span class="pre">numerosas</span> <span class="pre">observaciones</span></code>.</p></li>
</ul>
</section>
<section id="pca-como-filtro-de-ruido">
<h3>PCA como filtro de ruido<a class="headerlink" href="#pca-como-filtro-de-ruido" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>El PCA puede servir como técnica para <code class="docutils literal notranslate"><span class="pre">filtrar</span> <span class="pre">el</span> <span class="pre">ruido</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>. El concepto fundamental es que los <em><strong><code class="docutils literal notranslate"><span class="pre">componentes</span> <span class="pre">con</span> <span class="pre">una</span> <span class="pre">varianza</span> <span class="pre">significativamente</span> <span class="pre">mayor</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">impacto</span> <span class="pre">del</span> <span class="pre">ruido,</span> <span class="pre">están</span> <span class="pre">menos</span> <span class="pre">influidas</span> <span class="pre">por</span> <span class="pre">éste</span></code></strong></em>. Por consiguiente, si se reconstruyen los datos utilizando el <code class="docutils literal notranslate"><span class="pre">subconjunto</span> <span class="pre">primario</span> <span class="pre">de</span> <span class="pre">componentes</span> <span class="pre">principales</span> <span class="pre">con</span> <span class="pre">la</span> <span class="pre">mayor</span> <span class="pre">varianza,</span> <span class="pre">se</span> <span class="pre">conserva</span> <span class="pre">efectivamente</span> <span class="pre">la</span> <span class="pre">señal</span> <span class="pre">al</span> <span class="pre">tiempo</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">elimina</span> <span class="pre">el</span> <span class="pre">ruido</span></code>.</p></li>
<li><p>Para observar esto en acción con el conjunto de datos de <code class="docutils literal notranslate"><span class="pre">dígitos</span></code>, empezaremos generando <code class="docutils literal notranslate"><span class="pre">gráficos</span> <span class="pre">para</span> <span class="pre">varias</span> <span class="pre">instancias</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">originales</span> <span class="pre">libres</span> <span class="pre">de</span> <span class="pre">ruido</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_digits</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                             <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;xticks&#39;</span><span class="p">:[],</span> <span class="s1">&#39;yticks&#39;</span><span class="p">:[]},</span>
                             <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                  <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;binary&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">clim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_digits</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/83cef0b4fbbc31c4178d0dd999fe7f96907fca03a9783ce6674d6fcac7535146.png" src="_images/83cef0b4fbbc31c4178d0dd999fe7f96907fca03a9783ce6674d6fcac7535146.png" />
</div>
</div>
<ul class="simple">
<li><p>Ahora, <code class="docutils literal notranslate"><span class="pre">introduciremos</span> <span class="pre">ruido</span> <span class="pre">aleatorio</span> <span class="pre">para</span> <span class="pre">generar</span> <span class="pre">un</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">con</span> <span class="pre">ruido</span></code> y, a continuación, <code class="docutils literal notranslate"><span class="pre">volveremos</span> <span class="pre">a</span> <span class="pre">crear</span> <span class="pre">los</span> <span class="pre">gráficos</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">noisy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">noisy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1e16766ffa6a67b2fea1902cb4342d56ca90a786aba36f40e55f7780be7ddeab.png" src="_images/1e16766ffa6a67b2fea1902cb4342d56ca90a786aba36f40e55f7780be7ddeab.png" />
</div>
</div>
<ul class="simple">
<li><p>Las imágenes muestran un <code class="docutils literal notranslate"><span class="pre">ruido</span> <span class="pre">evidente</span> <span class="pre">e</span> <span class="pre">incluyen</span> <span class="pre">píxeles</span> <span class="pre">erróneos</span></code>. Procederemos entrenando un <code class="docutils literal notranslate"><span class="pre">PCA</span> <span class="pre">sobre</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">ruidosos</span></code>, especificando que <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">proyección</span> <span class="pre">debe</span> <span class="pre">mantener</span> <span class="pre">el</span> <span class="pre">50%</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">varianza</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mf">0.50</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">noisy</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">n_components_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>12
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>En este caso, el <code class="docutils literal notranslate"><span class="pre">50%</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">varianza</span> <span class="pre">corresponde</span> <span class="pre">a</span> <span class="pre">12</span> <span class="pre">componentes</span> <span class="pre">principales</span></code>. Para proceder, <code class="docutils literal notranslate"><span class="pre">calculamos</span> <span class="pre">estos</span> <span class="pre">componentes</span></code> y posteriormente utilizamos la <code class="docutils literal notranslate"><span class="pre">inversa</span> <span class="pre">del</span> <span class="pre">proceso</span> <span class="pre">de</span> <span class="pre">transformación</span> <span class="pre">para</span> <span class="pre">reconstruir</span> <span class="pre">los</span> <span class="pre">dígitos</span> <span class="pre">filtrados</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">noisy</span><span class="p">)</span>
<span class="n">filtered</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">components</span><span class="p">)</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">filtered</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/da26ead15aba2c1827da5e59cf3bc0c710a7bb01283d49729e5730b88edc3bd6.png" src="_images/da26ead15aba2c1827da5e59cf3bc0c710a7bb01283d49729e5730b88edc3bd6.png" />
</div>
</div>
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">capacidad</span> <span class="pre">de</span> <span class="pre">PCA</span> <span class="pre">para</span> <span class="pre">retener</span> <span class="pre">señales</span> <span class="pre">y</span> <span class="pre">filtrar</span> <span class="pre">el</span> <span class="pre">ruido</span> <span class="pre">la</span> <span class="pre">convierte</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">técnica</span> <span class="pre">muy</span> <span class="pre">valiosa</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">selección</span> <span class="pre">de</span> <span class="pre">características</span></code>. Por ejemplo, <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">lugar</span> <span class="pre">de</span> <span class="pre">entrenar</span> <span class="pre">un</span> <span class="pre">clasificador</span> <span class="pre">en</span> <span class="pre">datos</span> <span class="pre">con</span> <span class="pre">numerosas</span> <span class="pre">dimensiones</span></code>, se puede optar por <code class="docutils literal notranslate"><span class="pre">entrenar</span> <span class="pre">el</span> <span class="pre">clasificador</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">representación</span> <span class="pre">de</span> <span class="pre">dimensiones</span> <span class="pre">reducidas,</span> <span class="pre">que</span> <span class="pre">elimina</span> <span class="pre">intrínsecamente</span> <span class="pre">el</span> <span class="pre">ruido</span> <span class="pre">aleatorio</span></code> de las entradas.</p></li>
</ul>
</section>
<section id="ejemplo-eigenfaces">
<h3>Ejemplo Eigenfaces<a class="headerlink" href="#ejemplo-eigenfaces" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Examinamos la utilización de una <code class="docutils literal notranslate"><span class="pre">proyección</span> <span class="pre">PCA</span></code> como <code class="docutils literal notranslate"><span class="pre">selector</span> <span class="pre">de</span> <span class="pre">características</span></code> en el contexto del <code class="docutils literal notranslate"><span class="pre">reconocimiento</span> <span class="pre">facial</span></code>. Profundicemos en los detalles, trabajado con el conjunto de datos <code class="docutils literal notranslate"><span class="pre">Labeled</span> <span class="pre">Faces</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">Wild</span></code> proporcionado por <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_lfw_people</span>
<span class="n">faces</span> <span class="o">=</span> <span class="n">fetch_lfw_people</span><span class="p">(</span><span class="n">min_faces_per_person</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;Ariel Sharon&#39; &#39;Colin Powell&#39; &#39;Donald Rumsfeld&#39; &#39;George W Bush&#39;
 &#39;Gerhard Schroeder&#39; &#39;Hugo Chavez&#39; &#39;Junichiro Koizumi&#39; &#39;Tony Blair&#39;]
(1348, 62, 47)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">min_faces_per_person=60</span></code> asegura que <em><strong>solo se incluyan personas con al menos 60 imágenes disponibles</strong></em> en el conjunto de datos. <code class="docutils literal notranslate"><span class="pre">(1348,</span> <span class="pre">62,</span> <span class="pre">47)</span></code> está asociado con el <em><strong>shape de las imágenes de los rostros en el conjunto de datos</strong></em>. Indica que hay <em><strong>1348 imágenes en total</strong></em>. <em><strong>Cada imagen tiene dimensiones de 62 píxeles de alto por 47 píxeles de ancho</strong></em>. Esto significa que cada imagen tiene una <em><strong>resolución de 62x47 píxeles</strong></em> y que hay <em><strong>1348 imágenes en total</strong></em> en el conjunto de datos.</p></li>
</ul>
<ul class="simple">
<li><p>A continuación <code class="docutils literal notranslate"><span class="pre">examinaremos</span> <span class="pre">los</span> <span class="pre">ejes</span> <span class="pre">principales</span> <span class="pre">que</span> <span class="pre">engloban</span> <span class="pre">este</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span></code>. Debido a su <code class="docutils literal notranslate"><span class="pre">considerable</span> <span class="pre">tamaño</span></code>, emplearemos <code class="docutils literal notranslate"><span class="pre">randomized</span> <span class="pre">PCA</span></code>. Este enfoque emplea una <code class="docutils literal notranslate"><span class="pre">técnica</span> <span class="pre">aleatoria</span> <span class="pre">para</span> <span class="pre">aproximar</span> <span class="pre">rápidamente</span> <span class="pre">los</span></code> <span class="math notranslate nohighlight">\(N\)</span> <code class="docutils literal notranslate"><span class="pre">componentes</span> <span class="pre">principales</span> <span class="pre">iniciales</span></code>, lo que resulta especialmente ventajoso para datos de gran dimensión, como éste, con casi <code class="docutils literal notranslate"><span class="pre">3.000</span> <span class="pre">dimensiones.</span> <span class="pre">Nos</span> <span class="pre">centraremos</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">primeros</span> <span class="pre">150</span> <span class="pre">componentes</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="n">svd_solver</span><span class="o">=</span><span class="s1">&#39;randomized&#39;</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>PCA(n_components=150, svd_solver=&#x27;randomized&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">PCA</label><div class="sk-toggleable__content"><pre>PCA(n_components=150, svd_solver=&#x27;randomized&#x27;)</pre></div></div></div></div></div></div></div>
</div>
<ul class="simple">
<li><p>En este caso, resulta interesante <code class="docutils literal notranslate"><span class="pre">representar</span> <span class="pre">visualmente</span> <span class="pre">las</span> <span class="pre">imágenes</span> <span class="pre">correspondientes</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">componentes</span> <span class="pre">principales</span> <span class="pre">iniciales</span></code>. Formalmente denominados <code class="docutils literal notranslate"><span class="pre">&quot;vectores</span> <span class="pre">propios&quot;</span></code>, estos componentes suelen denominarse <code class="docutils literal notranslate"><span class="pre">&quot;caras</span> <span class="pre">propias&quot;</span> <span class="pre">cuando</span> <span class="pre">se</span> <span class="pre">trata</span> <span class="pre">de</span> <span class="pre">imágenes</span></code>. Como se muestra en la figura, estas caras propias tienen un aspecto bastante inquietante</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                         <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;xticks&#39;</span><span class="p">:[],</span> <span class="s1">&#39;yticks&#39;</span><span class="p">:[]},</span>
                         <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;bone&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d032e91d5804ca1b1fe3ed7d454baa2fe9c5a366b071f6c9ad45b267b4e0e6c6.png" src="_images/d032e91d5804ca1b1fe3ed7d454baa2fe9c5a366b071f6c9ad45b267b4e0e6c6.png" />
</div>
</div>
<ul class="simple">
<li><p>Nótese que las <code class="docutils literal notranslate"><span class="pre">caras</span> <span class="pre">propias</span> <span class="pre">iniciales</span></code> (empezando por la parte superior izquierda) parecen estar <em><strong><code class="docutils literal notranslate"><span class="pre">relacionadas</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">ángulo</span> <span class="pre">de</span> <span class="pre">iluminación</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">cara</span></code></strong></em>, mientras que los <em><strong><code class="docutils literal notranslate"><span class="pre">vectores</span> <span class="pre">principales</span> <span class="pre">posteriores</span> <span class="pre">empiezan</span> <span class="pre">a</span> <span class="pre">aislar</span> <span class="pre">rasgos</span> <span class="pre">específicos</span> <span class="pre">como</span> <span class="pre">los</span> <span class="pre">ojos,</span> <span class="pre">la</span> <span class="pre">nariz</span> <span class="pre">y</span> <span class="pre">los</span> <span class="pre">labios</span></code></strong></em>. Examinemos ahora la <code class="docutils literal notranslate"><span class="pre">varianza</span> <span class="pre">acumulada</span> <span class="pre">asociada</span> <span class="pre">a</span> <span class="pre">estos</span> <span class="pre">componentes</span></code> para calibrar hasta qué punto la proyección conserva la información de los datos</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;number of components&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cumulative explained variance&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b9097643a8ec912a20930d9d5b2e96224b1ceec321e2f99a17a954c98be2a298.png" src="_images/b9097643a8ec912a20930d9d5b2e96224b1ceec321e2f99a17a954c98be2a298.png" />
</div>
</div>
<ul class="simple">
<li><p>Se observa que estos <code class="docutils literal notranslate"><span class="pre">150</span> <span class="pre">componentes</span> <span class="pre">abarcan</span> <span class="pre">algo</span> <span class="pre">más</span> <span class="pre">del</span> <span class="pre">90%</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">varianza</span></code>. Esto sugiere que, utilizando estos 150 componentes, <code class="docutils literal notranslate"><span class="pre">probablemente</span> <span class="pre">recuperaríamos</span> <span class="pre">la</span> <span class="pre">mayoría</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">atributos</span> <span class="pre">críticos</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>. Para consolidar esta idea, podemos <code class="docutils literal notranslate"><span class="pre">comparar</span> <span class="pre">las</span> <span class="pre">imágenes</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">originales</span> <span class="pre">con</span> <span class="pre">las</span> <span class="pre">imágenes</span> <span class="pre">reconstruidas</span> <span class="pre">utilizando</span> <span class="pre">estos</span> <span class="pre">150</span> <span class="pre">componentes</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="n">svd_solver</span><span class="o">=</span><span class="s1">&#39;randomized&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">components</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">projected</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">components</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">),</span>
                       <span class="n">subplot_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;xticks&#39;</span><span class="p">:[],</span> <span class="s1">&#39;yticks&#39;</span><span class="p">:[]},</span>
                       <span class="n">gridspec_kw</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">faces</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;binary_r&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">projected</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">62</span><span class="p">,</span> <span class="mi">47</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;binary_r&#39;</span><span class="p">)</span>
    
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;full-dim</span><span class="se">\n</span><span class="s1">input&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;150-dim</span><span class="se">\n</span><span class="s1">reconstruction&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/054741864d514ffec4cf32d1c1b7e2b4e8c8d814439dd71d1ea63683754d89a7.png" src="_images/054741864d514ffec4cf32d1c1b7e2b4e8c8d814439dd71d1ea63683754d89a7.png" />
</div>
</div>
<ul class="simple">
<li><p>Las imágenes de la <code class="docutils literal notranslate"><span class="pre">fila</span> <span class="pre">superior</span> <span class="pre">representan</span> <span class="pre">la</span> <span class="pre">entrada</span> <span class="pre">original</span></code>, mientras que la <code class="docutils literal notranslate"><span class="pre">fila</span> <span class="pre">inferior</span> <span class="pre">muestra</span> <span class="pre">imágenes</span> <span class="pre">reconstruidas</span> <span class="pre">utilizando</span> <span class="pre">sólo</span> <span class="pre">150</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">aproximadamente</span> <span class="pre">3.000</span> <span class="pre">características</span> <span class="pre">iniciales</span></code>. Esta visualización ilustra eficazmente por qué la selección de características PCA, logra un éxito significativo.</p></li>
<li><p>A pesar de <code class="docutils literal notranslate"><span class="pre">reducir</span> <span class="pre">significativamente</span> <span class="pre">la</span> <span class="pre">dimensionalidad</span></code> de los datos en aproximadamente un factor de 20, <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">imágenes</span> <span class="pre">proyectadas</span> <span class="pre">conservan</span> <span class="pre">suficiente</span> <span class="pre">información</span> <span class="pre">para</span> <span class="pre">que</span> <span class="pre">podamos</span> <span class="pre">reconocer</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">individuos</span></code> en las imágenes mediante inspección visual.</p></li>
<li><p>Esto implica que nuestro <code class="docutils literal notranslate"><span class="pre">algoritmo</span> <span class="pre">de</span> <span class="pre">clasificación</span> <span class="pre">puede</span> <span class="pre">entrenarse</span> <span class="pre">con</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">150</span> <span class="pre">dimensiones</span> <span class="pre">en</span> <span class="pre">lugar</span> <span class="pre">de</span> <span class="pre">con</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">3.000</span> <span class="pre">dimensiones</span></code>. Dependiendo del algoritmo específico elegido, esto puede conducir a una <code class="docutils literal notranslate"><span class="pre">clasificación</span> <span class="pre">mucho</span> <span class="pre">más</span> <span class="pre">eficiente</span></code>.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "ml_tf"
        },
        kernelOptions: {
            name: "ml_tf",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ml_tf'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ann_model.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Redes Neuronales y Deep Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="model_evaluation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Evaluación de modelos</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concepto-basico">Concepto básico</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autovalores-autovectores-y-componentes-principales">Autovalores, Autovectores y Componentes Principales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proceso-de-derivacion-de-componentes-principales-y-propiedades">Proceso de Derivación de Componentes Principales y Propiedades</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#el-problema-de-los-autovalores-de-la-matriz-de-varianza-covarianza-muestral-y-los-componentes-principales">El problema de los autovalores de la matriz de varianza-covarianza muestral y los componentes principales</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autovalores-y-autovectores-de-una-matriz-simetrica">Autovalores y autovectores de una matriz simétrica</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estandarizacion-y-matriz-de-correlacion-muestral">Estandarización y matriz de correlación muestral</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reduccion-de-dimension-y-perdida-de-informacion">Reducción de dimensión y pérdida de información</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">Implementación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduccion-al-analisis-de-componentes-principales">Introducción al análisis de componentes principales</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-para-reduccion-de-dimension">PCA para reducción de dimensión</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-para-visualizacion-digitos-escritos-a-mano">PCA para visualización: Dígitos escritos a mano</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-del-numero-de-componentes">Selección del número de componentes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pca-como-filtro-de-ruido">PCA como filtro de ruido</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-eigenfaces">Ejemplo Eigenfaces</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lihki Rubio, Ph.D.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>Lihki Rubio, Ph.D. All rights reserved.</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>