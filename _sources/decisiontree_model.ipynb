{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Observación`**\n",
    "\n",
    "- Los árboles de clasificación `se basan en una idea simple, pero poderosa`, y se encuentran entre las `técnicas más populares de clasificación`. Son sistemas de varias etapas, y la clasificación de un patrón en una clase se realiza de forma secuencial. `A través de una serie de pruebas, las clases se rechazan de forma secuencial hasta que se llega a una decisión a favor de una clase restante`. \n",
    "\n",
    "- Cada una de las pruebas, cuyo resultado decide qué clases se rechazan, es de `tipo binario \"Sí\" o \"No\" y se aplica a una sola característica`. Nuestro objetivo es presentar la filosofía principal en torno a un tipo especial de árboles conocidos como **`árboles de clasificación binarios ordinarios (OBCT)`**. Estos, pertenecen a una clase más general de métodos que construyen árboles, tanto para la clasificación como para la regresión, conocidos como `árboles de clasificación y regresión (CART)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Formulación`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La idea básica de los `OBCT` es `dividir el espacio de características en (hiper)rectángulos`; es decir, el espacio se divide mediante hiperplanos, que son paralelos a los ejes. Esto se ilustra en la {numref}`fig_decision_hypplanes_obct`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./imgs/decision_hypplanes_obct.png\n",
    ":name: fig_decision_hypplanes_obct\n",
    ":align: center\n",
    ":scale: 60\n",
    "Partición de espacio de características. Árbol de clasificación (OBCT). Fuente {cite}`theodoridis2020machine`.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La partición del espacio en `(hiper)rectángulos` se realiza mediante una serie de `\"preguntas\"` de esta forma: `¿es el valor de la característica` $x_{i} < a$?. Este también se conoce como el `criterio de división`. La secuencia de preguntas puede realizarse de forma agradable mediante el uso de un árbol. La {numref}`fig_decision_tree_obct` muestra el árbol correspondiente al caso ilustrado\n",
    "en la {numref}`fig_decision_hypplanes_obct`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./imgs/decision_tree_obct.png\n",
    ":name: fig_decision_tree_obct\n",
    ":align: center\n",
    ":scale: 60\n",
    "Árbol de clasificación (OBCT). Partición del espacio para {numref}`fig_decision_hypplanes_obct`. Fuente {cite}`theodoridis2020machine`.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    "Cada nodo del árbol realiza una prueba contra una característica individual y, si este no es un `nodo hoja (sin división adicional)`, este es conectado a dos `nodos descendientes (nodo de decisión)`: uno está asociado a la `respuesta \"Yes\"` y el otro a la `respuesta \"No\"`.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partiendo del nodo raíz, se realiza un recorrido de decisiones sucesivas hasta llegar a un nodo hoja. `Cada nodo hoja está asociado a una única clase`. La `asignación de un punto a una clase se realiza según la etiqueta del nodo hoja correspondiente`. Este tipo de clasificación es conceptualmente simple y fácil de interpretar. Por ejemplo, en un `sistema de diagnóstico médico`, se puede empezar con una pregunta: `¿La temperatura es alta?` si la respuesta es afirmativa, una segunda pregunta puede ser: `¿Presenta moquea?`. El proceso continúa hasta que se llega a una `decisión final sobre la enfermedad`.\n",
    "\n",
    "- Además, los árboles son útiles para `construir sistemas de razonamiento en la inteligencia artificial`. Por ejemplo, la existencia de objetos específicos, que se deduce a través de una serie de preguntas relacionadas, basadas en los valores de ciertas características (de alto nivel), puede conducir al `reconocimiento de una escena o de un objeto representado en una imagen`.\n",
    "\n",
    "- Una vez desarrollado el árbol, la clasificación es sencilla. El mayor reto consiste en `construir el árbol, explotando la información que reside en el conjunto de datos de entrenamiento`. Las principales preguntas a las que uno se enfrenta al diseñar un árbol, entre otras que se discutirán más adelante, son:\n",
    "\n",
    "    - `¿Qué criterio de división debe adoptarse?`\n",
    "    - `¿Cuándo se debe detener el crecimiento de un árbol y declarar un nodo como final?`\n",
    "    - `¿Cómo se asocia un nodo hoja a una clase concreta?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Criterio de división`**: Ya hemos dicho que las preguntas que se hacen en cada nodo son del tipo\n",
    "\n",
    "$$\n",
    "\\text{¿es}~x_{i} < a\\text{?}\n",
    "$$\n",
    "\n",
    "- El objetivo es `seleccionar un valor adecuado para el umbral` $a$. Supongamos que, partiendo del nodo raíz, el árbol ha crecido hasta el `nodo actual` $t$. Cada nodo, $t$, está asociado a un subconjunto $X_{t}\\subseteq X$ del `conjunto de datos de entrenamiento`, $X$. Este es el `conjunto de los puntos de entrenamiento que han sobrevivido a este nodo, después de las pruebas que han tenido lugar en los nodos anteriores del árbol`. \n",
    "\n",
    "- Por ejemplo, en la {numref}`fig_decision_tree_obct`, `un número de puntos, que pertenecen, digamos, a la clase` $\\omega_{1}$, `no participarán en el nodo` $t_{1}$ porque `ya han sido asignados en un nodo hoja previamente etiquetado`. El propósito de un criterio de división es `dividir` $X_{t}$ `en dos subconjuntos disyuntos, digamos` $X_{tY}$`, y` $X_{tN}$`, dependiendo de la respuesta a la pregunta específica en el nodo` $t$. Para cada división, se cumple lo siguiente:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X_{tY}\\cap X_{tN}&=\\emptyset\\\\\n",
    "X_{tY}\\cup X_{tN}&=X_{t}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El objetivo en cada nodo es seleccionar qué característica se va a probar y también cuál es el mejor valor del umbral $a$. `La filosofía adoptada es hacer la elección de manera que cada división genere conjuntos,` $X_{tY}$,  $X_{tN}$`, que sean más homogéneos en cuanto a la clase en comparación con` $X_{t}$. En otras palabras, los datos en cada uno de los dos conjuntos descendientes deben mostrar una mayor preferencia por clases específicas, en comparación con el conjunto antecesor. \n",
    "\n",
    "- En la terminología adoptada, los conjuntos $X_{tY}$ y $X_{tN}$ `deben ser más puros en comparación con` $X_{t}$ . Así pues, primero debemos `seleccionar un criterio que mida la impureza` y, a continuación, `calcular el valor umbral y elegir la característica específica` (que se va a probar) para `maximizar la disminución de la impureza del nodo`.\n",
    "\n",
    "- Por ejemplo, una `medida común para cuantificar la impureza del nodo,` $t$`, es la entropía`, definida como\n",
    "\n",
    "$$\n",
    "I(t)=-\\sum_{m=1}^{M}P(\\omega_{m}|t)\\log_{2}P(\\omega_{m}|t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{figure} ./imgs/entropy_function.png\n",
    ":name: entropy_function_fig\n",
    ":align: center\n",
    ":scale: 20\n",
    "\n",
    "Función de entropía $I(t)$ para clasificación binaria.\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `El valor máximo de` $I(t)$ `se produce si todas las probabilidades son iguales (máxima impureza)`, y `el valor más pequeño, que es igual a cero, cuando sólo uno de los valores de probabilidad es uno y el resto es igual a cero`. Las probabilidades se aproximan como\n",
    "\n",
    "$$\n",
    "P(\\omega_{m}|t)=\\frac{N_{t}^{m}}{N_{t}},\\quad m=1,2,\\dots,M,\n",
    "$$\n",
    "\n",
    "- $N_{t}^{m}$ es el `número de puntos de la clase` $m$ `en` $X_{t}$, y $N_{t}$ el `número total de puntos en` $X_{t}$. La `disminución de la impureza del nodo`, tras dividir los datos en dos conjuntos, se define como\n",
    "\n",
    "    $$\n",
    "    \\Delta I(t)=I(t)-\\frac{N_{t_{Y}}}{N_{t}}I(t_{Y})-\\frac{N_{t_{N}}}{N_{t}}I(t_{N}),\n",
    "    $$\n",
    "\n",
    "    donde $I(t_{Y})$ y $I(t_{N})$ son las `impurezas asociadas a los dos nuevos conjuntos`, respectivamente. Esto es, disminución de la impureza del nodo es la `diferencia entre la entropía de un nodo padre y la suma ponderada de las entropías de sus nodos hijos`. A esta última expresión $\\Delta I(t)$, también se le conoce como `ganancia de información`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Ejemplo: Cálculo de Entropía\n",
    ":class: tip\n",
    "\n",
    "Calculemos el `índice de entropía` $I(t)$ para `3 casos diferentes` de un conjunto con `4 balotas` de dos colores diferentes, `rojo y azul`:\n",
    "\n",
    "- 4 balotas rojas y 0 balotas azules:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(t)&=-P(\\text{rojo})\\cdot\\log_{2}(P(\\text{rojo}))-P(\\text{azul})\\cdot\\log_{2}(P(\\text{azul}))\\\\\n",
    "&=-\\frac{4}{4}\\cdot\\log_{2}\\frac{4}{4}-\\frac{0}{4}\\cdot\\log_{2}\\frac{0}{4}=0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- 2 balotas rojas y 2 balotas azules:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(t)&=-P(\\text{rojo})\\cdot\\log_{2}(P(\\text{rojo}))-P(\\text{azul})\\cdot\\log_{2}(P(\\text{azul}))\\\\\n",
    "&=-\\frac{2}{4}\\cdot\\log_{2}\\frac{2}{4}-\\frac{2}{4}\\cdot\\log_{2}\\frac{2}{4}=1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- 3 balotas rojas y 1 balota azul:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(t)&=-P(\\text{rojo})\\cdot\\log_{2}(P(\\text{rojo}))-P(\\text{azul})\\cdot\\log_{2}(P(\\text{azul}))\\\\\n",
    "&=-\\frac{3}{4}\\cdot\\log_{2}\\frac{3}{4}-\\frac{1}{4}\\cdot\\log_{2}\\frac{1}{4}=0.811\n",
    "\\end{align*}\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{figure} ./imgs/entropy_decision_tree.png\n",
    ":align: center\n",
    ":name: trees_entropy_fig\n",
    ":scale: 40\n",
    "\n",
    "Pureza de nodos en un árbol de decisión. Fuente `towardsdatascience`.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El objetivo ahora es `seleccionar la característica específica` $x_{i}$ `y el umbral` $a_{t}$ tal que, $\\Delta I(t)$ sea máximo. Esto definirá ahora `dos nuevos nodos descendientes` de $t$, a saber, $t_{N}$ y $t_{Y}$; así, el árbol crece con dos nuevos nodos. Una forma de `buscar distintos valores umbral` es la siguiente:\n",
    "\n",
    "    1. `Para cada una de las características` $x_{i},~i=1,2\\dots,l$`, realice un ranking de los valores` $x_{in},~n=1,2,\\dots,N_{t}$`, que toma esta característica entre los puntos en` $X_{t}$.\n",
    "    2. A continuación, `defina una secuencia de valores umbral correspondientes`, $a_{in}$ `que estén en el medio, entre valores distintos consecutivos de` $x_{in}$\n",
    "    3. Seguidamente, `compruebe el cambio de impureza que se produce para cada uno de estos valores umbral` y quédese con el que consiga la `máxima disminución`.\n",
    "    4. `Repita el proceso para todas las características` y, por último, `quédese con la combinación que dé como resultado la mejor disminución máxima`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Además de la `entropía`, pueden utilizarse otros `índices de medición de impurezas`. Una alternativa popular, que da como resultado un `máximo ligeramente superior que el de entropía`, es el llamado `índice de Gini`, definido como\n",
    "\n",
    "$$\n",
    "I(t)=\\sum_{m=1}^{M}P(\\omega_{m}|t)(1-P(\\omega_{m}|t)).\n",
    "$$\n",
    "\n",
    "- Este índice también es `cero si uno de los valores de probabilidad es igual a 1 y el resto son cero`, y toma\n",
    "su valor `máximo cuando todas las clases son equiprobables`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Regla de detención de división (stop-splitting)\n",
    ":class: tip\n",
    "\n",
    "- La pregunta obvia cuando crece un árbol es `cuándo dejar de cultivarlo`. Una forma posible es `adoptar un valor umbral, `$T$`, y dejar de dividir un nodo una vez que el valor máximo` $\\Delta I(t)$`, para todas las divisiones posibles, sea menor que` $T$. \n",
    "- Otra posibilidad es parar cuando la cardinalidad de $X_{t}$ es menor que un número determinado o si el `nodo es puro, en el sentido de que todos los puntos que lo componen pertenecen a una única clase`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Regla de asignación de clase\n",
    ":class: tip\n",
    "Una vez que se declara que un `nodo` $t$` es un nodo hoja, se le asigna una etiqueta de clase`; normalmente por mayoría. Es decir, `se le asigna la etiqueta de la clase a la que pertenecen la mayoría de los datos de` $X_{t}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Podado del árbol\n",
    ":class: tip\n",
    "- La experiencia ha demostrado que `el crecimiento de un árbol y el uso de una regla de parada no siempre funciona bien en la práctica`; el crecimiento puede detenerse antes de tiempo o puede dar lugar a árboles de tamaño muy grande. \n",
    "- Una práctica común es `hacer crecer primero un árbol hasta un tamaño grande y luego adoptar una técnica de poda para eliminar nodos`. Se pueden utilizar diferentes criterios de poda; uno muy popular es `combinar una estimación de la probabilidad de error con un índice de medición de la complejidad` {cite}`breiman1984cart, ripley2007pattern`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:observation}\n",
    ":label: observation_dt3\n",
    "\n",
    "1. Entre las notables ventajas de los árboles de decisión está el hecho de que `pueden tratar de forma natural mezclas de variables numéricas y categóricas`. Además, `se adaptan bien a grandes conjuntos de datos`. `Pueden tratar eficazmente datos faltantes`. En muchos dominios, no se conocen todos los valores de las características para cada patrón. Los valores pueden no haber sido registrados, o pueden ser demasiado costosos de obtener.\n",
    "\n",
    "2. Debido a su simplicidad estructural, son `fácilmente interpretables`; en otras palabras, `es posible que un humano entienda la razón de la salida del algoritmo de aprendizaje`. En algunas, como en las decisiones financieras, esto es un requisito legal. Por otro lado, `el rendimiento de predicción de los clasificadores de árbol no es tan bueno como el de otros métodos`, como las `máquinas de soporte vectorial` y las `redes neuronales`, que se tratarán en posteriores capítulos\n",
    "\n",
    "3. Uno de los `principales inconvenientes asociados a los clasificadores de árbol es que son inestables`. Es decir, un `pequeño cambio en el conjunto de datos de entrenamiento puede dar lugar a un árbol muy diferente`. La razón de esto radica en la `naturaleza jerárquica de los clasificadores de árbol`. Un error que se produce en un nodo en un nivel alto del árbol se propaga hasta las hojas inferiores.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Bagging (Bootstrap Aggregating)\n",
    ":class: tip\n",
    "`Bagging (Bootstrap Aggregating)` es una técnica que puede `reducir la varianza y mejorar el rendimiento del error de generalización`.\n",
    "\n",
    "- La idea básica es `crear un número de variantes` $B$, $~X_{1}, X_{2},\\dots, X_{B}$`, del conjunto de entrenamiento,` $X$`, utilizando técnicas bootstrap`, mediante un muestreo uniforme de $X$ con reemplazo. \n",
    "- `Para cada una de las variantes del conjunto de entrenamiento,` $X_{i}$`, se construye un árbol,` $T_{i}$. \n",
    "- La decisión final para la clasificación de un punto dado es a favor de la `clase predicha por la mayoría de los arboles subclasificadores`, $T_{i}, i = 1, 2,\\dots, B$ {cite}`breiman1996bagging`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Bosques Aleatorios (Random Forest)\n",
    ":class: tip\n",
    "Los `Bosques Aleatorios (Random Forest) utilizan la idea de bagging junto con la selección aleatoria de características` {cite}`breiman2001random`. La diferencia con el bagging radica en la forma en que se construyen los árboles de decisión. \n",
    "\n",
    "- `La característica a dividir en cada nodo se selecciona como la mejor entre un conjunto de` $F$ `características elegidas al azar`, donde $F$ es un parámetro definido por el usuario. \n",
    "- Esta `aleatoriedad adicional introducida tiene un efecto sustancial en la mejora del rendimiento`. Los bosques aleatorios suelen tener una `precisión predictiva muy buena` y se han utilizado en una serie de aplicaciones, como el `reconocimiento de la postura del cuerpo en términos del popular sensor Kinect de Microsoft` {cite}`shotton2011real`.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Además de los métodos anteriores, recientemente, también se han sugerido `técnicas Bayesianas utilizadas para estabilizar el rendimiento de los árboles`; véase {cite}`chipman2010bart, wu2007bayesian`. Por supuesto, `el efecto de utilizar múltiples árboles, es perder una de las principales ventajas de los árboles, su facilidad de interpretación.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinación de clasificadores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hasta ahora, hemos revisado en detalle una serie de clasificadores, y se presentarán más métodos en las siguientes secciones, relativos a las `máquinas de vectores de soporte` y las `redes neuronales`. La pregunta obvia a la que se enfrenta un profesional/investigador sin experiencia es: `¿qué método uso entonces? Por desgracia, no hay una respuesta definitiva.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} No free lunch theorem\n",
    "\n",
    "- El objetivo del diseño de cualquier clasificador, y en general de cualquier esquema de aprendizaje es `proporcionar un buen rendimiento de generalización`. Sin embargo, no hay razones independientes del contexto o del uso para apoyar una técnica de aprendizaje en lugar de otra. \n",
    "\n",
    "- `Cada tarea de aprendizaje, representada por el conjunto de datos disponible, mostrará una preferencia por un esquema de aprendizaje específico` que se ajuste a las especificidades del problema en cuestión. \n",
    "\n",
    "- `Un algoritmo que obtiene la máxima puntuación en un problema puede obtener una puntuación baja en otro`. Esto se resume a veces como el **`Teorema de la no gratuidad (No free lunch theorem)`**\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- En la práctica, hay que `probar diferentes métodos de aprendizaje dentro de los disponibles`, cada uno optimizado para la tarea específica, y `probar su rendimiento de generalización` con un conjunto de datos independiente distinto del utilizado para el entrenamiento, utilizando, por ejemplo, el método de exclusión o cualquiera de sus variantes. \n",
    "\n",
    "- A continuación, `se mantiene y se utiliza el método que ha obtenido la mejor puntuación para la tarea específica`. Con este fin, hay una serie de esfuerzos importantes para comparar diferentes clasificadores contra diferentes conjuntos de datos y `medir el rendimiento \"medio\", mediante el uso de diferentes índices estadísticos para cuantificar el rendimiento global de cada clasificador` frente a los conjuntos de datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Esquemas de combinación de clasificadores`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una tendencia para `mejorar el rendimiento` es `combinar diferentes clasificadores y explotar sus ventajas individuales`. Una observación que justifica este enfoque es que, durante las pruebas, `hay patrones en los que incluso el mejor clasificador para una tarea concreta no logra predecir su verdadera clase`. En cambio, `los mismos patrones pueden ser clasificados correctamente por otros clasificadores, con un rendimiento global inferior`. \n",
    "\n",
    "- Esto muestra que puede haber cierta `complementariedad entre los distintos clasificadores`, y la combinación puede conducir a un `mayor rendimiento en comparación con el obtenido por el mejor (único) clasificador`. Recordemos que `el bagging mencionado anteriormente, es un tipo de combinación de clasificadores`. La cuestión que se plantea ahora es seleccionar un `esquema de combinación`. Hay diferentes esquemas, y los resultados que proporcionan pueden ser diferentes. A continuación, `resumimos los esquemas de combinación más populares`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Regla de la media aritmética\n",
    "Suponiendo que utilizamos $L$ clasificadores, en los que `cada uno da un valor de probabilidad posterior`, $P_{j}(\\omega_{i}|\\boldsymbol{x}), i = 1,2,\\dots,M,~j = 1, 2, . . . L$, la decisión sobre `la asignación de la clase se basa en la siguiente regla`:\n",
    "\n",
    "$$\n",
    "\\text{Asignar}~\\boldsymbol{x}~\\text{a la clase}~\\omega_{i}=\\textrm{arg}\\max_{k}\\frac{1}{L}\\sum_{j=1}^{L}P_{j}(\\omega_{k}|\\boldsymbol{x}),\\quad k=1,2,\\dots,M.\n",
    "$$\n",
    "\n",
    "Esta regla equivale a calcular la `probabilidad posterior \"final\"`, $P(\\omega_{i}|\\boldsymbol{x})$, por medio de `minimización de la distancia media de Kullback-Leibler`\n",
    "\n",
    "$$\n",
    "D_{av}=\\frac{1}{L}\\sum_{j=1}^{L}D_{j},~\\text{donde}~D_{j}=\\sum_{i=1}^{M}P_{j}(\\omega_{i}|\\boldsymbol{x})\\ln\\frac{P_{j}(\\omega_{i}|\\boldsymbol{x})}{P(\\omega_{i}|\\boldsymbol{x})}.\n",
    "$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Regla de promedio geométrica\n",
    "Esta regla es el resultado de `minimizar la formulación alternativa de distancia de Kullback-Leibler` (nótese que esta distancia no es simétrica); en otras palabras\n",
    "\n",
    "$$\n",
    "D_{j}=\\sum_{i=1}^{M}P(\\omega_{i}|\\boldsymbol{x})\\ln\\frac{P(\\omega_{i}|\\boldsymbol{x})}{P_{j}(\\omega_{i}|\\boldsymbol{x})},\n",
    "$$\n",
    "\n",
    "lo que da lugar a\n",
    "\n",
    "$$\n",
    "\\text{Asignar}~\\boldsymbol{x}~\\text{a la clase}~\\omega_{i}=\\textrm{arg}\\max_{k}\\prod_{j=1}^{L} P_{j}(\\omega_{k}|\\boldsymbol{x}),\\quad k=1,2,\\dots,M.\n",
    "$$\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Apilamiento (Stacking)\n",
    "Una forma alternativa es utilizar una `media ponderada de las salidas de los clasificadores individuales`, donde `los pesos de la combinación se obtienen de forma óptima utilizando los datos de entrenamiento`. Supongamos que la salida de cada `clasificador individual`, $f_{j}(x)$, es de tipo suave (`infinitamente diferenciable`); por ejemplo, una estimación de probabilidad posterior, como antes. Entonces, `la salida combinada viene dada por`\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{x})=\\sum_{j=1}^{L}\\omega_{j}f_{j}(\\boldsymbol{x}),\n",
    "$$\n",
    "\n",
    "donde `los pesos son estimados vía la siguiente tarea de optimización`:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\omega}}=\\textrm{arg}\\min_{\\boldsymbol{\\omega}}\\sum_{n=1}^{N}\\mathcal{L}(y_{n}, f(\\boldsymbol{x}_{n}))=\\textrm{arg}\\min_{\\boldsymbol{\\omega}}\\sum_{n=1}^{N}\\mathcal{L}\\left(y_{n}, \\sum_{j=1}^{L}\\omega_{j}f_{j}(\\boldsymbol{x}_{n})\\right)\n",
    "$$\n",
    "\n",
    "donde, $\\mathcal{L}(\\cdot,\\cdot)$ es una `función de pérdida`; por ejemplo, la del `error cuadrático`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sin embargo, `adoptar la anterior optimización, basada en el conjunto de datos de entrenamiento, puede conducir a un sobreajuste`. De acuerdo con el apilamiento {cite}`wolpert1992stacked` se adopta un `razonamiento de validación cruzada` y en lugar de $f_{j}(\\boldsymbol{x}_{n})$, empleamos $f_{j}^{(-n)}(\\boldsymbol{x}_{n})$, donde este último es la `salida del` $j$`-ésimo clasificador entrenado en los datos tras excluir el par` $(y_{n}, x_{n})$. En otras palabras, los pesos se estiman mediante \n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\omega}}=\\text{argmin}_{\\boldsymbol{\\omega}}\\sum_{n=1}^{N}\\mathcal{L}\\left(y_{n}, \\sum_{j=1}^{L}\\omega_{j}f_{j}^{(-n)}(\\boldsymbol{x}_{n})\\right).\n",
    "$$\n",
    "\n",
    "- A veces, `las ponderaciones tienen que ser positivas y sumar uno`, lo que da lugar a una tarea de `optimización restringida`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Regla de votación por mayoría\n",
    "Los métodos anteriores pertenecen a la familia de reglas `soft-type`. Una alternativa popular es una regla `hard-type`, que se basa en un `esquema de votación`. `Se decide a favor de la clase para la que hay consenso o al menos` $l_{c}$ `clasificadores están de acuerdo en la etiqueta de clase`, donde\n",
    "\n",
    "$$\n",
    "l_{c}=\n",
    "\\begin{cases}\n",
    "\\displaystyle{\\frac{L}{2}+1}, & L~\\text{es par}\\\\\n",
    "\\displaystyle{\\frac{L+1}{2}}, & L~\\text{es impar}\n",
    "\\end{cases}\n",
    "$$\n",
    "    \n",
    "`En caso contrario, la decisión es de rechazo` (es decir, no se toma ninguna decisión).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Reglas del Tipo: Hard-Soft\n",
    ":class: tip\n",
    "\n",
    "Una `regla de clasificación soft` generalmente `estima las probabilidades condicionales de clase de forma explícita y, a continuación, realiza la predicción de clase basada en la mayor probabilidad estimada`. Por el contrario, la `clasificación hard` omite el requisito de estimar la probabilidad de la clase y `estima directamente el límite de clasificación`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Regla de la mediana\n",
    "\n",
    "Cuando `valores atípicos están presentes`, se puede utilizar en su lugar el valor de la `mediana`:\n",
    "\n",
    "$$\n",
    "\\text{Asignar}~\\boldsymbol{x}~\\text{a la clase}~\\omega_{i}=\\textrm{arg}\\max_{k}\\textrm{median}\\{P_{j}(\\omega_{k}|\\boldsymbol{x})\\},~k=1,2,\\dots,M.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ocurre que, el `no free lunch theorem` también es válido para las reglas de combinación; `no hay una regla universalmente óptima`. Todo depende de los datos de que se disponga; véase {cite}`jain2000statistical`. Hay otras cuestiones relacionadas con la teoría de la combinación de clasificadores; por ejemplo, `¿cómo se eligen los clasificadores que se van a combinar? ¿Deben ser dependientes o independientes?`. \n",
    "\n",
    "- Además, `la combinación no implica necesariamente una mejora del rendimiento, en algunos casos, se puede experimentar una pérdida de rendimiento (mayor tasa de error) en comparación con el mejor clasificador (único)`. Por tanto, `la combinación debe realizarse con cuidado` {cite}`kuncheva2014combining, koutroumbas2008pattern`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:observation}\n",
    ":label: observation_dt1\n",
    "\n",
    "¿**Un algoritmo de aprendizaje débil**, es decir, uno que funciona ligeramente mejor que una adivinación aleatoria, **puede convertirse en uno fuerte con un buen índice de rendimiento**?.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El `enfoque boosting` es un procedimiento para `combinar o \"reforzar (boost)\" el rendimiento de los clasificadores débiles`(clasificadores cuyas estimaciones de parámetros suelen ser imprecisas y ofrecen un rendimiento deficiente) con el fin de `obtener un mejor clasificador`. Se diferencia del `bagging` en que es un *procedimiento determinista y genera conjuntos de entrenamiento y clasificadores secuencialmente, basándose en los resultados de la iteración anterior*. En cambio, el bagging genera los conjuntos de entrenamiento aleatoriamente y puede generar los clasificadores en paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} Observación\n",
    ":class: tip\n",
    "- **Boosting** *asigna un peso a cada patrón en el conjunto de entrenamiento, reflejando su importancia*, y *construye un clasificador utilizando el conjunto de entrenamiento y el conjunto de pesos*. Por lo tanto, **requiere un clasificador que pueda manejar pesos en las muestras de entrenamiento** (algunos clasificadores pueden ser incapaces de admitir patrones ponderados). \n",
    "\n",
    "- En este caso, se puede *muestrear un subconjunto de los ejemplos de entrenamiento de acuerdo con la distribución de los pesos* (ponderación de muestras de entrenamiento) y *utilizar estos ejemplos para entrenar al clasificador en la siguiente etapa de la iteración*. El aprendiz final se obtiene mediante una *media ponderada de todos los aprendices de base (débiles) jerárquicamente diseñados*. Por lo tanto, el boosting también puede considerarse un *esquema para combinar diferentes aprendices*.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dado un *número suficiente de iteraciones, se puede mejorar significativamente lo (pobre) del aprendiz débil*. Por ejemplo, en algunos casos de clasificación, el *error de entrenamiento puede tender a cero a medida que aumenta el número de iteraciones*. El entrenamiento de un clasificador *mediante una manipulación adecuada de los datos de entrenamiento* (de hecho, *el mecanismo de ponderación identifica las muestras difíciles, las que siguen fallando, y pone más énfasis en ellas*) *se puede obtener un clasificador fuerte*. Por supuesto, como veremos más adelante, el hecho de que el error de entrenamiento tienda a cero no significa necesariamente que el error de prueba llegue a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./imgs/boosted_tree.jpg\n",
    ":name: boosted_tree_numref\n",
    ":align: center\n",
    ":scale: 90\n",
    "Ilustración del `Arbol Boosting`: Fuente {cite}`kiangala2021effective`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Algoritmo AdaBoost (Adaptive Boosting)\n",
    "\n",
    "Considere una *tarea de clasificación de dos clases* y supongamos que se nos da un conjunto de $N$ *observaciones de entrenamiento*, $(y_{n}, \\boldsymbol{x}_{n}),~n=1,2,\\dots,N$, con $y_{n}\\in\\{-1, 1\\}$. Nuestro objetivo es *diseñar un clasificador binario*,\n",
    "\n",
    "$$\n",
    "f(\\boldsymbol{x})=\\text{sgn}\\{F(\\boldsymbol{x})\\}\n",
    "$$(sign_fn_adaboost_eq)\n",
    "\n",
    "donde\n",
    "\n",
    "$$\n",
    "F(\\boldsymbol{x}):=\\sum_{k=1}^{K}a_{k}\\phi(\\boldsymbol{x};\\boldsymbol{\\theta}_{k}),\n",
    "$$(adaboosting_model_eq)\n",
    "\n",
    "y $\\phi(\\boldsymbol{x}; \\boldsymbol{\\theta}_{k})\\in\\{-1, 1\\}$, es el *clasificador base en la iteración* $k$, definido en términos de un *conjunto de parámetros*, $\\boldsymbol{\\theta}_{k},~k=1,2,\\dots, K$ *a ser estimados*.\n",
    "\n",
    "- El clasificador base se selecciona como uno binario. El conjunto de *parámetros desconocidos se obtienen mediante* `step-wise` y de forma `greedy`; es decir, en cada iteración $i$, *solo optimizamos con respecto a un único par*, $(a_{i}, \\boldsymbol{\\theta}_{i})$ *manteniendo los parámetros* $a_{k}, \\boldsymbol{\\theta}_{k},~k=1,2,\\dots,i-1$, *obtenidos en los pasos anteriores, fijos*.\n",
    "\n",
    "- Nótese que *lo ideal sería optimizar respecto a todos los parámetros desconocidos*, $a_{k}$, $k=1,2,\\dots,K$, *simultáneamente*, sin embargo, esto conduciría a una *tarea de optimización muy exigente, desde el punto de vista computacional*. Los algoritmos `greedy` son muy populares debido a su *simplicidad computacional*, y conducen a un muy *buen rendimiento en una amplia gama de tareas de aprendizaje*. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supongamos que *nos encontramos en el paso de iteración* $i$*-ésimo*; consideremos la *suma parcial* de términos\n",
    "\n",
    "$$\n",
    "F_{i}(\\cdot)=\\sum_{k=1}^{i}a_{k}\\phi(\\cdot; \\boldsymbol{\\theta}_{k}).\n",
    "$$\n",
    "\n",
    "- Entonces, podemos escribir la siguiente *recursion*\n",
    "\n",
    "    $$\n",
    "    F_{i}(\\cdot)=F_{i-1}(\\cdot)+a_{i}\\phi(\\cdot; \\boldsymbol{\\theta}_{i}),\\quad i=1,2,\\dots,K,\n",
    "    $$\n",
    "  \n",
    "    partiendo de una *condición inicial*. Con base en el *razonamiento greedy*, $F_{i-1}(\\cdot)$ *se supone conocido* y el objetivo es *optimizar con respecto al conjunto de parámetros* $a_{i},~\\boldsymbol{\\theta}_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para la *tarea de optimización*, debe adoptarse una *función de pérdida*. Sin duda, *existen diferentes opciones, que dan distintos nombres al algoritmo derivado*. Una función de pérdida popular, utilizada para la clasificación, es la **pérdida exponencial**, definida como\n",
    "\n",
    "    $$\n",
    "    \\mathcal{L}(y, F(\\boldsymbol{x}))=\\exp(-yF(\\boldsymbol{x})):\\quad\\text{exponential loss function},\n",
    "    $$\n",
    "\n",
    "    y da lugar al **algoritmo boosting adaptativo (AdaBoost)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ./imgs/loss_fn_adaboost.png\n",
    ":name: loss_fn_adaboost_fig\n",
    ":align: center\n",
    ":scale: 80\n",
    "Pérdida 0-1, exponencial, log-loss y error al cuadrado. Fuente {cite}`theodoridis2020machine`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La función de *pérdida exponencial*, junto con la función de *pérdida 0-1*, se muestran en {numref}`loss_fn_adaboost_fig`. *La primera puede considerarse un límite superior (diferenciable) de la función de pérdida 0-1 (no diferenciable)*. \n",
    "\n",
    "- Observe que **la pérdida exponencial coloca más peso sobre los errores de clasificación** $(yF(\\boldsymbol{x}) < 0)$ **en comparación con los correctamente identificados** $(yF(\\boldsymbol{x}) > 0)$. Empleando la función de *pérdida exponencial*, el conjunto $a_{i},\\boldsymbol{\\theta}_{i}$ se obtiene mediante la respectiva **función de coste empírica**, de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(a_{i}, \\boldsymbol{\\theta}_{i})&=\\text{arg}\\text{min}_{a,\\boldsymbol{\\theta}}\\sum_{n=1}^{N}\\mathcal{L}(y_{n}, F(\\boldsymbol{x}_{n}))\\\\\n",
    "&=\\text{arg}\\text{min}_{a,\\boldsymbol{\\theta}}\\sum_{n=1}^{N}\\exp\\left(-y_{n}(F_{i-1}(\\boldsymbol{x}_{n})+a\\phi(\\boldsymbol{x}_{n};\\boldsymbol{\\theta}))\\right).\n",
    "\\end{align*}\n",
    "$$(costfn_adaboost_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Esta *tarea de optimización* también se realiza en dos pasos. En primer lugar, $a$ **se trata de forma fija y optimizamos con respecto a** $\\boldsymbol{\\theta}$,\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\boldsymbol{\\theta}_{i}&=\\text{arg}\\text{min}_{a,\\boldsymbol{\\theta}}\\sum_{n=1}^{N}\\exp\\left(-y_{n}(F_{i-1}(\\boldsymbol{x}_{n})+a\\phi(\\boldsymbol{x}_{n};\\boldsymbol{\\theta}))\\right)\\\\\n",
    "    &=\\text{argmin}_{\\boldsymbol{\\theta}}\\sum_{n=1}^{N}\\omega_{n}^{(i)}\\exp(-y_{n}a\\phi(\\boldsymbol{x}_{n};\\boldsymbol{\\theta})),\n",
    "    \\end{align*}\n",
    "    $$(weights_omega_eq)\n",
    "\n",
    "    donde $\\omega_{n}^{(i)}:=\\exp(-y_{n}F_{i-1}(\\boldsymbol{x}_{n})),~n=1,2,\\dots,N.$\n",
    "\n",
    "- Observe que $\\omega_{n}^{(i)}$ *no depende ni de* $a$ *ni de* $\\phi(x_{n}; \\boldsymbol{\\theta})$, por lo que puede considerarse un **peso asociado a la muestra** $n$. Además, *su valor depende por completo de los resultados obtenidos en las pruebas anteriores*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ahora nos centraremos en el coste de Ecuación {eq}`costfn_adaboost_eq`. La **optimización depende de la forma específica del\n",
    "clasificador base**. Nótese, sin embargo, que la *función de pérdida es de forma exponencial*. Además, el *clasificador\n",
    "es binario*, de modo que $\\phi(\\boldsymbol{x}, \\boldsymbol{\\theta})\\in\\{-1, 1\\}$.\n",
    "\n",
    "- Si *suponemos que* $a > 0$ (volveremos a ello pronto) la optimización de Ecuación {eq}`costfn_adaboost_eq` se ve fácilmente que es equivalente a *optimizar el siguiente coste*:\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol{\\theta}_{i}=\\text{argmin}_{\\theta}P_{i},~\\text{donde}~P_{i}:=\\sum_{n=1}^{N}\\omega_{n}^{(i)}\\chi_{(-\\infty, 0]}(y_{n}\\phi(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta})),\n",
    "    $$(loss_funtion_01)\n",
    "\n",
    "    y $\\chi_{(-\\infty, 0]}$ es la *función de pérdida 0-1*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En otras palabras, **solo contribuyen los puntos mal clasificados** ($y_{n}\\phi(\\boldsymbol{x}_{n}; \\boldsymbol{\\theta}) < 0$). Nótese que $P_{i}$ es el **error ponderado del clasificador empírico**. Claramente, cuando se minimiza el error de clasificación en la Ecuación {eq}`loss_funtion_01`, el coste de Ecuación {eq}`costfn_adaboost_eq` *también se minimiza, porque la pérdida exponencial coloca mayor peso a los puntos mal clasificados* (cota superior para $\\chi_{(-\\infty, 0]}$). \n",
    "\n",
    "- Para garantizar que $P_{i}$ permanece en el intervalo $[0, 1]$, *los pesos son normalizados a la unidad dividiendo por la suma respectiva*; nótese que esto no afecta al proceso de optimización. En otras palabras, $\\boldsymbol{\\theta}_{i}$ **puede calcularse para minimizar el error de clasificación empírico, cometido por el clasificador base**. Para clasificadores base de estructura muy simple, **dicha minimización es computacionalmente factible**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una vez calculado el $\\boldsymbol{\\theta}_{i}$ óptimo, a partir de las respectivas definiciones,\n",
    "\n",
    "    $$\n",
    "    \\sum_{y_{n}\\phi(\\boldsymbol{x}_{n}; \\boldsymbol{\\theta}_{i})<0}\\omega_{n}^{(i)}=P_{i},~\\text{y}~\\sum_{y_{n}\\phi(\\boldsymbol{x}_{n}; \\boldsymbol{\\theta}_{i})>0}\\omega_{n}^{(i)}=1-P_{i}.\n",
    "    $$(Pi_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Combinando* las Ecuaciones {eq}`Pi_eq`-{eq}`weights_omega_eq`, se demuestra fácilmente que\n",
    "\n",
    "$$\n",
    "a_{i}=\\text{argmin}_{a}\\{\\exp(-a)(1-P_{i})+\\exp(a)P_{i}\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Si se toma la derivada con respecto a* $a$ *y se iguala a cero*, se obtiene\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial a_{i}}(\\exp(-a_{i})(1-P_{i})+\\exp(a_{i})P_{i})=0\\Leftrightarrow a_{i}=\\frac{1}{2}\\ln\\frac{1-P_{i}}{P_{i}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Nótese que si* $P_{i} < 0.5$*, entonces* $a_{i} > 0$*, que es lo que se espera en la práctica*. **Una vez estimados** $a_{i}$ **y** $\\boldsymbol{\\theta}_{i}$ **las ponderaciones para la siguiente iteración vienen dadas por**\n",
    "\n",
    "    $$\n",
    "    \\omega_{n}^{(i+1)}=\\frac{\\exp(-y_{n}F_{i}(\\boldsymbol{x}_{n}))}{Z_{i}}=\\frac{\\omega_{n}^{(i)}\\exp(-y_{n}a_{i}\\phi(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta}_{i}))}{Z_{i}},\n",
    "    $$\n",
    "\n",
    "    donde $Z_{i}$ es el ***factor normalizado***\n",
    "\n",
    "    $$\n",
    "    Z_{i}:=\\sum_{n=1}^{N}\\omega_{n}^{(i)}\\exp(-y_{n}a_{i}\\phi(\\boldsymbol{x}_{n}; \\boldsymbol{\\theta}_{i})).\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si se observa la forma en que se forman los pesos, se puede comprender uno de los principales secretos subyacentes al algoritmo **AdaBoost: El peso asociado a una muestra de entrenamiento** $\\boldsymbol{x}_{n}$ **aumenta (disminuye) con respecto a su valor en la iteración anterior, dependiendo de si el patrón ha fallado (tenido éxito) respecto a su valor en la iteración anterior**.\n",
    "\n",
    "- Además, el **porcentaje de disminución (aumento) depende del valor de** $a_{i}$, **que controla la importancia relativa en la construcción del clasificador final**. Las *muestras difíciles, que siguen fallando en iteraciones sucesivas, ganan importancia en su participación en el valor de error empírico ponderado*.\n",
    "\n",
    "- Para el caso del *AdaBoost*, puede demostrarse que el error de entrenamiento tiende a cero exponencialmente rápido. El esquema se resume en el siguiente algoritmo, introducido por *Yoav Freund y Robert Schapire* quienes recibieron el prestigioso *premio Gödel* por este algoritmo en 2003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:algorithm} Algoritmo AdaBoost\n",
    ":label: adaboost_algo\n",
    "\n",
    "**Inicialización**\n",
    "\n",
    "1. Inicializa: $\\omega_{n}^{(1)}=1/N,~i=1,2,\\dots,N$\n",
    "2. Inicializa: $i=1$\n",
    "\n",
    "**Repeat**\n",
    "\n",
    "1. Calcular el óptimo $\\boldsymbol{\\theta}_{i}$ en $\\phi(\\cdot; \\boldsymbol{\\theta}_{i})$ minimizando $P_{i}$\n",
    "2. Calcular el óptimo $P_{i}$\n",
    "3. $a_{i}=\\displaystyle{1/2[\\ln(1-P_{i})/P_{i}]}$\n",
    "4. $Z_{i}=0$\n",
    "5. **For** $n=1,2,\\dots,N$, **Do**:\n",
    "\t1. $\\omega_{n}^{(i+1)}=\\omega_{n}^{(i)}\\exp(-y_{n}a_{i}\\phi(\\boldsymbol{x}_{n}; \\boldsymbol{\\theta}_{i}))$\n",
    "\t2. $Z_{i}=Z_{i}+\\omega_{n}^{(i+1)}$\n",
    "6. **End For**\n",
    "7. **For** $n=1,2,\\dots,N$, **Do**:\n",
    "\t1. $\\omega_{n}^{(i+1)}=\\omega_{n}^{(i+1)}/Z_{i}$\n",
    "8. **End For**\n",
    "9. $K=i$\n",
    "10. $i=i+1$\n",
    "11. **Until** Un criterio de parada se cumpla.\n",
    "12. $f(\\cdot)=\\text{sgn}\\left(\\sum_{k=1}^{K}a_{k}\\phi(\\cdot, \\boldsymbol{\\theta}_{k})\\right)$ \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La función Log-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En `AdaBoost`, se empleó la `función de pérdida exponencial`. Desde un punto de vista teórico, esto puede\n",
    "justificarse con el siguiente argumento. Consideremos el `valor medio con respecto a la etiqueta binaria,` $y$, de\n",
    "la `función de pérdida exponencial`\n",
    "\n",
    "$$\n",
    "\\mathbb{E}(\\exp(-yF(\\boldsymbol{x})))=P(y=1)\\exp(-F(\\boldsymbol{x}))+P(y=-1)\\exp(F(\\boldsymbol{x}))\n",
    "$$(mean_exp_loss_fn_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tomando `derivada con respecto a` $F(\\boldsymbol{x})$ `e igualando a cero`, fácilmente obtenemos el mínimo de {eq}`mean_exp_loss_fn_eq` el cual ocurre en\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial F(\\boldsymbol{x})}(\\mathbb{E}(\\exp(-yF(\\boldsymbol{x}))))=0\\Leftrightarrow F_{\\star}(\\boldsymbol{x})=\\text{argmin}_{f}\\mathbb{E}[\\exp(-yf)]=\\frac{1}{2}\\ln\\frac{P(y=1|\\boldsymbol{x})}{P(y=-1|\\boldsymbol{x})}\n",
    "$$(min_exp_loss_fn_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El logaritmo de la proporción del lado derecho se conoce como la `proporción log-odds`. Por lo tanto, si se considera la función de minimización en Ecuación {eq}`costfn_adaboost_eq` como la aproximación empírica del valor medio en Ecuación {eq}`mean_exp_loss_fn_eq`, `se justifica plenamente considerar el signo de la función en` Ecuación {eq}`sign_fn_adaboost_eq` `como regla de clasificación`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Un problema importante asociado a la `función de pérdida exponencial`, como se ve fácilmente en la {numref}`loss_fn_adaboost_fig`, es que `pondera en gran medida las muestras clasificadas erróneamente`, dependiendo del valor del margen respectivo, definido como\n",
    "\n",
    "$$\n",
    "m_{x}:=|yF(\\boldsymbol{x})|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tenga en cuenta que `cuanto más lejos esté el punto de la superficie de decisión` ($F(x) = 0$)`, mayor será el valor de` $|F(x)|$. Por lo tanto, los `puntos que se encuentran en el lado equivocado de la superficie de decisión` ($yF(x) < 0$) y lejos son (exponencialmente) grandes, y `su influencia en el proceso de optimización es grande` en comparación con los demás puntos. Así pues, `en presencia de valores atípicos, la pérdida exponencial no es la más adecuada`. De hecho, en tales entornos, el `rendimiento de AdaBoost puede degradarse drásticamente`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una función de pérdida alternativa es la `log-loss o desviación binomial` (ver Fig. {numref}`loss_fn_adaboost_fig`), definida como\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y, F(\\boldsymbol{x})):=\\ln(1+\\exp(-yF(\\boldsymbol{x}))):\\quad\\text{log-loss function},\n",
    "$$\n",
    "\n",
    "- Nótese que su `incremento es casi lineal para valores negativos grandes`. Tal función conduce a una `influencia más equilibrada de la pérdida entre todos los puntos`. Observe además que la función que minimiza la media de la pérdida logarítmica, con respecto a $y$, es la misma que la dada en Ecuación {eq}`min_exp_loss_fn_eq` (`verifíquelo`). Sin embargo, si se emplea la pérdida logarítmica en lugar de la exponencial, la tarea de optimización es más compleja, y hay que recurrir a esquemas de optimización de tipo `gradiente descendiente` o esquemas de `optimización de tipo Newton` (ver {cite}`friedman2001greedy`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arboles Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:observation}\n",
    ":label: observation_dt2\n",
    "\n",
    "- En la discusión sobre la comparación experimental de varios métodos, se afirmó que `los árboles boosting se encuentran entre los esquemas de aprendizaje más potentes para la clasificación y la minería de datos`. Por lo tanto, merece la pena dedicar más tiempo a este tipo especial de técnicas de boosting.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A partir de los conocimientos adquiridos hasta ahora, no es difícil ver que la `salida de un árbol puede escribirse de forma compacta como`\n",
    "\n",
    "    $$\n",
    "    T(\\boldsymbol{x}, \\boldsymbol{\\Theta})=\\sum_{j=1}^{J}\\hat{y}_{j}\\chi_{R_{j}}(\\boldsymbol{x}),\n",
    "    $$\n",
    "\n",
    "    donde $J$ es el `número de nodos hoja`, $R_{j}$ es la `región asociada a la hoja` $j$ tras la `partición espacial` impuesta por el árbol, $\\hat{y}_{j}$ es la `etiqueta respectiva asociada a` $R_{j}$ (valor de salida/predicción para la regresión), y $\\chi$ es nuestra `conocida función característica`. El conjunto de `parámetros`, $\\boldsymbol{\\Theta}$, se compone de $(\\hat{y}_{j}, R_{j} ), j = 1, 2,\\dots, J$, que `se estiman durante el entrenamiento`. Estos se pueden obtener mediante la `selección de una función de coste adecuada`. También se suelen emplear técnicas subóptimas, para construir un árbol, como las analizadas en la primera sección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En un modelo de árbol boosting, `el clasificador base está formado por un árbol`. En la práctica, se pueden emplear `árboles cuyo tamaño no debe ser muy grande, para acercarse más a un clasificador débil`. Normalmente, `valores de` $J$ `entre tres y ocho` son aconsejables. El modelo de `árbol boosting` puede escribirse como\n",
    "\n",
    "$$\n",
    "F(\\boldsymbol{x})=\\sum_{k=1}^{K}T(\\boldsymbol{x}; \\boldsymbol{\\Theta}_{k}),\\quad\\text{donde}\\quad T(\\boldsymbol{x}; \\boldsymbol{\\Theta}_{k})=\\sum_{j=1}^{J}\\hat{y}_{kj}\\chi_{R_{kj}}(\\boldsymbol{x}).\n",
    "$$(bost_tree_model_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La Ecuación {eq}`bost_tree_model_eq` es básicamente la misma que Ecuación {eq}`adaboosting_model_eq`, con el coeficiente $a_{k}$ igual a uno. Hemos supuesto que el `tamaño de todos los árboles es el mismo`, aunque no tiene por qué ser así. Adoptando una `función de pérdida` $\\mathcal{L}$ y la `lógica greedy` utilizada para el enfoque boosting más general, llegamos al siguiente esquema recursivo de optimización:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Theta}_{i}=\\text{argmin}_{\\boldsymbol{\\Theta}}\\sum_{n=1}^{N}\\mathcal{L}(y_{n}, F_{i-1}(\\boldsymbol{x}_{n})+T(\\boldsymbol{x}_{n}; \\boldsymbol{\\Theta})).\n",
    "$$(rec_optimization_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La `optimización` con respecto a $\\boldsymbol{\\Theta}$ tiene lugar en `dos pasos: uno con respecto a` $\\hat{y}_{ij},~j=1,2,\\dots,J$, dado $R_{ij}$ , y luego `uno con respecto a las regiones` $R_{ij}$. Esta última es una tarea difícil y sólo se simplifica en casos muy especiales. En la práctica, se pueden emplear varias aproximaciones. Nótese que en el caso de la `pérdida exponencial` y la `tarea de clasificación de dos clases`, lo anterior está directamente relacionado con el esquema `AdaBoost`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para casos más generales, se recurre a esquemas de optimización numérica. `El mismo razonamiento se aplica a los árboles de regresión, donde ahora se utilizan funciones de pérdida para la regresión, como el error al cuadrado o el valor de error absoluto`. Estos esquemas también se conocen como árboles de regresión aditiva múltiple (MART). Hay dos factores críticos en relación con los árboles boosting. Uno es el tamaño de los árboles, $J$, y el otro es la elección de $K$. `En cuanto al tamaño de los árboles, normalmente se prueban diferentes tamaños,` $4\\leq J \\leq 8$`, y se selecciona el mejor`. Con respecto al `número de iteraciones`, para valores grandes, el error de entrenamiento puede llegar a ser cercano a cero, pero el error de prueba puede aumentar debido al sobreajuste. Por lo tanto, `hay que parar lo suficientemente pronto, normalmente controlando el rendimiento`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra forma de hacer frente al sobreajuste es emplear `métodos de contracción (shrinkage), que suelen ser equivalentes a la regularización`. Por ejemplo, en la expansión por etapas de $F_{i}(x)$ utilizada en el paso de optimización Eq {eq}`rec_optimization_eq`, se puede adoptar lo siguiente:\n",
    "\n",
    "$$\n",
    "F_{i}(\\cdot)=F_{i-1}(\\cdot)+\\nu T(\\cdot; \\boldsymbol{\\theta}_{i}).\n",
    "$$\n",
    "\n",
    "- El parámetro $\\nu$ `toma valores pequeños y puede considerarse que controla la tasa de aprendizaje del procedimiento boosting`. Se aconsejan valores inferiores a $\\nu < 0.1$. Sin embargo, `cuanto menor sea el valor de` $\\nu$`, mayor deberá ser el valor de` $K$ `para garantizar un buen rendimiento`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control de complejidad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Al construir un árbol como se describe en esta sección, hasta que todas las hojas sean puras, da lugar a modelos muy complejos y muy ajustados a los datos de entrenamiento. `La presencia de hojas puras significa que un árbol es 100% preciso en el conjunto de entrenamiento`; cada punto de datos del conjunto de entrenamiento está en una hoja que tiene la clase mayoritaria correcta. \n",
    "\n",
    "- Hay dos estrategias comunes para evitar el overfitting: `detener la creación del árbol antes de tiempo (también llamada prepoda), o construir el árbol, pero luego eliminar o colapsar nodos que contienen poca información (también llamada poda posterior o simplemente poda)`. Los posibles criterios para la `prepoda` incluyen la `limitación de la profundidad máxima del árbol, limitar el número máximo de hojas, o exigir un número mínimo de puntos en un nodo para seguir dividiéndolo`.\n",
    "\n",
    "- Los árboles de decisión en `scikit-learn` se implementan en las clases `DecisionTreeRegressor` y `DecisionTreeClassifier`. `scikit-learn` sólo implementa la pre-poda, no la post-poda. Veamos el efecto de la poda a priori con más detalle en el conjunto de datos de cáncer de mama. Como siempre, importamos el conjunto de datos y lo dividimos en una parte de entrenamiento y otra de prueba. A continuación, construimos un  modelo utilizando la configuración por defecto de desarrollo completo del árbol (extendiendo el árbol hasta que todas las hojas sean puras). Fijamos el `random_state` en el árbol, que se utiliza para ruptura interna de de los lazos. Nótese que seleccionamos `stratify=cancer.target`, los datos se dividen de forma estratificada, utilizando `cancer.target` como las etiquetas de clase (ver [stratified](https://scikit-learn.org/stable/modules/cross_validation.html#stratification))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, \n",
    "                                                    stratify=cancer.target, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.937\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aquí la función `score` como en la mayoría de clasificadores, devuelve la precisión media en los datos de prueba y las etiquetas dadas. Como era de esperar, `la precisión en el conjunto de entrenamiento es del 100%`, ya que las hojas son puras, el árbol creció lo suficiente como para poder memorizar perfectamente todas las etiquetas de los datos de entrenamiento. `La precisión del conjunto de prueba es ligeramente peor que la de los modelos lineales` que vimos anteriormente, que tenían una precisión de alrededor del 95%.\n",
    "\n",
    "- Si no restringimos la profundidad de un árbol de decisión, el árbol puede llegar a ser arbitrariamente profundo y complejo. `Los árboles no podados son, por tanto, son propensos a sobreajustarse y a no generalizar bien a los nuevos datos`. Ahora apliquemos la pre-poda, la cual que dejará de desarrollar el árbol antes de ajustarse perfectamente a los datos de entrenamiento. Una opción es dejar de construir el árbol después de alcanzar una cierta profundidad. En este caso,`establecemos que la profundidad máxima sea de 4 (max_depth=4), lo que significa que sólo se pueden formular cuatro preguntas consecutivas`. La limitación de la profundidad del árbol disminuye el sobreajuste. Esto conduce a una menor precisión en el conjunto de entrenamiento, pero una mejora en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.988\n",
      "Accuracy on test set: 0.951\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los árboles de decisión"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos visualizar un árbol de decisión utilizando la función `export_graphviz` del módulo `tree`. Esta función escribe un archivo en el formato `.dot` de archivo de texto para almacenar gráficos. Establecemos una opción para colorear los nodos, para reflejar la clase mayoritaria en cada nodo y pasamos los nombres de las clases y las características para que el árbol pueda ser etiquetado correctamente. El argumento `impurity` está relacionado con la probabilidad de que clasifiquemos incorrectamente un nuevo punto de datos de forma incorrecta, normalmente calculada mediante la métrica de entropia `giny` la cual se aborda teóricamente en la sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(tree,\n",
    "                out_file=\"tree.dot\", \n",
    "                class_names=[\"malignant\", \"benign\"],\n",
    "                feature_names=cancer.feature_names, \n",
    "                impurity=False, \n",
    "                filled=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos leer este archivo y visualizarlo, utilizando el modulo `graphviz` (o puede utilizar cualquier programa que pueda leer archivos `.dot`). Para que funcione `Graphviz` deberá además realizar la siguiente instalación, la cual es la única dependencia (ver [Graphviz Instalación](https://forum.graphviz.org/t/new-simplified-installation-procedure-on-windows/224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    graph = graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\backend\\execute.py:79\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPIPE\n\u001b[1;32m---> 79\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43m_run_input_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\backend\\execute.py:99\u001b[0m, in \u001b[0;36m_run_input_lines\u001b[1;34m(cmd, input_lines, kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_input_lines\u001b[39m(cmd, input_lines, \u001b[38;5;241m*\u001b[39m, kwargs):\n\u001b[1;32m---> 99\u001b[0m     popen \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd, stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m     stdin_write \u001b[38;5;241m=\u001b[39m popen\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\subprocess.py:1436\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1436\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1438\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] El sistema no puede encontrar el archivo especificado",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\IPython\\core\\formatters.py:974\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    971\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\jupyter_integration.py:98\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_mimebundle_\u001b[1;34m(self, include, exclude, **_)\u001b[0m\n\u001b[0;32m     96\u001b[0m include \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(include) \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter_mimetype}\n\u001b[0;32m     97\u001b[0m include \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {mimetype: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_name)()\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mimetype, method_name \u001b[38;5;129;01min\u001b[39;00m MIME_TYPES\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01min\u001b[39;00m include}\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\jupyter_integration.py:98\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     96\u001b[0m include \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(include) \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter_mimetype}\n\u001b[0;32m     97\u001b[0m include \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {mimetype: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mimetype, method_name \u001b[38;5;129;01min\u001b[39;00m MIME_TYPES\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01min\u001b[39;00m include}\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\jupyter_integration.py:112\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_image_svg_xml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_image_svg_xml\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the rendered graph as SVG string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msvg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSVG_ENCODING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\piping.py:104\u001b[0m, in \u001b[0;36mPipe.pipe\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m          \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m          renderer: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m          engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m          encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the source piped through the Graphviz layout command.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m        '<?xml version='\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_legacy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\piping.py:121\u001b[0m, in \u001b[0;36mPipe._pipe_legacy\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@_tools\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecate_positional_args(supported_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe_legacy\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    114\u001b[0m                  \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m                  engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    120\u001b[0m                  encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_future\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\piping.py:149\u001b[0m, in \u001b[0;36mPipe._pipe_future\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mlookup(encoding) \u001b[38;5;129;01mis\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding):\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;66;03m# common case: both stdin and stdout need the same encoding\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_lines_string(\u001b[38;5;241m*\u001b[39margs, encoding\u001b[38;5;241m=\u001b[39mencoding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m         raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_lines(\u001b[38;5;241m*\u001b[39margs, input_encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\backend\\piping.py:212\u001b[0m, in \u001b[0;36mpipe_lines_string\u001b[1;34m(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet)\u001b[0m\n\u001b[0;32m    206\u001b[0m cmd \u001b[38;5;241m=\u001b[39m dot_command\u001b[38;5;241m.\u001b[39mcommand(engine, \u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    207\u001b[0m                           renderer\u001b[38;5;241m=\u001b[39mrenderer,\n\u001b[0;32m    208\u001b[0m                           formatter\u001b[38;5;241m=\u001b[39mformatter,\n\u001b[0;32m    209\u001b[0m                           neato_no_op\u001b[38;5;241m=\u001b[39mneato_no_op)\n\u001b[0;32m    210\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_lines\u001b[39m\u001b[38;5;124m'\u001b[39m: input_lines, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding}\n\u001b[1;32m--> 212\u001b[0m proc \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mrun_check(cmd, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, quiet\u001b[38;5;241m=\u001b[39mquiet, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\backend\\execute.py:84\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graphviz.sources.Source at 0x1d01f8767f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La visualización del árbol proporciona una `gran visión en profundidad de cómo el algoritmo realiza predicciones, y es un buen ejemplo de un algoritmo de aprendizaje automático que puede fácilmente explicarse a no expertos`. Sin embargo, incluso con un árbol de profundidad cuatro, como se ve aquí, el árbol puede resultar un poco abrumador. Los árboles más profundos (una profundidad de 10 puede ser común) son aún más difíciles de entender. \n",
    "\n",
    "- Un método de inspección del árbol que puede ser útil es, averiguar qué camino toma realmente la mayoría de los datos. `Las n_samples que se muestran en cada nodo de la figura, entregan el número de muestras en ese nodo`, mientras que `value provee el número de muestras por clase`. Siguiendo las ramas hacia la derecha, vemos que el `worst radius` $\\leq$ `16.795` crea un nodo que contiene sólo `8 muestras benignas pero 134 muestras malignas. El resto de este lado del árbol utiliza entonces algunas distinciones más finas para separar estas 8 muestras benignas restantes`. De las 142 muestras que fueron a la derecha en la división inicial, casi todas ellas (132) terminan en la hoja de la derecha. Tomando la izquierda en la raíz, para el `worst radio > 16.795` terminamos con `25 muestras malignas y 259 muestras benignas`. Casi todas las muestras benignas acaban en la segunda hoja de la derecha, y la mayoría de las demás hojas contienen muy pocas muestras."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Características importantes en los árboles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En lugar de mirar todo el árbol, lo que puede ser agotador, hay algunas `propiedades útiles que podemos derivar para resumir el funcionamiento del árbol`. El resumen más utilizado es el de las `características importantes, que califica la importancia de cada característica para la decisión que toma el árbol`. Es un número entre 0 y 1 para cada característica, donde 0 significa \"no se utiliza en absoluto\" y 1 significa \"predice perfectamente el objetivo\". Las características siempre suman 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Otra `excelente manera de visualizar predicciones a partir de un random forest por ejemplo, es utilizando la librería` [LIME](https://www.kaggle.com/code/prashant111/explain-your-model-predictions-with-lime/notebook) `de Python`. Con esta librería se pueden generar por cada instancias, figuras de cartaterísticas importantes y representar sus probabilidades de pertenecer a la clase predicha (`pruebela`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances:\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.01019737 0.04839825\n",
      " 0.         0.         0.0024156  0.         0.         0.\n",
      " 0.         0.         0.72682851 0.0458159  0.         0.\n",
      " 0.0141577  0.         0.018188   0.1221132  0.01188548 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature importances:\\n{}\".format(tree.feature_importances_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos visualizar las importancias de las características de forma similar a la forma en que visualizamos los coeficientes en el modelo lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances_cancer(model):\n",
    "    n_features = cancer.data.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), cancer.feature_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature_importances_cancer(tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aquí vemos que la característica utilizada en la división superior `(\"worst radio\")` es, con mucho, la más importante. Esto confirma nuestra observación al analizar el árbol de que el primer nivel ya separa bastante bien las dos clases. Sin embargo, si una característica tiene un `feature_importance` bajo, no significa que esta característica sea poco informativa. Sólo significa que la característica no fue elegida por el árbol, probablemente porque otra característica codifica la misma información.\n",
    "\n",
    "- A diferencia de los coeficientes de los modelos lineales, `las importancias de las características son siempre positivas y no codifican la clase de la que es indicativa una característica`. Las `feature_importance` nos dicen que el `\"worst radio\"` es importante, pero no si un radio alto es indicativo de que una muestra es benigna o maligna. De hecho, puede que no haya una relación tan sencilla entre las características y la clase, como se puede ver en el siguiente ejemplo. La siguiente figura muestra un conjunto de datos bidimensional en el que `la característica en el eje` $y$ `tiene una relación no monótona con la etiqueta de clase, y los límites de decisión encontrados por un árbol de decisión`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances: [0. 1.]\n"
     ]
    },
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\backend\\execute.py:79\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPIPE\n\u001b[1;32m---> 79\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43m_run_input_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\backend\\execute.py:99\u001b[0m, in \u001b[0;36m_run_input_lines\u001b[1;34m(cmd, input_lines, kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_input_lines\u001b[39m(cmd, input_lines, \u001b[38;5;241m*\u001b[39m, kwargs):\n\u001b[1;32m---> 99\u001b[0m     popen \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(cmd, stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    101\u001b[0m     stdin_write \u001b[38;5;241m=\u001b[39m popen\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\subprocess.py:1436\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1436\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1438\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] El sistema no puede encontrar el archivo especificado",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\IPython\\core\\formatters.py:974\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    971\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\jupyter_integration.py:98\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_mimebundle_\u001b[1;34m(self, include, exclude, **_)\u001b[0m\n\u001b[0;32m     96\u001b[0m include \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(include) \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter_mimetype}\n\u001b[0;32m     97\u001b[0m include \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {mimetype: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_name)()\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mimetype, method_name \u001b[38;5;129;01min\u001b[39;00m MIME_TYPES\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01min\u001b[39;00m include}\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\jupyter_integration.py:98\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     96\u001b[0m include \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(include) \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter_mimetype}\n\u001b[0;32m     97\u001b[0m include \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {mimetype: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mimetype, method_name \u001b[38;5;129;01min\u001b[39;00m MIME_TYPES\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01min\u001b[39;00m include}\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\jupyter_integration.py:112\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_image_svg_xml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_image_svg_xml\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the rendered graph as SVG string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msvg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSVG_ENCODING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\piping.py:104\u001b[0m, in \u001b[0;36mPipe.pipe\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     56\u001b[0m          \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m          renderer: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m          engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m          encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the source piped through the Graphviz layout command.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m        '<?xml version='\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_legacy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\piping.py:121\u001b[0m, in \u001b[0;36mPipe._pipe_legacy\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@_tools\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecate_positional_args(supported_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe_legacy\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    114\u001b[0m                  \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m                  engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    120\u001b[0m                  encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_future\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\piping.py:149\u001b[0m, in \u001b[0;36mPipe._pipe_future\u001b[1;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mlookup(encoding) \u001b[38;5;129;01mis\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding):\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;66;03m# common case: both stdin and stdout need the same encoding\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_lines_string(\u001b[38;5;241m*\u001b[39margs, encoding\u001b[38;5;241m=\u001b[39mencoding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m         raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_lines(\u001b[38;5;241m*\u001b[39margs, input_encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\backend\\piping.py:212\u001b[0m, in \u001b[0;36mpipe_lines_string\u001b[1;34m(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet)\u001b[0m\n\u001b[0;32m    206\u001b[0m cmd \u001b[38;5;241m=\u001b[39m dot_command\u001b[38;5;241m.\u001b[39mcommand(engine, \u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    207\u001b[0m                           renderer\u001b[38;5;241m=\u001b[39mrenderer,\n\u001b[0;32m    208\u001b[0m                           formatter\u001b[38;5;241m=\u001b[39mformatter,\n\u001b[0;32m    209\u001b[0m                           neato_no_op\u001b[38;5;241m=\u001b[39mneato_no_op)\n\u001b[0;32m    210\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_lines\u001b[39m\u001b[38;5;124m'\u001b[39m: input_lines, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding}\n\u001b[1;32m--> 212\u001b[0m proc \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mrun_check(cmd, capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, quiet\u001b[38;5;241m=\u001b[39mquiet, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\graphviz\\backend\\execute.py:84\u001b[0m, in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[1;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graphviz.sources.Source at 0x1d024f785e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree = mglearn.plots.plot_tree_not_monotone()\n",
    "display(tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El gráfico muestra un conjunto de datos con dos características y dos clases. Aquí, toda la información está contenida en `X[1]` , y `X[0]` no se utiliza en absoluto. Pero la relación entre X[1] y la clase de salida no es monótona, lo que significa que no podemos decir `\"un valor alto de X[0] significa la clase 0, y un valor bajo significa la clase 1\" (o viceversa)`. Aunque hemos centrado nuestra discusión aquí en los árboles de decisión para la clasificación, todo lo que se ha dicho es igualmente cierto para los árboles de decisión para la regresión, como se implementa en `DecisionTreeRegressor`. \n",
    "\n",
    "- El uso y análisis de los árboles de regresión es muy similar al de los árboles de clasificación. Hay una propiedad particular del uso de modelos basados en árboles para regresión que queremos señalar, sin embargo, `DecisionTreeRegressor (y todos los otros modelos de regresión basados en árboles) no son capaces de extrapolar, o hacer predicciones fuera del rango de los datos de entrenamiento`. Veamos esto con más detalle, utilizando un conjunto de datos de los precios históricos de la memoria de los ordenadores (RAM). La siguiente figura muestra el conjunto de datos, con la fecha en el eje $x$ y el precio de un megabyte de RAM en ese año en el eje $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/datasets/ram_price.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ram_prices \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:/datasets/ram_price.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogy(ram_prices\u001b[38;5;241m.\u001b[39mdate, ram_prices\u001b[38;5;241m.\u001b[39mprice)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/datasets/ram_price.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ram_prices = pd.read_csv(\"D:/datasets/ram_price.csv\")\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nótese que en la escala logarítmica del eje $y$, `la relación parece ser bastante lineal y, por tanto, debería ser relativamente fácil de predecir`. Vamos a hacer una predicción para los años posteriores al 2000 utilizando los datos históricos hasta esa fecha como única característica. Compararemos dos modelos sencillos: un `DecisionTreeRegressor` y `LinearRegression`. \n",
    "\n",
    "- Cambiamos la escala de los precios utilizando un logaritmo, para que la relación sea relativamente lineal. Esto no supone ninguna diferencia para el `DecisionTreeRegressor`, pero supone una gran diferencia para el `LinearRegression`. Después de entrenar los modelos y hacer predicciones, aplicamos la función exponencial para deshacer la transformación del logaritmo. Realizamos predicciones sobre todo el conjunto de datos para su visualización, pero para una evaluación cuantitativa, sólo consideraríamos el conjunto de datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilizamos los datos históricos para prever los precios después del año 2000. `Realizamos predicción de los precios en función de la fecha`. Utilizamos una transformación logarítmica para obtener una relación más sencilla de los datos con el objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test  = ram_prices[ram_prices.date >= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(data_train.date)[:, None] # Vector columna\n",
    "y_train = np.log(data_train.price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "X_all = np.array(ram_prices.date)[:, None]\n",
    "pred_tree = tree.predict(X_all)\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "\n",
    "price_tree = np.exp(pred_tree)\n",
    "price_lr = np.exp(pred_lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Realizamos una figura para comparar las predicciones del árbol de decisión y del modelo de regresión lineal con la los datos reales de entrenamiento y de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
    "plt.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
    "plt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
    "plt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La diferencia entre los modelos es bastante sorprendente. El modelo lineal se aproxima a los datos con una línea, como sabíamos que haría. Esta línea proporciona una previsión bastante buena para los datos de prueba (los años posteriores al 2000), mientras que pasa por alto algunas de las variaciones más finas en los datos de entrenamiento y de prueba. `El modelo de árbol, por su parte, hace predicciones perfectas sobre los datos de entrenamiento`, no restringimos la complejidad del árbol, por lo que aprendió de memoria todo el conjunto de datos. Sin embargo, `una vez que salimos del rango para el que el arbol tiene datos, el modelo simplemente sigue prediciendo el último punto conocido`. El árbol no tiene la capacidad para generar `\"nuevas\"` respuestas, fuera de lo que se vio en los datos de de entrenamiento. Esta deficiencia se aplica a todos los modelos basados en árboles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Puntos fuertes, puntos débiles y parámetros`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como ya se ha comentado, los parámetros que controlan la complejidad del modelo en los árboles de decisión son los parámetros de pre-selección que detienen la construcción del árbol antes de que esté completamente desarrollado. Por lo general, la elección de una de las estrategias de pre-poda y configuración de: `max_depth, max_leaf_nodes, min_samples_leaf` es suficiente para evitar el overfitting de la misma.\n",
    "\n",
    "- Los árboles de decisión tienen dos ventajas sobre muchos de los algoritmos que hemos analizado hasta ahora: `el modelo resultante puede ser fácilmente visualizado y entendido por personas no expertas (al menos para los árboles más pequeños)`, y los `algoritmos son completamente invariables a la escala de los datos`. Como `cada característica se procesa por separado`, y las posibles divisiones de los datos no dependen de la escala, no es necesario un preprocesamiento como la normalización o la estandarización de las características para los algoritmos de árboles de decisión. \n",
    "\n",
    "- En particular, los árboles de decisión funcionan bien cuando se tienen características que están en escalas completamente diferentes, o una mezcla de características binarias y continuas. `El principal inconveniente de los árboles de decisión es que, incluso con el uso de la pre-poda, tienden a sobreajustarse y a proporcionar un pobre rendimiento de generalización`. Por lo tanto, en la mayoría de las aplicaciones, los `métodos combiandos` que analizamos a continuación suelen utilizarse en lugar de un único árbol de decisión."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensamble de árboles de decisión"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Los ensambles son métodos que combinan múltiples modelos de aprendizaje automático para crear modelos más potentes`. Hay muchos modelos en la literatura de aprendizaje automático que pertenecen a esta categoría, pero hay dos modelos de ensamble que han demostrado su eficacia en una amplia gama de conjuntos de datos de clasificación y regresión, ambos utilizan árboles de decisión como bloques de construcción: los `random forest` y los `gradient boosted decision trees.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Bosques aleatorios`**\n",
    "\n",
    "- Como acabamos de observar, uno de los principales inconvenientes de `los árboles de decisión es que tienden a sobreajustar los datos de entrenamiento. Los bosques aleatorios son una forma de abordar este problema`. Un bosque aleatorio es esencialmente una colección de árboles de decisión, donde cada árbol es ligeramente diferente de de los demás. La idea detrás de los bosques aleatorios es que cada árbol puede hacer un trabajo de predicción relativamente bien, pero es probable que se ajuste demasiado a una parte de los datos. `Si construimos muchos árboles, todos los cuales funcionan bien y se ajustan en exceso de diferentes maneras, podemos reducir la cantidad de sobreajuste promediando sus resultados`. Esta reducción de overfitting, al tiempo que se mantiene el poder de predicción de los árboles se puede verificar matemáticamente.\n",
    "\n",
    "- Para poner en práctica esta estrategia, tenemos que construir muchos árboles de decisión. Cada árbol debe hacer un trabajo aceptable de predicción del objetivo, y también debe ser diferente de los otros árboles. Los bosques aleatorios reciben su nombre de la inyección de aleatoriedad en la construcción de árboles para garantizar que cada árbol sea diferente. Hay dos formas de aleatorizar los árboles de un bosque aleatorio: `seleccionando los puntos de datos utilizados para construir un árbol y seleccionando las características en cada prueba de división`. Veamos este proceso con más detalle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Construcción de bosques aleatorios`**\n",
    "\n",
    "- Para construir un modelo de bosque aleatorio (`random forest`), hay que decidir el número de árboles a construir (el parámetro `n_estimators` de `RandomForestRegressor` o `RandomForestClassifier`). Digamos que queremos construir 10 árboles. Estos árboles se construirán de forma completamente independiente unos de otros, y `el algoritmo hará elecciones aleatorias diferentes para cada árbol con el fin de asegurarse de que los árboles son distintos`. \n",
    "\n",
    "- Para construir un árbol, primero tomamos lo que se llama una `muestra bootstrap` de nuestros datos. Es decir, a partir de nuestras `n_samples` (puntos de datos), extraemos repetidamente un ejemplo al azar con reemplazo (lo que significa que la misma muestra puede ser elegida varias veces), `n_samples` veces. Esto creará un conjunto de datos que es tan grande como el conjunto de datos original, pero en el que faltarán algunos puntos de dato (aproximadamente un tercio), y algunos se repetirán.\n",
    "\n",
    "- Para ilustrarlo, digamos que queremos crear una `muestra bootstrap` de la lista `['a', 'b', 'c', 'd']`. Una posible muestra bootstrap sería `['b', 'd', 'd', 'c']`. Otra muestra posible sería `['d', 'a', 'd', 'a']`. A continuación, `se construye un árbol de decisión basado en este conjunto de datos (muestra bootstrap) recién creado`. Sin embargo, el algoritmo que hemos descrito para el árbol de decisión se ha modificado ligeramente. En lugar de buscar la mejor prueba para cada nodo, el algoritmo `selecciona aleatoriamente un subconjunto de características y busca la mejor prueba posible que incluya una de estas características`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El número de características que se seleccionan se controla con el parámetro `max_features`. La selección de un subconjunto de características se repite por separado en cada nodo, de modo que `cada nodo de un árbol puede tomar una decisión utilizando un subconjunto diferente de las características`. `El muestreo bootstrap hace que cada árbol de decisión del bosque aleatorio se construya en un conjunto de datos ligeramente diferente`. Debido a la selección de características en cada nodo, cada división de cada árbol opera con un subconjunto diferente de características. Juntos, estos dos mecanismos aseguran que todos los árboles del bosque aleatorio son diferentes.\n",
    "\n",
    "- En este proceso el parámetro `max_features` es crítico. Si establecemos `max_features` en `n_features`, `significa que cada división puede mirar todas las características del conjunto de datos, y no se inyectará aleatoriedad ninguna en la selección de características` (la aleatoriedad debida al bootstrap permanece, sin embargo). Si establecemos `max_features en 1, esto significa que las divisiones no tienen ninguna opción sobre qué característica probar`, y sólo pueden buscar sobre diferentes umbrales para la característica que fue seleccionada  al azar. Por lo tanto, un `max_feature` significa que los árboles del bosque aleatorio serán bastante similares y podrán con facilidad ajustarse a los datos, utilizando las características más distintivas. `Un max_features bajo significa que los árboles del bosque aleatorio serán bastante diferentes, y que cada árbol puede necesitar ser muy profundo para ajustarse bien a los datos`.\n",
    "\n",
    "- Para hacer una predicción utilizando el bosque aleatorio, el algoritmo realiza primero una predicción para cada árbol del bosque. `Para la regresión, podemos promediar estos resultados para obtener nuestra predicción final`. Para la clasificación, se utiliza una estrategia de `\"votación suave\"`. Esto significa que cada algoritmo hace una predicción `\"suave\"`, proporcionando una probabilidad para cada posible etiqueta de salida. `Las probabilidades predichas por todos los árboles se promedian y se predice la clase con la mayor probabilidad`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Análisis de los bosques aleatorios`**. Apliquemos un bosque aleatorio compuesto por cinco árboles al conjunto de datos `two_moons`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=2)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Los árboles que se construyen como parte del bosque aleatorio se almacenan en estimator_attribute`. Visualicemos los límites de decisión aprendidos por cada árbol, junto con su predicción agregada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
    "    ax.set_title(\"Tree {}\".format(i))\n",
    "    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1], alpha=.4)\n",
    "axes[-1, -1].set_title(\"Random Forest\")\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se puede ver claramente que los límites de decisión aprendidos por los cinco árboles son bastante diferentes. Cada uno de ellos comete algunos errores, ya que algunos de los puntos que se representan aquí no se incluyeron en los conjuntos de entrenamiento de los árboles, debido al muestreo bootstrap. `El bosque aleatorio se ajusta menos que cualquiera de los árboles por separado y proporciona un límite de decisión mucho más intuitivo`. En cualquier aplicación real, utilizaríamos muchos más árboles (a menudo cientos o miles), lo que daría lugar a límites aún más suaves."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como otro ejemplo, apliquemos un bosque aleatorio compuesto por 100 árboles en el conjunto de datos `Breast Cancer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `El bosque aleatorio nos da una precisión del 97%, mejor que los modelos lineales o un árbol de decisión único, sin necesidad de ajustar ningún parámetro`. Podríamos ajustar la configuración de `max_features`, o aplicar la pre-selección como hicimos con el árbol de decisión simple. Sin embargo, a menudo los parámetros por defecto del bosque aleatorio ya funcionan bastante bien. `Al igual que el árbol de decisión, el bosque aleatorio proporciona importancias de características, que se calculan agregando las importancias de las características en los árboles del bosque`. Normalmente, las `importancias de las características proporcionadas por el bosque aleatorio son más fiables que las proporcionadas por un solo árbol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_cancer(forest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como puede ver, `el bosque aleatorio da una importancia no nula a muchas más características que el árbol simple`. Al igual que el árbol de decisión simple, el bosque aleatorio también da importancia a la característica `\"worst radius\"`, pero en realidad elige `\"worst perimeter\"` como la `característica más informativa`. `La aleatoriedad en la construcción del bosque aleatorio obliga al algoritmo a considerar muchas explicaciones posibles`. El resultado es que el bosque aleatorio capta una imagen mucho más amplia de los datos que un árbol simple."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "La `importancia de la característica` se calcula como la disminución de la impureza del nodo ponderada por la probabilidad de alcanzar ese nodo. `La probabilidad del nodo puede calcularse mediante el número de muestras que llegan al nodo, dividido por el número total de muestras. Cuanto mayor sea el valor, más importante será la característica`. Si tenemos $C$ clases totales y $p(i)$ es la probabilidad de escoger un punto de datos con la clase $i$, entonces la `impureza de Gini` se calcula como\n",
    "\n",
    "$$\n",
    "G=\\sum_{i=1}^{C}p(i)(1-p(i)).\n",
    "$$\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Puntos fuertes, puntos débiles y parámetros`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los bosques aleatorios para la regresión y la clasificación se encuentran actualmente entre los métodos de aprendizaje automático más utilizados. `Son muy potentes, suelen funcionar bien sin necesidad de ajustar mucho los parámetros y no requieren el escalado de los datos`. Esencialmente, los bosques aleatorios comparten todos los beneficios de los árboles de decisión, mientras que compensan algunas de sus deficiencias. Una razón para seguir utilizando los árboles de decisión es, **`si se necesita una representación compacta del proceso de toma de decisiones`**. Es básicamente imposible interpretar decenas o cientos de árboles en detalle, y los árboles de los bosques aleatorios tienden a ser más profundos que los árboles de decisión (debido al uso de subconjuntos de características). Por lo tanto, si se necesita resumir las predicciones de forma visual para los no expertos, un único árbol de decisión puede ser la mejor opción. \n",
    "\n",
    "- Aunque la construcción de bosques aleatorios en grandes conjuntos de datos puede llevar algo de tiempo, `se puede paralelizar a través de múltiples núcleos de CPU dentro de un ordenador fácilmente`. Si utiliza un procesador multinúcleo (como casi todos los ordenadores modernos),  puede utilizar el parámetro `n_jobs` para ajustar el número de núcleos a utilizar. El uso de más núcleos de la CPU dará lugar a un aumento lineal de la velocidad (utilizando dos núcleos, el entrenamiento del bosque aleatorio será el doble de rápido), pero especificar `n_jobs` mayor que el número de núcleos no ayudará. `Puede establecer n_jobs=-1 para utilizar todos los núcleos en su ordenador`.\n",
    "\n",
    "- Debe tener en cuenta que los bosques aleatorios, por su naturaleza, son aleatorios, y establecen diferentes estados aleatorios (o no establecen el `random_state` en absoluto) puede cambiar drásticamente el modelo que se construye. `Cuanto más árboles haya en el bosque, más robusto será frente a la elección del estado aleatorio`. Si quiere tener resultados reproducibles, es importante fijar el `random_state` a caulquier número entero. `Los bosques aleatorios no tienden a funcionar bien en datos muy dimensionales y escasos, como los datos de texto`. Para este tipo de datos, los modelos lineales pueden ser más apropiados. Los bosques aleatorios suelen funcionar bien incluso en conjuntos de datos muy grandes, y el entrenamiento se puede paralelizar fácilmente en muchos núcleos de la CPU de un ordenador potente.  Sin embargo, `los bosques aleatorios requieren más memoria y son más lentos de entrenar y predecir que los modelos lineales`. Si el tiempo y la memoria son importantes en una aplicación, puede tener sentido utilizar un modelo lineal en su lugar.\n",
    "\n",
    "- Los parámetros importantes a ajustar son `n_estimators , max_features`, y posiblemente opciones de pre-poda como `max_depth`. En el caso de `n_estimators`, un número mayor es siempre mejor. `Promediar más árboles dará lugar a un conjunto más robusto al reducir el sobreajuste`. Sin embargo, hay rendimientos decrecientes, y más árboles necesitan más memoria y más tiempo para entrenar. Una regla común es construir \"tantos como tenga tiempo/memoria\".  Como se ha descrito anteriormente, `max_features` determina el grado de aleatoriedad de cada árbol, y un `max_features pequeño reduce el sobreajuste`. En general, es una buena regla general utilizar los valores por defecto: `max_features=sqrt(n_features)` para la clasificación y `max_features=log2(n_features)` para la regresión. Añadir `max_features` o `max_leaf_nodes` puede mejorar a veces el rendimiento. También puede reducir drásticamente los requisitos de espacio y tiempo para el entrenamiento y la predicción."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Árboles de regresión de gradiente reforzado (máquinas de gradiente reforzado)`**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El árbol de regresión de gradiente reforzado es otro método de conjunto que combina múltiples árboles de decisión. A pesar de la `\"regresión\"` en el nombre, estos modelos pueden utilizarse para la regresión y la clasificación. A diferencia del enfoque de bosque aleatorio, `el refuerzo de gradiente funciona construyendo árboles en forma de serie, en el que cada árbol trata de corregir los errores del anterior`. Por defecto, no hay aleatoriedad en los árboles de regresión de gradiente; en su lugar, `se utiliza una fuerte pre-poda`. Los árboles de impulso por gradiente suelen utilizar árboles muy poco profundos, de una a cinco profundidades, lo que hace que el modelo sea más pequeño en términos de memoria y que las predicciones sean más rápidas.\n",
    "\n",
    "- La idea principal del `gradient boosting` es combinar muchos modelos simples (en este contexto conocidos como aprendices débiles), como árboles poco profundos. Cada árbol sólo puede proporcionar buenas predicciones sobre una parte de los datos, por lo que se añaden más y más árboles para mejorar el rendimiento. `Los árboles con refuerzo de gradiente suelen ser los ganadores de los concursos de aprendizaje automático y se utilizan mucho en la industria`. Suelen ser un poco más sensibles a los ajustes de los parámetros que los bosques aleatorios, pero pueden proporcionar una mayor precisión si los parámetros se ajustan correctamente.\n",
    "\n",
    "- Aparte de la pre-poda y el número de árboles en el conjunto, otro parámetro importante del gradient boosting es la `tasa de aprendizaje, que controla la intensidad con la que cada árbol trata de corregir los errores de los árboles anteriores`. Una tasa de aprendizaje alta significa que cada árbol puede hacer correcciones más fuertes, lo que permite modelos más complejos. Si se añaden más árboles al conjunto, lo que puede conseguirse aumentando `n_estimators`, también aumenta la complejidad del modelo, ya que éste tiene más oportunidades de corregir errores en el conjunto de entrenamiento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Este es un ejemplo del uso del clasificador `GradientBoosting` en el conjunto de datos del `Breast Cancer`. Por defecto, se utilizan 100 árboles de profundidad máxima 3 y una tasa de aprendizaje de 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como la `precisión del conjunto de entrenamiento es del 100%, es probable que estemos sobreajustando. Para reducir el sobreajuste, podemos aplicar una pre-poda más fuerte limitando la profundidad máxima o reducir la tasa de aprendizaje`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ambos métodos para disminuir la complejidad del modelo `redujeron la precisión del conjunto de entrenamiento, como era de esperar`. En este caso, `la reducción de la profundidad máxima de los árboles proporcionó una mejora significativa del modelo, mientras que la reducción de la tasa de aprendizaje sólo aumentó la precisión del conjunto de entrenamiento ligeramente`. En cuanto a los otros modelos basados en árboles de decisión, podemos volver a visualizar las características para obtener más información sobre nuestro modelo. Como utilizamos 100 árboles, es poco práctico inspeccionarlos todos, aunque todos tengan una profundidad de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "plot_feature_importances_cancer(gbrt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos ver que las importancias de las características de los árboles `gradient-boost` son algo similares a las de los bosques aleatorios, aunque el refuerzo del gradiente ignora por completo algunas de las características. Como tanto el refuerzo de gradiente como los bosques aleatorios funcionan bien en tipos de datos similares, `un enfoque común es probar primero los bosques aleatorios, que funcionan con bastante solidez. Si los bosques aleatorios funcionan bien, pero el tiempo de predicción es un problema, o si es importante exprimir el último porcentaje de precisión del modelo de aprendizaje automático, pasar a la refuerzo por gradiente suele ser útil`.\n",
    "\n",
    "- **`Si quiere aplicar el refuerzo de gradiente a un problema a gran escala, puede que merezca la pena investigar el paquete xgboost y su interfaz de Python, que hasta el momento es más rápido (y a veces más fácil de usar) que la implementación de scikit-learn en muchos conjuntos de datos`**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Puntos fuertes, puntos débiles y parámetros`** \n",
    "\n",
    "- Los árboles de decisión con refuerzo de gradiente se encuentran entre los modelos más potentes y utilizados para el aprendizaje supervisado. `Su principal inconveniente es que requieren un ajuste cuidadoso de los parámetros y pueden tardar mucho tiempo de entrenamiento`. Al igual que otros modelos basados en árboles, el algoritmo funciona bien sin escalar y con una mezcla de características binarias y continuas. Al igual que otros modelos basados en árboles tampoco suele funcionar bien con datos dispersos de alta dimensión.\n",
    "\n",
    "- Los principales parámetros de los modelos de árbol de gradiente reforzado son el número de árboles, `n_estimators`, y el `learning_rate` que `controla el grado en que cada árbol puede corregir los errores de los árboles anteriores`. Estos dos parámetros están muy interconectados, ya que una tasa de aprendizaje más baja significa que se necesitan más árboles para construir un modelo de complejidad similar. A diferencia de los bosques aleatorios, en los que un valor de `n_estimators` es siempre mejor, el aumento de `n_estimators` en el `gradient boosting` conduce a un modelo más complejo, lo que puede llevar a un sobreajuste. Una práctica común es ajustar `n_estimators` dependiendo del presupuesto de tiempo y memoria, y luego buscar sobre diferentes tasas de aprendizaje.\n",
    "\n",
    "- Otro parámetro importante es `max_depth` (o alternativamente `max_leaf_nodes`), para reducir la complejidad de cada árbol. `Por lo general, la profundidad máxima es muy baja para los modelos de gradiente, a menudo no más allá de cinco divisiones`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_tf",
   "language": "python",
   "name": "ml_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
