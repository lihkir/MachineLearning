
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>8. Máquinas de vectores de soporte &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=530fe47d" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/custom-checkpoint.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'svm_model';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/custom.js?v=14184634"></script>
    <script src="_static/.ipynb_checkpoints/custom-checkpoint.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="10. Redes Neuronales y Deep Learning" href="ann_model.html" />
    <link rel="prev" title="5. Random Forest y XGBoost" href="decisiontree_model.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fotolihki.jpg" class="logo__image only-light" alt="Machine Learning - Home"/>
    <script>document.write(`<img src="_static/fotolihki.jpg" class="logo__image only-dark" alt="Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Profesor: Dr. Lihki Rubio
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="supervised_intro.html">1. Aprendizaje supervisado</a></li>
<li class="toctree-l1"><a class="reference internal" href="knn_model.html">2. <span class="math notranslate nohighlight">\(k\)</span>-vecinos más cercanos</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_model.html">3. Regresión Ridge y Lasso</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes_model.html">4. Clasificador Bayesiano</a></li>
<li class="toctree-l1"><a class="reference internal" href="decisiontree_model.html">5. Random Forest y XGBoost</a></li>


<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. Máquinas de vectores de soporte</a></li>

<li class="toctree-l1"><a class="reference internal" href="ann_model.html">10. Redes Neuronales y Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="practical_pca.html">11. Análisis de Componentes Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">12. Evaluación de modelos</a></li>
<li class="toctree-l1"><a class="reference internal" href="chains_pipelines.html">13. Cadenas de Algoritmos y Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">14. Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercises_sols.html">15. Soluciones a ejercicios</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">16. Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/svm_model.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Máquinas de vectores de soporte</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">8. Máquinas de vectores de soporte</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">8.1. Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#espacios-de-hilbert-con-kernel-reproductor">8.2. Espacios de Hilbert con Kernel reproductor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#construccion-de-kernels">8.3. Construcción de kernels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teorema-de-representacion">8.4. Teorema de representación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge-con-kernel">8.5. Regresión ridge con Kernel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-de-vectores-de-soporte">8.6. Regresión de vectores de soporte</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-optima-lineal-varepsilon-insensible">8.7. Regresión óptima lineal <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-kernel-ridge">8.8. Regresión Kernel Ridge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">8.9. Máquinas de vectores de soporte</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clases-linealmente-separables-clasificador-de-maximo-margen">8.10. Clases linealmente separables: Clasificador de máximo margen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clases-no-separables">8.11. Clases no separables</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ejercicios-para-el-estudiante">9. Ejercicios para el estudiante</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion">9.1. Aplicación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-lineales-y-caracteristicas-no-lineales">9.2. Modelos lineales y características no lineales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-kernel-trick">9.3. El Kernel Trick</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ajuste-de-los-parametros-de-svm">9.4. Ajuste de los parámetros de SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocesamiento-de-datos-para-svm">9.5. Preprocesamiento de datos para SVM</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="maquinas-de-vectores-de-soporte">
<h1><span class="section-number">8. </span>Máquinas de vectores de soporte<a class="headerlink" href="#maquinas-de-vectores-de-soporte" title="Link to this heading">#</a></h1>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Las <em><strong>máquinas de vectores de soporte (SVM)</strong></em> propuestas por <em><strong>Vapnik (1996, 1998)</strong></em>, se caracterizan por ser un proceso interno de <em><strong>construcción de reglas de clasificación, bastante diferente al de las de los métodos estadísticos</strong></em>. Las <em><strong>SVM</strong></em> suelen ser eficaces en casos en los que los métodos de clasificación tradicionales no lo son, como por ejemplo, los <em><strong>problemas basados en datos con estructura no lineal</strong></em>, además las <em><strong>SVM</strong></em> se han adaptado y aplicado en muchos campos de investigación. En esta sección, nos centramos en las <em><strong>SVM</strong></em> para la solución de <em><strong>problemas de regresión y clasificación</strong></em>, así como también el proceso de construcción, luego abordaremos los pasos de su extensión, de sistemas lineales a sistemas no lineales.</p></li>
<li><p>El paradigma algorítmico de las <em><strong>SVM</strong></em> aborda el reto de la complejidad de la muestra mediante la <strong>búsqueda de separadores de “mayor margen”</strong>. A grandes rasgos, un semiespacio separa un conjunto de entrenamiento con un gran margen si <em><strong>todos los ejemplos no sólo están en el lado correcto del hiperplano de separación, sino también lejos de él</strong></em>. Restringir el algoritmo a la salida de un separador de gran margen puede producir una complejidad de muestra pequeña, incluso si la dimensionalidad del espacio de características es alta (e incluso infinita). Introduciremos el concepto de margen y lo relacionamos con el <em><strong>paradigma de minimización de pérdidas regularizadas</strong></em>.</p></li>
</ul>
</div>
<section id="analisis">
<h2><span class="section-number">8.1. </span>Análisis<a class="headerlink" href="#analisis" title="Link to this heading">#</a></h2>
</section>
<section id="espacios-de-hilbert-con-kernel-reproductor">
<h2><span class="section-number">8.2. </span>Espacios de Hilbert con Kernel reproductor<a class="headerlink" href="#espacios-de-hilbert-con-kernel-reproductor" title="Link to this heading">#</a></h2>
<p>Sea <span class="math notranslate nohighlight">\(H\)</span> un <em><strong>espacio lineal de funciones reales</strong></em> definidas sobre <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathcal{R}^{l}\)</span>. Además, suponga que <span class="math notranslate nohighlight">\(H\)</span> es un <em><strong>espacio de Hilbert</strong></em>, con producto interno <span class="math notranslate nohighlight">\(\langle\cdot,\cdot\rangle_{H}\)</span> que induce la norma <span class="math notranslate nohighlight">\(\|\cdot\|_{H}\)</span>, con respecto a la cual <span class="math notranslate nohighlight">\(H\)</span> es completo.</p>
<div class="proof definition admonition" id="def_hilbert_rep">
<p class="admonition-title"><span class="caption-number">Definition 8.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Un espacio de Hilbert <span class="math notranslate nohighlight">\(H\)</span> es llamado <code class="docutils literal notranslate"><span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">Hilbert</span> <span class="pre">con</span> <span class="pre">kernel</span> <span class="pre">reproductor</span> <span class="pre">(RKHS)</span></code> si existe una función:</p>
<div class="math notranslate nohighlight">
\[
\kappa:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}
\]</div>
<p>con las siguientes propiedades:</p>
<ul class="simple">
<li><p>Para cada <span class="math notranslate nohighlight">\(\boldsymbol{x}\in\mathcal{X},~\kappa(\cdot, \boldsymbol{x})\)</span> pertenece a <span class="math notranslate nohighlight">\(H\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\kappa(\cdot, \cdot)\)</span> tiene la llamada <code class="docutils literal notranslate"><span class="pre">propiedad</span> <span class="pre">de</span> <span class="pre">reproducción</span></code>, esto es:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-prop-reproductora">
<span class="eqno">(8.1)<a class="headerlink" href="#equation-prop-reproductora" title="Link to this equation">#</a></span>\[
f(\boldsymbol{x})=\langle f, \kappa(\cdot, \boldsymbol{x})\rangle,\quad\forall f\in H,~\forall \boldsymbol{x}\in\mathcal{X}
\]</div>
</section>
</div><ul class="simple">
<li><p>Una consecuencia directa de la <em><strong>propiedad reproductora</strong></em> es, si definimos <span class="math notranslate nohighlight">\(f(\cdot)=\kappa(\cdot, \boldsymbol{y}),~\boldsymbol{y}\in\mathcal{X}\)</span> entonces</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\kappa(\boldsymbol{x}, \boldsymbol{y})=\langle\kappa(\cdot, \boldsymbol{y}), \kappa(\cdot, \boldsymbol{x})\rangle=\langle\kappa(\cdot, \boldsymbol{x}), \kappa(\cdot, \boldsymbol{y})\rangle=\kappa(\boldsymbol{y}, \boldsymbol{x})
\]</div>
<div class="proof definition admonition" id="def_features_map">
<p class="admonition-title"><span class="caption-number">Definition 8.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(H\)</span> un <code class="docutils literal notranslate"><span class="pre">(RKHS)</span></code> asociado con una función kernel <span class="math notranslate nohighlight">\(\kappa(\cdot, \cdot)\)</span> y <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> un conjunto de elementos. Entonces el mapeo</p>
<div class="math notranslate nohighlight">
\[
\mathcal{X}\ni\boldsymbol{x}\mapsto\Phi(\boldsymbol{x}):=\kappa(\cdot, \boldsymbol{x})\in H
\]</div>
<p>es conocido como <code class="docutils literal notranslate"><span class="pre">mapeo</span> <span class="pre">de</span> <span class="pre">características</span></code> y el espacio <span class="math notranslate nohighlight">\(H\)</span> es el <code class="docutils literal notranslate"><span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">características</span></code>. Esto es <span class="math notranslate nohighlight">\(\Phi\)</span> mapea cada vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\in\mathcal{X}\)</span> en el (RKHS) <span class="math notranslate nohighlight">\(H\)</span>.</p>
</section>
</div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H\)</span> puede ser de <code class="docutils literal notranslate"><span class="pre">dimensión</span> <span class="pre">finita</span> <span class="pre">o</span> <span class="pre">inifinita</span> <span class="pre">y</span> <span class="pre">sus</span> <span class="pre">elementos</span> <span class="pre">pueden</span> <span class="pre">ser</span> <span class="pre">funciones</span></code>. Esto es, cada <code class="docutils literal notranslate"><span class="pre">punto</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">es</span> <span class="pre">mapeado</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">funciones</span></code>. Si <span class="math notranslate nohighlight">\(H\)</span> es de dimensión finita, por ejemplo el Espacio Euclidiano <span class="math notranslate nohighlight">\(\mathbb{R}^{k},~\Phi(\boldsymbol{x})\in\mathbb{R}^{k}\)</span>.</p></li>
<li><p>Consideraremos el caso de dimensión infinita cuyas imágenes son funciones de <span class="math notranslate nohighlight">\(\Phi(\cdot)\)</span>. Veamos las ventajas de este <code class="docutils literal notranslate"><span class="pre">mapeo,</span> <span class="pre">del</span> <span class="pre">espacio</span> <span class="pre">original</span> <span class="pre">a</span> <span class="pre">otro</span> <span class="pre">de</span> <span class="pre">dimensión</span> <span class="pre">infinita</span> <span class="pre">(RKHS)</span></code>.</p></li>
</ul>
<div class="proof definition admonition" id="def_kernel_trick">
<p class="admonition-title"><span class="caption-number">Definition 8.3 </span></p>
<section class="definition-content" id="proof-content">
<p>Sean <span class="math notranslate nohighlight">\(\boldsymbol{x}, \boldsymbol{y}\in\mathcal{X}\subseteq\mathbb{R}^{l}\)</span>, entonces el <code class="docutils literal notranslate"><span class="pre">producto</span> <span class="pre">interno</span> <span class="pre">del</span> <span class="pre">respectivo</span> <span class="pre">mapeo</span> <span class="pre">de</span> <span class="pre">imágenes</span></code> es escrito como:</p>
<div class="math notranslate nohighlight">
\[
\langle\Phi(\boldsymbol{x}), \Phi(\boldsymbol{y})\rangle_{H}=\langle \kappa(\cdot, \boldsymbol{x}), \kappa(\cdot, \boldsymbol{y})\rangle
\]</div>
<p>o</p>
<div class="math notranslate nohighlight">
\[
\langle\Phi(\boldsymbol{x}), \Phi(\boldsymbol{y})\rangle_{H}=\kappa(\boldsymbol{x}, \boldsymbol{y}),\quad\text{Kernel trick}
\]</div>
</section>
</div><figure class="align-center" id="fig-kernel-map-ilust">
<a class="reference internal image-reference" href="_images/kernel_map_ilust.png"><img alt="_images/kernel_map_ilust.png" src="_images/kernel_map_ilust.png" style="width: 463.4px; height: 354.2px;" /></a>
</figure>
<ul class="simple">
<li><p>Empleando este tipo de mapeo a nuestro problema, calculamos <code class="docutils literal notranslate"><span class="pre">operaciones</span> <span class="pre">de</span> <span class="pre">producto</span> <span class="pre">interno</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\(H\)</span> en una manera eficiente, vía <code class="docutils literal notranslate"><span class="pre">evaluación</span> <span class="pre">de</span> <span class="pre">funciones</span> <span class="pre">sobre</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">baja</span> <span class="pre">dimensión</span></code>.</p></li>
</ul>
<figure class="align-center" id="non-linear-classifier-using-kernel-trick-fig">
<img alt="_images/non-linear-classifier-using-kernel-trick.png" src="_images/non-linear-classifier-using-kernel-trick.png" />
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">Ilustración de la propiedad <strong>kernel trick</strong>. Fuente <span id="id1">[<a class="reference internal" href="biblio.html#id57" title="Marouane Hachimi, Georges Kaddoum, Ghyslain Gagnon, and Poulmanogo Illy. Multi-stage jamming attacks detection using deep learning combined with kernelized support vector machine in 5g cloud radio access networks. In 2020 international symposium on networks, computers and communications (ISNCC), 1–5. IEEE, 2020.">Hachimi <em>et al.</em>, 2020</a>]</span>.</span><a class="headerlink" href="#non-linear-classifier-using-kernel-trick-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof property admonition" id="props_features_map">
<p class="admonition-title"><span class="caption-number">Property 8.1 </span></p>
<section class="property-content" id="proof-content">
<p>El mapeo <span class="math notranslate nohighlight">\(\Phi(\cdot)\)</span> satisface las siguientes propiedades</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Mapea</span> <span class="pre">implícitamente</span> <span class="pre">inputs</span> <span class="pre">(datos</span> <span class="pre">de</span> <span class="pre">entrenamiento)</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">espacio</span> <span class="pre">RKHS</span></code></p></li>
<li><p>Soluciona tareas de <code class="docutils literal notranslate"><span class="pre">estimación</span> <span class="pre">lineal</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\(H\)</span>, involucrando las imágenes:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\Phi(x_{n}),~n=1,2,\dots,N.
\]</div>
<ul class="simple">
<li><p><strong>Proyecta el algoritmo que soluciona problemas de parámetros desconocidos, en términos del producto interno</strong> en la forma:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\langle\Phi(x_{i}), \Phi(x_{j})\rangle,\quad i,j=1,2,\dots,N
\]</div>
<ul class="simple">
<li><p>Considera la <code class="docutils literal notranslate"><span class="pre">evaluación</span> <span class="pre">kernel</span> <span class="pre">como</span> <span class="pre">el</span> <span class="pre">producto</span> <span class="pre">interno</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\langle\Phi(x_{i}),\Phi(x_{j})\rangle=\kappa(x_{i}, x_{j}).
\]</div>
</section>
</div><ul class="simple">
<li><p>Nótese que este procedimiento de mapeo explicito es necesario para calcular la operación kernel en el último paso. La forma especifica de <span class="math notranslate nohighlight">\(\kappa(\cdot,\cdot)\)</span> no concierne en el análisis.</p></li>
</ul>
<div class="proof example admonition" id="ej_feature_map">
<p class="admonition-title"><span class="caption-number">Example 8.1 </span></p>
<section class="example-content" id="proof-content">
<p>Considere el caso del <code class="docutils literal notranslate"><span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">2</span> <span class="pre">dimensiones</span></code> y el mapeo</p>
<div class="math notranslate nohighlight">
\[
\mathbb{R}^{2}\ni \boldsymbol{x}\mapsto\Phi(x)=(x_{1}^{2}, \sqrt{2}x_{1}x_{2}, x_{2}^{2})\in\mathbb{R}^{3}
\]</div>
<p>Entonces, dados los vectores <span class="math notranslate nohighlight">\(\boldsymbol{x}=(x_{1}, x_{2})^{T}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{y}=(y_{1}, y_{2})^{T}\)</span>, es facil ver que</p>
<div class="math notranslate nohighlight">
\[
\kappa(\boldsymbol{x}, \boldsymbol{y})=\langle\Phi(\boldsymbol{x}), \Phi(\boldsymbol{y})\rangle_{\mathbb{R}^{3}}=\Phi(\boldsymbol{x})^{T}\Phi(\boldsymbol{y})=(x_{1}y_{1}+x_{2}y_{2})^{2}=(\boldsymbol{x}^{T}\boldsymbol{y})^{2}
\]</div>
<p>Es decir, el producto interior en el espacio tridimensional, después del mapeo, está dado en términos de una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">variables</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">original</span></code>.</p>
</section>
</div><div class="proof property admonition" id="prop_kcompacto">
<p class="admonition-title"><span class="caption-number">Property 8.2 </span></p>
<section class="property-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> un conjunto de puntos. Típicamente, <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^{l}\)</span> es <strong>compacto, esto es, cada sucesión en</strong> <span class="math notranslate nohighlight">\(X\)</span> <strong>tiene una subsucesión convergente</strong>. Sea <span class="math notranslate nohighlight">\(\kappa\)</span> la función</p>
<div class="math notranslate nohighlight">
\[
\kappa:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}.
\]</div>
<p>La función <span class="math notranslate nohighlight">\(\kappa\)</span> es llamada <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">definido</span> <span class="pre">positivo</span> <span class="pre">si</span> <span class="pre">satisface</span></code></p>
<div class="math notranslate nohighlight">
\[
\sum_{n=1}^{N}\sum_{m=1}^{N}a_{n}a_{m}\kappa(x_{n}, x_{m})\geq0,
\]</div>
<p>para cualquier número real <span class="math notranslate nohighlight">\(a_{n}, a_{m}\)</span> y cualquier punto <span class="math notranslate nohighlight">\(x_{n}, x_{m}\in\mathcal{X}\)</span> y cualquier <span class="math notranslate nohighlight">\(N\in\mathbb{N}\)</span>. O equivalentemente, si definimos la matriz <span class="math notranslate nohighlight">\(K\)</span>, de orden <span class="math notranslate nohighlight">\(N\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
K=
\begin{pmatrix}
k(x_{1}, x_{1}) &amp; \cdots &amp; k(x_{1}, x_{N})\\
\vdots &amp; \vdots &amp; \vdots\\
k(x_{N}, x_{1}) &amp; \cdots &amp; k(x_{N}, x_{N})\\
\end{pmatrix}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(a^{T}Ka\geq0\)</span>, donde <span class="math notranslate nohighlight">\(a=(a_{1}, a_{2},\dots, a_{N})^{T}\)</span>.</p>
</section>
</div><div class="proof example admonition" id="ej_kernel_functions">
<p class="admonition-title"><span class="caption-number">Example 8.2 </span></p>
<section class="example-content" id="proof-content">
<p>Los siguientes son algunos <code class="docutils literal notranslate"><span class="pre">ejemplos</span> <span class="pre">típicos</span> <span class="pre">de</span> <span class="pre">funciones</span> <span class="pre">kernel</span></code>, las cuales son comúnmente usadas en varias aplicaciones. Estas funciones kernel son utilizadas en <code class="docutils literal notranslate"><span class="pre">Python</span></code> para los modelos <code class="docutils literal notranslate"><span class="pre">SVC</span></code> y <code class="docutils literal notranslate"><span class="pre">SVR</span></code> (ver <a class="reference external" href="https://scikit-learn.org/stable/modules/svm.html#kernel-functions">kernel-functions</a>).</p>
<ul>
<li><p>El <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">gaussiano</span></code> está entre las más populares y está dado por la función</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\exp\left(-\frac{\|\boldsymbol{x}-\boldsymbol{y}\|^{2}}{2\sigma^2}\right),\quad\sigma&gt;0
    \]</div>
</li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">polinomial</span></code> homogéneo tiene la forma</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=(\boldsymbol{x}^{T}\boldsymbol{y})^{r},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(r\)</span> es un parámetro.</p>
</li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">polinómico</span> <span class="pre">no-homogéneo</span></code> tiene la forma</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=(\boldsymbol{x}^{T}\boldsymbol{y}+c)^{r},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(c\geq0,~r\)</span> son parámetros.</p>
</li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">laplaciano</span></code> está dado por</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\exp(-t\|\boldsymbol{x}-\boldsymbol{y}\|),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(t&gt;0\)</span> es un parámetro.</p>
</li>
</ul>
</section>
</div><figure class="align-center" id="sphx-glr-plot-iris-svc-001-fig">
<img alt="_images/sphx_glr_plot_iris_svc_001.png" src="_images/sphx_glr_plot_iris_svc_001.png" />
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">SVC</span></code> y <code class="docutils literal notranslate"><span class="pre">LinearSVC</span></code> para clasificación en un conjunto de datos. Ver <a class="reference external" href="https://scikit-learn.org/stable/modules/svm.html#classification">scikit-learn</a>.</span><a class="headerlink" href="#sphx-glr-plot-iris-svc-001-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="construccion-de-kernels">
<h2><span class="section-number">8.3. </span>Construcción de kernels<a class="headerlink" href="#construccion-de-kernels" title="Link to this heading">#</a></h2>
<p>Además de los ejemplos previos, se pueden <code class="docutils literal notranslate"><span class="pre">construir</span> <span class="pre">otros</span> <span class="pre">kernels</span> <span class="pre">aplicando</span> <span class="pre">las</span> <span class="pre">siguientes</span> <span class="pre">propiedades</span></code></p>
<ul>
<li><p>Si</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \kappa_{1}(\boldsymbol{x}, \boldsymbol{y}):\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}\\
    \kappa_{1}(\boldsymbol{x}, \boldsymbol{y}):\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}\end{split}\]</div>
<p>son kernels, entonces</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\kappa_{1}(\boldsymbol{x}, \boldsymbol{y})+\kappa_{2}(\boldsymbol{x}, \boldsymbol{y})\\
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\alpha\kappa_{1}(\boldsymbol{x}, \boldsymbol{y})\end{split}\]</div>
<p>y</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\kappa_{1}(\boldsymbol{x}, \boldsymbol{y})\kappa_{2}(\boldsymbol{x}, \boldsymbol{y}),
    \]</div>
<p>son kernels.</p>
</li>
</ul>
<ul>
<li><p>Sea</p>
<div class="math notranslate nohighlight">
\[
    f:\mathcal{X}\mapsto\mathbb{R}
    \]</div>
<p>entonces</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=f(\boldsymbol{x})f(\boldsymbol{y})
    \]</div>
<p>es un kernel</p>
</li>
</ul>
<ul>
<li><p>Sea una función</p>
<div class="math notranslate nohighlight">
\[
    g:\mathcal{X}\mapsto\mathbb{R}^{l}
    \]</div>
<p>y una función kernel</p>
<div class="math notranslate nohighlight">
\[
    \kappa_{1}(\cdot, \cdot):\mathbb{R}^{l}\times\mathbb{R}^{l}\mapsto\mathbb{R}.
    \]</div>
<p>Entonces</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\kappa_{1}(g(\boldsymbol{x}), g(\boldsymbol{y}))
    \]</div>
<p>es también un kernel.</p>
</li>
</ul>
<ul>
<li><p>Sea <span class="math notranslate nohighlight">\(A\)</span> una <code class="docutils literal notranslate"><span class="pre">matriz</span> <span class="pre">definida</span> <span class="pre">positiva</span></code> con dimensión <span class="math notranslate nohighlight">\(l\times l\)</span>. Entonces</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\boldsymbol{x}^{T}A\boldsymbol{y}
    \]</div>
<p>es un kernel.</p>
</li>
</ul>
<ul>
<li><p>Si</p>
<div class="math notranslate nohighlight">
\[
    \kappa_{1}(\boldsymbol{x}, \boldsymbol{y}):\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R},
    \]</div>
<p>entonces</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\exp(\kappa_{1}(\boldsymbol{x}, \boldsymbol{y}))
    \]</div>
<p>es también un kernel, y si <span class="math notranslate nohighlight">\(p(\cdot)\)</span> es un polinomio con coeficientes no negativos,</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=p(\kappa_{1}(\boldsymbol{x}, \boldsymbol{y}))
    \]</div>
<p>también es un kernel.</p>
</li>
</ul>
<p>Para mas información sobre los kernel y su construción (ver <span id="id2">[<a class="reference internal" href="biblio.html#id12" title="Thomas Hofmann, Bernhard Schölkopf, and Alexander J Smola. Kernel methods in machine learning. The annals of statistics, 36(3):1171–1220, 2008.">Hofmann <em>et al.</em>, 2008</a>, <a class="reference internal" href="biblio.html#id13" title="John Shawe-Taylor, Nello Cristianini, and others. Kernel methods for pattern analysis. Cambridge university press, 2004.">Shawe-Taylor <em>et al.</em>, 2004</a>, <a class="reference internal" href="biblio.html#id14" title="Konstantinos Slavakis, Pantelis Bouboulis, and Sergios Theodoridis. Online learning in reproducing kernel hilbert spaces. In Academic Press Library in Signal Processing, volume 1, pages 883–987. Elsevier, 2014.">Slavakis <em>et al.</em>, 2014</a>]</span>).</p>
</section>
<section id="teorema-de-representacion">
<h2><span class="section-number">8.4. </span>Teorema de representación<a class="headerlink" href="#teorema-de-representacion" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>El teorema que se presentará en esta sección es de gran importancia desde un punto de vista práctico. Este nos permite llevar a cabo,  <strong>optimización empírica de funciones de pérdida</strong>, basado en un conjunto finito de puntos de entrenamiento, en una manera muy eficiente <em>si la función a estimar pertenece a un espacio de dimensión alta</em> o incluso infinita <span class="math notranslate nohighlight">\(H\)</span>.</p></li>
</ul>
<div class="proof theorem admonition" id="th_representation">
<p class="admonition-title"><span class="caption-number">Theorem 8.1 </span></p>
<section class="theorem-content" id="proof-content">
<p>Sea</p>
<div class="math notranslate nohighlight">
\[
\Omega: [0, +\infty)\longmapsto\mathbb{R}
\]</div>
<p>una función arbitraria, <code class="docutils literal notranslate"><span class="pre">estrictamente</span> <span class="pre">monotona</span> <span class="pre">creciente</span></code>. Sea también</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}:\mathbb{R}^{2}\longmapsto\mathbb{R}\cup\{\infty\}
\]</div>
<p>una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span></code>. Entonces, cada minimizador <span class="math notranslate nohighlight">\(f\in H\)</span>, de una <code class="docutils literal notranslate"><span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">minimización</span> <span class="pre">regularizada</span></code></p>
<div class="math notranslate nohighlight" id="equation-eq-min-reg">
<span class="eqno">(8.2)<a class="headerlink" href="#equation-eq-min-reg" title="Link to this equation">#</a></span>\[
\min_{f\in H} J(f)=\sum_{n=1}^{N}\mathcal{L}(y_{n}, f(\boldsymbol{x}_{n}))+\lambda\Omega(\|f\|^{2})
\]</div>
<p>admite una <code class="docutils literal notranslate"><span class="pre">representación</span></code> de la forma</p>
<div class="math notranslate nohighlight">
\[
f(\cdot)=\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n}),
\]</div>
<p>donde <span class="math notranslate nohighlight">\(\theta_{n}\in\mathbb{R},~n=1,2,\dots,N\)</span>. La regularización de la forma <span class="math notranslate nohighlight">\(\Omega(\|f\|^{2})\)</span> es una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">cuadrática,</span> <span class="pre">y</span> <span class="pre">por</span> <span class="pre">lo</span> <span class="pre">tanto</span> <span class="pre">estrictamente</span> <span class="pre">monotona</span></code> en el intervalo <span class="math notranslate nohighlight">\([0, \infty)\)</span>.</p>
</section>
</div><p><strong><code class="docutils literal notranslate"><span class="pre">Demostración</span></code></strong></p>
<p>Sea</p>
<div class="math notranslate nohighlight">
\[
A=\text{span}\{\kappa(\cdot, \boldsymbol{x}_{1}), \kappa(\cdot, \boldsymbol{x}_{2}),\dots,\kappa(\cdot, \boldsymbol{x}_{N})\}.
\]</div>
<p>Dado que cada <span class="math notranslate nohighlight">\(\kappa(\cdot, \boldsymbol{x}_{i})\in H,~i=1,2,\dots,N,~A\subseteq H\)</span> y <span class="math notranslate nohighlight">\(N&lt;\infty\)</span>, entonces, <span class="math notranslate nohighlight">\(A\)</span> es cerrado.</p>
<p>Por <strong>Teorema de descomposición ortogonal</strong>, como <span class="math notranslate nohighlight">\(A\)</span> es un subespacio cerrado de <span class="math notranslate nohighlight">\(H\)</span>, entonces, <span class="math notranslate nohighlight">\(H=A\oplus A^{T}\)</span>. Esto es, si <span class="math notranslate nohighlight">\(f\in H\)</span> entonces</p>
<div class="math notranslate nohighlight">
\[
f(\cdot)=\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+f_{\perp}
\]</div>
<p>donde <span class="math notranslate nohighlight">\(f_{\perp}\)</span> es la parte de <span class="math notranslate nohighlight">\(f\)</span> que es ortogonal a <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>Usando la <strong>propiedad reproductora</strong> <a class="reference internal" href="#equation-prop-reproductora">(8.1)</a> se tiene que <span class="math notranslate nohighlight">\(\forall f\in H\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
f(\boldsymbol{x}_{m})&amp;=\langle f, \kappa(\cdot, \boldsymbol{x}_{m})\rangle\\
&amp;=\left\langle\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+f_{\perp}, \kappa(\cdot, \boldsymbol{x}_{m})\right\rangle\\
&amp;=\left\langle\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n}), \kappa(\cdot, \boldsymbol{x}_{m})\right\rangle\\
&amp;=\sum_{n=1}^{N}\theta_{n}\langle\kappa(\cdot, \boldsymbol{x}_{n}), \kappa(\cdot, \boldsymbol{x}_{m})\rangle\\
&amp;=\sum_{n=1}^{N}\theta_{n}\kappa(\boldsymbol{x}_{m}, \boldsymbol{x}_{n}).
\end{align}
\end{split}\]</div>
<p>Nótese que <strong>la propiedad de reproducción garantiza que en los puntos de entrenamiento el valor de</strong> <span class="math notranslate nohighlight">\(f\)</span> <strong>no depende de</strong> <span class="math notranslate nohighlight">\(f_{\perp}\)</span>, y, por lo tanto, tampoco el primer término de <span class="math notranslate nohighlight">\(\min_{f\in H}J(f)\)</span> en <a class="reference internal" href="#equation-eq-min-reg">(8.2)</a>.</p>
<p>Además, para todo <span class="math notranslate nohighlight">\(f_{\perp}\)</span> tenemos que:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\Omega(\|f\|^{2})&amp;=\Omega\left(\left\langle\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+f_{\perp}, \sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+f_{\perp}\right\rangle\right)\\
&amp;=\Omega\left(\left\langle\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n}), \sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})\right\rangle+\langle f_{\perp}, f_{\perp}\rangle\right)\\
&amp;=\Omega\left(\left\|\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})\right\|^{2}+\|f_{\perp}\|^{2}\right)\\
&amp;\geq\Omega\left(\left\|\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})\right\|^{2}\right)
\end{align*}
\end{split}\]</div>
<p>Por lo tanto <code class="docutils literal notranslate"><span class="pre">para</span> <span class="pre">cualquier</span> <span class="pre">selección</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(\theta_{n},~n=1,2,\dots,N\)</span>, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span> <span class="pre">es</span> <span class="pre">minimizada</span> <span class="pre">por</span></code> <span class="math notranslate nohighlight">\(f_{\perp}=0\)</span>.</p>
<div class="proof observation admonition" id="observation-8">
<p class="admonition-title"><span class="caption-number">Observation 8.1 </span></p>
<section class="observation-content" id="proof-content">
<ul>
<li><p>La importancia de este teorema radica en que, para optimizar <span class="math notranslate nohighlight">\(J(f)\)</span> en <a class="reference internal" href="#equation-eq-min-reg">(8.2)</a> con respecto a <span class="math notranslate nohighlight">\(f\)</span>, usamos la expresión <span class="math notranslate nohighlight">\(f(\cdot)\)</span> en <a class="reference internal" href="#equation-prop-reproductora">(8.1)</a> y <strong>la minimización es llevada a cabo con respecto a los parámetros</strong> <span class="math notranslate nohighlight">\(\theta_{n},~n=1,2,\dots,N\)</span>.</p></li>
<li><p>En casos que la regularización no es necesaria, usualmente un término de sesgo es agregado y se supone que la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">a</span> <span class="pre">minimizar</span> <span class="pre">admite</span> <span class="pre">la</span> <span class="pre">representación</span></code></p>
<div class="math notranslate nohighlight">
\[
    \tilde{f}=f+b,\quad f(\cdot)=\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})
    \]</div>
</li>
</ul>
</section>
</div><ul class="simple">
<li><p>La esencia del siguiente teorema es expandir la solución en dos partes. <strong>Una que pertenece al RKHS</strong>, <span class="math notranslate nohighlight">\(H\)</span>, y otra que está dada como una <strong>combinación lineal de un conjunto de funciones preseleccionadas</strong>.</p></li>
</ul>
<div class="proof theorem admonition" id="th_rep_semipar">
<p class="admonition-title"><span class="caption-number">Theorem 8.2 </span> (Teorema de representación semiparamétrico)</p>
<section class="theorem-content" id="proof-content">
<p>Supongamos que adicionalmente a los supuestos adoptados en el <a class="reference internal" href="#th_representation">Theorem 8.1</a>, es dado el siguiente <strong>conjunto de funciones reales</strong></p>
<div class="math notranslate nohighlight">
\[
\varphi_{m}:\mathcal{X}\longmapsto\mathbb{R},\quad m=1,2,\dots,M,
\]</div>
<p>con la propiedad que la matriz de <span class="math notranslate nohighlight">\(N\times M\)</span> con elementos <span class="math notranslate nohighlight">\(\varphi_{m}(\boldsymbol{x}_{n})\)</span>, <span class="math notranslate nohighlight">\(~n=1,2,\dots,N\)</span>, <span class="math notranslate nohighlight">\(~m=1,2,\dots,M\)</span>, tiene rango <span class="math notranslate nohighlight">\(M\)</span>. Entonces, cualquier</p>
<div class="math notranslate nohighlight">
\[
\tilde{f}=f+h,\quad f\in H,~ h\in\text{span}\{\varphi_{m}, m=1,2,\dots,M\},
\]</div>
<p>que soluciona la tarea de minimización</p>
<div class="math notranslate nohighlight">
\[
\min_{\tilde{f}} J(\tilde{f}):=\sum_{n=1}^{N}\mathcal{L}(y_{n}, \tilde{f}(\boldsymbol{x}_{n}))+\Omega(\|f\|^{2}),
\]</div>
<p><strong>admite la representación</strong></p>
<div class="math notranslate nohighlight">
\[
\tilde{f}(\cdot)=\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+\sum_{m=1}^{M}b_{m}\psi_{m}(\cdot).
\]</div>
</section>
</div><div class="proof observation admonition" id="observation-10">
<p class="admonition-title"><span class="caption-number">Observation 8.2 </span></p>
<section class="observation-content" id="proof-content">
<p>Un ejemplo de <code class="docutils literal notranslate"><span class="pre">aplicación</span> <span class="pre">exitosa</span> <span class="pre">de</span> <span class="pre">este</span> <span class="pre">teorema</span> <span class="pre">fue</span> <span class="pre">demostrada</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">contexto</span> <span class="pre">de</span> <span class="pre">procesamiento</span> <span class="pre">de</span> <span class="pre">imágenes</span></code> <span id="id3">[<a class="reference internal" href="biblio.html#id15" title="Pantelis Bouboulis, Konstantinos Slavakis, and Sergios Theodoridis. Adaptive kernel-based image denoising employing semi-parametric regularization. IEEE Transactions on Image Processing, 19(6):1465–1479, 2010.">Bouboulis <em>et al.</em>, 2010</a>]</span>. Un conjunto de funciones no lineales en lugar de <span class="math notranslate nohighlight">\(\psi_{m}\)</span> fueron usadas para <strong>detección de bordes en una imagen con (saltos no suaves)</strong>. La parte de <span class="math notranslate nohighlight">\(f\)</span> que pertenece al espacio <code class="docutils literal notranslate"><span class="pre">RKHS</span></code> es usada para las partes suaves en la imagen.</p>
</section>
</div></section>
<section id="regresion-ridge-con-kernel">
<h2><span class="section-number">8.5. </span>Regresión ridge con Kernel<a class="headerlink" href="#regresion-ridge-con-kernel" title="Link to this heading">#</a></h2>
<ul>
<li><p>En esta sección abordaremos la tarea de <em><strong>regresión ridge en un espacio general RKHS</strong></em>. El camino a seguir es el usado típicamente para <em><strong>extender técnicas, las cuales han sido desarrolladas para modelos lineales, a espacios más generales RKSH</strong></em>.</p></li>
<li><p>Sea <span class="math notranslate nohighlight">\((y_{n}, \boldsymbol{x}_{n})\in\mathbb{R}\times\mathbb{R}^{l}\)</span> la representación de un mecanismo de generación de datos, modelados vía tarea de <em><strong>regresión no lineal</strong></em></p>
<div class="math notranslate nohighlight">
\[
    y_{n}=g(\boldsymbol{x}_{n})+\eta_{n},\quad n=1,2,\dots,N.
    \]</div>
</li>
<li><p>Denotemos por <span class="math notranslate nohighlight">\(f\)</span> es el <em><strong>estimador de la función</strong></em> <span class="math notranslate nohighlight">\(g\)</span> desconocida. <span class="math notranslate nohighlight">\(f\)</span> <em><strong>es llamada la hipótesis</strong></em> y <span class="math notranslate nohighlight">\(H\)</span> <em><strong>el espacio de hipótesis</strong></em>, donde <span class="math notranslate nohighlight">\(f\)</span> es buscada. Supongamos que <span class="math notranslate nohighlight">\(f\)</span> está en <em><strong>RKHS</strong></em> y está asociada con el kernel</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\kappa:\mathbb{R}^{l}\times\mathbb{R}^{l}\longmapsto\mathbb{R}.
\]</div>
<ul class="simple">
<li><p>Por el <em><strong>teorema de representación</strong></em> se tiene que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f(\boldsymbol{x})=\sum_{n=1}^{N}\theta_{n}\kappa(\boldsymbol{x}, \boldsymbol{x}_{n}).
\]</div>
<ul>
<li><p>De acuerdo a la <em><strong>regresión ridge con kernel</strong></em>, los coeficientes desconocidos son estimados por medio de la siguiente tarea</p>
<div class="math notranslate nohighlight" id="equation-eq-task-ridge-reg">
<span class="eqno">(8.3)<a class="headerlink" href="#equation-eq-task-ridge-reg" title="Link to this equation">#</a></span>\[
    \hat{\theta}=\text{argmin}_{\theta} J(\theta),\quad J(\theta)=\sum_{n=1}^{N}\left(y_{n}-\sum_{m=1}^{N}\theta_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})\right)^{2}+C\langle f, f\rangle,
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(C\)</span> es el <em><strong>parámetro de regularización</strong></em>. La Ecuación <a class="reference internal" href="#equation-eq-task-ridge-reg">(8.3)</a> puede reescribirse como</p>
<div class="math notranslate nohighlight" id="equation-loss-ridge-kernel">
<span class="eqno">(8.4)<a class="headerlink" href="#equation-loss-ridge-kernel" title="Link to this equation">#</a></span>\[
    J(\theta)=(\boldsymbol{y}-K\theta)^{T}(\boldsymbol{y}-K\theta)+C\theta^{T}K\theta,
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\boldsymbol{y}=(y_{1}, y_{2},\dots, y_{N})^{T}\)</span> y <span class="math notranslate nohighlight">\(\theta=(\theta_{1}, \theta_{2}, \dots,\theta_{M})^{T}\)</span>, <span class="math notranslate nohighlight">\(K\)</span> es la matriz kernel tal que <span class="math notranslate nohighlight">\(\langle\kappa(\cdot, \boldsymbol{x}_{m}), \kappa(\cdot, \boldsymbol{x}_{m})\rangle=\kappa(\boldsymbol{x}_{m}, \boldsymbol{x}_{n})\)</span>, y <span class="math notranslate nohighlight">\(y\)</span> es <em><strong>determinada por la función kernel y los valores de entrenamiento</strong></em>.</p>
</li>
</ul>
<ul class="simple">
<li><p><em><strong>Minimizando</strong></em> <span class="math notranslate nohighlight">\(J(\theta)\)</span> con respecto a <span class="math notranslate nohighlight">\(\theta\)</span> y <em><strong>suponiendo que</strong></em> <span class="math notranslate nohighlight">\(K^{T}=K\)</span> <em><strong>es invertible</strong></em>, utilizando el <em><strong>mismo procedimiento en la regresión ridge</strong></em>, tenemos que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
(K^{T}K+CK^{T})\hat{\theta}=K^{T}\boldsymbol{y}\quad\text{o}\quad\boldsymbol{y} = (K+CI)\hat{\theta}.
\]</div>
<ul>
<li><p>Una vez <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> es obtenido, dado un vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\in\mathbb{R}^{l}\)</span> la correspondiente <em><strong>predicción de la variable dependiente</strong></em> está dada por:</p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{y}}=\sum_{n=1}^{N}\hat{\theta}_{n}\kappa(\boldsymbol{x}, \boldsymbol{x}_{n})=\theta^{T}\boldsymbol{\kappa}(\boldsymbol{x})=\boldsymbol{y}^{T}(K+CI)^{-1}\boldsymbol{\kappa}(\boldsymbol{x}),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\boldsymbol{\kappa}(\boldsymbol{x})=(\kappa(\boldsymbol{x},\boldsymbol{x}_{1}),\dots,\boldsymbol{\kappa}(\boldsymbol{x}, \boldsymbol{x}_{N}))^{T}\)</span>.</p>
</li>
</ul>
</section>
<section id="regresion-de-vectores-de-soporte">
<h2><span class="section-number">8.6. </span>Regresión de vectores de soporte<a class="headerlink" href="#regresion-de-vectores-de-soporte" title="Link to this heading">#</a></h2>
<div class="admonition-metodo-de-minimos-modulos admonition">
<p class="admonition-title">Método de mínimos módulos</p>
<ul>
<li><p>El método de <em><strong>mínimos cuadrados no es siempre el mejor criterio de optimización</strong></em>, debido a que, <em><strong>en presencia de ruido no Gaussiano</strong></em> con colas largas y por lo tanto número creciente de outliers, <em><strong>la dependencia cuadrática de el método se sesga hacia valores asociados con presencia de outliers</strong></em></p></li>
<li><p>Una manera de darle solución a este problema es escoger una <em><strong>función de pérdida que se ajuste mejor al modelo del ruido</strong></em> (ver Huber 1992, <span id="id4">[<a class="reference internal" href="biblio.html#id16" title="Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pages 492–518. Springer, 1992.">Huber, 1992</a>]</span>). Bajo el <em><strong>supuesto de que el ruido tiene función de densidad de probabilidad (fdp) simétrica</strong></em>, la tarea optima de regresión es obtenida minimizando la función de pérdida</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(y, f(\boldsymbol{x}))=|y-f(\boldsymbol{x})|
    \]</div>
<p>la cual es conocida como el <em><strong>método de mínimos módulos</strong></em>.</p>
</li>
</ul>
</div>
<div class="admonition-perdida-de-huber admonition">
<p class="admonition-title">Pérdida de Huber</p>
<ul>
<li><p>Huber demostró que <em><strong>si el ruido tiene dos componentes: Gaussiana y otra fdp arbitraria (simétrica)</strong></em>, entonces la <em><strong>mejor función de pérdida en el sentido minimax</strong></em> está dada por:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathcal{L}(y, f(\boldsymbol{x}))=
    \begin{cases}
    \displaystyle{\varepsilon|y-f(\boldsymbol{x})|-\varepsilon^{2}/2} &amp; \text{si}~|y-f(\boldsymbol{x})|&gt;\varepsilon,\\[2mm]
    \displaystyle{|y-f(\boldsymbol{x})|/2-\varepsilon^{2}/2} &amp; \text{si}~|y-f(\boldsymbol{x})|\leq\varepsilon,
    \end{cases}
    \end{split}\]</div>
<p>para algún parámetro <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Esta es conocida como función de <em><strong>pérdida de Huber</strong></em>.</p>
</li>
</ul>
</div>
<div class="admonition-perdida-varepsilon-insensible admonition">
<p class="admonition-title">Pérdida <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible</p>
<ul>
<li><p>Una <em><strong>función de pérdida que se aproxima a la de Huber</strong></em> y tiene excelentes propiedades computacionales es llamada <em><strong>función de pérdida lineal</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>-<em><strong>insensible</strong></em> y está dada por</p>
<div class="math notranslate nohighlight" id="equation-eq-lin-ins">
<span class="eqno">(8.5)<a class="headerlink" href="#equation-eq-lin-ins" title="Link to this equation">#</a></span>\[\begin{split}
    \mathcal{L}(y, f(\boldsymbol{x}))=
    \begin{cases}
    |y-f(\boldsymbol{x})|-\varepsilon &amp; \text{si}~|y-f(\boldsymbol{x})|&gt;\varepsilon\\[2mm]
    0 &amp; \text{si}~|y-f(\boldsymbol{x})|\leq\varepsilon
    \end{cases}
    \end{split}\]</div>
<p>y la función de pérdida <em><strong>cuadrática</strong></em> <span class="math notranslate nohighlight">\(\epsilon\)</span><em><strong>-insensible</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathcal{L}(y, f(\boldsymbol{x}))=
    \begin{cases}
    |y-f(\boldsymbol{x})|^{2}-\varepsilon &amp; \text{si}~|y-f(\boldsymbol{x})|&gt;\varepsilon\\[2mm]
    0 &amp; \text{si}~|y-f(\boldsymbol{x})|\leq\varepsilon
    \end{cases}
    \end{split}\]</div>
<p>la cuales <em><strong>coinciden con la función de pérdida de mínimos módulos para</strong></em> <span class="math notranslate nohighlight">\(\varepsilon=0\)</span> y es <em><strong>cercana a la función de pérdida de Huber para</strong></em> <span class="math notranslate nohighlight">\(\varepsilon&lt;1\)</span>.</p>
</li>
</ul>
</div>
<figure class="align-center" id="huber-loss-plot">
<a class="reference internal image-reference" href="_images/huber_loss_plot.png"><img alt="_images/huber_loss_plot.png" src="_images/huber_loss_plot.png" style="width: 411.3px; height: 271.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">Función de pérdida de Huber (gris punteado). Función de pérdida lineal <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible (gris completo). Función de pérdida cuadrática <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible (rojo) para <span class="math notranslate nohighlight">\(\varepsilon = 0.7\)</span>. Fuente <span id="id5">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#huber-loss-plot" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La función de pérdida <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensible se utiliza en escenarios donde <em><strong>queremos que nuestro modelo tolere errores hasta cierto punto</strong></em>. La pérdida <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensible permite que el modelo sea <em><strong>menos sensible a valores atípicos o puntos de datos ruidosos</strong></em></p></li>
<li><p>La función de pérdida <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensible proporciona una herramienta útil para <em><strong>construir modelos que sean robustos, flexibles y capaces de equilibrar la precisión con la generalización</strong></em>, especialmente en <em><strong>situaciones donde los datos pueden contener ruido o valores atípicos</strong></em>.</p></li>
</ul>
</div>
</section>
<section id="regresion-optima-lineal-varepsilon-insensible">
<h2><span class="section-number">8.7. </span>Regresión óptima lineal <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible<a class="headerlink" href="#regresion-optima-lineal-varepsilon-insensible" title="Link to this heading">#</a></h2>
<ul>
<li><p>Usaremos la <em><strong>función de pérdida lineal</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible <a class="reference internal" href="#equation-eq-lin-ins">(8.5)</a> para <em><strong>cuantificar el error de desajuste del modelo</strong></em>. El valor de <span class="math notranslate nohighlight">\(\epsilon\)</span> <em>determina la cantidad de “slack” (grado de error) que se permite</em>.</p></li>
<li><p>Trataremos la tarea de regresión</p>
<div class="math notranslate nohighlight">
\[
    y=g(\boldsymbol{x})+\eta_{n},
    \]</div>
<p>usando un model lineal de la forma:</p>
<div class="math notranslate nohighlight">
\[
    f(\boldsymbol{x})=\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}.
    \]</div>
<p>La solución más general en la que <span class="math notranslate nohighlight">\(f\)</span> <em><strong>es una función en RKHS será obtenida vía kernel trick</strong></em> (producto interno reemplazado por <em><strong>evaluación de kernel</strong></em>).</p>
</li>
<li><p>Introducimos ahora <em><strong>dos conjuntos de variables auxiliares</strong></em></p>
<div class="math notranslate nohighlight" id="equation-eq-restric-1">
<span class="eqno">(8.6)<a class="headerlink" href="#equation-eq-restric-1" title="Link to this equation">#</a></span>\[
    \text{Si}~ y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}\geq\varepsilon,~\text{definimos}~\tilde{\xi}_{n}~\text{tal que},\quad y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}\leq\varepsilon+\tilde{\xi}_{n}.
    \]</div>
<p>Nótese que <em><strong>idealmente, deseamos seleccionar</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}, \theta_{0}\)</span> <em><strong>tales que</strong></em> <span class="math notranslate nohighlight">\(\tilde{\xi}_{n}=0\)</span>, dado que este valor entregaría la <em><strong>contribución del respectivo término en la función de pérdida igual a cero</strong></em>, pues <span class="math notranslate nohighlight">\(\mathcal{L}(y, f(\boldsymbol{x}))=0\)</span> si <span class="math notranslate nohighlight">\(|y-f(\boldsymbol{x})|\leq\varepsilon\)</span>. Análogamente,</p>
<div class="math notranslate nohighlight" id="equation-eq-restric-2">
<span class="eqno">(8.7)<a class="headerlink" href="#equation-eq-restric-2" title="Link to this equation">#</a></span>\[
    \text{Si}~ y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}\leq-\varepsilon,~\text{definimos}~\xi_{n}~\text{tal que},\quad\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}\leq\varepsilon+\xi_{n}.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Estamos ahora listos para formular la <em><strong>tarea de minimización para el correspondiente coste empírico</strong></em>, regularizado por la norma de <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, el cual es <em><strong>proyectado en términos de las variables auxiliares</strong></em> como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
\text{minimizar} &amp; J(\boldsymbol{\theta}, \theta_{0}, \xi, \tilde{\xi})=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\left(\displaystyle{\sum_{n=1}^{N}\xi_{n}+\sum_{n=1}^{N}\tilde{\xi}_{n}}\right),\\[2mm]
\text{sujeto a} &amp; y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}\leq\varepsilon+\tilde{\xi}_{n},\quad n=1,2,\dots,N,\\[2mm]
 &amp; \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}\leq\varepsilon+\xi_{n},\quad n=1,2,\dots,N,\\[2mm]
&amp;\tilde{\xi}_{n}\geq0,\quad\tilde{\xi}_{n}\geq0,~ n=1,2,\dots,N.
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>El término <span class="math notranslate nohighlight">\(\frac{1}{2}\|\boldsymbol{\theta}\|^{2}\)</span>, surge de la necesidad de <em><strong>maximizar el margen asociado al hiperplano que separa dos clases</strong></em> (ver sección <em><strong>máquinas de vectores de soporte</strong></em> <em>Vapnik and Chervonenkis 1960s</em>).</p></li>
<li><p>Dicho margen: <span class="math notranslate nohighlight">\(2/\|\boldsymbol{\theta}\|\)</span>, el cual, en <em><strong>problemas de clasificación, se desea maximizar, es equivalente a minimizar la norma</strong></em> <span class="math notranslate nohighlight">\(\frac{1}{2}\|\boldsymbol{\theta}\|^2\)</span> para el caso de <em><strong>modelos regresivos</strong></em>. El factor <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> es incluido de forma numéricamente conveniente, y no altera la solución. A este tipo de problemas se les denomina de <em><strong>programación cuadrática</strong></em>.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">¿Por qué maximizar el margen?</p>
<ul class="simple">
<li><p>Maximizar el margen permite <em><strong>lograr un buen rendimiento de generalización</strong></em>. El <em><strong>margen representa el nivel de confianza del clasificador, y maximizarlo ayuda a minimizar el error de generalización</strong></em>.</p></li>
<li><p>Maximizar el margen corresponde a <em><strong>minimizar la</strong></em> <a class="reference external" href="https://es.wikipedia.org/wiki/Dimensi%C3%B3n_VC">Dimensión VC (Vapnik-Chervonenkis)</a>, que es una <em><strong>medida de la capacidad o complejidad del modelo</strong></em>. Al maximizar el margen, SVM busca <em><strong>encontrar el límite de decisión más simple que separa las clases de manera efectiva</strong></em>, lo que conduce a una <em><strong>mejor generalización a datos no vistos</strong></em>.</p></li>
<li><p>Maximizar el margen ayuda a <em><strong>crear un límite de decisión que es robusto al ruido y a los valores atípicos en los datos de entrenamiento</strong></em>. Al enfocarse en los puntos de datos cerca del margen, <em><strong>SVM puede tolerar mejor el ruido y los valores atípicos, lo que lleva a un clasificador más confiable y preciso</strong></em>.</p></li>
</ul>
</div>
<figure class="align-center" id="largest-margin-fig">
<a class="reference internal image-reference" href="_images/largest_margin.png"><img alt="_images/largest_margin.png" src="_images/largest_margin.png" style="width: 541.8px; height: 253.79999999999998px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.4 </span><span class="caption-text">Dos <em><strong>líneas de separación</strong></em>: una buena con un <em><strong>margen grande</strong></em> (derecha) y una línea de separación menos aceptable con un <em><strong>margen pequeño</strong></em> (izquierda).</span><a class="headerlink" href="#largest-margin-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Las variables auxiliares, <span class="math notranslate nohighlight">\(\tilde{\xi}_{n}\)</span> y <span class="math notranslate nohighlight">\(\xi_{n},~ n=1,2,\dots,N\)</span> que <em><strong>miden el exceso de error con respecto a</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>, son conocidas como <em><strong>variables de relajación</strong></em>. Nótese que cualquier contribución de la función de coste, para un error obtenido con un valor absoluto menor o igual que <span class="math notranslate nohighlight">\(\varepsilon\)</span> es cero. La tarea de optimización previa, intenta <em><strong>estimar</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}, \theta_{0}\)</span> <em><strong>tal que valores de error mayores que</strong></em> <span class="math notranslate nohighlight">\(\epsilon\)</span> <em><strong>y menores que</strong></em> <span class="math notranslate nohighlight">\(-\epsilon\)</span> <em><strong>sean minimizados</strong></em>.</p></li>
<li><p>Por lo tanto, la tarea de optimización es equivalente a <em><strong>minimizar la función de pérdida empírica</strong></em></p>
<div class="math notranslate nohighlight">
\[
    \frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\sum_{n=1}^{N}\mathcal{L}(y_{n}, \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}),
    \]</div>
<p>donde la función de pérdida <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> es <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible.</p>
</li>
<li><p>Nótese que debido a que el problema de optimización con restricción <em><strong>involucra las variables de relajación</strong></em> con valores históricos, empleamos el <em><strong>kernel trick</strong></em>.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Solución</span></code></strong></p>
<ul>
<li><p>La solución de la tarea de optimización es obtenida introduciendo los <em><strong>multiplicadores de Lagrange</strong></em> y formando el correspondiente <em><strong>Lagrangiano</strong></em>. <em><strong>Habiendo obtenido los multiplicadores, la solución estaría dada por</strong></em></p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{\theta}}=\sum_{n=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})\boldsymbol{x}_{n},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\tilde{\lambda}_{n},\lambda_{n},~ n=1,2,\dots,N\)</span>, son los <em><strong>multiplicadores asociados con cada una de las restricciones</strong></em>.</p>
</li>
</ul>
<ul class="simple">
<li><p>Los <em><strong>multiplicadores de Lagrange son distintos de cero, solo para puntos</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span>, que corresponden a <em><strong>valores de error ya sean iguales o mayores que</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Estos son conocidos como <em><strong>vectores de soporte</strong></em>.</p></li>
<li><p>Puntos para los que el valor del <em><strong>puntaje de error es menor que</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>, <em><strong>corresponden a multiplicadores de Lagrange ceros</strong></em> y por lo tanto <em><strong>no son vectores de soporte y no participan en la construcción de la función de decisión</strong></em></p></li>
</ul>
<figure class="align-center" id="epsilon-tube-fig">
<a class="reference internal image-reference" href="_images/epsilon_tube.png"><img alt="_images/epsilon_tube.png" src="_images/epsilon_tube.png" style="width: 414.4px; height: 323.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.5 </span><span class="caption-text">Parámetros utilizados en la <em><strong>regresión de vectores de soporte (unidimensional)</strong></em>. Cuadrados rellenos son vectores de soporte, y los vacíos no lo son. Por lo tanto, los <em><strong>SV sólo pueden aparecer en el límite del tubo o fuera del tubo</strong></em></span><a class="headerlink" href="#epsilon-tube-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>El término de sesgo <span class="math notranslate nohighlight">\(\hat{\theta}_{0}\)</span> <em><strong>puede ser obtenido a partir de las ecuaciones</strong></em></p>
<div class="math notranslate nohighlight" id="equation-eq-sistem-bias">
<span class="eqno">(8.8)<a class="headerlink" href="#equation-eq-sistem-bias" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align}
    y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}&amp;=\varepsilon,\\[2mm]
    \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}&amp;=\varepsilon,
    \end{align} 
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(n\)</span> recorre todos los puntos que forman un <em><strong>subconjunto de vectores de soporte</strong></em>, esto es, puntos asociados con:</p>
<div class="math notranslate nohighlight">
\[
    \tilde{\lambda}_{n}&gt;0,~(\lambda_{n}&gt;0)\quad\text{y}\quad\tilde{\xi}_{n}=0,~(\xi_{n}=0).
    \]</div>
<p>En la práctica, <span class="math notranslate nohighlight">\(\hat{\theta}_{0}\)</span> <em><strong>es obtenido a partir del promedio de todas las ecuaciones en</strong></em> <a class="reference internal" href="#equation-eq-sistem-bias">(8.8)</a>.</p>
</li>
</ul>
<ul>
<li><p>Una vez <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}, \hat{\theta}_{0}\)</span> han sido obtenidos, estamos listos para <em><strong>realizar predicciones</strong></em>. Dado un valor <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, primero desarrollamos el <em><strong>mapeo (implícito) usando el mapeo de características</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\longmapsto\kappa(\cdot, \boldsymbol{x})\)</span> obtenemos</p>
<div class="math notranslate nohighlight">
\[
    \hat{y}(\boldsymbol{x})=\langle\hat{\boldsymbol{\theta}}, \kappa(\cdot, \boldsymbol{x})\rangle+\hat{\theta}_{0},
    \]</div>
<p>o bien,</p>
<div class="math notranslate nohighlight">
\[
    \hat{y}(\boldsymbol{x})=\sum_{n=1}^{N_{s}}(\tilde{\lambda}_{n}-\tilde{\lambda}_{n})\kappa(\boldsymbol{x}, \boldsymbol{x}_{n})+\hat{\theta}_{0},\quad\text{Predicción SVR},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(N_{s}\leq N\)</span>, es el <em><strong>número de multiplicadores de Lagrange distintos de cero</strong></em>. Nótese que esta última es una expansión en términos de funciones kernel no lineales.</p>
</li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Solución</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code></strong></p>
<ul>
<li><p>La tarea de <em><strong>optimización con desigualdades de restricción</strong></em>, satisface las <em><strong>condiciones de Karush–Kuhn–Tucker (KKT)</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    &amp;\frac{\partial L}{\partial\boldsymbol{\theta}}=\boldsymbol{0},~\frac{\partial L}{\partial\theta_{0}}=0,~\frac{\partial L}{\partial\tilde{\xi}_{n}}=0,~\frac{\partial L}{\partial\xi_{n}}=0,\\[2mm]
    &amp;\tilde{\lambda}_{n}(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon-\tilde{\xi}_{n})=0,\quad n=1,2,\dots,N,\\[2mm]
    &amp;\lambda_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}-\varepsilon-\xi_{n})=0,\quad n=1,2,\dots,N,\\[2mm]
    &amp;\tilde{\mu}_{n}\tilde{\xi}_{n}=0,\quad n=1,2,\dots,N,\\[2mm]
    &amp;\mu_{n}\xi_{n}=0,\quad n=1,2,\dots,N,\\[2mm]
    &amp;\tilde{\lambda}_{n}\geq0,~\lambda_{n}\geq0,~\tilde{\mu}_{n}\geq0,~\mu_{n}\geq0,\quad n=1,2,\dots,N,
    \end{align*}
    \end{split}\]</div>
<p>con respecto al <em><strong>Lagrangiano</strong></em></p>
<div class="math notranslate nohighlight" id="equation-eq-lagrangiano-general">
<span class="eqno">(8.9)<a class="headerlink" href="#equation-eq-lagrangiano-general" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    L(\boldsymbol{\theta}, \theta_{0}, \tilde{\xi}, \xi, \lambda, \mu)&amp;=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\left(\sum_{n=1}^{N}{\xi}_{n}+\sum_{n=1}^{N}\tilde{\xi}_{n}\right)\\
    &amp;+\sum_{n=1}^{N}\tilde{\lambda}_{n}(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon-\tilde{\xi}_{n})\\
    &amp;+\sum_{n=1}^{N}\lambda_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}-\varepsilon-\xi_{n})\\
    &amp;-\sum_{n=1}^{N}\tilde{\mu}_{n}\tilde{\xi}_{n}-\sum_{n=1}^{N}\mu_{n}\xi_{n},
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\tilde{\lambda}_{n},~\lambda_{n},~\tilde{\mu}_{n},~\mu_{n}\)</span> son los <em><strong>multiplicadores de Lagrange</strong></em>.</p>
</li>
</ul>
<ul>
<li><p><em><strong>Sumando las restricciones</strong></em> (multiplicando cada restricción por <span class="math notranslate nohighlight">\(\tilde{\lambda}_{n},\lambda_{n}, \tilde{\xi}_{n}, \xi_{n}\)</span>), se tiene que:</p>
<div class="math notranslate nohighlight">
\[
    \tilde{\lambda}_{n}\lambda_{n}((y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon)(\xi_{n}-\tilde{\xi}_{n})-2\tilde{\xi}_{n}\xi_{n})=0,
    \]</div>
<p>dado que los <em><strong>multiplicadores de Lagrange que participan en la solución</strong></em> <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> son aquellos <em><strong>distintos de cero</strong></em>, para puntos <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> correspondientes a valores de <em><strong>errores que sean mayores o iguales que</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>, esto es, por ejemplo <span class="math notranslate nohighlight">\((y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0})\geq\varepsilon,~\lambda_{n}\neq0\)</span>.</p>
<p>Si <span class="math notranslate nohighlight">\(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}=\varepsilon\)</span> (similar para <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}=\varepsilon\)</span>), entonces</p>
<div class="math notranslate nohighlight">
\[
    (i)~\tilde{\lambda}_{n}\lambda_{n}=0\quad\text{o}\quad(ii)~(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon)(\xi_{n}-\tilde{\xi}_{n})-2\tilde{\xi}_{n}\xi_{n}=0
    \]</div>
<p>Si <span class="math notranslate nohighlight">\(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}&gt;\varepsilon\)</span> (similar para <span class="math notranslate nohighlight">\(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}&lt;-\varepsilon\)</span>), entonces <span class="math notranslate nohighlight">\((i)~\tilde{\lambda}_{n}\lambda_{n}=0\)</span>. Además, <em><strong>basados en la tarea de optimización</strong></em>, seleccionando <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}, \theta_{0}\)</span> tales que <span class="math notranslate nohighlight">\(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon\rightarrow0\)</span> se tiene que</p>
<div class="math notranslate nohighlight">
\[
    (ii)~(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon)(\xi_{n}-\tilde{\xi}_{n})-2\tilde{\xi}_{n}\xi_{n}=0\Rightarrow\tilde{\xi}_{n}\xi_{n}=0.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Esto es</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-xin-xintilde-product">
<span class="eqno">(8.10)<a class="headerlink" href="#equation-xin-xintilde-product" title="Link to this equation">#</a></span>\[
\tilde{\xi}_{n}\xi_{n}=0,~\tilde{\lambda}_{n}\lambda_{n}=0,~n=1,2,\dots,N.
\]</div>
<ul class="simple">
<li><p><em><strong>Derivando el Lagrangiano</strong></em> <a class="reference internal" href="#equation-eq-lagrangiano-general">(8.9)</a> tenemos los resultados</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\boldsymbol{\theta}}&amp;=\boldsymbol{0}\Leftrightarrow\hat{\boldsymbol{\theta}}=\sum_{n=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})\boldsymbol{x}_{n};\quad\frac{\partial}{\partial\boldsymbol{\theta}}(\frac{1}{2}\|\boldsymbol{\theta}\|^{2})=\boldsymbol{\theta},~\frac{\partial}{\partial\boldsymbol{\theta}}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n})=\boldsymbol{x}_{n}\\
\frac{\partial L}{\partial\theta_{0}}&amp;=0\Leftrightarrow\sum_{n=1}^{N}\tilde{\lambda}_{n}=\sum_{n=1}^{N}\lambda_{n};\quad\frac{\partial}{\partial\theta_{0}}(\theta_{0})=1,\\
\frac{\partial L}{\partial\tilde{\xi}_{n}}&amp;=0\Leftrightarrow C-\tilde{\lambda}_{n}-\tilde{\mu}_{n}=0;\quad
\frac{\partial}{\partial\tilde{\xi}_{n}}\left(\sum_{n=1}^{N}\tilde{\xi}_{n}\right)=1\\
\frac{\partial L}{\partial\xi_{n}}&amp;=0\Leftrightarrow C-\lambda_{n}-\mu_{n}=0.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Para proceder a <em><strong>calcular los multiplicadores asociados a la tarea de optimización</strong></em>, consideraremos la <em><strong><code class="docutils literal notranslate"><span class="pre">Representación</span> <span class="pre">dual</span> <span class="pre">de</span> <span class="pre">Walfe</span></code></strong></em>. En esta representación es considerado el <em><strong>término de sesgo</strong></em> <span class="math notranslate nohighlight">\(\theta_{0}=0\)</span>. Además, <em><strong>consideraremos solo el uso de vectores de soportes</strong></em>, esto es valores asociados a <span class="math notranslate nohighlight">\(\boldsymbol{\xi}_{n}=\tilde{\boldsymbol{\xi}}_{n}=0\)</span>, para proceder con la <em><strong>representación dual respectiva</strong></em>. Esta representación es obtenida a partir del Lagrangiano <span class="math notranslate nohighlight">\(L(\boldsymbol{\theta}, \theta_{0}, \tilde{\xi}, \xi, \lambda, \mu)\)</span>.</p></li>
</ul>
<div class="proof property admonition" id="walfe_prop">
<p class="admonition-title"><span class="caption-number">Property 8.3 </span> (Representación dual de Walfe)</p>
<section class="property-content" id="proof-content">
<ul>
<li><p>Un <em><strong>problema de programación convexa</strong></em> es equivalente a</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \max_{\lambda\geq0}&amp;\quad L(\boldsymbol{\theta}, \boldsymbol{\lambda}),\\
    \text{sujeto a}&amp;\quad\frac{\partial}{\partial\boldsymbol{\theta}}L(\boldsymbol{\theta},\boldsymbol{\lambda})=\boldsymbol{0}.
    \end{split}\]</div>
<p>La última ecuación garantiza que <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> es un <em><strong>mínimo del Lagrangiano</strong></em>.</p>
</li>
<li><p>Considere el siguiente <em><strong>problema cuadrático</strong></em>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \text{minimizar:}&amp;\quad\frac{1}{2}\boldsymbol{\theta}^{T}\boldsymbol{\theta},\\
    \text{sujeto a:}&amp;\quad A\boldsymbol{\theta}\geq b.
    \end{align*}
    \end{split}\]</div>
<p>el cual admite la siguiente <em><strong>representación de Wolfe</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \text{minimizar:}&amp;\quad\frac{1}{2}\boldsymbol{\theta}^{T}\boldsymbol{\theta}-\lambda^{T}(A\boldsymbol{\theta}-\boldsymbol{b}),\\
    \text{sujeto a:}&amp;\quad\theta-A^{T}\lambda=0.
    \end{align*}
    \end{split}\]</div>
</li>
<li><p><em><strong>Solucionando la tarea de optimización de Wolfe</strong></em> con respecto a <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, podemos <em><strong>escribir el problema dual involucrando solo multiplicadores de Lagrange</strong></em>. De esta forma obtenemos otro problema cuadrático, pero más simple</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    
    \begin{align*}
    \max_{\lambda}&amp;\left\{\displaystyle{-\frac{1}{2}\boldsymbol{\lambda}^{T}AA^{T}\boldsymbol{\lambda}+\boldsymbol{\lambda}^{T}\boldsymbol{b}}\right\}\\
    \text{sujeto a:}&amp;\quad\lambda\geq0.
    \end{align*}
    \end{split}\]</div>
</li>
</ul>
</section>
</div><ul class="simple">
<li><p>Nótese en la propiedad anterior que, <span class="math notranslate nohighlight">\(\partial_{\boldsymbol{\theta}}(\frac{1}{2}\boldsymbol{\theta}^{T}\boldsymbol{\theta}-\lambda^{T}(A\boldsymbol{\theta}-\boldsymbol{b}))=\boldsymbol{\theta}-A^{T}\lambda\)</span>. Consideremos ahora la <em><strong>representación dual de Wolfe</strong></em> asociada a la tarea de optimización <a class="reference internal" href="#equation-eq-lagrangiano-general">(8.9)</a>. <em><strong>Con este fin, consideremos</strong></em> <span class="math notranslate nohighlight">\(A\boldsymbol{\theta}=\boldsymbol{\theta}\boldsymbol{x_{n}}+\theta_{0}\)</span> y <span class="math notranslate nohighlight">\(b=y_{n}-\varepsilon\)</span>, con <span class="math notranslate nohighlight">\(\tilde{\xi}_{n}, \xi_{n}=0\)</span>, con base en el estimador obtenido para <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span>, <em><strong>la tarea de optimización se convierte en la siguiente representación dual de Walfe</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-optimization-task-innerprod">
<span class="eqno">(8.11)<a class="headerlink" href="#equation-optimization-task-innerprod" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\text{minimizar respecto a}~\lambda,\tilde{\lambda}:&amp;\quad-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})(\tilde{\lambda}_{m}-\lambda_{m})\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}\\
&amp;\quad+\sum_{n=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})y_{n}-\varepsilon(\tilde{\lambda}_{n}+\lambda_{n})\\
\text{sujeto a:}&amp;\quad 0\leq\tilde{\lambda}_{n}\leq C\quad\text{y}\quad 0\leq\lambda_{n}\leq C,\quad n=1,2,\dots,N.\\
&amp;\quad~\sum_{n=1}^{N}\tilde{\lambda}_{n}=\sum_{n=1}^{N}\lambda_{n}.
\end{align*}
\end{split}\]</div>
<ul>
<li><p>El primer producto interior es reemplazado por <span class="math notranslate nohighlight">\(\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})\)</span> vía <em><strong>kernel trick</strong></em>. Nótese que la <em><strong>primera restricción proviene de la igualdad</strong></em> <span class="math notranslate nohighlight">\(C-\tilde{\lambda}-\tilde{\mu}_{n}=0\)</span>, con la cual se tiene que</p>
<div class="math notranslate nohighlight">
\[
    0\leq\tilde{\lambda}_{n}\leq\tilde{\lambda}_{n}+\tilde{\mu}_{n}\leq C\Rightarrow 0\leq\tilde{\lambda}_{n}\leq C,
    \]</div>
<p>análogamente, <span class="math notranslate nohighlight">\(0\leq\lambda_{n}\leq C,\quad n=1,2,\dots,N.\)</span></p>
</li>
</ul>
<ul class="simple">
<li><p>El <em><strong>primer término proviene la siguiente igualdad obtenida a partir del Lagrangiano</strong></em>, el <em><strong>segundo resulta de factorizar</strong></em> los términos que incluyen los autovalores, con <span class="math notranslate nohighlight">\(\xi_{n}=\tilde{\xi}_{n}=0\)</span> (<code class="docutils literal notranslate"><span class="pre">verifíquelo</span></code>) (<em><strong>vectores de soporte</strong></em>)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\|\boldsymbol{\theta}\|^{2}=\sum_{n=1}^{N}\sum_{m=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})(\tilde{\lambda}_{m}-\lambda_{m})\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}
\]</div>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La belleza de la forma de <em><strong>representación dual</strong></em> radica en que <em><strong>involucra los vectores de observación en forma de operaciones de producto interno</strong></em>. Así, cuando se resuelve la tarea en un <code class="docutils literal notranslate"><span class="pre">RKHS</span></code>, <a class="reference internal" href="#equation-optimization-task-innerprod">(8.11)</a> se convierte en</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{minimizar respecto a}~\lambda,\tilde{\lambda}:&amp;\quad-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})(\tilde{\lambda}_{m}-\lambda_{m})\kappa(\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m})\\
&amp;\quad+\sum_{n=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})y_{n}-\varepsilon(\tilde{\lambda}_{n}+\lambda_{n})
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Las condiciones <code class="docutils literal notranslate"><span class="pre">KKT</span></code> transmiten información importante. Los <em><strong>multiplicadores de Lagrange</strong></em>, <span class="math notranslate nohighlight">\(\tilde{\lambda}_{n},\lambda_{n}\)</span>, para puntos que obtienen un error menor que <span class="math notranslate nohighlight">\(\varepsilon\)</span>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
|\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}|&lt;\varepsilon,~\text{son cero}.
\]</div>
<ul class="simple">
<li><p>Por lo tanto, los <em><strong>multiplicadores de Lagrange son distintos de cero solo para puntos que obtienen un error igual a</strong></em> <span class="math notranslate nohighlight">\(\varepsilon~(\tilde{\xi}_{n}, \xi_{n} = 0)\)</span> o valores más grandes (<span class="math notranslate nohighlight">\(\tilde{\xi}_{n}, \xi_{n}&gt;0\)</span>) (<em><strong>vectores de soporte</strong></em>).</p></li>
<li><p>Debido a <a class="reference internal" href="#equation-xin-xintilde-product">(8.10)</a>, ya sea <span class="math notranslate nohighlight">\(\tilde{\xi}_{n}\)</span> o <span class="math notranslate nohighlight">\(\xi_{n}\)</span> <em><strong>pueden ser distintos de cero, pero no ambos</strong></em>. Esto también se aplica a los multiplicadores de Lagrange correspondientes.</p></li>
</ul>
</div>
</section>
<section id="regresion-kernel-ridge">
<h2><span class="section-number">8.8. </span>Regresión Kernel Ridge<a class="headerlink" href="#regresion-kernel-ridge" title="Link to this heading">#</a></h2>
<ul>
<li><p>En esta sección abordaremos la <em><strong>regresión kernel ridge vía su representación dual</strong></em>. La regresión ridge en su <em><strong>representación primal</strong></em> puede ser proyectada como:</p>
<div class="math notranslate nohighlight" id="equation-eq-kernel-ridge-reg">
<span class="eqno">(8.12)<a class="headerlink" href="#equation-eq-kernel-ridge-reg" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \text{minimizar con respecto a}\quad\boldsymbol{\theta}, \boldsymbol{\xi}:&amp;\quad J(\boldsymbol{\theta}, \boldsymbol{\xi})=\sum_{n=1}^{N}\xi_{n}^{2}+C\|\boldsymbol{\theta}\|^{2}\\
    \text{sujeto a}:&amp;\quad y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}=\boldsymbol{\xi}_{n},\quad n=1,2,\dots,N,
    \end{align*}
    \end{split}\]</div>
<p>con el siguiente <em><strong>Lagrangiano</strong></em></p>
<div class="math notranslate nohighlight">
\[
    L(\boldsymbol{\theta}, \boldsymbol{\xi}, \lambda)=\sum_{n=1}^{N}\xi_{n}^{2}+C\|\boldsymbol{\theta}\|^{2}+\sum_{n=1}^{N}\lambda_{n}(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\xi_{n}),\quad n=1,2,\dots,N.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Calculando las <em><strong>derivadas parciales</strong></em> <span class="math notranslate nohighlight">\(\partial_{\boldsymbol{\theta},\xi_{n}}=0,\, n=1,2,\dots,N\)</span> se tiene que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\boldsymbol{\theta}}&amp;=0\Leftrightarrow 2C\boldsymbol{\theta}-\sum_{n=1}^{N}\lambda_{n}\boldsymbol{x}_{n}=0\Leftrightarrow\boldsymbol{\hat{\theta}}=\frac{1}{2C}\sum_{n=1}^{N}\lambda_{n}\boldsymbol{x}_{n}\\
\frac{\partial L}{\partial\xi_{n}}&amp;=0\Leftrightarrow 2\xi_{n}-\lambda_{n}=0\Leftrightarrow\xi_{n}=\frac{\lambda_{n}}{2},\quad n=1,2,\dots,N.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Para obtener los multiplicadores de Lagrange, consideramos la siguiente <em><strong>formulación dual, obtenida al reemplazar los estimadores</strong></em> <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span> y <span class="math notranslate nohighlight">\(\xi_{n}\)</span> en la tarea de optimización <a class="reference internal" href="#equation-eq-kernel-ridge-reg">(8.12)</a></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{n=1}^{N}\xi_{n}^{2}+C\|\boldsymbol{\theta}\|^{2}&amp;+\sum_{n=1}^{N}\lambda_{n}(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\xi_{n})\\
&amp;=\frac{1}{4}\sum_{n=1}^{N}\lambda_{n}^{2}+C\frac{1}{4C^{2}}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})+\sum_{n=1}^{N}\lambda_{n}y_{n}\\
&amp;-\frac{1}{2C}\sum_{n=1}^{N}\lambda_{n}\lambda_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})-\frac{1}{2}\sum_{n=1}^{N}\lambda_{n}^{2}\\
&amp;=\sum_{n=1}^{N}\lambda_{n}y_{n}-\frac{1}{4C}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})-\frac{1}{4}\sum_{n=1}^{N}\lambda_{n}^{2}
\end{align*}
\end{split}\]</div>
<ul>
<li><p>La <em><strong>representación dual</strong></em> entrega la siguiente <em><strong>formulación</strong></em></p>
<div class="math notranslate nohighlight">
\[
    \text{minimizar respecto a}~\lambda:\quad\sum_{n=1}^{N}\lambda_{n}y_{n}-\frac{1}{4C}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})-\frac{1}{4}\sum_{n=1}^{N}\lambda_{n}^{2},
    \]</div>
<p>aquí <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}\)</span> fue reemplazado por <span class="math notranslate nohighlight">\(\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})\)</span>, de acuerdo al kernel trick.</p>
</li>
</ul>
<ul class="simple">
<li><p><em><strong>Diferenciando con respecto a</strong></em> <span class="math notranslate nohighlight">\(\lambda\)</span> obtenemos:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{y}-\frac{1}{2C}K\boldsymbol{\lambda}-\frac{1}{2}\boldsymbol{\lambda}=0&amp;\Rightarrow\left(\frac{1}{2C}K+\frac{1}{2}I\right)\boldsymbol{\lambda}=\boldsymbol{y}\\
&amp;\Rightarrow(K+CI)\boldsymbol{\lambda}=2C\boldsymbol{y}\\[2mm]
&amp;\Rightarrow\boldsymbol{\lambda}=2C(K+CI)^{-1}\boldsymbol{y}.
\end{align*}
\end{split}\]</div>
<ul>
<li><p>Dado que <span class="math notranslate nohighlight">\(\displaystyle{\boldsymbol{\hat{\theta}}=\frac{1}{2C}\sum_{n=1}^{N}\lambda_{n}\boldsymbol{x}_{n}}\)</span>, <em><strong>usando kernel trick</strong></em> se tiene que</p>
<div class="math notranslate nohighlight">
\[
    \hat{y}(\boldsymbol{x})=\langle\boldsymbol{\hat{\theta}}, \kappa(\cdot,\boldsymbol{x})\rangle=\left\langle\frac{1}{2C}\sum_{n=1}^{N}2C(\kappa(\cdot, \boldsymbol{x}_{n})+C)^{-1}y_{n}, \kappa(\cdot, \boldsymbol{x}_{n})\right\rangle=\boldsymbol{y}^{T}(K+CI)^{-1}\kappa(\boldsymbol{x}).
    \]</div>
<p>Nótese que en este caso no <em><strong>fue necesario el supuesto de invertibilidad</strong></em> para la matriz <span class="math notranslate nohighlight">\(K\)</span>.</p>
</li>
</ul>
<figure class="align-center" id="nonlinear-reg-curve">
<img alt="_images/nonlinear_reg_curve.png" src="_images/nonlinear_reg_curve.png" />
<figcaption>
<p><span class="caption-number">Fig. 8.6 </span><span class="caption-text"><em><strong>Tubo alrededor de la curva de regresión no lineal</strong></em>. Los puntos fuera del tubo (denotados por estrellas) tienen o bien <span class="math notranslate nohighlight">\(\tilde{\xi} &gt; 0\)</span> y
<span class="math notranslate nohighlight">\(\xi = 0\)</span> o <span class="math notranslate nohighlight">\(\xi &gt; 0\)</span> y <span class="math notranslate nohighlight">\(\tilde{\xi} = 0\)</span>. El resto de los puntos tienen <span class="math notranslate nohighlight">\(\tilde{\xi} = \xi = 0\)</span> (SV). Los <em><strong>puntos que están dentro del tubo corresponden a multiplicadores de Lagrange iguales a cero</strong></em>.</span><a class="headerlink" href="#nonlinear-reg-curve" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="id6">
<h2><span class="section-number">8.9. </span>Máquinas de vectores de soporte<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Antes de abordar el <em><strong>modelo de clasificación</strong></em>, <em><strong>máquinas de vectores de soporte (SVM)</strong></em> revisemos la definición de un clasificador</p></li>
</ul>
<div class="proof definition admonition" id="def_clasificador_general">
<p class="admonition-title"><span class="caption-number">Definition 8.4 </span></p>
<section class="definition-content" id="proof-content">
<p>Un <em><strong>clasificador, es una función</strong></em> definida como</p>
<div class="math notranslate nohighlight">
\[
\omega(x):=\text{argmax}_{\omega}f_{\omega}(x)
\]</div>
<p>donde para cada clase <span class="math notranslate nohighlight">\(\omega\)</span>, se define su <em><strong>función discriminante</strong></em> <span class="math notranslate nohighlight">\(f_{\omega}\)</span>. El <em><strong>grado de pertenencia del valor</strong></em> <span class="math notranslate nohighlight">\(x\)</span> <em><strong>a la clase</strong></em> <span class="math notranslate nohighlight">\(\omega\)</span> es <span class="math notranslate nohighlight">\(f_{\omega}(x)\)</span>. Ademas <span class="math notranslate nohighlight">\(\omega(x)\)</span> es la <em><strong>clase a la que el objeto</strong></em> <span class="math notranslate nohighlight">\(x\)</span> <em><strong>pertenece en mayor grado</strong></em>.</p>
</section>
</div><ul class="simple">
<li><p>Cuando en un método de <em><strong>clasificación Bayesiana</strong></em> desconocemos estadísticos subyacentes, una alternativa es usar <em><strong>técnicas de aprendizaje discriminante</strong></em>, y adoptar una función discriminante <span class="math notranslate nohighlight">\(f\)</span>, que <em><strong>efectúa la clasificación correspondiente y trata de optimizarla</strong></em>, como se minimiza la función de pérdida empírica</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
J(f)=\sum_{n=1}^{N}\mathcal{L}(y_{n}, f(\boldsymbol{x}_{n})),\quad\text{donde}\quad y_{n}=
\begin{cases}
+1,&amp; \text{si}~\boldsymbol{x}_{n}\in\omega_{1},\\
-1,&amp; \text{si}~\boldsymbol{x}_{n}\in\omega_{2}.
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>Para una tarea de <em><strong>clasificación binaria</strong></em>, la función de pérdida <span class="math notranslate nohighlight">\((0, 1)\)</span>, definida de la siguiente forma, puede ser utilizada en la tarea de optimización</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L}(y, f(x))=
\begin{cases}
1,&amp; \text{si}~ yf(x)\leq0\\
0,&amp; \text{otro caso}
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>El principal problema de esta función de pérdida es que <em><strong>su optimización resulta ser una tarea compleja, debido a que es discontinua</strong></em>. Se han utilizado alternativas para esta función de pérdida, con el fin de solventar este problema. En esta sección centraremos nuestra atención en la <em><strong>función de pérdida hinge</strong></em> definida como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-loss-hinge">
<span class="eqno">(8.13)<a class="headerlink" href="#equation-eq-loss-hinge" title="Link to this equation">#</a></span>\[
\mathcal{L}(y, f(x))=\max(0, \rho-yf(x))
\]</div>
<ul class="simple">
<li><p>Esto es, si el <em><strong>signo del producto</strong></em> entre el label real <span class="math notranslate nohighlight">\((y)\)</span> y el predicho por la función discriminante <span class="math notranslate nohighlight">\((f(x))\)</span> es <em><strong>positivo y mayor que un umbral definido</strong></em> por el usuario, <span class="math notranslate nohighlight">\(\rho\geq0\)</span>, <em><strong>la función de pérdida es cero</strong></em>. En caso contrario, la función de pérdida exhibe un crecimiento lineal.</p></li>
</ul>
<figure class="align-center" id="fig-hinge-lossfn">
<a class="reference internal image-reference" href="_images/hinge_lossfn.png"><img alt="_images/hinge_lossfn.png" src="_images/hinge_lossfn.png" style="width: 307.20000000000005px; height: 232.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.7 </span><span class="caption-text">Función de pérdida <em><strong>hinge</strong></em>, para la tarea de clasificación, <span class="math notranslate nohighlight">\(\tau = y\boldsymbol{\theta}^{T}\boldsymbol{x}\)</span>.</span><a class="headerlink" href="#fig-hinge-lossfn" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Consideraremos en esta sección, la <em><strong>función discriminante en el espacio RKHS</strong></em></p>
<div class="math notranslate nohighlight">
\[
    f(\boldsymbol{x})=\theta_{0}+\langle\boldsymbol{\theta}, \phi(\boldsymbol{x})\rangle,
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\phi(\boldsymbol{x})\)</span> es el mapeo de características. <em><strong>Proyectamos la tarea de optimización como un problema lineal en el espacio de inputs</strong></em>, <span class="math notranslate nohighlight">\(\mathbb{R}^{l}\)</span>, y al final la información del kernel será implantada usando el <em><strong>kernel trick</strong></em>.</p>
</li>
<li><p>La tarea de diseñar un clasificador lineal, ahora es <em><strong>equivalente a minimizar la función de costo</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-cost-linear-class">
<span class="eqno">(8.14)<a class="headerlink" href="#equation-eq-cost-linear-class" title="Link to this equation">#</a></span>\[
J(\boldsymbol{\theta}, \theta_{0})=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\sum_{n=1}^{N}\mathcal{L}_{p}(y_{n}, \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})
\]</div>
<ul class="simple">
<li><p>Alternativamente,  empleando <em><strong>variables de relajación (slack)</strong></em> y <em><strong>siguiendo un razonamiento similar a la regresión de vectores de soporte (SVR)</strong></em>, minimizar <a class="reference internal" href="#equation-eq-cost-linear-class">(8.14)</a> es equivalente a:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{minimizar con respecto a}~\boldsymbol{\theta}, \theta_{0}, \boldsymbol{\xi}:&amp;\quad J(\boldsymbol{\theta}, \boldsymbol{\xi})=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\sum_{n=1}^{N}\boldsymbol{\xi}_{n}\\
\text{sujeto a:}&amp;\quad y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\geq\rho-\boldsymbol{\xi}_{n},\\[2mm]
&amp;\quad\boldsymbol{\xi}_{n}\geq 0,\quad n=1,2,\dots,N.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>La constante <span class="math notranslate nohighlight">\(C\)</span> se encarga de <em><strong>maximizar el margen mientras que se evita el sobreajuste</strong></em>. <em><strong>Adoptaremos de ahora en adelante</strong></em> <span class="math notranslate nohighlight">\(\rho=1\)</span> sin pérdida de generalidad. Nótese que <span class="math notranslate nohighlight">\(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\geq1\)</span> si <span class="math notranslate nohighlight">\(\xi_{n}=0\)</span>, en este caso el <em><strong>margen de error sería nulo</strong></em>. Por otro lado, un <em><strong>margen de error es cometido si</strong></em> <span class="math notranslate nohighlight">\(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\leq 1\)</span>, correspondiente a <span class="math notranslate nohighlight">\(\xi_{n}&gt;0\)</span>. Por lo tanto, nuestro objetivo para la tarea de optimización es, encontrar <span class="math notranslate nohighlight">\(\boldsymbol{\theta}, \theta_{0}, \xi_{n}\)</span> óptimos, tales que <span class="math notranslate nohighlight">\(\xi_{n}\)</span> <em><strong>sea tan pequeño como sea posible</strong></em>.</p></li>
</ul>
</section>
<section id="clases-linealmente-separables-clasificador-de-maximo-margen">
<h2><span class="section-number">8.10. </span>Clases linealmente separables: Clasificador de máximo margen<a class="headerlink" href="#clases-linealmente-separables-clasificador-de-maximo-margen" title="Link to this heading">#</a></h2>
<ul>
<li><p>Asumiendo que las <em><strong>clases son linealmente separables</strong></em>, hay un <em><strong>número infinito de clasificadores que solucionan la tarea exactamente</strong></em>, sin error, en el conjunto de entrenamiento. Se puede ver que de estos infinitos hiperplanos que solucionan la tarea, podemos identificar un subconjunto tal que</p>
<div class="math notranslate nohighlight">
\[
    y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\geq 1,\quad n=1,2,\dots,N,
    \]</div>
<p>el cual <em><strong>garantiza que</strong></em> <span class="math notranslate nohighlight">\(\xi_{n}=0,~n=1,2,\dots,N\)</span>. Por lo tanto, para <em><strong>clases linealmente separables, la tarea de optimización previa es equivalente a</strong></em></p>
<div class="math notranslate nohighlight" id="equation-eq-opt-task-linear-class-sep">
<span class="eqno">(8.15)<a class="headerlink" href="#equation-eq-opt-task-linear-class-sep" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \text{minimizar con respecto a}~\theta:&amp;\quad\frac{1}{2}\|\boldsymbol{\theta}\|^{2}\\
    \text{sujeto a:}&amp;\quad y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\geq 1,\quad n=1,2,\dots,N.
    \end{align*}
    \end{split}\]</div>
</li>
</ul>
<ul>
<li><p>En otras palabras, de este conjunto infinito de clasificadores lineales (ver <a class="reference internal" href="#inf-correct-classification"><span class="std std-numref">Fig. 8.8</span></a>), los cuales <em><strong>solucionan la tarea de optimización</strong></em>, y clasifican correctamente todos los patrones, <em><strong>se selecciona aquel que tiene mínima norma</strong></em>. Veremos mas adelante que, <em><strong>la norma</strong></em> <span class="math notranslate nohighlight">\(\|\boldsymbol{\theta}\|\)</span> <em><strong>está directamente relacionada con el margen</strong></em> formado por el respectivo clasificador. Cada hiperplano en el espacio está descrito por la ecuación</p>
<div class="math notranslate nohighlight">
\[
    
    f(\boldsymbol{x})=\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}=0.
    \]</div>
</li>
</ul>
<figure class="align-center" id="inf-correct-classification">
<a class="reference internal image-reference" href="_images/inf_correct_classification.png"><img alt="_images/inf_correct_classification.png" src="_images/inf_correct_classification.png" style="width: 433.6px; height: 310.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.8 </span><span class="caption-text">Número infinito de clasificadores lineales que pueden clasificar correctamente todos los patrones de una clase linealmente separable.</span><a class="headerlink" href="#inf-correct-classification" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>De la <em><strong>geometría analítica</strong></em> sabemos que la <em><strong>dirección en espacio es controlada por</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> y su posición por <span class="math notranslate nohighlight">\(\theta_{0}\)</span>. De todos los hiperplanos que solucionan la tarea de optimización y tienen la misma dirección (comparten <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>), seleccionamos <span class="math notranslate nohighlight">\(\theta_{0}\)</span> tal que, el <em><strong>hiperplano queda dentro de dos clases</strong></em>, de modo que su <em><strong>distancia a los puntos más cercanos es siempre la misma</strong></em> para las dos clases.</p></li>
</ul>
<figure class="align-center" id="fig-svm-class-sep">
<a class="reference internal image-reference" href="_images/svm_class_sep.png"><img alt="_images/svm_class_sep.png" src="_images/svm_class_sep.png" style="width: 463.20000000000005px; height: 320.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.9 </span><span class="caption-text">Las líneas punteadas, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x} + \theta_{0} =±1\)</span>, que pasan por los puntos más cercanos, son paralelas al clasificador respectivo y definen el margen (ancho de banda) <span class="math notranslate nohighlight">\(2/\|\boldsymbol{\theta}\|\)</span>.</span><a class="headerlink" href="#fig-svm-class-sep" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>De la <em><strong>geometría básica</strong></em> sabemos que la <em><strong>distancia de un punto</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> <em><strong>al hiperplano</strong></em> está dada por</p>
<div class="math notranslate nohighlight">
\[
    z=\frac{|\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}|}{\|\boldsymbol{\theta}\|}
    \]</div>
<p>la cual es claramente cero cuando <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> está dentro del hiperplano <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}=0\)</span>. Donde los <em><strong>parámetros</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> <em><strong>y</strong></em> <span class="math notranslate nohighlight">\(\theta_{0}\)</span> <em><strong>pueden ser escalados apropiadamente</strong></em>, por ejemplo, usando un factor, digamos, <span class="math notranslate nohighlight">\(a\)</span>, sin afectar la geometría del plano.</p>
</li>
<li><p>De esta forma, podemos hacer la distancia del punto más cercano, de los puntos más cercanos entre las dos clases al hiperplano, igual a <span class="math notranslate nohighlight">\(1/\|\boldsymbol{\theta}\|\)</span>, donde <span class="math notranslate nohighlight">\(|\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}|=1\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>Equivalentemente, el <em><strong>escalamiento garantiza que</strong></em>: <span class="math notranslate nohighlight">\(f(\boldsymbol{x})=\pm 1\)</span>, <em><strong>si</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> <em><strong>es el punto más cercano al hiperplano</strong></em>, y dependiendo de si el punto <span class="math notranslate nohighlight">\(x\)</span> pertenece a <span class="math notranslate nohighlight">\(\omega_{1} (+1)\)</span> o <span class="math notranslate nohighlight">\(\omega_{2} (-1)\)</span>. Los dos hiperplanos se pueden visualizar en la <a class="reference internal" href="#fig-svm-class-sep"><span class="std std-numref">Fig. 8.9</span></a> <span class="math notranslate nohighlight">\((f(\boldsymbol{x})=\pm1)\)</span>. <em><strong>Estos dos hiperplanos definen el margen correspondiente de longitud</strong></em> <span class="math notranslate nohighlight">\(2/\|\boldsymbol{\theta}\|\)</span> para cada iteración.</p></li>
</ul>
<ul>
<li><p><em><strong>Cualquier clasificador construido de la forma anterior, soluciona la tarea de optimización</strong></em>, y además satisface las siguientes propiedades:</p>
<ul class="simple">
<li><p>Tiene un margen de longitud igual a: <span class="math notranslate nohighlight">\(\displaystyle{1/\|\boldsymbol{\theta}\|+1/\|\boldsymbol{\theta}}\|\)</span></p></li>
<li><p>Además:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}\geq 1,~\boldsymbol{x}_{n}\in\omega_{1}~\text{y}~\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}\leq-1,~\boldsymbol{x}_{n}\in\omega_{2}
    \]</div>
<p>Por lo tanto la tarea de optimización <a class="reference internal" href="#equation-eq-opt-task-linear-class-sep">(8.15)</a> <em><strong>calcula el clasificador lineal que maximiza el margen sujeto a las respectivas restricciones</strong></em>.</p>
</li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Solución</span></code></strong></p>
<ul>
<li><p>Siguiendo <em><strong>pasos similares a SVR</strong></em>, la solución esta dada por, <em><strong>combinación lineal de un subconjunto de muestras de entrenamiento</strong></em>, esto es:</p>
<div class="math notranslate nohighlight">
\[
    
    \hat{\boldsymbol{\theta}}=\sum_{n=1}^{N_{s}}\lambda_{n}y_{n}\boldsymbol{x}_{n},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(N_{s}\)</span> es el número de multiplicadores de Lagrange distintos de cero. <em><strong>Multiplicadores de Lagrange asociados a los puntos más cercanos al clasificador, esto es, puntos que satisfacen la restricción con igualdad</strong></em> <span class="math notranslate nohighlight">\((y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})=1)\)</span>, <em><strong>son distintos de cero</strong></em>. Estos son conocidos como <em><strong>vectores de soporte</strong></em>. Multiplicadores de Lagrange correspondientes a puntos tales que <span class="math notranslate nohighlight">\((y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})&gt;1)\)</span>, son iguales a cero.</p>
</li>
</ul>
<ul>
<li><p>Para el caso mas general, <em><strong>en el espacio RKHS</strong></em>, tenemos que:</p>
<div class="math notranslate nohighlight">
\[
    
    \hat{\theta}(\cdot)=\sum_{n=1}^{N_{s}}\lambda_{n}y_{n}\kappa(\cdot, \boldsymbol{x}_{n}),
    \]</div>
<p>estimación la cual entrega la siguiente regla de predicción. <em><strong>Dado un valor desconocido</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, <em><strong>su clase es predicha de acuerdo al signo de</strong></em>:</p>
<div class="math notranslate nohighlight">
\[
    \hat{y}(\boldsymbol{x})=\sum_{n=1}^{N_{s}}\lambda_{n}y_{n}\kappa(\boldsymbol{x}, \boldsymbol{x}_{n})+\hat{\theta}_{0}:\quad\text{Predicción SVM},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\hat{\theta}_{0}\)</span> <em><strong>es obtenido solucionando todas las restricciones con</strong></em> <span class="math notranslate nohighlight">\(\lambda_{n}\neq0\)</span>, correspondientes a:</p>
<div class="math notranslate nohighlight">
\[
    y_{n}(\hat{\boldsymbol{\theta}}^{T}\boldsymbol{x}_{n}+\hat{\theta}_{0})-1=0,\quad n=1,2,\dots,N_{s},
    \]</div>
<p>la cual <em><strong>para el caso RKHS se convierte</strong></em> en:</p>
<div class="math notranslate nohighlight" id="equation-mean-sol-fortheta0">
<span class="eqno">(8.16)<a class="headerlink" href="#equation-mean-sol-fortheta0" title="Link to this equation">#</a></span>\[
    y_{n}\left(\sum_{m=1}^{N_{s}}\lambda_{m}y_{m}\kappa(\boldsymbol{x}_{m}, \boldsymbol{x}_{n})+\hat{\theta}_{0}\right)-1=0,\quad n=1,2,\dots,N_{s},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\hat{\theta}_{0}\)</span> <em><strong>es obtenido por medio del promedio entre las soluciones de</strong></em> <a class="reference internal" href="#equation-mean-sol-fortheta0">(8.16)</a>. Aunque la solución es única, los correspondientes multiplicadores <span class="math notranslate nohighlight">\(\lambda_{m}\)</span> pueden no ser únicos. Finalmente <em><strong>el número de vectores de soporte está relacionado con la capacidad de generalización del clasificador</strong></em>. Entre mas pequeño es el número de vectores de soporte, mejor será la generalización esperada (ver <span id="id7">[<a class="reference internal" href="biblio.html#id19" title="Konstantinos Koutroumbas and Sergios Theodoridis. Pattern recognition. Academic Press, 2008.">Koutroumbas and Theodoridis, 2008</a>, <a class="reference internal" href="biblio.html#id18" title="佐土原健. N. cristianini and j. shawe-taylor: an introduction to support vector machines, cambridge university press (2000). 人工知能, 16(2):337–337, 2001.">佐土原健, 2001</a>]</span>).</p>
</li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code></strong></p>
<ul class="simple">
<li><p>Procedemos de forma <em><strong>similar a SVR</strong></em>. El <em><strong>Lagrangiano asociado a la tarea de clasificación</strong></em> está dado por:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(\boldsymbol{\theta}, \theta_{0}, \lambda)=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}-\sum_{n=1}^{N}\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1),
\]</div>
<ul class="simple">
<li><p>Calculando las <em><strong>derivadas parciales</strong></em> <span class="math notranslate nohighlight">\(\partial_{\boldsymbol{\theta}}=0\)</span> y <span class="math notranslate nohighlight">\(\partial_{\theta_{0}}=0\)</span>, se tienen las condiciones siguientes</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-opt-task-svm-class">
<span class="eqno">(8.17)<a class="headerlink" href="#equation-opt-task-svm-class" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\boldsymbol{\theta}}&amp;=0\Leftrightarrow\boldsymbol{\theta}-\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}=0\Rightarrow\hat{\boldsymbol{\theta}}=\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}\\
\frac{\partial L}{\partial\theta_{0}}&amp;=0\Leftrightarrow-\sum_{n=1}^{N}\lambda_{n}y_{n}=0\Rightarrow\sum_{n=1}^{N}\lambda_{n}y_{n}=0,\\
\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1)&amp;=0,\quad n=1,2,\dots,N,\\[2mm]
\lambda_{n}&amp;\geq0,\quad n=1,2,\dots,N.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Los <em><strong>multiplicadores son obtenidos por medio de la representación dual</strong></em> de <a class="reference internal" href="#equation-opt-task-svm-class">(8.17)</a> en el Lagrangiano</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
L(\boldsymbol{\theta}, \theta_{0}, \lambda)&amp;=\frac{1}{2}\left\langle\sum_{m=1}^{N}\lambda_{m}y_{m}\boldsymbol{x}_{m},\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}\right\rangle-\sum_{n=1}^{N}\lambda_{n}(y_{n}(\boldsymbol{\theta}\boldsymbol{x}_{n}+\theta_{0})-1)\\
&amp;=\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}-\sum_{n=1}^{N}\sum_{n=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}+\sum_{n=1}^{N}\lambda_{n}\\
&amp;=\sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>En base a lo anterior, <em><strong>la siguiente es la representación dual de la tarea de optimización</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{minimizar con respecto a }~\lambda:&amp;\quad\sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}\\
\text{sujeto a}:&amp;\quad\lambda_{n}\geq0,\quad\sum_{n=1}^{N}\lambda_{n}y_{n}=0.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Para el caso en que la <em><strong>tarea de optimización original es mapeada en el espacio RKHS</strong></em>, la función de costo viene dada por</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})
\]</div>
<ul class="simple">
<li><p>De acuerdo con la restricción: <span class="math notranslate nohighlight">\(\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1)=0,~n=1,2,\dots,N\)</span>, si <span class="math notranslate nohighlight">\(\lambda_{n}\neq0\)</span>, entonces necesariamente <span class="math notranslate nohighlight">\(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})=1\)</span>. Estos son los <em><strong>puntos mas cercanos, desde cada clase al clasificador</strong></em> (distancia <span class="math notranslate nohighlight">\(1/\|\boldsymbol{\theta}\|\)</span>). Estos puntos caen en cualquiera de los hiperplanos formando el borde del margen. <em><strong>Estos puntos se conocen como los vectores de soporte</strong></em> y las respectivas restricciones son conocidas como <em><strong>restricciones activas</strong></em>. El resto de puntos asociados con <span class="math notranslate nohighlight">\(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})&gt;1\)</span>, los cuales quedan por fuera del margen corresponden a <span class="math notranslate nohighlight">\(\lambda_{n}=0\)</span>, y las restricciones asociadas son conocidas como las <em><strong>restricciones inactivas</strong></em>.</p></li>
</ul>
</section>
<section id="clases-no-separables">
<h2><span class="section-number">8.11. </span>Clases no separables<a class="headerlink" href="#clases-no-separables" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Centraremos ahora nuestra atención en un <em><strong>caso mas realista en el que las clases se superponen</strong></em>. En este caso <em><strong>no existe un clasificador lineal que pueda clasificar correctamente todos los puntos</strong></em>, algunos errores de clasificación pueden ocurrir. Existen tres tipos de puntos mal clasificados.</p></li>
</ul>
<ol class="arabic">
<li><p><em><strong>Puntos que se encuentran en la frontera o fuera del margen y en el lado correcto del clasificador</strong></em>, esto es</p>
<div class="math notranslate nohighlight">
\[
    y_{n}f(\boldsymbol{x}_{n})\geq1.
    \]</div>
<p>Estos puntos <em><strong>no cometen margen de error</strong></em>, esto es: <span class="math notranslate nohighlight">\( \xi_{n}=0.\)</span></p>
</li>
<li><p><em><strong>Puntos que se encuentran en el lado correcto del clasificador, pero se encuentran dentro del margen</strong></em>, esto es</p>
<div class="math notranslate nohighlight">
\[
    0&lt;y_{n}f(\boldsymbol{x}_{n})&lt;1.
    \]</div>
<p>Estos puntos <em><strong>cometen un margen de error</strong></em>, y se tiene que: <span class="math notranslate nohighlight">\(0&lt;\xi_{n}&lt;1\)</span>.</p>
</li>
<li><p><em><strong>Puntos que se encuentran en el lugar equivocado del clasificador</strong></em>, esto es,</p>
<div class="math notranslate nohighlight">
\[
    y_{n}f(\boldsymbol{x}_{n})\leq0.
    \]</div>
<p>Estos puntos <em><strong>cometen un error</strong></em>, y se tiene que: <span class="math notranslate nohighlight">\(\xi_{n}\geq1.\)</span></p>
</li>
</ol>
<figure class="align-center" id="non-separable-classes-svm">
<a class="reference internal image-reference" href="_images/non_separable_classes_svm.png"><img alt="_images/non_separable_classes_svm.png" src="_images/non_separable_classes_svm.png" style="width: 405.90000000000003px; height: 317.7px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.10 </span><span class="caption-text">(1) <em><strong>puntos fuera o en los límites del margen y se clasifican correctamente</strong></em> (<span class="math notranslate nohighlight">\(\xi_{n} = 0\)</span>); (2) <em><strong>puntos dentro del margen y clasificados correctamente</strong></em> (<span class="math notranslate nohighlight">\(0 &lt; \xi_{n} &lt; 1\)</span>), denotados por círculos; y (3) <em><strong>puntos mal clasificados</strong></em>, denotados por un cuadrado <span class="math notranslate nohighlight">\((\xi_{n}\geq 1\)</span>).</span><a class="headerlink" href="#non-separable-classes-svm" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Nuestro objetivo es <em><strong>estimar el hiperplano clasificador que maximiza el margen</strong></em> y al mismo tiempo <em><strong>mantiene el número de errores (incluyendo el margen del error) tan pequeño como sea posible</strong></em>.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Solución</span></code></strong></p>
<ul>
<li><p>Una vez mas la solución está dada por una <em><strong>combinación lineal de un subconjunto de puntos de entrenamiento</strong></em></p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\hat{\theta}}=\sum_{n=1}^{N_{s}}\lambda_{n}y_{n}\boldsymbol{x}_{n},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\lambda_{n},~n=1,2,\dots,N\)</span>, son los <em><strong>multiplicadores de Lagrange asociados con los vectores de soporte</strong></em>. Los puntos <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> pueden satisfacer cualquiera de los <em><strong>tres casos para tipos de datos de entrenamiento</strong></em>.</p>
</li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code></strong></p>
<ul class="simple">
<li><p>El <em><strong>Lagrangiano asociado con la tarea de optimización</strong></em> está dado por:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(\boldsymbol{\theta}, \theta_{0}, \xi, \lambda)=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\sum_{n=1}^{N}\xi_{n}-\sum_{n=1}^{N}\mu_{n}\xi_{n}-\sum_{n=1}^{N}\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1+\xi_{n})
\]</div>
<ul class="simple">
<li><p>Entonces</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-class-svm-nonsep">
<span class="eqno">(8.18)<a class="headerlink" href="#equation-eq-class-svm-nonsep" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\boldsymbol{\theta}}&amp;=0\Leftrightarrow\boldsymbol{\theta}-\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}=0\Rightarrow\hat{\boldsymbol{\theta}}=\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}\\
\frac{\partial L}{\partial\theta_{0}}&amp;=0\Leftrightarrow-\sum_{n=1}^{N}\lambda_{n}y_{n}=0\Rightarrow\sum_{n=1}^{N}\lambda_{n}y_{n}=0\\
\frac{\partial L}{\partial\xi_{n}}&amp;=0\Leftrightarrow C-\mu_{n}-\lambda_{n}=0,\\[2mm]
\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1+\xi_{n})&amp;=0,\quad n=1,2,\dots,N,\\[2mm]
\mu_{n}\xi_{n}&amp;=0,\quad n=1,2,\dots,N,\\[2mm]
\mu_{n}\geq,~\lambda_{n}\geq&amp;0,\quad n=1,2,\dots,N.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>El <em><strong>problema dual</strong></em> entonces es proyectado como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{maximizar con respecto a }~\lambda:&amp;\quad\sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}\\
\text{sujeto a}:&amp;\quad0\leq\lambda_{n}\leq C, n=1,2,\dots,N,\quad\sum_{n=1}^{N}\lambda_{n}y_{n}=0.
\end{align*}
\end{split}\]</div>
<ul>
<li><p>Cuando se trabaja <em><strong>en un espacio RKHS, la función de coste se convierte en</strong></em></p>
<div class="math notranslate nohighlight">
\[
    \sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})
    \]</div>
<p>Obsérvese que la única <em><strong>diferencia con respecto a su contraparte linealmente separable</strong></em> es la <em><strong>existencia de</strong></em> <span class="math notranslate nohighlight">\(C\)</span> <em><strong>en las restricciones de desigualdad</strong></em> para <span class="math notranslate nohighlight">\(\lambda_{n}\)</span>.</p>
</li>
</ul>
<ul>
<li><p>A partir de <a class="reference internal" href="#equation-eq-class-svm-nonsep">(8.18)</a> concluimos que para todos los <em><strong>puntos fuera del margen, y en el lado correcto del clasificador</strong></em>, que corresponden a <span class="math notranslate nohighlight">\(\xi_{n} = 0\)</span>, tenemos</p>
<div class="math notranslate nohighlight">
\[
    y_{n}(\boldsymbol{\theta}\boldsymbol{x}_{n}+\theta_{0})&gt;1,
    \]</div>
<p>por lo tanto, <span class="math notranslate nohighlight">\(\lambda_{n}=0\)</span>. Esto es, estos <em><strong>puntos no participan en la formación de la solución</strong></em> <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span> en <a class="reference internal" href="#equation-eq-class-svm-nonsep">(8.18)</a>.</p>
</li>
<li><p>Tenemos <span class="math notranslate nohighlight">\(\lambda_{n}\neq0\)</span> sólo para los <em><strong>puntos en los hiperplanos de la frontera, dentro o fuera del margen</strong></em>, pero en el lado equivocado del clasificador. Estos son los <em><strong>vectores de soporte</strong></em>.</p></li>
<li><p>Para los <em><strong>puntos que se encuentran dentro del margen o fuera pero en el lado equivocado</strong></em>, <span class="math notranslate nohighlight">\(\xi_{n}&gt;0\)</span>; por lo tanto, a partir de <a class="reference internal" href="#equation-eq-class-svm-nonsep">(8.18)</a> se tiene que <span class="math notranslate nohighlight">\(\mu_{n} = 0\)</span> y además <span class="math notranslate nohighlight">\(\lambda_{n}=C\)</span>.</p></li>
<li><p>Los <em><strong>vectores de soporte que se encuentran en los hiperplanos de la frontera del margen</strong></em> satisfacen <span class="math notranslate nohighlight">\(\xi_{n} = 0\)</span> y, por tanto, <span class="math notranslate nohighlight">\(\mu_{n}\)</span> puede ser distinto de cero, lo que lleva a <span class="math notranslate nohighlight">\(0\leq\lambda_{n}\leq C\)</span>.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="ejercicios-para-el-estudiante">
<h1><span class="section-number">9. </span>Ejercicios para el estudiante<a class="headerlink" href="#ejercicios-para-el-estudiante" title="Link to this heading">#</a></h1>
<ol class="arabic simple">
<li><p>Demuestre que el <em><strong>kernel reproductor</strong></em> <span class="math notranslate nohighlight">\(\kappa(\cdot, \cdot)\)</span> es definido positivo.</p></li>
</ol>
<ol class="arabic simple" start="2">
<li><p>Demuestre que si <span class="math notranslate nohighlight">\(\kappa(\cdot, \cdot)\)</span> es el kernel reproductor en un espacio de Hilbert(RKHS) <span class="math notranslate nohighlight">\(\mathbb{H}\)</span>, entonces</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\mathbb{H} = \text{span} \{\kappa(\cdot, x) \mid x \in X\}.
\]</div>
<ol class="arabic simple" start="3">
<li><p>Demuestre la <em><strong>desigualdad de Cauchy–Schwarz para kernels</strong></em>, es decir,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\kappa(x, y)^2 \leq \kappa(x, x) \kappa(y, y).
\]</div>
<ol class="arabic" start="4">
<li><p>Demuestre que si</p>
<div class="math notranslate nohighlight">
\[
    \kappa_i (\cdot, \cdot) : X \times X \to \mathbb{R}, \quad i = 1,2,
    \]</div>
<p>son kernels, entonces:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\kappa(x, y) = \kappa_1(x, y) + \kappa_2(x, y)\)</span> también es un kernel;</p></li>
<li><p><span class="math notranslate nohighlight">\(a\kappa(x, y)\)</span>, con <span class="math notranslate nohighlight">\(a &gt; 0\)</span>, también es un kernel;</p></li>
<li><p><span class="math notranslate nohighlight">\(\kappa(x, y) = \kappa_1(x, y) \kappa_2(x, y)\)</span> también es un kernel.</p></li>
</ul>
</li>
</ol>
<ol class="arabic simple" start="5">
<li><p>Demuestre la ecuación <a class="reference internal" href="#equation-loss-ridge-kernel">(8.4)</a></p></li>
</ol>
<ol class="arabic" start="6">
<li><p>Demuestre que la solución para los parámetros <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> en la <em><strong>regresión de ridge con kernel</strong></em>, cuando se incluye un término de sesgo <span class="math notranslate nohighlight">\(b\)</span>, está dada por</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{bmatrix}
    \theta \\
    b
    \end{bmatrix}
    =
    \begin{bmatrix}
    K + C I &amp; 1 \\
    1^T K &amp; N
    \end{bmatrix}^{-1}
    \begin{bmatrix}
    y \\
    y^T 1
    \end{bmatrix},
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(1\)</span> es el vector cuyas entradas son todas iguales a uno. Asuma la invertibilidad de la matriz de kernel.</p>
</li>
</ol>
<section id="aplicacion">
<h2><span class="section-number">9.1. </span>Aplicación<a class="headerlink" href="#aplicacion" title="Link to this heading">#</a></h2>
</section>
<section id="modelos-lineales-y-caracteristicas-no-lineales">
<h2><span class="section-number">9.2. </span>Modelos lineales y características no lineales<a class="headerlink" href="#modelos-lineales-y-caracteristicas-no-lineales" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Como se ha visto antes, los <em><strong>modelos lineales pueden ser bastante limitados en espacios de baja dimensión</strong></em>, ya que las líneas y los hiperplanos tienen una <em><strong>flexibilidad limitada</strong></em>. Una forma de hacer que un modelo lineal sea mas flexible es añadiendo más características, por ejemplo, <em><strong>añadiendo interacciones o polinomios de las características de entrada</strong></em>. Veamos el dataset sintético utilizado en <em><strong>feature importance</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>El vector de <em><strong>labels y</strong></em>, está compuesto por las clases <em><strong>0, 1, 2, 3</strong></em>. Convertimos el problema, en uno binario, usando la <em><strong>clase residual modulo 2</strong></em>, mediante: <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">y</span> <span class="pre">%</span> <span class="pre">2</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">%</span> <span class="mi">2</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ad2bec39a800d5c47424259935dedf2a2502260c83dc9cb0d1bf581837a6b82d.png" src="_images/ad2bec39a800d5c47424259935dedf2a2502260c83dc9cb0d1bf581837a6b82d.png" />
</div>
</div>
<ul class="simple">
<li><p>Un `modelo lineal de clasificación sólo puede separar los puntos mediante una línea***, por lo tanto, no podrá hacer un buen trabajo en este conjunto de datos</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_svm</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">linear_svm</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/660acd8750caf21555d5b4d32b4649240ecbcc059d8b2d73c5ab60f676443d1e.png" src="_images/660acd8750caf21555d5b4d32b4649240ecbcc059d8b2d73c5ab60f676443d1e.png" />
</div>
</div>
<ul class="simple">
<li><p>Ahora vamos a <em><strong>ampliar el conjunto de características de entrada, digamos que añadiendo también feature1 2</strong></em>, el cuadrado de la segunda característica, como una nueva característica. En lugar de representar cada punto de datos como un punto bidimensional, (<code class="docutils literal notranslate"><span class="pre">feature0</span></code>, <code class="docutils literal notranslate"><span class="pre">feature1</span></code>), ahora lo representamos como un punto tridimensional, (<code class="docutils literal notranslate"><span class="pre">feature0</span></code>, <code class="docutils literal notranslate"><span class="pre">feature1</span></code>, <code class="docutils literal notranslate"><span class="pre">feature1</span> <span class="pre">**</span> <span class="pre">2</span></code>). Esta nueva representación se ilustra en el siguiente grafico de dispersión tridimensional</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Nótese que al nuevo array <code class="docutils literal notranslate"><span class="pre">X_new</span></code> <em><strong>se le agregó una tercera columna</strong></em> utilizando la segunda columna de <code class="docutils literal notranslate"><span class="pre">X</span></code>, <code class="docutils literal notranslate"><span class="pre">X[:,</span> <span class="pre">1:]</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-1.72161036, -1.48033142,  2.19138111],
       [-3.6573384 , -9.5482383 , 91.16885455],
       [ 7.0778163 ,  0.99508772,  0.99019957],
       [-1.36579859, -0.3148625 ,  0.09913839],
       [-2.66521206, -3.12591651,  9.77135405]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span><span class="p">,</span> <span class="n">axes3d</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><em><strong>Visualizamos los datos en 3D</strong></em> y trazamos primero todos los puntos con <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">==</span> <span class="pre">0</span></code>, luego todos con <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">==</span> <span class="pre">1</span></code>. Si está interesado en conocer mas sobre los argumentos que puede utilizar en la función <code class="docutils literal notranslate"><span class="pre">scatter</span></code> (ver <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html">matplotlib.pyplot.scatter</a>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">figure</span><span class="p">,</span> <span class="n">elev</span><span class="o">=-</span><span class="mi">152</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">26</span><span class="p">)</span> <span class="c1">#elev, azim: vertical and horizontal angle respectivly</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">0</span> <span class="c1"># booleano para etiquetado</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span> <span class="c1"># s=60: points size; cmap=mglearn.cm2:color palette  </span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;feature0&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;feature1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;feature1 ** 2&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f7d3315c4db3a563720c4f3fb0915df8dbb4433830823e88100c9cd9b5e64ae1.png" src="_images/f7d3315c4db3a563720c4f3fb0915df8dbb4433830823e88100c9cd9b5e64ae1.png" />
</div>
</div>
<ul class="simple">
<li><p>En esta nueva representación de los datos, <em><strong>ahora sí es posible separar las dos clases mediante un modelo lineal, un plano en tres dimensiones</strong></em>. Podemos confirmarlo <em><strong>ajustando un modelo lineal a los datos aumentados</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_svm_3d</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="n">linear_svm_3d</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">linear_svm_3d</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Puede imprimir los coeficientes e intercepto (<code class="docutils literal notranslate"><span class="pre">coef,</span> <span class="pre">intercept</span></code>) del plano de la forma <span class="math notranslate nohighlight">\(c_{0}x+c_{1}y+c_{2}z+\text{intercept}\)</span>, que separa las dos clases. <em><strong>Nótese que en este caso se está realizando un mapeo de características a un espacio de Hilbert, de mayor dimensión</strong></em>, donde se linealiza nuestro problema de clasificación.</p></li>
<li><p>Aquí <code class="docutils literal notranslate"><span class="pre">rstride,</span> <span class="pre">cstride</span></code> son utilizados para realizar divisiones uniformes en el plano <code class="docutils literal notranslate"><span class="pre">XY</span></code>, <code class="docutils literal notranslate"><span class="pre">elev</span></code> y <code class="docutils literal notranslate"><span class="pre">azim</span></code> son ángulos de rotación en grados que giran la imagen en <code class="docutils literal notranslate"><span class="pre">XY</span></code> y <code class="docutils literal notranslate"><span class="pre">Z</span></code> respectivamente, . Para mas información acerca de como usar <code class="docutils literal notranslate"><span class="pre">plot_surface</span></code> (ver <a class="reference external" href="https://matplotlib.org/stable/gallery/mplot3d/surface3d.html">3D surface (colormap)</a>). Es <em><strong>tarea del estudiante profundizar en el uso de cada parámetro</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">figure</span><span class="p">,</span> <span class="n">elev</span><span class="o">=-</span><span class="mi">152</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">26</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">XX</span><span class="p">,</span> <span class="n">YY</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">)</span>
<span class="n">ZZ</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">XX</span> <span class="o">+</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">YY</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="o">-</span><span class="n">coef</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="c1">#ecuación del plano</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">ZZ</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> <span class="c1">#rstride/cstride: control row/column stride</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;feature0&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;feature1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;feature0 ** 2&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/38e7802984338415ded9297e2fe4aea704fc0de38cbd7fa844e1440573042600.png" src="_images/38e7802984338415ded9297e2fe4aea704fc0de38cbd7fa844e1440573042600.png" />
</div>
</div>
<ul class="simple">
<li><p>Como función de las características originales, el modelo <em><strong>SVM lineal ya no es lineal</strong></em>. No es una línea, sino <em><strong>más bien una elipse</strong></em>, como se puede ver en el gráfico creado aquí. Para dibujar la función de decisión utilizamos la clase <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> de <code class="docutils literal notranslate"><span class="pre">LinearSVC()</span></code>.</p></li>
<li><p>La función <em><strong>ravel(), devuelve un array 1-D que contiene los elementos de la entrada</strong></em>. Se hace una copia sólo si se necesita. Para conocer más métodos que pueden ser utilizados a partir de la clase <em><strong>SVC</strong></em> (ver <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">sklearn.svm.SVC</a>). Aquí <em><strong>levels indica niveles específicos en los que las curvas de nivel serán dibujadas</strong></em>, los valores deben estar en orden creciente. Para mas información acerca de dibujos de contorno (ver <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html">matplotlib.pyplot.contourf</a>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ZZ</span> <span class="o">=</span> <span class="n">YY</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">dec</span> <span class="o">=</span> <span class="n">linear_svm_3d</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">XX</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">YY</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">ZZ</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">dec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">XX</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="n">dec</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dec</span><span class="o">.</span><span class="n">max</span><span class="p">()],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/73e81f664e165ab2508f87f2efbbb8907664106b71db6f195f2c52b44537eb39.png" src="_images/73e81f664e165ab2508f87f2efbbb8907664106b71db6f195f2c52b44537eb39.png" />
</div>
</div>
</section>
<section id="el-kernel-trick">
<h2><span class="section-number">9.3. </span>El Kernel Trick<a class="headerlink" href="#el-kernel-trick" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Durante el entrenamiento, <em><strong>SVM aprende cuan importante son cada uno de los puntos de datos de entrenamiento para representar el límite de decisión entre las dos clases</strong></em>. Recuerde que, <em><strong>sólo un subconjunto de los puntos de entrenamiento es importante para definir la frontera de decisión</strong></em>, los que se encuentran en la frontera entre las clases (<em><strong>vectores de soporte</strong></em>).</p></li>
<li><p>Para hacer una predicción de un nuevo punto, se mide la distancia a cada uno de los vectores de soporte. <em><strong>Se toma una decisión de clasificación basada en las distancias a los vectores de soporte y en la importancia de los vectores de soporte, aprendida durante el entrenamiento</strong></em> (almacenado en el atributo <code class="docutils literal notranslate"><span class="pre">dual_coef_</span></code> de <code class="docutils literal notranslate"><span class="pre">SVC</span></code>). La distancia entre los puntos de datos se mide, por ejemplo, mediante el <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">Gaussiano</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
k_{\text{rbf}}=\exp(\gamma\|x_{1}-x_{2}\|^2).
\]</div>
<ul class="simple">
<li><p>Aquí, <span class="math notranslate nohighlight">\(x_{1}\)</span> y <span class="math notranslate nohighlight">\(x_{2}\)</span> son puntos de datos, <span class="math notranslate nohighlight">\(\|x_{1} - x_{2}\|\)</span> denota la distancia euclidiana, y <span class="math notranslate nohighlight">\(\gamma\)</span> es un parámetro que controla el ancho del kernel gaussiano. <span class="math notranslate nohighlight">\(\gamma\)</span><code class="docutils literal notranslate"><span class="pre">=gamma</span></code> define un <em><strong>factor de escala general para la noción de distancia entre dos puntos de la SVM</strong></em>; esto, a su vez, define <em><strong>cómo un vector de apoyo da forma al límite de decisión en su vecindad cercana</strong></em>. La siguiente figura muestra el resultado del entrenamiento de una máquina de vectores de soporte en un conjunto de datos bidimensional de dos clases. <em><strong>El límite de decisión se muestra en negro, y los vectores de soporte son puntos más grandes con el contorno ancho</strong></em>. El siguiente código crea este gráfico entrenando una <em><strong>SVM</strong></em> en el conjunto de datos <em><strong>forge</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">imp</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">make_handcrafted_dataset</span><span class="p">()</span>
<span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X.shape =&quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;; y.shape&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X.shape = (26, 2) ; y.shape (26,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Dibujamos los vectores de soporte. <em><strong>Las etiquetas de clase de los vectores soporte vienen dadas por el signo de los coeficientes duales</strong></em> obtenidos en Python por medio de la función <code class="docutils literal notranslate"><span class="pre">dual_coef_</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">svm</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">sv</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">support_vectors_</span>
<span class="n">sv_labels</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">dual_coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">sv</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sv</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">sv_labels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b94a1f5e1838509fc8071bbf1c66cbd7652bd4fcc228023abff3230398efe654.png" src="_images/b94a1f5e1838509fc8071bbf1c66cbd7652bd4fcc228023abff3230398efe654.png" />
</div>
</div>
<ul class="simple">
<li><p>En este caso, <em><strong>SVM produce un límite muy suave y no lineal</strong></em>. Aquí ajustamos dos parámetros: el parámetro <code class="docutils literal notranslate"><span class="pre">C</span></code> y el parámetro <code class="docutils literal notranslate"><span class="pre">gamma</span></code>, que ahora discutiremos en detalle. Puede <em><strong>hacer uso de la validación cruzada y pipeline</strong></em> estudiados en la sección de evaluación de modelos.</p></li>
</ul>
</section>
<section id="ajuste-de-los-parametros-de-svm">
<h2><span class="section-number">9.4. </span>Ajuste de los parámetros de SVM<a class="headerlink" href="#ajuste-de-los-parametros-de-svm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>El <em><strong>parámetro gamma</strong></em> es el que se muestra en la fórmula dada en la sección anterior, que <em><strong>controla la anchura del kernel Gaussiano</strong></em>. Determina la <em><strong>escala de lo que significa que los puntos estén próximos entre sí</strong></em>. El parámetro <em><strong>C es un parámetro de regularización, similar al utilizado en los modelos lineales</strong></em>. Limita la importancia de cada punto (o más precisamente, su <code class="docutils literal notranslate"><span class="pre">dual_coef_</span></code>). Veamos qué ocurre cuando variamos estos parámetros:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">C</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">gamma</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)):</span>
        <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_svm</span><span class="p">(</span><span class="n">log_C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">log_gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">a</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;class 0&quot;</span><span class="p">,</span> <span class="s2">&quot;class 1&quot;</span><span class="p">,</span> <span class="s2">&quot;sv class 0&quot;</span><span class="p">,</span> <span class="s2">&quot;sv class 1&quot;</span><span class="p">],</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/898da292eefd7f095f83abb7555ad98f6bcc19937a1e1f027443d8ae1c0ba666.png" src="_images/898da292eefd7f095f83abb7555ad98f6bcc19937a1e1f027443d8ae1c0ba666.png" />
</div>
</div>
<ul class="simple">
<li><p>De izquierda a derecha, aumentamos el valor del <em><strong>parámetro gamma de 0.1 a 10</strong></em>. Un <em><strong>gamma pequeño significa un radio grande para el kernel gaussiano, lo que significa que muchos puntos se consideran cercanos</strong></em>. Esto se refleja en unos <em><strong>límites de decisión muy suaves a la izquierda, y límites que se centran más en puntos individuales más a la derecha</strong></em>. Un valor bajo de <em><strong>gamma</strong></em> significa que el límite de decisión variará lentamente, lo que produce un modelo de baja complejidad, mientras que <em><strong>un valor alto de gamma da lugar a un modelo más complejo</strong></em>.</p></li>
<li><p>De arriba a abajo, aumentamos el parámetro <code class="docutils literal notranslate"><span class="pre">C</span></code> de 0.1 a 1000. Al igual que con los modelos lineales, un valor de <code class="docutils literal notranslate"><span class="pre">C</span></code> pequeño <em><strong>corresponde a un modelo muy restringido, en el que cada punto de datos sólo puede tener una influencia muy limitada</strong></em>. Se puede ver que en la parte superior izquierda el límite de decisión parece casi lineal, y los puntos mal clasificados apenas influyen en la línea.</p></li>
<li><p><em><strong>Aumentar C, como se muestra en la parte inferior derecha, permite que estos puntos tengan una influencia en el modelo y hace que el límite de decisión se doble para clasificarlos correctamente</strong></em>. Apliquemos <em><strong>SVM</strong></em> de núcleo <em><strong>RBF</strong></em> al conjunto de datos <strong><code class="docutils literal notranslate"><span class="pre">breast</span> <span class="pre">cancer</span></code></strong>. Por defecto, <code class="docutils literal notranslate"><span class="pre">C=1</span></code> y <code class="docutils literal notranslate"><span class="pre">gamma=1/n_features</span></code>. Queda como tarea para el estudiante hiperparametrizar cada uno de estos parámetros por medio del uso de <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> y <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.90
Accuracy on test set: 0.94
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Aunque las <code class="docutils literal notranslate"><span class="pre">SVM</span></code> suelen funcionar bastante bien, <em><strong>son muy sensibles a los ajustes de los parámetros y al escalado de los datos</strong></em>. En particular, requieren que <em><strong>todas las características varíen en una escala similar</strong></em>. Veamos los valores mínimos y máximos de cada característica, trazados en escala logarítmica</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature magnitude&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/700ecae98132d85939512b9d8006e2b717c837a045a32d3fc605631492b0b8f1.png" src="_images/700ecae98132d85939512b9d8006e2b717c837a045a32d3fc605631492b0b8f1.png" />
</div>
</div>
<ul class="simple">
<li><p>A partir de este gráfico podemos determinar que <em><strong>las características del conjunto de datos de cáncer de mama son de órdenes de magnitud completamente diferentes</strong></em>. Esto puede ser un problema para otros modelos (como los modelos lineales), pero <em><strong>tiene efectos devastadores para SVM con kernel</strong></em>. Examinemos algunas formas de resolver este problema.</p></li>
</ul>
</section>
<section id="preprocesamiento-de-datos-para-svm">
<h2><span class="section-number">9.5. </span>Preprocesamiento de datos para SVM<a class="headerlink" href="#preprocesamiento-de-datos-para-svm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Una forma de resolver este problema es <em><strong>reescalar cada característica para que todas estén aproximadamente en la misma escala</strong></em>. Un método común de reescalado para <em><strong>SVMs</strong></em> con kernel consiste en <em><strong>escalar los datos de manera que todas las características estén entre 0 y 1</strong></em>. Veremos cómo hacer esto usando el método de preprocesamiento <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code>, <span class="math notranslate nohighlight">\(X_{\textsf{scaled}}=(X-X_{\textsf{min}})/(X_{\textsf{max}}-X_{\textsf{min}})\)</span>. Con el fin de comprender su funcionamiento, vamos a <em><strong>implementar el preprocesamiento “de forma manual”</strong></em></p></li>
</ul>
<ul class="simple">
<li><p>Calculamos el valor <em><strong>mínimo por característica en el conjunto de entrenamiento</strong></em>. Dado que <code class="docutils literal notranslate"><span class="pre">X_train</span></code> contiene 30 columnas de características, el resultado va a ser un array de dimensión 30. Para obtenerlo usamos <code class="docutils literal notranslate"><span class="pre">min(axis=0)</span></code>, para que el mínimo sea buscado sobre las filas de cada columna.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(426, 30)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">min_on_training</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">min_on_training</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(30,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Calculamos el <em><strong>vector de rangos para cada característica (max - min)</strong></em> en el conjunto de entrenamiento. El <em><strong>máximo sobre cada columna es calculado usando la función max(axis=0)</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">range_on_training</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="n">min_on_training</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">range_on_training</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(30,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><em><strong>Restamos con el mínimo, cada una de las características en X_train</strong></em> y después dividimos por el <em><strong>rango para cada característica</strong></em> resultante</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="n">min_on_training</span><span class="p">)</span> <span class="o">/</span> <span class="n">range_on_training</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Minimum for each feature</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minimum for each feature
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximum for each feature</span><span class="se">\n</span><span class="s2"> </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Maximum for each feature
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1.]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><em><strong>Utilizamos la misma transformación en el conjunto de prueba</strong></em>, utilizando el mínimo y el rango del conjunto de entrenamiento</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span> <span class="o">-</span> <span class="n">min_on_training</span><span class="p">)</span> <span class="o">/</span> <span class="n">range_on_training</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Utilizando ahora los <em><strong>datos transformados, entrenamos el modelo SVC()</strong></em> y verificamos valores de <code class="docutils literal notranslate"><span class="pre">accuracy</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.984
Accuracy on test set: 0.972
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Escalar los datos supuso una gran diferencia. Ahora estamos en un régimen de <em><strong>overfitting donde el rendimiento del conjunto de entrenamiento es ligeramente mayor que el de prueba</strong></em>, pero menos cercano al 100% de <code class="docutils literal notranslate"><span class="pre">accuracy</span></code>. A partir de aquí, podemos intentar disminuir <code class="docutils literal notranslate"><span class="pre">C</span></code> o <code class="docutils literal notranslate"><span class="pre">gamma</span></code> para ajustar un modelo regularizado. Por ejemplo:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc_C01</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">svc_C01</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C01</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C01</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.948
Accuracy on test set: 0.958
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Nótese que sucede cuando seguimos disminuyendo el valor de <span class="math notranslate nohighlight">\(C\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc_C001</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">svc_C001</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.643
Accuracy on test set: 0.636
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Si usamos un parámetro de regularización <em><strong>C demasiado extremos, podemos caer nuevamente en un mayor overfitting</strong></em>, o en un modelo demasiado simple, que entrega valores de <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> bastantes bajos para los datos de entrenamiento y prueba, esto es, <em><strong>el modelo no será capaz de generalizarse correctamente, underfitting</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc_C100</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">svc_C100</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C100</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C100</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 1.000
Accuracy on test set: 0.965
</pre></div>
</div>
</div>
</div>
<div class="admonition-puntos-fuertes-puntos-debiles-y-parametros admonition">
<p class="admonition-title">Puntos fuertes, puntos débiles y parámetros</p>
<ul class="simple">
<li><p>Las máquinas de vectores de soporte kernelizadas son modelos potentes y funcionan bien en una variedad de conjuntos de datos. <em><strong>Las SVM permiten establecer límites de decisión complejos, incluso si los datos sólo tienen  unas pocas características</strong></em>. Funcionan bien con datos de baja y alta dimensión (es decir, pocas y muchas características), pero no escalan muy bien con el número de muestras. Ejecutar SVM con datos de hasta 10.000 muestras puede funcionar bien, pero <em><strong>trabajar con conjuntos de datos de 100.000 o más puede ser un reto en términos de tiempo de ejecución y uso de memoria</strong></em>.</p></li>
<li><p>Otra desventaja de SVM es que <em><strong>requieren un cuidadoso preprocesamiento de los datos y el ajuste de los parámetros</strong></em>. Por eso, hoy en día, algunos investigadores utilizan modelos basados en árboles, como los bosques aleatorios o el gradient boosting (que requieren poco o ningún preprocesamiento) en muchas aplicaciones. Además, los modelos <em><strong>SVM</strong></em> son difíciles de inspeccionar, <em><strong>puede ser difícil entender por qué se ha hecho una predicción concreta, y puede ser difícil explicar el modelo a un inexperto</strong></em>. Aun así, puede valer la pena probar las <em><strong>SVM</strong></em>, sobre todo si todas las características representan medidas en unidades similares (por ejemplo, todas son intensidades de píxeles) y están en escalas similares.</p></li>
<li><p>Los <em><strong>parámetros importantes en las SVM con kernel son el parámetro de regularización C, la elección del kernel, y los parámetros específicos del kernel</strong></em>. Aunque principalmente nos centramos en el núcleo <em><strong>RBF</strong></em>, hay otras opciones disponibles en <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. El núcleo <code class="docutils literal notranslate"><span class="pre">RBF</span></code> tiene sólo un parámetro,<code class="docutils literal notranslate"> <span class="pre">gamma</span></code>, que es la inversa de la anchura del kernel de Gaus. <code class="docutils literal notranslate"><span class="pre">gamma</span></code> y <code class="docutils literal notranslate"><span class="pre">C</span></code> controlan la complejidad del modelo, <em><strong>con valores grandes en cualquiera de ellos obtenemos un modelo más complejo</strong></em>. Por lo tanto, los buenos ajustes para los dos parámetros suelen estar fuertemente correlacionados, y <code class="docutils literal notranslate"><span class="pre">C</span></code> y <code class="docutils literal notranslate"><span class="pre">gamma</span></code> deben ajustarse juntos.</p></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "tf"
        },
        kernelOptions: {
            name: "tf",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'tf'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="decisiontree_model.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Random Forest y XGBoost</p>
      </div>
    </a>
    <a class="right-next"
       href="ann_model.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">10. </span>Redes Neuronales y Deep Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">8. Máquinas de vectores de soporte</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">8.1. Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#espacios-de-hilbert-con-kernel-reproductor">8.2. Espacios de Hilbert con Kernel reproductor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#construccion-de-kernels">8.3. Construcción de kernels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teorema-de-representacion">8.4. Teorema de representación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge-con-kernel">8.5. Regresión ridge con Kernel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-de-vectores-de-soporte">8.6. Regresión de vectores de soporte</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-optima-lineal-varepsilon-insensible">8.7. Regresión óptima lineal <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-kernel-ridge">8.8. Regresión Kernel Ridge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">8.9. Máquinas de vectores de soporte</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clases-linealmente-separables-clasificador-de-maximo-margen">8.10. Clases linealmente separables: Clasificador de máximo margen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clases-no-separables">8.11. Clases no separables</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ejercicios-para-el-estudiante">9. Ejercicios para el estudiante</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion">9.1. Aplicación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-lineales-y-caracteristicas-no-lineales">9.2. Modelos lineales y características no lineales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-kernel-trick">9.3. El Kernel Trick</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ajuste-de-los-parametros-de-svm">9.4. Ajuste de los parámetros de SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocesamiento-de-datos-para-svm">9.5. Preprocesamiento de datos para SVM</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lihki Rubio, Ph.D.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>Lihki Rubio, Ph.D. All rights reserved.</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>