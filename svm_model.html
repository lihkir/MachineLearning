
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Máquinas de vectores de soporte &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=530fe47d" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/custom-checkpoint.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'svm_model';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/custom.js?v=14184634"></script>
    <script src="_static/.ipynb_checkpoints/custom-checkpoint.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Redes Neuronales y Deep Learning" href="ann_model.html" />
    <link rel="prev" title="Árboles de decisión" href="decisiontree_model.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fotolihki.jpg" class="logo__image only-light" alt="Machine Learning - Home"/>
    <script>document.write(`<img src="_static/fotolihki.jpg" class="logo__image only-dark" alt="Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Profesor: Dr. Lihki Rubio
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="supervised_intro.html">Aprendizaje supervisado</a></li>
<li class="toctree-l1"><a class="reference internal" href="knn_model.html"><span class="math notranslate nohighlight">\(k\)</span>-Vecinos más cercanos</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_model.html">Modelos lineales</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes_model.html">Clasificadores Naive Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="decisiontree_model.html">Árboles de decisión</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">Máquinas de vectores de soporte</a></li>
<li class="toctree-l1"><a class="reference internal" href="ann_model.html">Redes Neuronales y Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="practical_pca.html">Análisis de Componentes Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">Evaluación de modelos</a></li>
<li class="toctree-l1"><a class="reference internal" href="chains_pipelines.html">Cadenas de Algoritmos y Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/svm_model.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Máquinas de vectores de soporte</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#espacios-de-hilbert-con-kernel-reproductor">Espacios de Hilbert con Kernel reproductor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#construccion-de-kernels">Construcción de kernels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teorema-de-representacion">Teorema de representación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge-con-kernel">Regresión ridge con Kernel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-de-vectores-de-soporte">Regresión de vectores de soporte</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-optima-lineal-varepsilon-insensible">Regresión óptima lineal <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-kernel-ridge">Regresión Kernel Ridge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Máquinas de vectores de soporte</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clases-linealmente-separables-clasificador-de-maximo-margen">Clases linealmente separables: Clasificador de máximo margen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clases-no-separables">Clases no separables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion">Aplicación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-lineales-y-caracteristicas-no-lineales">Modelos lineales y características no lineales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-kernel-trick">El Kernel Trick</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ajuste-de-los-parametros-de-svm">Ajuste de los parámetros de SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocesamiento-de-datos-para-svm">Preprocesamiento de datos para SVM</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="maquinas-de-vectores-de-soporte">
<h1>Máquinas de vectores de soporte<a class="headerlink" href="#maquinas-de-vectores-de-soporte" title="Link to this heading">#</a></h1>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Las <em><strong>máquinas de vectores de soporte (SVM)</strong></em> propuestas por <em><strong>Vapnik (1996, 1998)</strong></em>, se caracterizan por ser un proceso interno de <em><strong>construcción de reglas de clasificación, bastante diferente al de las de los métodos estadísticos</strong></em>. Las <em><strong>SVM</strong></em> suelen ser eficaces en casos en los que los métodos de clasificación tradicionales no lo son, como por ejemplo, los <em><strong>problemas basados en datos con estructura no lineal</strong></em>, además las <em><strong>SVM</strong></em> se han adaptado y aplicado en muchos campos de investigación. En esta sección, nos centramos en las <em><strong>SVM</strong></em> para la solución de <em><strong>problemas de regresión y clasificación</strong></em>, así como también el proceso de construcción, luego abordaremos los pasos de su extensión, de sistemas lineales a sistemas no lineales.</p></li>
<li><p>El paradigma algorítmico de las <em><strong>SVM</strong></em> aborda el reto de la complejidad de la muestra mediante la <strong>búsqueda de separadores de “mayor margen”</strong>. A grandes rasgos, un semiespacio separa un conjunto de entrenamiento con un gran margen si <em><strong>todos los ejemplos no sólo están en el lado correcto del hiperplano de separación, sino también lejos de él</strong></em>. Restringir el algoritmo a la salida de un separador de gran margen puede producir una complejidad de muestra pequeña, incluso si la dimensionalidad del espacio de características es alta (e incluso infinita). Introduciremos el concepto de margen y lo relacionamos con el <em><strong>paradigma de minimización de pérdidas regularizadas</strong></em>.</p></li>
</ul>
</div>
<section id="analisis">
<h2>Análisis<a class="headerlink" href="#analisis" title="Link to this heading">#</a></h2>
</section>
<section id="espacios-de-hilbert-con-kernel-reproductor">
<h2>Espacios de Hilbert con Kernel reproductor<a class="headerlink" href="#espacios-de-hilbert-con-kernel-reproductor" title="Link to this heading">#</a></h2>
<p>Sea <span class="math notranslate nohighlight">\(H\)</span> un <em><strong>espacio lineal de funciones reales</strong></em> definidas sobre <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathcal{R}^{l}\)</span>. Además, suponga que <span class="math notranslate nohighlight">\(H\)</span> es un <em><strong>espacio de Hilbert</strong></em>, con producto interno <span class="math notranslate nohighlight">\(\langle\cdot,\cdot\rangle_{H}\)</span> que induce la norma <span class="math notranslate nohighlight">\(\|\cdot\|_{H}\)</span>, con respecto a la cual <span class="math notranslate nohighlight">\(H\)</span> es completo.</p>
<div class="proof definition admonition" id="def_hilbert_rep">
<p class="admonition-title"><span class="caption-number">Definition 6 </span></p>
<section class="definition-content" id="proof-content">
<p>Un espacio de Hilbert <span class="math notranslate nohighlight">\(H\)</span> es llamado <code class="docutils literal notranslate"><span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">Hilbert</span> <span class="pre">con</span> <span class="pre">kernel</span> <span class="pre">reproductor</span> <span class="pre">(RKHS)</span></code> si existe una función:</p>
<div class="math notranslate nohighlight">
\[
\kappa:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}
\]</div>
<p>con las siguientes propiedades:</p>
<ul class="simple">
<li><p>Para cada <span class="math notranslate nohighlight">\(\boldsymbol{x}\in\mathcal{X},~\kappa(\cdot, \boldsymbol{x})\)</span> pertenece a <span class="math notranslate nohighlight">\(H\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\kappa(\cdot, \cdot)\)</span> tiene la llamada <code class="docutils literal notranslate"><span class="pre">propiedad</span> <span class="pre">de</span> <span class="pre">reproducción</span></code>, esto es:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-prop-reproductora">
<span class="eqno">(34)<a class="headerlink" href="#equation-prop-reproductora" title="Link to this equation">#</a></span>\[
f(\boldsymbol{x})=\langle f, \kappa(\cdot, \boldsymbol{x})\rangle,\quad\forall f\in H,~\forall \boldsymbol{x}\in\mathcal{X}
\]</div>
</section>
</div><ul class="simple">
<li><p>Una consecuencia directa de la <em><strong>propiedad reproductora</strong></em> es, si definimos <span class="math notranslate nohighlight">\(f(\cdot)=\kappa(\cdot, \boldsymbol{y}),~\boldsymbol{y}\in\mathcal{X}\)</span> entonces</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\kappa(\boldsymbol{x}, \boldsymbol{y})=\langle\kappa(\cdot, \boldsymbol{y}), \kappa(\cdot, \boldsymbol{x})\rangle=\langle\kappa(\cdot, \boldsymbol{x}), \kappa(\cdot, \boldsymbol{y})\rangle=\kappa(\boldsymbol{y}, \boldsymbol{x})
\]</div>
<div class="proof definition admonition" id="def_features_map">
<p class="admonition-title"><span class="caption-number">Definition 7 </span></p>
<section class="definition-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(H\)</span> un <code class="docutils literal notranslate"><span class="pre">(RKHS)</span></code> asociado con una función kernel <span class="math notranslate nohighlight">\(\kappa(\cdot, \cdot)\)</span> y <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> un conjunto de elementos. Entonces el mapeo</p>
<div class="math notranslate nohighlight">
\[
\mathcal{X}\ni\boldsymbol{x}\mapsto\Phi(\boldsymbol{x}):=\kappa(\cdot, \boldsymbol{x})\in H
\]</div>
<p>es conocido como <code class="docutils literal notranslate"><span class="pre">mapeo</span> <span class="pre">de</span> <span class="pre">características</span></code> y el espacio <span class="math notranslate nohighlight">\(H\)</span> es el <code class="docutils literal notranslate"><span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">características</span></code>. Esto es <span class="math notranslate nohighlight">\(\Phi\)</span> mapea cada vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\in\mathcal{X}\)</span> en el (RKHS) <span class="math notranslate nohighlight">\(H\)</span>.</p>
</section>
</div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H\)</span> puede ser de <code class="docutils literal notranslate"><span class="pre">dimensión</span> <span class="pre">finita</span> <span class="pre">o</span> <span class="pre">inifinita</span> <span class="pre">y</span> <span class="pre">sus</span> <span class="pre">elementos</span> <span class="pre">pueden</span> <span class="pre">ser</span> <span class="pre">funciones</span></code>. Esto es, cada <code class="docutils literal notranslate"><span class="pre">punto</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">es</span> <span class="pre">mapeado</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">funciones</span></code>. Si <span class="math notranslate nohighlight">\(H\)</span> es de dimensión finita, por ejemplo el Espacio Euclidiano <span class="math notranslate nohighlight">\(\mathbb{R}^{k},~\Phi(\boldsymbol{x})\in\mathbb{R}^{k}\)</span>.</p></li>
<li><p>Consideraremos el caso de dimensión infinita cuyas imágenes son funciones de <span class="math notranslate nohighlight">\(\Phi(\cdot)\)</span>. Veamos las ventajas de este <code class="docutils literal notranslate"><span class="pre">mapeo,</span> <span class="pre">del</span> <span class="pre">espacio</span> <span class="pre">original</span> <span class="pre">a</span> <span class="pre">otro</span> <span class="pre">de</span> <span class="pre">dimensión</span> <span class="pre">infinita</span> <span class="pre">(RKHS)</span></code>.</p></li>
</ul>
<div class="proof definition admonition" id="def_kernel_trick">
<p class="admonition-title"><span class="caption-number">Definition 8 </span></p>
<section class="definition-content" id="proof-content">
<p>Sean <span class="math notranslate nohighlight">\(\boldsymbol{x}, \boldsymbol{y}\in\mathcal{X}\subseteq\mathbb{R}^{l}\)</span>, entonces el <code class="docutils literal notranslate"><span class="pre">producto</span> <span class="pre">interno</span> <span class="pre">del</span> <span class="pre">respectivo</span> <span class="pre">mapeo</span> <span class="pre">de</span> <span class="pre">imágenes</span></code> es escrito como:</p>
<div class="math notranslate nohighlight">
\[
\langle\Phi(\boldsymbol{x}), \Phi(\boldsymbol{y})\rangle_{H}=\langle \kappa(\cdot, \boldsymbol{x}), \kappa(\cdot, \boldsymbol{y})\rangle
\]</div>
<p>o</p>
<div class="math notranslate nohighlight">
\[
\langle\Phi(\boldsymbol{x}), \Phi(\boldsymbol{y})\rangle_{H}=\kappa(\boldsymbol{x}, \boldsymbol{y}),\quad\text{Kernel trick}
\]</div>
</section>
</div><figure class="align-center" id="fig-kernel-map-ilust">
<a class="reference internal image-reference" href="_images/kernel_map_ilust.png"><img alt="_images/kernel_map_ilust.png" src="_images/kernel_map_ilust.png" style="width: 463.4px; height: 354.2px;" />
</a>
</figure>
<ul class="simple">
<li><p>Empleando este tipo de mapeo a nuestro problema, calculamos <code class="docutils literal notranslate"><span class="pre">operaciones</span> <span class="pre">de</span> <span class="pre">producto</span> <span class="pre">interno</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\(H\)</span> en una manera eficiente, vía <code class="docutils literal notranslate"><span class="pre">evaluación</span> <span class="pre">de</span> <span class="pre">funciones</span> <span class="pre">sobre</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">baja</span> <span class="pre">dimensión</span></code>.</p></li>
</ul>
<figure class="align-center" id="non-linear-classifier-using-kernel-trick-fig">
<img alt="_images/non-linear-classifier-using-kernel-trick.png" src="_images/non-linear-classifier-using-kernel-trick.png" />
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Ilustración de la propiedad <strong>kernel trick</strong>. Fuente <span id="id1">[<a class="reference internal" href="biblio.html#id57" title="Marouane Hachimi, Georges Kaddoum, Ghyslain Gagnon, and Poulmanogo Illy. Multi-stage jamming attacks detection using deep learning combined with kernelized support vector machine in 5g cloud radio access networks. In 2020 international symposium on networks, computers and communications (ISNCC), 1–5. IEEE, 2020.">Hachimi <em>et al.</em>, 2020</a>]</span>.</span><a class="headerlink" href="#non-linear-classifier-using-kernel-trick-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof property admonition" id="props_features_map">
<p class="admonition-title"><span class="caption-number">Property 1 </span></p>
<section class="property-content" id="proof-content">
<p>El mapeo <span class="math notranslate nohighlight">\(\Phi(\cdot)\)</span> satisface las siguientes propiedades</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Mapea</span> <span class="pre">implícitamente</span> <span class="pre">inputs</span> <span class="pre">(datos</span> <span class="pre">de</span> <span class="pre">entrenamiento)</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">espacio</span> <span class="pre">RKHS</span></code></p></li>
<li><p>Soluciona tareas de <code class="docutils literal notranslate"><span class="pre">estimación</span> <span class="pre">lineal</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\(H\)</span>, involucrando las imágenes:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\Phi(x_{n}),~n=1,2,\dots,N.
\]</div>
<ul class="simple">
<li><p><strong>Proyecta el algoritmo que soluciona problemas de parámetros desconocidos, en términos del producto interno</strong> en la forma:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\langle\Phi(x_{i}), \Phi(x_{j})\rangle,\quad i,j=1,2,\dots,N
\]</div>
<ul class="simple">
<li><p>Considera la <code class="docutils literal notranslate"><span class="pre">evaluación</span> <span class="pre">kernel</span> <span class="pre">como</span> <span class="pre">el</span> <span class="pre">producto</span> <span class="pre">interno</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\langle\Phi(x_{i}),\Phi(x_{j})\rangle=\kappa(x_{i}, x_{j}).
\]</div>
</section>
</div><ul class="simple">
<li><p>Nótese que este procedimiento de mapeo explicito es necesario para calcular la operación kernel en el último paso. La forma especifica de <span class="math notranslate nohighlight">\(\kappa(\cdot,\cdot)\)</span> no concierne en el análisis.</p></li>
</ul>
<div class="proof example admonition" id="ej_feature_map">
<p class="admonition-title"><span class="caption-number">Example 1 </span></p>
<section class="example-content" id="proof-content">
<p>Considere el caso del <code class="docutils literal notranslate"><span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">2</span> <span class="pre">dimensiones</span></code> y el mapeo</p>
<div class="math notranslate nohighlight">
\[
\mathbb{R}^{2}\ni \boldsymbol{x}\mapsto\Phi(x)=(x_{1}^{2}, \sqrt{2}x_{1}x_{2}, x_{2}^{2})\in\mathbb{R}^{3}
\]</div>
<p>Entonces, dados los vectores <span class="math notranslate nohighlight">\(\boldsymbol{x}=(x_{1}, x_{2})^{T}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{y}=(y_{1}, y_{2})^{T}\)</span>, es facil ver que</p>
<div class="math notranslate nohighlight">
\[
\kappa(\boldsymbol{x}, \boldsymbol{y})=\langle\Phi(\boldsymbol{x}), \Phi(\boldsymbol{y})\rangle_{\mathbb{R}^{3}}=\Phi(\boldsymbol{x})^{T}\Phi(\boldsymbol{y})=(x_{1}y_{1}+x_{2}y_{2})^{2}=(\boldsymbol{x}^{T}\boldsymbol{y})^{2}
\]</div>
<p>Es decir, el producto interior en el espacio tridimensional, después del mapeo, está dado en términos de una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">variables</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">original</span></code>.</p>
</section>
</div><div class="proof property admonition" id="prop_kcompacto">
<p class="admonition-title"><span class="caption-number">Property 2 </span></p>
<section class="property-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> un conjunto de puntos. Típicamente, <span class="math notranslate nohighlight">\(\mathcal{X}\subseteq\mathbb{R}^{l}\)</span> es <strong>compacto, esto es, cada sucesión en</strong> <span class="math notranslate nohighlight">\(X\)</span> <strong>tiene una subsucesión convergente</strong>. Sea <span class="math notranslate nohighlight">\(\kappa\)</span> la función</p>
<div class="math notranslate nohighlight">
\[
\kappa:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}.
\]</div>
<p>La función <span class="math notranslate nohighlight">\(\kappa\)</span> es llamada <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">definido</span> <span class="pre">positivo</span> <span class="pre">si</span> <span class="pre">satisface</span></code></p>
<div class="math notranslate nohighlight">
\[
\sum_{n=1}^{N}\sum_{m=1}^{N}a_{n}a_{m}\kappa(x_{n}, x_{m})\geq0,
\]</div>
<p>para cualquier número real <span class="math notranslate nohighlight">\(a_{n}, a_{m}\)</span> y cualquier punto <span class="math notranslate nohighlight">\(x_{n}, x_{m}\in\mathcal{X}\)</span> y cualquier <span class="math notranslate nohighlight">\(N\in\mathbb{N}\)</span>. O equivalentemente, si definimos la matriz <span class="math notranslate nohighlight">\(K\)</span>, de orden <span class="math notranslate nohighlight">\(N\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
K=
\begin{pmatrix}
k(x_{1}, x_{1}) &amp; \cdots &amp; k(x_{1}, x_{N})\\
\vdots &amp; \vdots &amp; \vdots\\
k(x_{N}, x_{1}) &amp; \cdots &amp; k(x_{N}, x_{N})\\
\end{pmatrix}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(a^{T}Ka\geq0\)</span>, donde <span class="math notranslate nohighlight">\(a=(a_{1}, a_{2},\dots, a_{N})^{T}\)</span>.</p>
</section>
</div><div class="proof example admonition" id="ej_kernel_functions">
<p class="admonition-title"><span class="caption-number">Example 2 </span></p>
<section class="example-content" id="proof-content">
<p>Los siguientes son algunos <code class="docutils literal notranslate"><span class="pre">ejemplos</span> <span class="pre">típicos</span> <span class="pre">de</span> <span class="pre">funciones</span> <span class="pre">kernel</span></code>, las cuales son comúnmente usadas en varias aplicaciones. Estas funciones kernel son utilizadas en <code class="docutils literal notranslate"><span class="pre">Python</span></code> para los modelos <code class="docutils literal notranslate"><span class="pre">SVC</span></code> y <code class="docutils literal notranslate"><span class="pre">SVR</span></code> (ver <a class="reference external" href="https://scikit-learn.org/stable/modules/svm.html#kernel-functions">kernel-functions</a>).</p>
<ul>
<li><p>El <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">gaussiano</span></code> está entre las más populares y está dado por la función</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\exp\left(-\frac{\|\boldsymbol{x}-\boldsymbol{y}\|^{2}}{2\sigma^2}\right),\quad\sigma&gt;0
    \end{split}\]</div>
</li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">polinomial</span></code> homogéneo tiene la forma</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \kappa(\boldsymbol{x}, \boldsymbol{y})=(\boldsymbol{x}^{T}\boldsymbol{y})^{r},
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(r\)</span> es un parámetro.</p>
</li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">polinómico</span> <span class="pre">no-homogéneo</span></code> tiene la forma</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \kappa(\boldsymbol{x}, \boldsymbol{y})=(\boldsymbol{x}^{T}\boldsymbol{y}+c)^{r},
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(c\geq0,~r\)</span> son parámetros.</p>
</li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">laplaciano</span></code> está dado por</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[2mm]
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\exp(-t\|\boldsymbol{x}-\boldsymbol{y}\|),
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(t&gt;0\)</span> es un parámetro.</p>
</li>
</ul>
</section>
</div><figure class="align-center" id="sphx-glr-plot-iris-svc-001-fig">
<img alt="_images/sphx_glr_plot_iris_svc_001.png" src="_images/sphx_glr_plot_iris_svc_001.png" />
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">SVC</span></code> y <code class="docutils literal notranslate"><span class="pre">LinearSVC</span></code> para clasificación en un conjunto de datos. Ver <a class="reference external" href="https://scikit-learn.org/stable/modules/svm.html#classification">scikit-learn</a>.</span><a class="headerlink" href="#sphx-glr-plot-iris-svc-001-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="construccion-de-kernels">
<h2>Construcción de kernels<a class="headerlink" href="#construccion-de-kernels" title="Link to this heading">#</a></h2>
<p>Además de los ejemplos previos, se pueden <code class="docutils literal notranslate"><span class="pre">construir</span> <span class="pre">otros</span> <span class="pre">kernels</span> <span class="pre">aplicando</span> <span class="pre">las</span> <span class="pre">siguientes</span> <span class="pre">propiedades</span></code></p>
<ul>
<li><p>Si</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \kappa_{1}(\boldsymbol{x}, \boldsymbol{y}):\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}\\
    \kappa_{1}(\boldsymbol{x}, \boldsymbol{y}):\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}\end{split}\]</div>
<p>son kernels, entonces</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\kappa_{1}(\boldsymbol{x}, \boldsymbol{y})+\kappa_{2}(\boldsymbol{x}, \boldsymbol{y})\\
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\alpha\kappa_{1}(\boldsymbol{x}, \boldsymbol{y})\end{split}\]</div>
<p>y</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\kappa_{1}(\boldsymbol{x}, \boldsymbol{y})\kappa_{2}(\boldsymbol{x}, \boldsymbol{y}),
    \]</div>
<p>son kernels.</p>
</li>
</ul>
<ul>
<li><p>Sea</p>
<div class="math notranslate nohighlight">
\[
    f:\mathcal{X}\mapsto\mathbb{R}
    \]</div>
<p>entonces</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=f(\boldsymbol{x})f(\boldsymbol{y})
    \]</div>
<p>es un kernel</p>
</li>
</ul>
<ul>
<li><p>Sea una función</p>
<div class="math notranslate nohighlight">
\[
    g:\mathcal{X}\mapsto\mathbb{R}^{l}
    \]</div>
<p>y una función kernel</p>
<div class="math notranslate nohighlight">
\[
    \kappa_{1}(\cdot, \cdot):\mathbb{R}^{l}\times\mathbb{R}^{l}\mapsto\mathbb{R}.
    \]</div>
<p>Entonces</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\kappa_{1}(g(\boldsymbol{x}), g(\boldsymbol{y}))
    \]</div>
<p>es también un kernel.</p>
</li>
</ul>
<ul>
<li><p>Sea <span class="math notranslate nohighlight">\(A\)</span> una <code class="docutils literal notranslate"><span class="pre">matriz</span> <span class="pre">definida</span> <span class="pre">positiva</span></code> con dimensión <span class="math notranslate nohighlight">\(l\times l\)</span>. Entonces</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\boldsymbol{x}^{T}A\boldsymbol{y}
    \end{split}\]</div>
<p>es un kernel.</p>
</li>
</ul>
<ul>
<li><p>Si</p>
<div class="math notranslate nohighlight">
\[
    \kappa_{1}(\boldsymbol{x}, \boldsymbol{y}):\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R},
    \]</div>
<p>entonces</p>
<div class="math notranslate nohighlight">
\[
    \kappa(\boldsymbol{x}, \boldsymbol{y})=\exp(\kappa_{1}(\boldsymbol{x}, \boldsymbol{y}))
    \]</div>
<p>es también un kernel, y si <span class="math notranslate nohighlight">\(p(\cdot)\)</span> es un polinomio con coeficientes no negativos,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \kappa(\boldsymbol{x}, \boldsymbol{y})=p(\kappa_{1}(\boldsymbol{x}, \boldsymbol{y}))
    \end{split}\]</div>
<p>también es un kernel.</p>
</li>
</ul>
<p>Para mas información sobre los kernel y su construción (ver <span id="id2">[<a class="reference internal" href="biblio.html#id12" title="Thomas Hofmann, Bernhard Schölkopf, and Alexander J Smola. Kernel methods in machine learning. The annals of statistics, 36(3):1171–1220, 2008.">Hofmann <em>et al.</em>, 2008</a>, <a class="reference internal" href="biblio.html#id13" title="John Shawe-Taylor, Nello Cristianini, and others. Kernel methods for pattern analysis. Cambridge university press, 2004.">Shawe-Taylor <em>et al.</em>, 2004</a>, <a class="reference internal" href="biblio.html#id14" title="Konstantinos Slavakis, Pantelis Bouboulis, and Sergios Theodoridis. Online learning in reproducing kernel hilbert spaces. In Academic Press Library in Signal Processing, volume 1, pages 883–987. Elsevier, 2014.">Slavakis <em>et al.</em>, 2014</a>]</span>).</p>
</section>
<section id="teorema-de-representacion">
<h2>Teorema de representación<a class="headerlink" href="#teorema-de-representacion" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>El teorema que se presentará en esta sección es de gran importancia desde un punto de vista práctico. Este nos permite llevar a cabo,  <strong>optimización empírica de funciones de pérdida</strong>, basado en un conjunto finito de puntos de entrenamiento, en una manera muy eficiente <em>si la función a estimar pertenece a un espacio de dimensión alta</em> o incluso infinita <span class="math notranslate nohighlight">\(H\)</span>.</p></li>
</ul>
<div class="proof theorem admonition" id="th_representation">
<p class="admonition-title"><span class="caption-number">Theorem 3 </span></p>
<section class="theorem-content" id="proof-content">
<p>Sea</p>
<div class="math notranslate nohighlight">
\[
\Omega: [0, +\infty)\longmapsto\mathbb{R}
\]</div>
<p>una función arbitraria, <code class="docutils literal notranslate"><span class="pre">estrictamente</span> <span class="pre">monotona</span> <span class="pre">creciente</span></code>. Sea también</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}:\mathbb{R}^{2}\longmapsto\mathbb{R}\cup\{\infty\}
\]</div>
<p>una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span></code>. Entonces, cada minimizador <span class="math notranslate nohighlight">\(f\in H\)</span>, de una <code class="docutils literal notranslate"><span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">minimización</span> <span class="pre">regularizada</span></code></p>
<div class="math notranslate nohighlight" id="equation-eq-min-reg">
<span class="eqno">(35)<a class="headerlink" href="#equation-eq-min-reg" title="Link to this equation">#</a></span>\[
\min_{f\in H} J(f)=\sum_{n=1}^{N}\mathcal{L}(y_{n}, f(\boldsymbol{x}_{n}))+\lambda\Omega(\|f\|^{2})
\]</div>
<p>admite una <code class="docutils literal notranslate"><span class="pre">representación</span></code> de la forma</p>
<div class="math notranslate nohighlight">
\[
f(\cdot)=\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n}),
\]</div>
<p>donde <span class="math notranslate nohighlight">\(\theta_{n}\in\mathbb{R},~n=1,2,\dots,N\)</span>. La regularización de la forma <span class="math notranslate nohighlight">\(\Omega(\|f\|^{2})\)</span> es una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">cuadrática,</span> <span class="pre">y</span> <span class="pre">por</span> <span class="pre">lo</span> <span class="pre">tanto</span> <span class="pre">estrictamente</span> <span class="pre">monotona</span></code> en el intervalo <span class="math notranslate nohighlight">\([0, \infty)\)</span>.</p>
</section>
</div><p><strong><code class="docutils literal notranslate"><span class="pre">Demostración</span></code></strong></p>
<p>Sea</p>
<div class="math notranslate nohighlight">
\[
A=\text{span}\{\kappa(\cdot, \boldsymbol{x}_{1}), \kappa(\cdot, \boldsymbol{x}_{2}),\dots,\kappa(\cdot, \boldsymbol{x}_{N})\}.
\]</div>
<p>Dado que cada <span class="math notranslate nohighlight">\(\kappa(\cdot, \boldsymbol{x}_{i})\in H,~i=1,2,\dots,N,~A\subseteq H\)</span> y <span class="math notranslate nohighlight">\(N&lt;\infty\)</span>, entonces, <span class="math notranslate nohighlight">\(A\)</span> es cerrado.</p>
<p>Por <strong>Teorema de descomposición ortogonal</strong>, como <span class="math notranslate nohighlight">\(A\)</span> es un subespacio cerrado de <span class="math notranslate nohighlight">\(H\)</span>, entonces, <span class="math notranslate nohighlight">\(H=A\oplus A^{T}\)</span>. Esto es, si <span class="math notranslate nohighlight">\(f\in H\)</span> entonces</p>
<div class="math notranslate nohighlight">
\[
f(\cdot)=\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+f_{\perp}
\]</div>
<p>donde <span class="math notranslate nohighlight">\(f_{\perp}\)</span> es la parte de <span class="math notranslate nohighlight">\(f\)</span> que es ortogonal a <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>Usando la <strong>propiedad reproductora</strong> <a class="reference internal" href="#equation-prop-reproductora">(34)</a> se tiene que <span class="math notranslate nohighlight">\(\forall f\in H\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
f(\boldsymbol{x}_{m})&amp;=\langle f, \kappa(\cdot, \boldsymbol{x}_{m})\rangle\\
&amp;=\left\langle\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+f_{\perp}, \kappa(\cdot, \boldsymbol{x}_{m})\right\rangle\\
&amp;=\left\langle\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n}), \kappa(\cdot, \boldsymbol{x}_{m})\right\rangle\\
&amp;=\sum_{n=1}^{N}\theta_{n}\langle\kappa(\cdot, \boldsymbol{x}_{n}), \kappa(\cdot, \boldsymbol{x}_{m})\rangle\\
&amp;=\sum_{n=1}^{N}\theta_{n}\kappa(\boldsymbol{x}_{m}, \boldsymbol{x}_{n}).
\end{align}
\end{split}\]</div>
<p>Nótese que <strong>la propiedad de reproducción garantiza que en los puntos de entrenamiento el valor de</strong> <span class="math notranslate nohighlight">\(f\)</span> <strong>no depende de</strong> <span class="math notranslate nohighlight">\(f_{\perp}\)</span>, y, por lo tanto, tampoco el primer término de <span class="math notranslate nohighlight">\(\min_{f\in H}J(f)\)</span> en <a class="reference internal" href="#equation-eq-min-reg">(35)</a>.</p>
<p>Además, para todo <span class="math notranslate nohighlight">\(f_{\perp}\)</span> tenemos que:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\Omega(\|f\|^{2})&amp;=\Omega\left(\left\langle\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+f_{\perp}, \sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+f_{\perp}\right\rangle\right)\\
&amp;=\Omega\left(\left\langle\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n}), \sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})\right\rangle+\langle f_{\perp}, f_{\perp}\rangle\right)\\
&amp;=\Omega\left(\left\|\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})\right\|^{2}+\|f_{\perp}\|^{2}\right)\\
&amp;\geq\Omega\left(\left\|\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})\right\|^{2}\right)
\end{align*}
\end{split}\]</div>
<p>Por lo tanto <code class="docutils literal notranslate"><span class="pre">para</span> <span class="pre">cualquier</span> <span class="pre">selección</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(\theta_{n},~n=1,2,\dots,N\)</span>, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span> <span class="pre">es</span> <span class="pre">minimizada</span> <span class="pre">por</span></code> <span class="math notranslate nohighlight">\(f_{\perp}=0\)</span>.</p>
<div class="proof observation admonition" id="observation-8">
<p class="admonition-title"><span class="caption-number">Observation 7 </span></p>
<section class="observation-content" id="proof-content">
<ul>
<li><p>La importancia de este teorema radica en que, para optimizar <span class="math notranslate nohighlight">\(J(f)\)</span> en <a class="reference internal" href="#equation-eq-min-reg">(35)</a> con respecto a <span class="math notranslate nohighlight">\(f\)</span>, usamos la expresión <span class="math notranslate nohighlight">\(f(\cdot)\)</span> en <a class="reference internal" href="#equation-prop-reproductora">(34)</a> y <strong>la minimización es llevada a cabo con respecto a los parámetros</strong> <span class="math notranslate nohighlight">\(\theta_{n},~n=1,2,\dots,N\)</span>.</p></li>
<li><p>En casos que la regularización no es necesaria, usualmente un término de sesgo es agregado y se supone que la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">a</span> <span class="pre">minimizar</span> <span class="pre">admite</span> <span class="pre">la</span> <span class="pre">representación</span></code></p>
<div class="math notranslate nohighlight">
\[
    \tilde{f}=f+b,\quad f(\cdot)=\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})
    \]</div>
</li>
</ul>
</section>
</div><ul class="simple">
<li><p>La esencia del siguiente teorema es expandir la solución en dos partes. <strong>Una que pertenece al RKHS</strong>, <span class="math notranslate nohighlight">\(H\)</span>, y otra que está dada como una <strong>combinación lineal de un conjunto de funciones preseleccionadas</strong>.</p></li>
</ul>
<div class="proof theorem admonition" id="th_rep_semipar">
<p class="admonition-title"><span class="caption-number">Theorem 4 </span> (Teorema de representación semiparamétrico)</p>
<section class="theorem-content" id="proof-content">
<p>Supongamos que adicionalmente a los supuestos adoptados en el <a class="reference internal" href="#th_representation">Theorem 3</a>, es dado el siguiente <strong>conjunto de funciones reales</strong></p>
<div class="math notranslate nohighlight">
\[
\varphi_{m}:\mathcal{X}\longmapsto\mathbb{R},\quad m=1,2,\dots,M,
\]</div>
<p>con la propiedad que la matriz de <span class="math notranslate nohighlight">\(N\times M\)</span> con elementos <span class="math notranslate nohighlight">\(\varphi_{m}(\boldsymbol{x}_{n})\)</span>, <span class="math notranslate nohighlight">\(~n=1,2,\dots,N\)</span>, <span class="math notranslate nohighlight">\(~m=1,2,\dots,M\)</span>, tiene rango <span class="math notranslate nohighlight">\(M\)</span>. Entonces, cualquier</p>
<div class="math notranslate nohighlight">
\[
\tilde{f}=f+h,\quad f\in H,~ h\in\text{span}\{\varphi_{m}, m=1,2,\dots,M\},
\]</div>
<p>que soluciona la tarea de minimización</p>
<div class="math notranslate nohighlight">
\[
\min_{\tilde{f}} J(\tilde{f}):=\sum_{n=1}^{N}\mathcal{L}(y_{n}, \tilde{f}(\boldsymbol{x}_{n}))+\Omega(\|f\|^{2}),
\]</div>
<p><strong>admite la representación</strong></p>
<div class="math notranslate nohighlight">
\[
\tilde{f}(\cdot)=\sum_{n=1}^{N}\theta_{n}\kappa(\cdot, \boldsymbol{x}_{n})+\sum_{m=1}^{M}b_{m}\psi_{m}(\cdot).
\]</div>
</section>
</div><div class="proof observation admonition" id="observation-10">
<p class="admonition-title"><span class="caption-number">Observation 8 </span></p>
<section class="observation-content" id="proof-content">
<p>Un ejemplo de <code class="docutils literal notranslate"><span class="pre">aplicación</span> <span class="pre">exitosa</span> <span class="pre">de</span> <span class="pre">este</span> <span class="pre">teorema</span> <span class="pre">fue</span> <span class="pre">demostrada</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">contexto</span> <span class="pre">de</span> <span class="pre">procesamiento</span> <span class="pre">de</span> <span class="pre">imágenes</span></code> <span id="id3">[<a class="reference internal" href="biblio.html#id15" title="Pantelis Bouboulis, Konstantinos Slavakis, and Sergios Theodoridis. Adaptive kernel-based image denoising employing semi-parametric regularization. IEEE Transactions on Image Processing, 19(6):1465–1479, 2010.">Bouboulis <em>et al.</em>, 2010</a>]</span>. Un conjunto de funciones no lineales en lugar de <span class="math notranslate nohighlight">\(\psi_{m}\)</span> fueron usadas para <strong>detección de bordes en una imagen con (saltos no suaves)</strong>. La parte de <span class="math notranslate nohighlight">\(f\)</span> que pertenece al espacio <code class="docutils literal notranslate"><span class="pre">RKHS</span></code> es usada para las partes suaves en la imagen.</p>
</section>
</div></section>
<section id="regresion-ridge-con-kernel">
<h2>Regresión ridge con Kernel<a class="headerlink" href="#regresion-ridge-con-kernel" title="Link to this heading">#</a></h2>
<ul>
<li><p>En esta sección abordaremos la tarea de <em><strong>regresión ridge en un espacio general RKHS</strong></em>. El camino a seguir es el usado típicamente para <em><strong>extender técnicas, las cuales han sido desarrolladas para modelos lineales, a espacios más generales RKSH</strong></em>.</p></li>
<li><p>Sea <span class="math notranslate nohighlight">\((y_{n}, \boldsymbol{x}_{n})\in\mathbb{R}\times\mathbb{R}^{l}\)</span> la representación de un mecanismo de generación de datos, modelados vía tarea de <em><strong>regresión no lineal</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    y_{n}=g(\boldsymbol{x}_{n})+\eta_{n},\quad n=1,2,\dots,N.
    \end{split}\]</div>
</li>
<li><p>Denotemos por <span class="math notranslate nohighlight">\(f\)</span> es el <em><strong>estimador de la función</strong></em> <span class="math notranslate nohighlight">\(g\)</span> desconocida. <span class="math notranslate nohighlight">\(f\)</span> <em><strong>es llamada la hipótesis</strong></em> y <span class="math notranslate nohighlight">\(H\)</span> <em><strong>el espacio de hipótesis</strong></em>, donde <span class="math notranslate nohighlight">\(f\)</span> es buscada. Supongamos que <span class="math notranslate nohighlight">\(f\)</span> está en <em><strong>RKHS</strong></em> y está asociada con el kernel</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\kappa:\mathbb{R}^{l}\times\mathbb{R}^{l}\longmapsto\mathbb{R}.
\]</div>
<ul class="simple">
<li><p>Por el <em><strong>teorema de representación</strong></em> se tiene que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f(\boldsymbol{x})=\sum_{n=1}^{N}\theta_{n}\kappa(\boldsymbol{x}, \boldsymbol{x}_{n}).
\]</div>
<ul>
<li><p>De acuerdo a la <em><strong>regresión ridge con kernel</strong></em>, los coeficientes desconocidos son estimados por medio de la siguiente tarea</p>
<div class="math notranslate nohighlight" id="equation-eq-task-ridge-reg">
<span class="eqno">(36)<a class="headerlink" href="#equation-eq-task-ridge-reg" title="Link to this equation">#</a></span>\[
    \hat{\theta}=\text{argmin}_{\theta} J(\theta),\quad J(\theta)=\sum_{n=1}^{N}\left(y_{n}-\sum_{m=1}^{N}\theta_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})\right)^{2}+C\langle f, f\rangle,
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(C\)</span> es el <em><strong>parámetro de regularización</strong></em>. La Ecuación <a class="reference internal" href="#equation-eq-task-ridge-reg">(36)</a> puede reescribirse como</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    J(\theta)=(\boldsymbol{y}-K\theta)^{T}(\boldsymbol{y}-K\theta)+C\theta^{T}K\theta,
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\boldsymbol{y}=(y_{1}, y_{2},\dots, y_{N})^{T}\)</span> y <span class="math notranslate nohighlight">\(\theta=(\theta_{1}, \theta_{2}, \dots,\theta_{M})^{T}\)</span>, <span class="math notranslate nohighlight">\(K\)</span> es la matriz kernel tal que <span class="math notranslate nohighlight">\(\langle\kappa(\cdot, \boldsymbol{x}_{m}), \kappa(\cdot, \boldsymbol{x}_{m})\rangle=\kappa(\boldsymbol{x}_{m}, \boldsymbol{x}_{n})\)</span>, y <span class="math notranslate nohighlight">\(y\)</span> es <em><strong>determinada por la función kernel y los valores de entrenamiento</strong></em>.</p>
</li>
</ul>
<ul class="simple">
<li><p><em><strong>Minimizando</strong></em> <span class="math notranslate nohighlight">\(J(\theta)\)</span> con respecto a <span class="math notranslate nohighlight">\(\theta\)</span> y <em><strong>suponiendo que</strong></em> <span class="math notranslate nohighlight">\(K^{T}=K\)</span> <em><strong>es invertible</strong></em>, utilizando el <em><strong>mismo procedimiento en la regresión ridge</strong></em>, tenemos que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
(K^{T}K+CK^{T})\hat{\theta}=K^{T}\boldsymbol{y}\quad\text{o}\quad\boldsymbol{y} = (K+CI)\hat{\theta}.
\]</div>
<ul>
<li><p>Una vez <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> es obtenido, dado un vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\in\mathbb{R}^{l}\)</span> la correspondiente <em><strong>predicción de la variable dependiente</strong></em> está dada por:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \hat{\boldsymbol{y}}=\sum_{n=1}^{N}\hat{\theta}_{n}\kappa(\boldsymbol{x}, \boldsymbol{x}_{n})=\theta^{T}\boldsymbol{\kappa}(\boldsymbol{x})=\boldsymbol{y}^{T}(K+CI)^{-1}\boldsymbol{\kappa}(\boldsymbol{x}),
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\boldsymbol{\kappa}(\boldsymbol{x})=(\kappa(\boldsymbol{x},\boldsymbol{x}_{1}),\dots,\boldsymbol{\kappa}(\boldsymbol{x}, \boldsymbol{x}_{N}))^{T}\)</span>.</p>
</li>
</ul>
</section>
<section id="regresion-de-vectores-de-soporte">
<h2>Regresión de vectores de soporte<a class="headerlink" href="#regresion-de-vectores-de-soporte" title="Link to this heading">#</a></h2>
<div class="admonition-metodo-de-minimos-modulos admonition">
<p class="admonition-title">Método de mínimos módulos</p>
<ul>
<li><p>El método de <em><strong>mínimos cuadrados no es siempre el mejor criterio de optimización</strong></em>, debido a que, <em><strong>en presencia de ruido no Gaussiano</strong></em> con colas largas y por lo tanto número creciente de outliers, <em><strong>la dependencia cuadrática de el método se sesga hacia valores asociados con presencia de outliers</strong></em></p></li>
<li><p>Una manera de darle solución a este problema es escoger una <em><strong>función de pérdida que se ajuste mejor al modelo del ruido</strong></em> (ver Huber 1992, <span id="id4">[<a class="reference internal" href="biblio.html#id16" title="Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pages 492–518. Springer, 1992.">Huber, 1992</a>]</span>). Bajo el <em><strong>supuesto de que el ruido tiene función de densidad de probabilidad (fdp) simétrica</strong></em>, la tarea optima de regresión es obtenida minimizando la función de pérdida</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[2mm]
    \mathcal{L}(y, f(\boldsymbol{x}))=|y-f(\boldsymbol{x})|
    \end{split}\]</div>
<p>la cual es conocida como el <em><strong>método de mínimos módulos</strong></em>.</p>
</li>
</ul>
</div>
<div class="admonition-perdida-de-huber admonition">
<p class="admonition-title">Pérdida de Huber</p>
<ul>
<li><p>Huber demostró que <em><strong>si el ruido tiene dos componentes: Gaussiana y otra fdp arbitraria (simétrica)</strong></em>, entonces la <em><strong>mejor función de pérdida en el sentido minimax</strong></em> está dada por:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \mathcal{L}(y, f(\boldsymbol{x}))=
    \begin{cases}
    \displaystyle{\varepsilon|y-f(\boldsymbol{x})|-\varepsilon^{2}/2} &amp; \text{si}~|y-f(\boldsymbol{x})|&gt;\varepsilon,\\[2mm]
    \displaystyle{|y-f(\boldsymbol{x})|/2-\varepsilon^{2}/2} &amp; \text{si}~|y-f(\boldsymbol{x})|\leq\varepsilon,
    \end{cases}
    \end{split}\]</div>
<p>para algún parámetro <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Esta es conocida como función de <em><strong>pérdida de Huber</strong></em>.</p>
</li>
</ul>
</div>
<div class="admonition-perdida-varepsilon-insensible admonition">
<p class="admonition-title">Pérdida <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible</p>
<ul>
<li><p>Una <em><strong>función de pérdida que se aproxima a la de Huber</strong></em> y tiene excelentes propiedades computacionales es llamada <em><strong>función de pérdida lineal</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>-<em><strong>insensible</strong></em> y está dada por</p>
<div class="math notranslate nohighlight" id="equation-eq-lin-ins">
<span class="eqno">(37)<a class="headerlink" href="#equation-eq-lin-ins" title="Link to this equation">#</a></span>\[\begin{split}
    \\[1mm]
    \mathcal{L}(y, f(\boldsymbol{x}))=
    \begin{cases}
    |y-f(\boldsymbol{x})|-\varepsilon &amp; \text{si}~|y-f(\boldsymbol{x})|&gt;\varepsilon\\[2mm]
    0 &amp; \text{si}~|y-f(\boldsymbol{x})|\leq\varepsilon
    \end{cases}
    \end{split}\]</div>
<p>y la función de pérdida <em><strong>cuadrática</strong></em> <span class="math notranslate nohighlight">\(\epsilon\)</span><em><strong>-insensible</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathcal{L}(y, f(\boldsymbol{x}))=
    \begin{cases}
    |y-f(\boldsymbol{x})|^{2}-\varepsilon &amp; \text{si}~|y-f(\boldsymbol{x})|&gt;\varepsilon\\[2mm]
    0 &amp; \text{si}~|y-f(\boldsymbol{x})|\leq\varepsilon
    \end{cases}
    \end{split}\]</div>
<p>la cuales <em><strong>coinciden con la función de pérdida de mínimos módulos para</strong></em> <span class="math notranslate nohighlight">\(\varepsilon=0\)</span> y es <em><strong>cercana a la función de pérdida de Huber para</strong></em> <span class="math notranslate nohighlight">\(\varepsilon&lt;1\)</span>.</p>
</li>
</ul>
</div>
<figure class="align-center" id="huber-loss-plot">
<a class="reference internal image-reference" href="_images/huber_loss_plot.png"><img alt="_images/huber_loss_plot.png" src="_images/huber_loss_plot.png" style="width: 411.3px; height: 271.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Función de pérdida de Huber (gris punteado). Función de pérdida lineal <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible (gris completo). Función de pérdida cuadrática <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible (rojo) para <span class="math notranslate nohighlight">\(\varepsilon = 0.7\)</span>. Fuente <span id="id5">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#huber-loss-plot" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La función de pérdida <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensible se utiliza en escenarios donde <em><strong>queremos que nuestro modelo tolere errores hasta cierto punto</strong></em>. La pérdida <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensible permite que el modelo sea <em><strong>menos sensible a valores atípicos o puntos de datos ruidosos</strong></em></p></li>
<li><p>La función de pérdida <span class="math notranslate nohighlight">\(\epsilon\)</span>-insensible proporciona una herramienta útil para <em><strong>construir modelos que sean robustos, flexibles y capaces de equilibrar la precisión con la generalización</strong></em>, especialmente en <em><strong>situaciones donde los datos pueden contener ruido o valores atípicos</strong></em>.</p></li>
</ul>
</div>
</section>
<section id="regresion-optima-lineal-varepsilon-insensible">
<h2>Regresión óptima lineal <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible<a class="headerlink" href="#regresion-optima-lineal-varepsilon-insensible" title="Link to this heading">#</a></h2>
<ul>
<li><p>Usaremos la <em><strong>función de pérdida lineal</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible <a class="reference internal" href="#equation-eq-lin-ins">(37)</a> para <em><strong>cuantificar el error de desajuste del modelo</strong></em>. El valor de <span class="math notranslate nohighlight">\(\epsilon\)</span> <em>determina la cantidad de “slack” (grado de error) que se permite</em>.</p></li>
<li><p>Trataremos la tarea de regresión</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    y=g(\boldsymbol{x})+\eta_{n},
    \end{split}\]</div>
<p>usando un model lineal de la forma:</p>
<div class="math notranslate nohighlight">
\[
    f(\boldsymbol{x})=\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}.
    \]</div>
<p>La solución más general en la que <span class="math notranslate nohighlight">\(f\)</span> <em><strong>es una función en RKHS será obtenida vía kernel trick</strong></em> (producto interno reemplazado por <em><strong>evaluación de kernel</strong></em>).</p>
</li>
<li><p>Introducimos ahora <em><strong>dos conjuntos de variables auxiliares</strong></em></p>
<div class="math notranslate nohighlight" id="equation-eq-restric-1">
<span class="eqno">(38)<a class="headerlink" href="#equation-eq-restric-1" title="Link to this equation">#</a></span>\[\begin{split}
    \\[1mm]
    \text{Si}~ y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}\geq\varepsilon,~\text{definimos}~\tilde{\xi}_{n}~\text{tal que},\quad y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}\leq\varepsilon+\tilde{\xi}_{n}.
    \end{split}\]</div>
<p>Nótese que <em><strong>idealmente, deseamos seleccionar</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}, \theta_{0}\)</span> <em><strong>tales que</strong></em> <span class="math notranslate nohighlight">\(\tilde{\xi}_{n}=0\)</span>, dado que este valor entregaría la <em><strong>contribución del respectivo término en la función de pérdida igual a cero</strong></em>, pues <span class="math notranslate nohighlight">\(\mathcal{L}(y, f(\boldsymbol{x}))=0\)</span> si <span class="math notranslate nohighlight">\(|y-f(\boldsymbol{x})|\leq\varepsilon\)</span>. Análogamente,</p>
<div class="math notranslate nohighlight" id="equation-eq-restric-2">
<span class="eqno">(39)<a class="headerlink" href="#equation-eq-restric-2" title="Link to this equation">#</a></span>\[\begin{split}
    \\[1mm]
    \text{Si}~ y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}\leq-\varepsilon,~\text{definimos}~\xi_{n}~\text{tal que},\quad\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}\leq\varepsilon+\xi_{n}.
    \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>Estamos ahora listos para formular la <em><strong>tarea de minimización para el correspondiente coste empírico</strong></em>, regularizado por la norma de <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, el cual es <em><strong>proyectado en términos de las variables auxiliares</strong></em> como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
\text{minimizar} &amp; J(\boldsymbol{\theta}, \theta_{0}, \xi, \tilde{\xi})=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\left(\displaystyle{\sum_{n=1}^{N}\xi_{n}+\sum_{n=1}^{N}\tilde{\xi}_{n}}\right),\\[2mm]
\text{sujeto a} &amp; y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}\leq\varepsilon+\tilde{\xi}_{n},\quad n=1,2,\dots,N,\\[2mm]
 &amp; \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}\leq\varepsilon+\xi_{n},\quad n=1,2,\dots,N,\\[2mm]
&amp;\tilde{\xi}_{n}\geq0,\quad\tilde{\xi}_{n}\geq0,~ n=1,2,\dots,N.
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>El término <span class="math notranslate nohighlight">\(\frac{1}{2}\|\boldsymbol{\theta}\|^{2}\)</span>, surge de la necesidad de <em><strong>maximizar el margen asociado al hiperplano que separa dos clases</strong></em> (ver sección <em><strong>máquinas de vectores de soporte</strong></em> <em>Vapnik and Chervonenkis 1960s</em>).</p></li>
<li><p>Dicho margen: <span class="math notranslate nohighlight">\(2/\|\boldsymbol{\theta}\|\)</span>, el cual, en <em><strong>problemas de clasificación, se desea maximizar, es equivalente a minimizar la norma</strong></em> <span class="math notranslate nohighlight">\(\frac{1}{2}\|\boldsymbol{\theta}\|^2\)</span> para el caso de <em><strong>modelos regresivos</strong></em>. El factor <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> es incluido de forma numéricamente conveniente, y no altera la solución. A este tipo de problemas se les denomina de <em><strong>programación cuadrática</strong></em>.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">¿Por qué maximizar el margen?</p>
<ul class="simple">
<li><p>Maximizar el margen permite <em><strong>lograr un buen rendimiento de generalización</strong></em>. El <em><strong>margen representa el nivel de confianza del clasificador, y maximizarlo ayuda a minimizar el error de generalización</strong></em>.</p></li>
<li><p>Maximizar el margen corresponde a <em><strong>minimizar la</strong></em> <a class="reference external" href="https://es.wikipedia.org/wiki/Dimensi%C3%B3n_VC">Dimensión VC (Vapnik-Chervonenkis)</a>, que es una <em><strong>medida de la capacidad o complejidad del modelo</strong></em>. Al maximizar el margen, SVM busca <em><strong>encontrar el límite de decisión más simple que separa las clases de manera efectiva</strong></em>, lo que conduce a una <em><strong>mejor generalización a datos no vistos</strong></em>.</p></li>
<li><p>Maximizar el margen ayuda a <em><strong>crear un límite de decisión que es robusto al ruido y a los valores atípicos en los datos de entrenamiento</strong></em>. Al enfocarse en los puntos de datos cerca del margen, <em><strong>SVM puede tolerar mejor el ruido y los valores atípicos, lo que lleva a un clasificador más confiable y preciso</strong></em>.</p></li>
</ul>
</div>
<figure class="align-center" id="largest-margin-fig">
<a class="reference internal image-reference" href="_images/largest_margin.png"><img alt="_images/largest_margin.png" src="_images/largest_margin.png" style="width: 541.8px; height: 253.79999999999998px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Dos <em><strong>líneas de separación</strong></em>: una buena con un <em><strong>margen grande</strong></em> (derecha) y una línea de separación menos aceptable con un <em><strong>margen pequeño</strong></em> (izquierda).</span><a class="headerlink" href="#largest-margin-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Las variables auxiliares, <span class="math notranslate nohighlight">\(\tilde{\xi}_{n}\)</span> y <span class="math notranslate nohighlight">\(\xi_{n},~ n=1,2,\dots,N\)</span> que <em><strong>miden el exceso de error con respecto a</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>, son conocidas como <em><strong>variables de relajación</strong></em>. Nótese que cualquier contribución de la función de coste, para un error obtenido con un valor absoluto menor o igual que <span class="math notranslate nohighlight">\(\varepsilon\)</span> es cero. La tarea de optimización previa, intenta <em><strong>estimar</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}, \theta_{0}\)</span> <em><strong>tal que valores de error mayores que</strong></em> <span class="math notranslate nohighlight">\(\epsilon\)</span> <em><strong>y menores que</strong></em> <span class="math notranslate nohighlight">\(-\epsilon\)</span> <em><strong>sean minimizados</strong></em>.</p></li>
<li><p>Por lo tanto, la tarea de optimización es equivalente a <em><strong>minimizar la función de pérdida empírica</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\sum_{n=1}^{N}\mathcal{L}(y_{n}, \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}),
    \end{split}\]</div>
<p>donde la función de pérdida <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> es <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible.</p>
</li>
<li><p>Nótese que debido a que el problema de optimización con restricción <em><strong>involucra las variables de relajación</strong></em> con valores históricos, empleamos el <em><strong>kernel trick</strong></em>.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Solución</span></code></strong></p>
<ul>
<li><p>La solución de la tarea de optimización es obtenida introduciendo los <em><strong>multiplicadores de Lagrange</strong></em> y formando el correspondiente <em><strong>Lagrangiano</strong></em>. <em><strong>Habiendo obtenido los multiplicadores, la solución estaría dada por</strong></em></p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{\theta}}=\sum_{n=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})\boldsymbol{x}_{n},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\tilde{\lambda}_{n},\lambda_{n},~ n=1,2,\dots,N\)</span>, son los <em><strong>multiplicadores asociados con cada una de las restricciones</strong></em>.</p>
</li>
</ul>
<ul class="simple">
<li><p>Los <em><strong>multiplicadores de Lagrange son distintos de cero, solo para puntos</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span>, que corresponden a <em><strong>valores de error ya sean iguales o mayores que</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Estos son conocidos como <em><strong>vectores de soporte</strong></em>.</p></li>
<li><p>Puntos para los que el valor del <em><strong>puntaje de error es menor que</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>, <em><strong>corresponden a multiplicadores de Lagrange ceros</strong></em> y por lo tanto <em><strong>no son vectores de soporte y no participan en la construcción de la función de decisión</strong></em></p></li>
</ul>
<figure class="align-center" id="epsilon-tube-fig">
<a class="reference internal image-reference" href="_images/epsilon_tube.png"><img alt="_images/epsilon_tube.png" src="_images/epsilon_tube.png" style="width: 414.4px; height: 323.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Parámetros utilizados en la <em><strong>regresión de vectores de soporte (unidimensional)</strong></em>. Cuadrados rellenos son vectores de soporte, y los vacíos no lo son. Por lo tanto, los <em><strong>SV sólo pueden aparecer en el límite del tubo o fuera del tubo</strong></em></span><a class="headerlink" href="#epsilon-tube-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>El término de sesgo <span class="math notranslate nohighlight">\(\hat{\theta}_{0}\)</span> <em><strong>puede ser obtenido a partir de las ecuaciones</strong></em></p>
<div class="math notranslate nohighlight" id="equation-eq-sistem-bias">
<span class="eqno">(40)<a class="headerlink" href="#equation-eq-sistem-bias" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align}
    y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}&amp;=\varepsilon,\\[2mm]
    \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}&amp;=\varepsilon,
    \end{align} 
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(n\)</span> recorre todos los puntos que forman un <em><strong>subconjunto de vectores de soporte</strong></em>, esto es, puntos asociados con:</p>
<div class="math notranslate nohighlight">
\[
    \tilde{\lambda}_{n}&gt;0,~(\lambda_{n}&gt;0)\quad\text{y}\quad\tilde{\xi}_{n}=0,~(\xi_{n}=0).
    \]</div>
<p>En la práctica, <span class="math notranslate nohighlight">\(\hat{\theta}_{0}\)</span> <em><strong>es obtenido a partir del promedio de todas las ecuaciones en</strong></em> <a class="reference internal" href="#equation-eq-sistem-bias">(40)</a>.</p>
</li>
</ul>
<ul>
<li><p>Una vez <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}, \hat{\theta}_{0}\)</span> han sido obtenidos, estamos listos para <em><strong>realizar predicciones</strong></em>. Dado un valor <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, primero desarrollamos el <em><strong>mapeo (implícito) usando el mapeo de características</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\longmapsto\kappa(\cdot, \boldsymbol{x})\)</span> obtenemos</p>
<div class="math notranslate nohighlight">
\[
    \hat{y}(\boldsymbol{x})=\langle\hat{\boldsymbol{\theta}}, \kappa(\cdot, \boldsymbol{x})\rangle+\hat{\theta}_{0},
    \]</div>
<p>o bien,</p>
<div class="math notranslate nohighlight">
\[
    \hat{y}(\boldsymbol{x})=\sum_{n=1}^{N_{s}}(\tilde{\lambda}_{n}-\tilde{\lambda}_{n})\kappa(\boldsymbol{x}, \boldsymbol{x}_{n})+\hat{\theta}_{0},\quad\text{Predicción SVR},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(N_{s}\leq N\)</span>, es el <em><strong>número de multiplicadores de Lagrange distintos de cero</strong></em>. Nótese que esta última es una expansión en términos de funciones kernel no lineales.</p>
</li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Solución</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code></strong></p>
<ul>
<li><p>La tarea de <em><strong>optimización con desigualdades de restricción</strong></em>, satisface las <em><strong>condiciones de Karush–Kuhn–Tucker (KKT)</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \begin{align*}
    &amp;\frac{\partial L}{\partial\boldsymbol{\theta}}=\boldsymbol{0},~\frac{\partial L}{\partial\theta_{0}}=0,~\frac{\partial L}{\partial\tilde{\xi}_{n}}=0,~\frac{\partial L}{\partial\xi_{n}}=0,\\[2mm]
    &amp;\tilde{\lambda}_{n}(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon-\tilde{\xi}_{n})=0,\quad n=1,2,\dots,N,\\[2mm]
    &amp;\lambda_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}-\varepsilon-\xi_{n})=0,\quad n=1,2,\dots,N,\\[2mm]
    &amp;\tilde{\mu}_{n}\tilde{\xi}_{n}=0,\quad n=1,2,\dots,N,\\[2mm]
    &amp;\mu_{n}\xi_{n}=0,\quad n=1,2,\dots,N,\\[2mm]
    &amp;\tilde{\lambda}_{n}\geq0,~\lambda_{n}\geq0,~\tilde{\mu}_{n}\geq0,~\mu_{n}\geq0,\quad n=1,2,\dots,N,
    \end{align*}
    \end{split}\]</div>
<p>con respecto al <em><strong>Lagrangiano</strong></em></p>
<div class="math notranslate nohighlight" id="equation-eq-lagrangiano-general">
<span class="eqno">(41)<a class="headerlink" href="#equation-eq-lagrangiano-general" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    L(\boldsymbol{\theta}, \theta_{0}, \tilde{\xi}, \xi, \lambda, \mu)&amp;=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\left(\sum_{n=1}^{N}{\xi}_{n}+\sum_{n=1}^{N}\tilde{\xi}_{n}\right)\\
    &amp;+\sum_{n=1}^{N}\tilde{\lambda}_{n}(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon-\tilde{\xi}_{n})\\
    &amp;+\sum_{n=1}^{N}\lambda_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}-\varepsilon-\xi_{n})\\
    &amp;-\sum_{n=1}^{N}\tilde{\mu}_{n}\tilde{\xi}_{n}-\sum_{n=1}^{N}\mu_{n}\xi_{n},
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\tilde{\lambda}_{n},~\lambda_{n},~\tilde{\mu}_{n},~\mu_{n}\)</span> son los <em><strong>multiplicadores de Lagrange</strong></em>.</p>
</li>
</ul>
<ul>
<li><p><em><strong>Sumando las restricciones</strong></em> (multiplicando cada restricción por <span class="math notranslate nohighlight">\(\tilde{\lambda}_{n},\lambda_{n}, \tilde{\xi}_{n}, \xi_{n}\)</span>), se tiene que:</p>
<div class="math notranslate nohighlight">
\[
    \tilde{\lambda}_{n}\lambda_{n}((y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon)(\xi_{n}-\tilde{\xi}_{n})-2\tilde{\xi}_{n}\xi_{n})=0,
    \]</div>
<p>dado que los <em><strong>multiplicadores de Lagrange que participan en la solución</strong></em> <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> son aquellos <em><strong>distintos de cero</strong></em>, para puntos <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> correspondientes a valores de <em><strong>errores que sean mayores o iguales que</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>, esto es, por ejemplo <span class="math notranslate nohighlight">\((y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0})\geq\varepsilon,~\lambda_{n}\neq0\)</span>.</p>
<p>Si <span class="math notranslate nohighlight">\(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}=\varepsilon\)</span> (similar para <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}=\varepsilon\)</span>), entonces</p>
<div class="math notranslate nohighlight">
\[
    (i)~\tilde{\lambda}_{n}\lambda_{n}=0\quad\text{o}\quad(ii)~(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon)(\xi_{n}-\tilde{\xi}_{n})-2\tilde{\xi}_{n}\xi_{n}=0
    \]</div>
<p>Si <span class="math notranslate nohighlight">\(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}&gt;\varepsilon\)</span> (similar para <span class="math notranslate nohighlight">\(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}&lt;-\varepsilon\)</span>), entonces <span class="math notranslate nohighlight">\((i)~\tilde{\lambda}_{n}\lambda_{n}=0\)</span>. Además, <em><strong>basados en la tarea de optimización</strong></em>, seleccionando <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}, \theta_{0}\)</span> tales que <span class="math notranslate nohighlight">\(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon\rightarrow0\)</span> se tiene que</p>
<div class="math notranslate nohighlight">
\[
    (ii)~(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\theta_{0}-\varepsilon)(\xi_{n}-\tilde{\xi}_{n})-2\tilde{\xi}_{n}\xi_{n}=0\Rightarrow\tilde{\xi}_{n}\xi_{n}=0.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Esto es</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-xin-xintilde-product">
<span class="eqno">(42)<a class="headerlink" href="#equation-xin-xintilde-product" title="Link to this equation">#</a></span>\[
\tilde{\xi}_{n}\xi_{n}=0,~\tilde{\lambda}_{n}\lambda_{n}=0,~n=1,2,\dots,N.
\]</div>
<ul class="simple">
<li><p><em><strong>Derivando el Lagrangiano</strong></em> <a class="reference internal" href="#equation-eq-lagrangiano-general">(41)</a> tenemos los resultados</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\boldsymbol{\theta}}&amp;=\boldsymbol{0}\Leftrightarrow\hat{\boldsymbol{\theta}}=\sum_{n=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})\boldsymbol{x}_{n};\quad\frac{\partial}{\partial\boldsymbol{\theta}}(\frac{1}{2}\|\boldsymbol{\theta}\|^{2})=\boldsymbol{\theta},~\frac{\partial}{\partial\boldsymbol{\theta}}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n})=\boldsymbol{x}_{n}\\
\frac{\partial L}{\partial\theta_{0}}&amp;=0\Leftrightarrow\sum_{n=1}^{N}\tilde{\lambda}_{n}=\sum_{n=1}^{N}\lambda_{n};\quad\frac{\partial}{\partial\theta_{0}}(\theta_{0})=1,\\
\frac{\partial L}{\partial\tilde{\xi}_{n}}&amp;=0\Leftrightarrow C-\tilde{\lambda}_{n}-\tilde{\mu}_{n}=0;\quad
\frac{\partial}{\partial\tilde{\xi}_{n}}\left(\sum_{n=1}^{N}\tilde{\xi}_{n}\right)=1\\
\frac{\partial L}{\partial\xi_{n}}&amp;=0\Leftrightarrow C-\lambda_{n}-\mu_{n}=0.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Para proceder a <em><strong>calcular los multiplicadores asociados a la tarea de optimización</strong></em>, consideraremos la <em><strong><code class="docutils literal notranslate"><span class="pre">Representación</span> <span class="pre">dual</span> <span class="pre">de</span> <span class="pre">Walfe</span></code></strong></em>. En esta representación es considerado el <em><strong>término de sesgo</strong></em> <span class="math notranslate nohighlight">\(\theta_{0}=0\)</span>. Además, <em><strong>consideraremos solo el uso de vectores de soportes</strong></em>, esto es valores asociados a <span class="math notranslate nohighlight">\(\boldsymbol{\xi}_{n}=\tilde{\boldsymbol{\xi}}_{n}=0\)</span>, para proceder con la <em><strong>representación dual respectiva</strong></em>. Esta representación es obtenida a partir del Lagrangiano <span class="math notranslate nohighlight">\(L(\boldsymbol{\theta}, \theta_{0}, \tilde{\xi}, \xi, \lambda, \mu)\)</span>.</p></li>
</ul>
<div class="proof property admonition" id="walfe_prop">
<p class="admonition-title"><span class="caption-number">Property 3 </span> (Representación dual de Walfe)</p>
<section class="property-content" id="proof-content">
<ul>
<li><p>Un <em><strong>problema de programación convexa</strong></em> es equivalente a</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \max_{\lambda\geq0}&amp;\quad L(\boldsymbol{\theta}, \boldsymbol{\lambda}),\\
    \text{sujeto a}&amp;\quad\frac{\partial}{\partial\boldsymbol{\theta}}L(\boldsymbol{\theta},\boldsymbol{\lambda})=\boldsymbol{0}.
    \end{split}\]</div>
<p>La última ecuación garantiza que <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> es un <em><strong>mínimo del Lagrangiano</strong></em>.</p>
</li>
<li><p>Considere el siguiente <em><strong>problema cuadrático</strong></em>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \text{minimizar:}&amp;\quad\frac{1}{2}\boldsymbol{\theta}^{T}\boldsymbol{\theta},\\
    \text{sujeto a:}&amp;\quad A\boldsymbol{\theta}\geq b.
    \end{align*}
    \end{split}\]</div>
<p>el cual admite la siguiente <em><strong>representación de Wolfe</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \text{minimizar:}&amp;\quad\frac{1}{2}\boldsymbol{\theta}^{T}\boldsymbol{\theta}-\lambda^{T}(A\boldsymbol{\theta}-\boldsymbol{b}),\\
    \text{sujeto a:}&amp;\quad\theta-A^{T}\lambda=0.
    \end{align*}
    \end{split}\]</div>
</li>
<li><p><em><strong>Solucionando la tarea de optimización de Wolfe</strong></em> con respecto a <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, podemos <em><strong>escribir el problema dual involucrando solo multiplicadores de Lagrange</strong></em>. De esta forma obtenemos otro problema cuadrático, pero más simple</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \begin{align*}
    \max_{\lambda}&amp;\left\{\displaystyle{-\frac{1}{2}\boldsymbol{\lambda}^{T}AA^{T}\boldsymbol{\lambda}+\boldsymbol{\lambda}^{T}\boldsymbol{b}}\right\}\\
    \text{sujeto a:}&amp;\quad\lambda\geq0.
    \end{align*}
    \end{split}\]</div>
</li>
</ul>
</section>
</div><ul class="simple">
<li><p>Nótese en la propiedad anterior que, <span class="math notranslate nohighlight">\(\partial_{\boldsymbol{\theta}}(\frac{1}{2}\boldsymbol{\theta}^{T}\boldsymbol{\theta}-\lambda^{T}(A\boldsymbol{\theta}-\boldsymbol{b}))=\boldsymbol{\theta}-A^{T}\lambda\)</span>. Consideremos ahora la <em><strong>representación dual de Wolfe</strong></em> asociada a la tarea de optimización <a class="reference internal" href="#equation-eq-lagrangiano-general">(41)</a>. <em><strong>Con este fin, consideremos</strong></em> <span class="math notranslate nohighlight">\(A\boldsymbol{\theta}=\boldsymbol{\theta}\boldsymbol{x_{n}}+\theta_{0}\)</span> y <span class="math notranslate nohighlight">\(b=y_{n}-\varepsilon\)</span>, con <span class="math notranslate nohighlight">\(\tilde{\xi}_{n}, \xi_{n}=0\)</span>, con base en el estimador obtenido para <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span>, <em><strong>la tarea de optimización se convierte en la siguiente representación dual de Walfe</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-optimization-task-innerprod">
<span class="eqno">(43)<a class="headerlink" href="#equation-optimization-task-innerprod" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\text{minimizar respecto a}~\lambda,\tilde{\lambda}:&amp;\quad-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})(\tilde{\lambda}_{m}-\lambda_{m})\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}\\
&amp;\quad+\sum_{n=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})y_{n}-\varepsilon(\tilde{\lambda}_{n}+\lambda_{n})\\
\text{sujeto a:}&amp;\quad 0\leq\tilde{\lambda}_{n}\leq C\quad\text{y}\quad 0\leq\lambda_{n}\leq C,\quad n=1,2,\dots,N.\\
&amp;\quad~\sum_{n=1}^{N}\tilde{\lambda}_{n}=\sum_{n=1}^{N}\lambda_{n}.
\end{align*}
\end{split}\]</div>
<ul>
<li><p>El primer producto interior es reemplazado por <span class="math notranslate nohighlight">\(\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})\)</span> vía <em><strong>kernel trick</strong></em>. Nótese que la <em><strong>primera restricción proviene de la igualdad</strong></em> <span class="math notranslate nohighlight">\(C-\tilde{\lambda}-\tilde{\mu}_{n}=0\)</span>, con la cual se tiene que</p>
<div class="math notranslate nohighlight">
\[
    0\leq\tilde{\lambda}_{n}\leq\tilde{\lambda}_{n}+\tilde{\mu}_{n}\leq C\Rightarrow 0\leq\tilde{\lambda}_{n}\leq C,
    \]</div>
<p>análogamente, <span class="math notranslate nohighlight">\(0\leq\lambda_{n}\leq C,\quad n=1,2,\dots,N.\)</span></p>
</li>
</ul>
<ul class="simple">
<li><p>El <em><strong>primer término proviene la siguiente igualdad obtenida a partir del Lagrangiano</strong></em>, el <em><strong>segundo resulta de factorizar</strong></em> los términos que incluyen los autovalores, con <span class="math notranslate nohighlight">\(\xi_{n}=\tilde{\xi}_{n}=0\)</span> (<code class="docutils literal notranslate"><span class="pre">verifíquelo</span></code>) (<em><strong>vectores de soporte</strong></em>)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\|\boldsymbol{\theta}\|^{2}=\sum_{n=1}^{N}\sum_{m=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})(\tilde{\lambda}_{m}-\lambda_{m})\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}
\]</div>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La belleza de la forma de <em><strong>representación dual</strong></em> radica en que <em><strong>involucra los vectores de observación en forma de operaciones de producto interno</strong></em>. Así, cuando se resuelve la tarea en un <code class="docutils literal notranslate"><span class="pre">RKHS</span></code>, <a class="reference internal" href="#equation-optimization-task-innerprod">(43)</a> se convierte en</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{minimizar respecto a}~\lambda,\tilde{\lambda}:&amp;\quad-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})(\tilde{\lambda}_{m}-\lambda_{m})\kappa(\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m})\\
&amp;\quad+\sum_{n=1}^{N}(\tilde{\lambda}_{n}-\lambda_{n})y_{n}-\varepsilon(\tilde{\lambda}_{n}+\lambda_{n})
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Las condiciones <code class="docutils literal notranslate"><span class="pre">KKT</span></code> transmiten información importante. Los <em><strong>multiplicadores de Lagrange</strong></em>, <span class="math notranslate nohighlight">\(\tilde{\lambda}_{n},\lambda_{n}\)</span>, para puntos que obtienen un error menor que <span class="math notranslate nohighlight">\(\varepsilon\)</span>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
|\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}-y_{n}|&lt;\varepsilon,~\text{son cero}.
\]</div>
<ul class="simple">
<li><p>Por lo tanto, los <em><strong>multiplicadores de Lagrange son distintos de cero solo para puntos que obtienen un error igual a</strong></em> <span class="math notranslate nohighlight">\(\varepsilon~(\tilde{\xi}_{n}, \xi_{n} = 0)\)</span> o valores más grandes (<span class="math notranslate nohighlight">\(\tilde{\xi}_{n}, \xi_{n}&gt;0\)</span>) (<em><strong>vectores de soporte</strong></em>).</p></li>
<li><p>Debido a <a class="reference internal" href="#equation-xin-xintilde-product">(42)</a>, ya sea <span class="math notranslate nohighlight">\(\tilde{\xi}_{n}\)</span> o <span class="math notranslate nohighlight">\(\xi_{n}\)</span> <em><strong>pueden ser distintos de cero, pero no ambos</strong></em>. Esto también se aplica a los multiplicadores de Lagrange correspondientes.</p></li>
</ul>
</div>
</section>
<section id="regresion-kernel-ridge">
<h2>Regresión Kernel Ridge<a class="headerlink" href="#regresion-kernel-ridge" title="Link to this heading">#</a></h2>
<ul>
<li><p>En esta sección abordaremos la <em><strong>regresión kernel ridge vía su representación dual</strong></em>. La regresión ridge en su <em><strong>representación primal</strong></em> puede ser proyectada como:</p>
<div class="math notranslate nohighlight" id="equation-eq-kernel-ridge-reg">
<span class="eqno">(44)<a class="headerlink" href="#equation-eq-kernel-ridge-reg" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \text{minimizar con respecto a}\quad\boldsymbol{\theta}, \boldsymbol{\xi}:&amp;\quad J(\boldsymbol{\theta}, \boldsymbol{\xi})=\sum_{n=1}^{N}\xi_{n}^{2}+C\|\boldsymbol{\theta}\|^{2}\\
    \text{sujeto a}:&amp;\quad y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}=\boldsymbol{\xi}_{n},\quad n=1,2,\dots,N,
    \end{align*}
    \end{split}\]</div>
<p>con el siguiente <em><strong>Lagrangiano</strong></em></p>
<div class="math notranslate nohighlight">
\[
    L(\boldsymbol{\theta}, \boldsymbol{\xi}, \lambda)=\sum_{n=1}^{N}\xi_{n}^{2}+C\|\boldsymbol{\theta}\|^{2}+\sum_{n=1}^{N}\lambda_{n}(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\xi_{n}),\quad n=1,2,\dots,N.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Calculando las <em><strong>derivadas parciales</strong></em> <span class="math notranslate nohighlight">\(\partial_{\boldsymbol{\theta},\xi_{n}}=0,\, n=1,2,\dots,N\)</span> se tiene que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\boldsymbol{\theta}}&amp;=0\Leftrightarrow 2C\boldsymbol{\theta}-\sum_{n=1}^{N}\lambda_{n}\boldsymbol{x}_{n}=0\Leftrightarrow\boldsymbol{\hat{\theta}}=\frac{1}{2C}\sum_{n=1}^{N}\lambda_{n}\boldsymbol{x}_{n}\\
\frac{\partial L}{\partial\xi_{n}}&amp;=0\Leftrightarrow 2\xi_{n}-\lambda_{n}=0\Leftrightarrow\xi_{n}=\frac{\lambda_{n}}{2},\quad n=1,2,\dots,N.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Para obtener los multiplicadores de Lagrange, consideramos la siguiente <em><strong>formulación dual, obtenida al reemplazar los estimadores</strong></em> <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span> y <span class="math notranslate nohighlight">\(\xi_{n}\)</span> en la tarea de optimización <a class="reference internal" href="#equation-eq-kernel-ridge-reg">(44)</a></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\sum_{n=1}^{N}\xi_{n}^{2}+C\|\boldsymbol{\theta}\|^{2}&amp;+\sum_{n=1}^{N}\lambda_{n}(y_{n}-\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}-\xi_{n})\\
&amp;=\frac{1}{4}\sum_{n=1}^{N}\lambda_{n}^{2}+C\frac{1}{4C^{2}}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})+\sum_{n=1}^{N}\lambda_{n}y_{n}\\
&amp;-\frac{1}{2C}\sum_{n=1}^{N}\lambda_{n}\lambda_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})-\frac{1}{2}\sum_{n=1}^{N}\lambda_{n}^{2}\\
&amp;=\sum_{n=1}^{N}\lambda_{n}y_{n}-\frac{1}{4C}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})-\frac{1}{4}\sum_{n=1}^{N}\lambda_{n}^{2}
\end{align*}
\end{split}\]</div>
<ul>
<li><p>La <em><strong>representación dual</strong></em> entrega la siguiente <em><strong>formulación</strong></em></p>
<div class="math notranslate nohighlight">
\[
    \text{minimizar respecto a}~\lambda:\quad\sum_{n=1}^{N}\lambda_{n}y_{n}-\frac{1}{4C}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})-\frac{1}{4}\sum_{n=1}^{N}\lambda_{n}^{2},
    \]</div>
<p>aquí <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}\)</span> fue reemplazado por <span class="math notranslate nohighlight">\(\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})\)</span>, de acuerdo al kernel trick.</p>
</li>
</ul>
<ul class="simple">
<li><p><em><strong>Diferenciando con respecto a</strong></em> <span class="math notranslate nohighlight">\(\lambda\)</span> obtenemos:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{y}-\frac{1}{2C}K\boldsymbol{\lambda}-\frac{1}{2}\boldsymbol{\lambda}=0&amp;\Rightarrow\left(\frac{1}{2C}K+\frac{1}{2}I\right)\boldsymbol{\lambda}=\boldsymbol{y}\\
&amp;\Rightarrow(K+CI)\boldsymbol{\lambda}=2C\boldsymbol{y}\\[2mm]
&amp;\Rightarrow\boldsymbol{\lambda}=2C(K+CI)^{-1}\boldsymbol{y}.
\end{align*}
\end{split}\]</div>
<ul>
<li><p>Dado que <span class="math notranslate nohighlight">\(\displaystyle{\boldsymbol{\hat{\theta}}=\frac{1}{2C}\sum_{n=1}^{N}\lambda_{n}\boldsymbol{x}_{n}}\)</span>, <em><strong>usando kernel trick</strong></em> se tiene que</p>
<div class="math notranslate nohighlight">
\[
    \hat{y}(\boldsymbol{x})=\langle\boldsymbol{\hat{\theta}}, \kappa(\cdot,\boldsymbol{x})\rangle=\left\langle\frac{1}{2C}\sum_{n=1}^{N}2C(\kappa(\cdot, \boldsymbol{x}_{n})+C)^{-1}y_{n}, \kappa(\cdot, \boldsymbol{x}_{n})\right\rangle=\boldsymbol{y}^{T}(K+CI)^{-1}\kappa(\boldsymbol{x}).
    \]</div>
<p>Nótese que en este caso no <em><strong>fue necesario el supuesto de invertibilidad</strong></em> para la matriz <span class="math notranslate nohighlight">\(K\)</span>.</p>
</li>
</ul>
<figure class="align-center" id="nonlinear-reg-curve">
<img alt="_images/nonlinear_reg_curve.png" src="_images/nonlinear_reg_curve.png" />
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text"><em><strong>Tubo alrededor de la curva de regresión no lineal</strong></em>. Los puntos fuera del tubo (denotados por estrellas) tienen o bien <span class="math notranslate nohighlight">\(\tilde{\xi} &gt; 0\)</span> y
<span class="math notranslate nohighlight">\(\xi = 0\)</span> o <span class="math notranslate nohighlight">\(\xi &gt; 0\)</span> y <span class="math notranslate nohighlight">\(\tilde{\xi} = 0\)</span>. El resto de los puntos tienen <span class="math notranslate nohighlight">\(\tilde{\xi} = \xi = 0\)</span> (SV). Los <em><strong>puntos que están dentro del tubo corresponden a multiplicadores de Lagrange iguales a cero</strong></em>.</span><a class="headerlink" href="#nonlinear-reg-curve" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="id6">
<h2>Máquinas de vectores de soporte<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Antes de abordar el <code class="docutils literal notranslate"><span class="pre">modelo</span> <span class="pre">de</span> <span class="pre">clasificación</span></code>, <code class="docutils literal notranslate"><span class="pre">máquinas</span> <span class="pre">de</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span> <span class="pre">(SVM)</span></code> revisemos la definición de un clasificador</p></li>
</ul>
<div class="proof definition admonition" id="def_clasificador_general">
<p class="admonition-title"><span class="caption-number">Definition 9 </span></p>
<section class="definition-content" id="proof-content">
<p>Un <code class="docutils literal notranslate"><span class="pre">clasificador,</span> <span class="pre">es</span> <span class="pre">una</span> <span class="pre">función</span></code> definida como</p>
<div class="math notranslate nohighlight">
\[
\omega(x):=\text{argmax}_{\omega}f_{\omega}(x)
\]</div>
<p>donde para cada clase <span class="math notranslate nohighlight">\(\omega\)</span>, se define su <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">discriminante</span></code> <span class="math notranslate nohighlight">\(f_{\omega}\)</span>. El <code class="docutils literal notranslate"><span class="pre">grado</span> <span class="pre">de</span> <span class="pre">pertenencia</span> <span class="pre">del</span> <span class="pre">valor</span></code> <span class="math notranslate nohighlight">\(x\)</span> <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">la</span> <span class="pre">clase</span></code> <span class="math notranslate nohighlight">\(\omega\)</span> es <span class="math notranslate nohighlight">\(f_{\omega}(x)\)</span>. Ademas <span class="math notranslate nohighlight">\(\omega(x)\)</span> es la <code class="docutils literal notranslate"><span class="pre">clase</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">objeto</span></code> <span class="math notranslate nohighlight">\(x\)</span> <code class="docutils literal notranslate"><span class="pre">pertenece</span> <span class="pre">en</span> <span class="pre">mayor</span> <span class="pre">grado</span></code>.</p>
</section>
</div><ul class="simple">
<li><p>Cuando en un método de <code class="docutils literal notranslate"><span class="pre">clasificación</span> <span class="pre">Bayesiana</span></code> desconocemos estadísticos subyacentes, una alternativa es usar <code class="docutils literal notranslate"><span class="pre">técnicas</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">discriminante</span></code>, y adoptar una función discriminante <span class="math notranslate nohighlight">\(f\)</span>, que <code class="docutils literal notranslate"><span class="pre">efectúa</span> <span class="pre">la</span> <span class="pre">clasificación</span> <span class="pre">correspondiente</span> <span class="pre">y</span> <span class="pre">trata</span> <span class="pre">de</span> <span class="pre">optimizarla</span></code>, como se minimiza la función de pérdida empírica</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
J(f)=\sum_{n=1}^{N}\mathcal{L}(y_{n}, f(\boldsymbol{x}_{n})),\quad\text{donde}\quad y_{n}=
\begin{cases}
+1,&amp; \text{si}~\boldsymbol{x}_{n}\in\omega_{1},\\
-1,&amp; \text{si}~\boldsymbol{x}_{n}\in\omega_{2}.
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>Para una tarea de <code class="docutils literal notranslate"><span class="pre">clasificación</span> <span class="pre">binaria</span></code>, la función de pérdida <span class="math notranslate nohighlight">\((0, 1)\)</span>, definida de la siguiente forma, puede ser utilizada en la tarea de optimización</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L}(y, f(x))=
\begin{cases}
1,&amp; \text{si}~ yf(x)\leq0\\
0,&amp; \text{otro caso}
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>El principal problema de esta función de pérdida es que <code class="docutils literal notranslate"><span class="pre">su</span> <span class="pre">optimización</span> <span class="pre">resulta</span> <span class="pre">ser</span> <span class="pre">una</span> <span class="pre">tarea</span> <span class="pre">compleja,</span> <span class="pre">debido</span> <span class="pre">a</span> <span class="pre">que</span> <span class="pre">es</span> <span class="pre">discontinua</span></code>. Se han utilizado alternativas para esta función de pérdida, con el fin de solventar este problema. En esta sección centraremos nuestra atención en la <strong><code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span> <span class="pre">hinge</span></code></strong> definida como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-loss-hinge">
<span class="eqno">(45)<a class="headerlink" href="#equation-eq-loss-hinge" title="Link to this equation">#</a></span>\[
\mathcal{L}(y, f(x))=\max(0, \rho-yf(x))
\]</div>
<ul class="simple">
<li><p>Esto es, si el <code class="docutils literal notranslate"><span class="pre">signo</span> <span class="pre">del</span> <span class="pre">producto</span></code> entre el label real <span class="math notranslate nohighlight">\((y)\)</span> y el predicho por la función discriminante <span class="math notranslate nohighlight">\((f(x))\)</span> es <code class="docutils literal notranslate"><span class="pre">positivo</span> <span class="pre">y</span> <span class="pre">mayor</span> <span class="pre">que</span> <span class="pre">un</span> <span class="pre">umbral</span> <span class="pre">definido</span></code> por el usuario, <span class="math notranslate nohighlight">\(\rho\geq0\)</span>, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span> <span class="pre">es</span> <span class="pre">cero</span></code>. En caso contrario, la función de pérdida exhibe un crecimiento lineal.</p></li>
</ul>
<figure class="align-center" id="fig-hinge-lossfn">
<a class="reference internal image-reference" href="_images/hinge_lossfn.png"><img alt="_images/hinge_lossfn.png" src="_images/hinge_lossfn.png" style="width: 307.20000000000005px; height: 232.0px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">Función de pérdida <code class="docutils literal notranslate"><span class="pre">hinge</span></code>, para la tarea de clasificación, <span class="math notranslate nohighlight">\(\tau = y\boldsymbol{\theta}^{T}\boldsymbol{x}\)</span>.</span><a class="headerlink" href="#fig-hinge-lossfn" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Consideraremos en esta sección, la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">discriminante</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">RKHS</span></code></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    f(\boldsymbol{x})=\theta_{0}+\langle\boldsymbol{\theta}, \phi(\boldsymbol{x})\rangle,
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\phi(\boldsymbol{x})\)</span> es el mapeo de características. <code class="docutils literal notranslate"><span class="pre">Proyectamos</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span> <span class="pre">como</span> <span class="pre">un</span> <span class="pre">problema</span> <span class="pre">lineal</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">inputs</span></code>, <span class="math notranslate nohighlight">\(\mathbb{R}^{l}\)</span>, y al final la información del kernel será implantada usando el <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">trick</span></code>.</p>
</li>
<li><p>La tarea de diseñar un clasificador lineal, ahora es equivalente a minimizar la función de costo</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-cost-linear-class">
<span class="eqno">(46)<a class="headerlink" href="#equation-eq-cost-linear-class" title="Link to this equation">#</a></span>\[
J(\boldsymbol{\theta}, \theta_{0})=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\sum_{n=1}^{N}\mathcal{L}_{p}(y_{n}, \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})
\]</div>
<ul class="simple">
<li><p>Alternativamente,  empleando <code class="docutils literal notranslate"><span class="pre">variables</span> <span class="pre">de</span> <span class="pre">relajación</span> <span class="pre">(slack)</span></code> y <code class="docutils literal notranslate"><span class="pre">siguiendo</span> <span class="pre">un</span> <span class="pre">razonamiento</span> <span class="pre">similar</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">regresión</span> <span class="pre">de</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span> <span class="pre">(SVR)</span></code>, minimizar <a class="reference internal" href="#equation-eq-cost-linear-class">(46)</a> es equivalente a:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{minimizar con respecto a}~\boldsymbol{\theta}, \theta_{0}, \boldsymbol{\xi}:&amp;\quad J(\boldsymbol{\theta}, \boldsymbol{\xi})=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\sum_{n=1}^{N}\boldsymbol{\xi}_{n}\\
\text{sujeto a:}&amp;\quad y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\geq\rho-\boldsymbol{\xi}_{n},\\[2mm]
&amp;\quad\boldsymbol{\xi}_{n}\geq 0,\quad n=1,2,\dots,N.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Adoptaremos</span> <span class="pre">de</span> <span class="pre">ahora</span> <span class="pre">en</span> <span class="pre">adelante</span></code> <span class="math notranslate nohighlight">\(\rho=1\)</span> sin pérdida de generalidad. Nótese que <span class="math notranslate nohighlight">\(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\geq1\)</span> si <span class="math notranslate nohighlight">\(\xi_{n}=0\)</span>, en este caso el <code class="docutils literal notranslate"><span class="pre">margen</span> <span class="pre">de</span> <span class="pre">error</span> <span class="pre">sería</span> <span class="pre">nulo</span></code>. Por otro lado, un <code class="docutils literal notranslate"><span class="pre">margen</span> <span class="pre">de</span> <span class="pre">error</span> <span class="pre">es</span> <span class="pre">cometido</span> <span class="pre">si</span></code> <span class="math notranslate nohighlight">\(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\leq 1\)</span>, correspondiente a <span class="math notranslate nohighlight">\(\xi_{n}&gt;0\)</span>. Por lo tanto, nuestro objetivo para la tarea de optimización es, encontrar <span class="math notranslate nohighlight">\(\boldsymbol{\theta}, \theta_{0}, \xi_{n}\)</span> óptimos, tales que <span class="math notranslate nohighlight">\(\xi_{n}\)</span> <code class="docutils literal notranslate"><span class="pre">sea</span> <span class="pre">tan</span> <span class="pre">pequeño</span> <span class="pre">como</span> <span class="pre">sea</span> <span class="pre">posible</span></code>.</p></li>
</ul>
</section>
<section id="clases-linealmente-separables-clasificador-de-maximo-margen">
<h2>Clases linealmente separables: Clasificador de máximo margen<a class="headerlink" href="#clases-linealmente-separables-clasificador-de-maximo-margen" title="Link to this heading">#</a></h2>
<ul>
<li><p>Asumiendo que las <code class="docutils literal notranslate"><span class="pre">clases</span> <span class="pre">son</span> <span class="pre">linealmente</span> <span class="pre">separables</span></code>, hay un <code class="docutils literal notranslate"><span class="pre">número</span> <span class="pre">infinito</span> <span class="pre">de</span> <span class="pre">clasificadores</span> <span class="pre">que</span> <span class="pre">solucionan</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">exactamente</span></code>, sin error, en el conjunto de entrenamiento. Se puede ver que de estos infinitos hiperplanos que solucionan la tarea, podemos identificar un subconjunto tal que</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\geq 1,\quad n=1,2,\dots,N,
    \end{split}\]</div>
<p>el cual garantiza que <span class="math notranslate nohighlight">\(\xi_{n}=0,~n=1,2,\dots,N\)</span>. Por lo tanto, para <code class="docutils literal notranslate"><span class="pre">clases</span> <span class="pre">linealmente</span> <span class="pre">separables,</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span> <span class="pre">previa</span> <span class="pre">es</span> <span class="pre">equivalente</span> <span class="pre">a</span></code></p>
<div class="math notranslate nohighlight" id="equation-eq-opt-task-linear-class-sep">
<span class="eqno">(47)<a class="headerlink" href="#equation-eq-opt-task-linear-class-sep" title="Link to this equation">#</a></span>\[\begin{split}
    \\[1mm]
    \begin{align*}
    \text{minimizar con respecto a}~\theta:&amp;\quad\frac{1}{2}\|\boldsymbol{\theta}\|^{2}\\
    \text{sujeto a:}&amp;\quad y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})\geq 1,\quad n=1,2,\dots,N.
    \end{align*}
    \end{split}\]</div>
</li>
</ul>
<ul>
<li><p>En otras palabras, de este conjunto infinito de clasificadores lineales (ver <a class="reference internal" href="#inf-correct-classification"><span class="std std-numref">Fig. 22</span></a>), los cuales <code class="docutils literal notranslate"><span class="pre">solucionan</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code>, y clasifican correctamente todos los patrones, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">selecciona</span> <span class="pre">aquel</span> <span class="pre">que</span> <span class="pre">tiene</span> <span class="pre">mínima</span> <span class="pre">norma</span></code>. Veremos mas adelante que, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">norma</span></code> <span class="math notranslate nohighlight">\(\|\boldsymbol{\theta}\|\)</span> <code class="docutils literal notranslate"><span class="pre">está</span> <span class="pre">directamente</span> <span class="pre">relacionada</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">margen</span></code> formado por el respectivo clasificador. Cada hiperplano en el espacio está descrito por la ecuación</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    f(\boldsymbol{x})=\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}=0.
    \end{split}\]</div>
</li>
</ul>
<figure class="align-center" id="inf-correct-classification">
<a class="reference internal image-reference" href="_images/inf_correct_classification.png"><img alt="_images/inf_correct_classification.png" src="_images/inf_correct_classification.png" style="width: 433.6px; height: 310.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">Número infinito de clasificadores lineales que pueden clasificar correctamente todos los patrones de una clase linealmente separable.</span><a class="headerlink" href="#inf-correct-classification" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>De la <code class="docutils literal notranslate"><span class="pre">geometría</span> <span class="pre">analítica</span></code> sabemos que la <code class="docutils literal notranslate"><span class="pre">dirección</span> <span class="pre">en</span> <span class="pre">espacio</span> <span class="pre">es</span> <span class="pre">controlada</span> <span class="pre">por</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> y su posición por <span class="math notranslate nohighlight">\(\theta_{0}\)</span>. De todos los hiperplanos que solucionan la tarea de optimización y tienen la misma dirección (comparten <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>), seleccionamos <span class="math notranslate nohighlight">\(\theta_{0}\)</span> tal que, el <code class="docutils literal notranslate"><span class="pre">hiperplano</span> <span class="pre">queda</span> <span class="pre">dentro</span> <span class="pre">de</span> <span class="pre">dos</span> <span class="pre">clases</span></code>, de modo que su <code class="docutils literal notranslate"><span class="pre">distancia</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">más</span> <span class="pre">cercanos</span> <span class="pre">es</span> <span class="pre">siempre</span> <span class="pre">la</span> <span class="pre">misma</span></code> para las dos clases.</p></li>
</ul>
<figure class="align-center" id="fig-svm-class-sep">
<a class="reference internal image-reference" href="_images/svm_class_sep.png"><img alt="_images/svm_class_sep.png" src="_images/svm_class_sep.png" style="width: 463.20000000000005px; height: 320.8px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 23 </span><span class="caption-text">Las líneas punteadas, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x} + \theta_{0} =±1\)</span>, que pasan por los puntos más cercanos, son paralelas al clasificador respectivo y definen el margen (ancho de banda) <span class="math notranslate nohighlight">\(2/\|\boldsymbol{\theta}\|\)</span>.</span><a class="headerlink" href="#fig-svm-class-sep" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>De la <code class="docutils literal notranslate"><span class="pre">geometría</span> <span class="pre">básica</span></code> sabemos que la <code class="docutils literal notranslate"><span class="pre">distancia</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">punto</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> <code class="docutils literal notranslate"><span class="pre">al</span> <span class="pre">hiperplano</span></code> está dada por</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    z=\frac{|\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}|}{\|\boldsymbol{\theta}\|}
    \end{split}\]</div>
<p>la cual es claramente cero cuando <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> está dentro del hiperplano <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}=0\)</span>. Donde los <code class="docutils literal notranslate"><span class="pre">parámetros</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> <code class="docutils literal notranslate"><span class="pre">y</span></code> <span class="math notranslate nohighlight">\(\theta_{0}\)</span> <code class="docutils literal notranslate"><span class="pre">pueden</span> <span class="pre">ser</span> <span class="pre">escalados</span> <span class="pre">apropiadamente</span></code>, por ejemplo, usando un factor, digamos, <span class="math notranslate nohighlight">\(a\)</span>, sin afectar la geometría del plano.</p>
</li>
<li><p>De esta forma, podemos hacer la distancia del punto más cercano, de los puntos más cercanos entre las dos clases al hiperplano, igual a <span class="math notranslate nohighlight">\(1/\|\boldsymbol{\theta}\|\)</span>, donde <span class="math notranslate nohighlight">\(|\boldsymbol{\theta}^{T}\boldsymbol{x}+\theta_{0}|=1\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>Equivalentemente, el <code class="docutils literal notranslate"><span class="pre">escalamiento</span> <span class="pre">garantiza</span> <span class="pre">que</span></code>: <span class="math notranslate nohighlight">\(f(\boldsymbol{x})=\pm 1\)</span>, <code class="docutils literal notranslate"><span class="pre">si</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">el</span> <span class="pre">punto</span> <span class="pre">más</span> <span class="pre">cercano</span> <span class="pre">al</span> <span class="pre">hiperplano</span></code>, y dependiendo de si el punto <span class="math notranslate nohighlight">\(x\)</span> pertenece a <span class="math notranslate nohighlight">\(\omega_{1} (+1)\)</span> o <span class="math notranslate nohighlight">\(\omega_{2} (-1)\)</span>. Los dos hiperplanos se pueden visualizar en la <a class="reference internal" href="#fig-svm-class-sep"><span class="std std-numref">Fig. 23</span></a> <span class="math notranslate nohighlight">\((f(\boldsymbol{x})=\pm1)\)</span>. <code class="docutils literal notranslate"><span class="pre">Estos</span> <span class="pre">dos</span> <span class="pre">hiperplanos</span> <span class="pre">definen</span> <span class="pre">el</span> <span class="pre">margen</span> <span class="pre">correspondiente</span> <span class="pre">de</span> <span class="pre">longitud</span></code> <span class="math notranslate nohighlight">\(2/\|\boldsymbol{\theta}\|\)</span> para cada iteración.</p></li>
</ul>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Cualquier</span> <span class="pre">clasificador</span> <span class="pre">construido</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">forma</span> <span class="pre">anterior,</span> <span class="pre">soluciona</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code>, y además satisface las siguientes propiedades:</p>
<ul class="simple">
<li><p>Tiene un margen de longitud igual a: <span class="math notranslate nohighlight">\(\displaystyle{1/\|\boldsymbol{\theta}\|+1/\|\boldsymbol{\theta}}\|\)</span></p></li>
<li><p>Además:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}\geq 1,~\boldsymbol{x}_{n}\in\omega_{1}~\text{y}~\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0}\leq-1,~\boldsymbol{x}_{n}\in\omega_{2}
    \]</div>
<p>Por lo tanto la tarea de optimización <a class="reference internal" href="#equation-eq-opt-task-linear-class-sep">(47)</a> <code class="docutils literal notranslate"><span class="pre">calcula</span> <span class="pre">el</span> <span class="pre">clasificador</span> <span class="pre">lineal</span> <span class="pre">que</span> <span class="pre">maximiza</span> <span class="pre">el</span> <span class="pre">margen</span> <span class="pre">sujeto</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">respectivas</span> <span class="pre">restricciones</span></code>.</p>
</li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Solución</span></code></strong></p>
<ul>
<li><p>Siguiendo <code class="docutils literal notranslate"><span class="pre">pasos</span> <span class="pre">similares</span> <span class="pre">a</span> <span class="pre">SVR</span></code>, la solución esta dada por, <code class="docutils literal notranslate"><span class="pre">combinación</span> <span class="pre">lineal</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">subconjunto</span> <span class="pre">de</span> <span class="pre">muestras</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>, esto es:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \hat{\boldsymbol{\theta}}=\sum_{n=1}^{N_{s}}\lambda_{n}y_{n}\boldsymbol{x}_{n},
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(N_{s}\)</span> es el número de multiplicadores de Lagrange distintos de cero. <code class="docutils literal notranslate"><span class="pre">Multiplicadores</span> <span class="pre">de</span> <span class="pre">Lagrange</span> <span class="pre">asociados</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">más</span> <span class="pre">cercanos</span> <span class="pre">al</span> <span class="pre">clasificador,</span> <span class="pre">esto</span> <span class="pre">es,</span> <span class="pre">puntos</span> <span class="pre">que</span> <span class="pre">satisfacen</span> <span class="pre">la</span> <span class="pre">restricción</span> <span class="pre">con</span> <span class="pre">igualdad</span></code> <span class="math notranslate nohighlight">\((y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})=1)\)</span>, <code class="docutils literal notranslate"><span class="pre">son</span> <span class="pre">distintos</span> <span class="pre">de</span> <span class="pre">cero</span></code>. Estos son conocidos como <code class="docutils literal notranslate"><span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span></code>. Multiplicadores de Lagrange correspondientes a puntos tales que <span class="math notranslate nohighlight">\((y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})&gt;1)\)</span>, son iguales a cero.</p>
</li>
</ul>
<ul>
<li><p>Para el caso mas general, <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">RKHS</span></code>, tenemos que:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \hat{\theta}(\cdot)=\sum_{n=1}^{N_{s}}\lambda_{n}y_{n}\kappa(\cdot, \boldsymbol{x}_{n}),
    \end{split}\]</div>
<p>estimación la cual entrega la siguiente regla de predicción. <code class="docutils literal notranslate"><span class="pre">Dado</span> <span class="pre">un</span> <span class="pre">valor</span> <span class="pre">desconocido</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, <code class="docutils literal notranslate"><span class="pre">su</span> <span class="pre">clase</span> <span class="pre">es</span> <span class="pre">predicha</span> <span class="pre">de</span> <span class="pre">acuerdo</span> <span class="pre">al</span> <span class="pre">signo</span> <span class="pre">de</span></code>:</p>
<div class="math notranslate nohighlight">
\[
    \hat{y}(\boldsymbol{x})=\sum_{n=1}^{N_{s}}\lambda_{n}y_{n}\kappa(\boldsymbol{x}, \boldsymbol{x}_{n})+\hat{\theta}_{0}:\quad\text{Predicción SVM},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\hat{\theta}_{0}\)</span> <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">obtenido</span> <span class="pre">solucionando</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">restricciones</span> <span class="pre">con</span></code> <span class="math notranslate nohighlight">\(\lambda_{n}\neq0\)</span>, correspondientes a:</p>
<div class="math notranslate nohighlight">
\[
    y_{n}(\hat{\boldsymbol{\theta}}^{T}\boldsymbol{x}_{n}+\hat{\theta}_{0})-1=0,\quad n=1,2,\dots,N_{s},
    \]</div>
<p>la cual <code class="docutils literal notranslate"><span class="pre">para</span> <span class="pre">el</span> <span class="pre">caso</span> <span class="pre">RKHS</span> <span class="pre">se</span> <span class="pre">convierte</span></code> en:</p>
<div class="math notranslate nohighlight" id="equation-mean-sol-fortheta0">
<span class="eqno">(48)<a class="headerlink" href="#equation-mean-sol-fortheta0" title="Link to this equation">#</a></span>\[
    y_{n}\left(\sum_{m=1}^{N_{s}}\lambda_{m}y_{m}\kappa(\boldsymbol{x}_{m}, \boldsymbol{x}_{n})+\hat{\theta}_{0}\right)-1=0,\quad n=1,2,\dots,N_{s},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\hat{\theta}_{0}\)</span> <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">obtenido</span> <span class="pre">por</span> <span class="pre">medio</span> <span class="pre">del</span> <span class="pre">promedio</span> <span class="pre">entre</span> <span class="pre">las</span> <span class="pre">soluciones</span> <span class="pre">de</span></code> <a class="reference internal" href="#equation-mean-sol-fortheta0">(48)</a>. Aunque la solución es única, los correspondientes multiplicadores <span class="math notranslate nohighlight">\(\lambda_{m}\)</span> pueden no ser únicos. Finalmente <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span> <span class="pre">está</span> <span class="pre">relacionado</span> <span class="pre">con</span> <span class="pre">la</span> <span class="pre">capacidad</span> <span class="pre">de</span> <span class="pre">generalización</span> <span class="pre">del</span> <span class="pre">clasificador</span></code>. Entre mas pequeño es el número de vectores de soporte, mejor será la generalización esperada (ver <span id="id7">[<a class="reference internal" href="biblio.html#id19" title="Konstantinos Koutroumbas and Sergios Theodoridis. Pattern recognition. Academic Press, 2008.">Koutroumbas and Theodoridis, 2008</a>, <a class="reference internal" href="biblio.html#id18" title="佐土原健. N. cristianini and j. shawe-taylor: an introduction to support vector machines, cambridge university press (2000). 人工知能, 16(2):337–337, 2001.">佐土原健, 2001</a>]</span>).</p>
</li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code></strong></p>
<ul class="simple">
<li><p>Procedemos de forma <code class="docutils literal notranslate"><span class="pre">similar</span> <span class="pre">a</span> <span class="pre">SVR</span></code>. El <code class="docutils literal notranslate"><span class="pre">Lagrangiano</span> <span class="pre">asociado</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">clasificación</span></code> está dado por:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(\boldsymbol{\theta}, \theta_{0}, \lambda)=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}-\sum_{n=1}^{N}\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1),
\]</div>
<ul class="simple">
<li><p>Calculando las <code class="docutils literal notranslate"><span class="pre">derivadas</span> <span class="pre">parciales</span></code> <span class="math notranslate nohighlight">\(\partial_{\boldsymbol{\theta}}=0\)</span> y <span class="math notranslate nohighlight">\(\partial_{\theta_{0}}=0\)</span>, se tienen las condiciones siguientes</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-opt-task-svm-class">
<span class="eqno">(49)<a class="headerlink" href="#equation-opt-task-svm-class" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\boldsymbol{\theta}}&amp;=0\Leftrightarrow\boldsymbol{\theta}-\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}=0\Rightarrow\hat{\boldsymbol{\theta}}=\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}\\
\frac{\partial L}{\partial\theta_{0}}&amp;=0\Leftrightarrow-\sum_{n=1}^{N}\lambda_{n}y_{n}=0\Rightarrow\sum_{n=1}^{N}\lambda_{n}y_{n}=0,\\
\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1)&amp;=0,\quad n=1,2,\dots,N,\\[2mm]
\lambda_{n}&amp;\geq0,\quad n=1,2,\dots,N.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Los <code class="docutils literal notranslate"><span class="pre">multiplicadores</span> <span class="pre">son</span> <span class="pre">obtenidos</span> <span class="pre">por</span> <span class="pre">medio</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">representación</span> <span class="pre">dual</span></code> de <a class="reference internal" href="#equation-opt-task-svm-class">(49)</a> en el Lagrangiano</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
L(\boldsymbol{\theta}, \theta_{0}, \lambda)&amp;=\frac{1}{2}\left\langle\sum_{m=1}^{N}\lambda_{m}y_{m}\boldsymbol{x}_{m},\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}\right\rangle-\sum_{n=1}^{N}\lambda_{n}(y_{n}(\boldsymbol{\theta}\boldsymbol{x}_{n}+\theta_{0})-1)\\
&amp;=\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}-\sum_{n=1}^{N}\sum_{n=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}+\sum_{n=1}^{N}\lambda_{n}\\
&amp;=\sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>En base a lo anterior, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">siguiente</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">representación</span> <span class="pre">dual</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{minimizar con respecto a }~\lambda:&amp;\quad\sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}\\
\text{sujeto a}:&amp;\quad\lambda_{n}\geq0,\quad\sum_{n=1}^{N}\lambda_{n}y_{n}=0.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Para el caso en que la <code class="docutils literal notranslate"><span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span> <span class="pre">original</span> <span class="pre">es</span> <span class="pre">mapeada</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">RKHS</span></code>, la función de costo viene dada por</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})
\]</div>
<ul class="simple">
<li><p>De acuerdo con la restricción: <span class="math notranslate nohighlight">\(\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1)=0,~n=1,2,\dots,N\)</span>, si <span class="math notranslate nohighlight">\(\lambda_{n}\neq0\)</span>, entonces necesariamente <span class="math notranslate nohighlight">\(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})=1\)</span>. Estos son los <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">mas</span> <span class="pre">cercanos,</span> <span class="pre">desde</span> <span class="pre">cada</span> <span class="pre">clase</span> <span class="pre">al</span> <span class="pre">clasificador</span></code> (distancia <span class="math notranslate nohighlight">\(1/\|\boldsymbol{\theta}\|\)</span>). Estos puntos caen en cualquiera de los hiperplanos formando el borde del margen. <code class="docutils literal notranslate"><span class="pre">Estos</span> <span class="pre">puntos</span> <span class="pre">se</span> <span class="pre">conocen</span> <span class="pre">como</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span></code> y las respectivas restricciones son conocidas como <code class="docutils literal notranslate"><span class="pre">restricciones</span> <span class="pre">activas</span></code>. El resto de puntos asociados con <span class="math notranslate nohighlight">\(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})&gt;1\)</span>, los cuales quedan por fuera del margen corresponden a <span class="math notranslate nohighlight">\(\lambda_{n}=0\)</span>, y las restricciones asociadas son conocidas como las <code class="docutils literal notranslate"><span class="pre">restricciones</span> <span class="pre">inactivas</span></code>.</p></li>
</ul>
</section>
<section id="clases-no-separables">
<h2>Clases no separables<a class="headerlink" href="#clases-no-separables" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Centraremos ahora nuestra atención en un <code class="docutils literal notranslate"><span class="pre">caso</span> <span class="pre">mas</span> <span class="pre">realista</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">que</span> <span class="pre">las</span> <span class="pre">clases</span> <span class="pre">se</span> <span class="pre">superponen</span></code>. En este caso <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">existe</span> <span class="pre">un</span> <span class="pre">clasificador</span> <span class="pre">lineal</span> <span class="pre">que</span> <span class="pre">pueda</span> <span class="pre">clasificar</span> <span class="pre">correctamente</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">puntos</span></code>, algunos errores de clasificación pueden ocurrir. Existen tres tipos de puntos mal clasificados.</p></li>
</ul>
<ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">Puntos</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">encuentran</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">frontera</span> <span class="pre">o</span> <span class="pre">fuera</span> <span class="pre">del</span> <span class="pre">margen</span> <span class="pre">y</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">lado</span> <span class="pre">correcto</span> <span class="pre">del</span> <span class="pre">clasificador</span></code>, esto es</p>
<div class="math notranslate nohighlight">
\[
    y_{n}f(\boldsymbol{x}_{n})\geq1.
    \]</div>
<p>Estos puntos <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">cometen</span> <span class="pre">margen</span> <span class="pre">de</span> <span class="pre">error</span></code>, esto es: <span class="math notranslate nohighlight">\( \xi_{n}=0.\)</span></p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Puntos</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">encuentran</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">lado</span> <span class="pre">correcto</span> <span class="pre">del</span> <span class="pre">clasificador,</span> <span class="pre">pero</span> <span class="pre">se</span> <span class="pre">encuentran</span> <span class="pre">dentro</span> <span class="pre">del</span> <span class="pre">margen</span></code>, esto es</p>
<div class="math notranslate nohighlight">
\[
    0&lt;y_{n}f(\boldsymbol{x}_{n})&lt;1.
    \]</div>
<p>Estos puntos <code class="docutils literal notranslate"><span class="pre">cometen</span> <span class="pre">un</span> <span class="pre">margen</span> <span class="pre">de</span> <span class="pre">error</span></code>, y se tiene que: <span class="math notranslate nohighlight">\(0&lt;\xi_{n}&lt;1\)</span>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Puntos</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">encuentran</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">lugar</span> <span class="pre">equivocado</span> <span class="pre">del</span> <span class="pre">clasificador</span></code>, esto es,</p>
<div class="math notranslate nohighlight">
\[
    y_{n}f(\boldsymbol{x}_{n})\leq0.
    \]</div>
<p>Estos puntos <code class="docutils literal notranslate"><span class="pre">cometen</span> <span class="pre">un</span> <span class="pre">eror</span></code>, y se tiene que: <span class="math notranslate nohighlight">\(\xi_{n}\geq1.\)</span></p>
</li>
</ol>
<figure class="align-center" id="non-separable-classes-svm">
<a class="reference internal image-reference" href="_images/non_separable_classes_svm.png"><img alt="_images/non_separable_classes_svm.png" src="_images/non_separable_classes_svm.png" style="width: 405.90000000000003px; height: 317.7px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 24 </span><span class="caption-text">(1) <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">fuera</span> <span class="pre">o</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">límites</span> <span class="pre">del</span> <span class="pre">margen</span> <span class="pre">y</span> <span class="pre">se</span> <span class="pre">clasifican</span> <span class="pre">correctamente</span></code> (<span class="math notranslate nohighlight">\(\xi_{n} = 0\)</span>); (2) <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">dentro</span> <span class="pre">del</span> <span class="pre">margen</span> <span class="pre">y</span> <span class="pre">clasificados</span> <span class="pre">correctamente</span></code> (<span class="math notranslate nohighlight">\(0 &lt; \xi_{n} &lt; 1\)</span>), denotados por círculos; y (3) <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">mal</span> <span class="pre">clasificados</span></code>, denotados por un cuadrado <span class="math notranslate nohighlight">\((\xi_{n}\geq 1\)</span>).</span><a class="headerlink" href="#non-separable-classes-svm" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Nuestro objetivo es <code class="docutils literal notranslate"><span class="pre">estimar</span> <span class="pre">el</span> <span class="pre">hiperplano</span> <span class="pre">clasificador</span> <span class="pre">que</span> <span class="pre">maximiza</span> <span class="pre">el</span> <span class="pre">margen</span></code> y al mismo tiempo <code class="docutils literal notranslate"><span class="pre">mantiene</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">errores</span> <span class="pre">(incluyendo</span> <span class="pre">el</span> <span class="pre">margen</span> <span class="pre">del</span> <span class="pre">error)</span> <span class="pre">tan</span> <span class="pre">pequeño</span> <span class="pre">como</span> <span class="pre">sea</span> <span class="pre">posible</span></code>.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Solución</span></code></strong></p>
<ul>
<li><p>Una vez mas la solución está dada por una <code class="docutils literal notranslate"><span class="pre">combinación</span> <span class="pre">lineal</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">subconjunto</span> <span class="pre">de</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code></p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\hat{\theta}}=\sum_{n=1}^{N_{s}}\lambda_{n}y_{n}\boldsymbol{x}_{n},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\lambda_{n},~n=1,2,\dots,N\)</span>, son los <code class="docutils literal notranslate"><span class="pre">multiplicadores</span> <span class="pre">de</span> <span class="pre">Lagrange</span> <span class="pre">asociados</span> <span class="pre">con</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span></code>. Los puntos <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> pueden satisfacer cualquiera de los <code class="docutils literal notranslate"><span class="pre">tres</span> <span class="pre">casos</span> <span class="pre">para</span> <span class="pre">tipos</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>.</p>
</li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code></strong></p>
<ul class="simple">
<li><p>El <code class="docutils literal notranslate"><span class="pre">Lagrangiano</span> <span class="pre">asociado</span> <span class="pre">con</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code> está dado por:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L(\boldsymbol{\theta}, \theta_{0}, \xi, \lambda)=\frac{1}{2}\|\boldsymbol{\theta}\|^{2}+C\sum_{n=1}^{N}\xi_{n}-\sum_{n=1}^{N}\mu_{n}\xi_{n}-\sum_{n=1}^{N}\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1+\xi_{n})
\]</div>
<ul class="simple">
<li><p>Entonces</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-class-svm-nonsep">
<span class="eqno">(50)<a class="headerlink" href="#equation-eq-class-svm-nonsep" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\boldsymbol{\theta}}&amp;=0\Leftrightarrow\boldsymbol{\theta}-\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}=0\Rightarrow\hat{\boldsymbol{\theta}}=\sum_{n=1}^{N}\lambda_{n}y_{n}\boldsymbol{x}_{n}\\
\frac{\partial L}{\partial\theta_{0}}&amp;=0\Leftrightarrow-\sum_{n=1}^{N}\lambda_{n}y_{n}=0\Rightarrow\sum_{n=1}^{N}\lambda_{n}y_{n}=0\\
\frac{\partial L}{\partial\xi_{n}}&amp;=0\Leftrightarrow C-\mu_{n}-\lambda_{n}=0,\\[2mm]
\lambda_{n}(y_{n}(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}+\theta_{0})-1+\xi_{n})&amp;=0,\quad n=1,2,\dots,N,\\[2mm]
\mu_{n}\xi_{n}&amp;=0,\quad n=1,2,\dots,N,\\[2mm]
\mu_{n}\geq,~\lambda_{n}\geq&amp;0,\quad n=1,2,\dots,N.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>El <code class="docutils literal notranslate"><span class="pre">problema</span> <span class="pre">dual</span></code> entonces es proyectado como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\text{maximizar con respecto a }~\lambda:&amp;\quad\sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\boldsymbol{x}_{n}^{T}\boldsymbol{x}_{m}\\
\text{sujeto a}:&amp;\quad0\leq\lambda_{n}\leq C, n=1,2,\dots,N,\quad\sum_{n=1}^{N}\lambda_{n}y_{n}=0.
\end{align*}
\end{split}\]</div>
<ul>
<li><p>Cuando se trabaja <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">un</span> <span class="pre">espacio</span> <span class="pre">RKHS,</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span> <span class="pre">se</span> <span class="pre">convierte</span> <span class="pre">en</span></code></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \sum_{n=1}^{N}\lambda_{n}-\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\lambda_{n}\lambda_{m}y_{n}y_{m}\kappa(\boldsymbol{x}_{n}, \boldsymbol{x}_{m})
    \end{split}\]</div>
<p>Obsérvese que la única <code class="docutils literal notranslate"><span class="pre">diferencia</span> <span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">su</span> <span class="pre">contraparte</span> <span class="pre">linealmente</span> <span class="pre">separable</span></code> es la <code class="docutils literal notranslate"><span class="pre">existencia</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(C\)</span> <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">las</span> <span class="pre">restricciones</span> <span class="pre">de</span> <span class="pre">desigualdad</span></code> para <span class="math notranslate nohighlight">\(\lambda_{n}\)</span>.</p>
</li>
</ul>
<ul>
<li><p>A partir de <a class="reference internal" href="#equation-eq-class-svm-nonsep">(50)</a> concluimos que para todos los <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">fuera</span> <span class="pre">del</span> <span class="pre">margen,</span> <span class="pre">y</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">lado</span> <span class="pre">correcto</span> <span class="pre">del</span> <span class="pre">clasificador</span></code>, que corresponden a <span class="math notranslate nohighlight">\(\xi_{n} = 0\)</span>, tenemos</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    y_{n}(\boldsymbol{\theta}\boldsymbol{x}_{n}+\theta_{0})&gt;1,
    \end{split}\]</div>
<p>por lo tanto, <span class="math notranslate nohighlight">\(\lambda_{n}=0\)</span>. Esto es, estos <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">no</span> <span class="pre">participan</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">formación</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">solución</span></code> <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span> en <a class="reference internal" href="#equation-eq-class-svm-nonsep">(50)</a>.</p>
</li>
<li><p>Tenemos <span class="math notranslate nohighlight">\(\lambda_{n}\neq0\)</span> sólo para los <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">hiperplanos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">frontera,</span> <span class="pre">dentro</span> <span class="pre">o</span> <span class="pre">fuera</span> <span class="pre">del</span> <span class="pre">margen</span></code>, pero en el lado equivocado del clasificador. Estos son los <code class="docutils literal notranslate"><span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span></code>.</p></li>
<li><p>Para los <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">encuentran</span> <span class="pre">dentro</span> <span class="pre">del</span> <span class="pre">margen</span> <span class="pre">o</span> <span class="pre">fuera</span> <span class="pre">pero</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">lado</span> <span class="pre">equivocado</span></code>, <span class="math notranslate nohighlight">\(\xi_{n}&gt;0\)</span>; por lo tanto, a partir de <a class="reference internal" href="#equation-eq-class-svm-nonsep">(50)</a> se tiene que <span class="math notranslate nohighlight">\(\mu_{n} = 0\)</span> y además <span class="math notranslate nohighlight">\(\lambda_{n}=C\)</span>.</p></li>
<li><p>Los <code class="docutils literal notranslate"><span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">encuentran</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">hiperplanos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">frontera</span> <span class="pre">del</span> <span class="pre">margen</span></code> satisfacen <span class="math notranslate nohighlight">\(\xi_{n} = 0\)</span> y, por tanto, <span class="math notranslate nohighlight">\(\mu_{n}\)</span> puede ser distinto de cero, lo que lleva a <span class="math notranslate nohighlight">\(0\leq\lambda_{n}\leq C\)</span>.</p></li>
</ul>
</section>
<section id="aplicacion">
<h2>Aplicación<a class="headerlink" href="#aplicacion" title="Link to this heading">#</a></h2>
</section>
<section id="modelos-lineales-y-caracteristicas-no-lineales">
<h2>Modelos lineales y características no lineales<a class="headerlink" href="#modelos-lineales-y-caracteristicas-no-lineales" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Como se ha visto antes, los <code class="docutils literal notranslate"><span class="pre">modelos</span> <span class="pre">lineales</span> <span class="pre">pueden</span> <span class="pre">ser</span> <span class="pre">bastante</span> <span class="pre">limitados</span> <span class="pre">en</span> <span class="pre">espacios</span> <span class="pre">de</span> <span class="pre">baja</span> <span class="pre">dimensión</span></code>, ya que las líneas y los hiperplanos tienen una <code class="docutils literal notranslate"><span class="pre">flexibilidad</span> <span class="pre">limitada</span></code>. Una forma de hacer que un modelo lineal sea mas flexible es añadiendo más características, por ejemplo, <code class="docutils literal notranslate"><span class="pre">añadiendo</span> <span class="pre">interacciones</span> <span class="pre">o</span> <span class="pre">polinomios</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">de</span> <span class="pre">entrada</span></code>. Veamos el dataset sintético utilizado en <code class="docutils literal notranslate"><span class="pre">feature</span> <span class="pre">importance</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>El vector de <code class="docutils literal notranslate"><span class="pre">labels</span> <span class="pre">y</span></code>, está compuesto por las clases <code class="docutils literal notranslate"><span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3</span></code>. Convertimos el problema, en uno binario, usando la <code class="docutils literal notranslate"><span class="pre">clase</span> <span class="pre">residual</span> <span class="pre">modulo</span> <span class="pre">2</span></code>, mediante: <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">y</span> <span class="pre">%</span> <span class="pre">2</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">%</span> <span class="mi">2</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ad2bec39a800d5c47424259935dedf2a2502260c83dc9cb0d1bf581837a6b82d.png" src="_images/ad2bec39a800d5c47424259935dedf2a2502260c83dc9cb0d1bf581837a6b82d.png" />
</div>
</div>
<ul class="simple">
<li><p>Un <code class="docutils literal notranslate"><span class="pre">modelo</span> <span class="pre">lineal</span> <span class="pre">de</span> <span class="pre">clasificación</span> <span class="pre">sólo</span> <span class="pre">puede</span> <span class="pre">separar</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">mediante</span> <span class="pre">una</span> <span class="pre">línea</span></code>, por lo tanto, no podrá hacer un buen trabajo en este conjunto de datos</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_svm</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">linear_svm</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a749ec442a6c6dea12195548b29b61a7c0df32ca86bdb5c6dd3e578b93dc00f3.png" src="_images/a749ec442a6c6dea12195548b29b61a7c0df32ca86bdb5c6dd3e578b93dc00f3.png" />
</div>
</div>
<ul class="simple">
<li><p>Ahora vamos a <code class="docutils literal notranslate"><span class="pre">ampliar</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">características</span> <span class="pre">de</span> <span class="pre">entrada,</span> <span class="pre">digamos</span> <span class="pre">que</span> <span class="pre">añadiendo</span> <span class="pre">también</span> <span class="pre">feature1</span> <span class="pre">**</span> <span class="pre">2</span></code>, el cuadrado de la segunda característica, como una nueva característica. En lugar de representar cada punto de datos como un punto bidimensional, (<code class="docutils literal notranslate"><span class="pre">feature0</span></code>, <code class="docutils literal notranslate"><span class="pre">feature1</span></code>), ahora lo representamos como un punto tridimensional, (<code class="docutils literal notranslate"><span class="pre">feature0</span></code>, <code class="docutils literal notranslate"><span class="pre">feature1</span></code>, <code class="docutils literal notranslate"><span class="pre">feature1</span> <span class="pre">**</span> <span class="pre">2</span></code>). Esta nueva representación se ilustra en el siguiente grafico de dispersión tridimensional</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Nótese que al nuevo array <code class="docutils literal notranslate"><span class="pre">X_new</span> <span class="pre">se</span> <span class="pre">le</span> <span class="pre">agregó</span> <span class="pre">una</span> <span class="pre">tercera</span> <span class="pre">columna</span></code> utilizando la segunda columna de <code class="docutils literal notranslate"><span class="pre">X</span></code>, <code class="docutils literal notranslate"><span class="pre">X[:,</span> <span class="pre">1:]</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[-1.72161036, -1.48033142,  2.19138111],
       [-3.6573384 , -9.5482383 , 91.16885455],
       [ 7.0778163 ,  0.99508772,  0.99019957],
       [-1.36579859, -0.3148625 ,  0.09913839],
       [-2.66521206, -3.12591651,  9.77135405]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span><span class="p">,</span> <span class="n">axes3d</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Visualizamos</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">en</span> <span class="pre">3D</span></code> y trazamos primero todos los puntos con <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">==</span> <span class="pre">0</span></code>, luego todos con <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">==</span> <span class="pre">1</span></code>. Si está interesado en conocer mas sobre los argumentos que puede utilizar en la función <code class="docutils literal notranslate"><span class="pre">scatter</span></code> (ver <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html">matplotlib.pyplot.scatter</a>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">figure</span><span class="p">,</span> <span class="n">elev</span><span class="o">=-</span><span class="mi">152</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">26</span><span class="p">)</span> <span class="c1">#elev, azim: vertical and horizontal angle respectivly</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">0</span> <span class="c1"># booleano para etiquetado</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span> <span class="c1"># s=60: points size; cmap=mglearn.cm2:color palette  </span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;feature0&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;feature1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;feature1 ** 2&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f7d3315c4db3a563720c4f3fb0915df8dbb4433830823e88100c9cd9b5e64ae1.png" src="_images/f7d3315c4db3a563720c4f3fb0915df8dbb4433830823e88100c9cd9b5e64ae1.png" />
</div>
</div>
<ul class="simple">
<li><p>En esta nueva representación de los datos, <code class="docutils literal notranslate"><span class="pre">ahora</span> <span class="pre">sí</span> <span class="pre">es</span> <span class="pre">posible</span> <span class="pre">separar</span> <span class="pre">las</span> <span class="pre">dos</span> <span class="pre">clases</span> <span class="pre">mediante</span> <span class="pre">un</span> <span class="pre">modelo</span> <span class="pre">lineal,</span> <span class="pre">un</span> <span class="pre">plano</span> <span class="pre">en</span> <span class="pre">tres</span> <span class="pre">dimensiones</span></code>. Podemos confirmarlo <code class="docutils literal notranslate"><span class="pre">ajustando</span> <span class="pre">un</span> <span class="pre">modelo</span> <span class="pre">lineal</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">aumentados</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_svm_3d</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="n">linear_svm_3d</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">linear_svm_3d</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Puede imprimir los coeficientes e intercepto (<code class="docutils literal notranslate"><span class="pre">coef,</span> <span class="pre">intercept</span></code>) del plano de la forma <span class="math notranslate nohighlight">\(c_{0}x+c_{1}y+c_{2}z+\text{intercept}\)</span>, que separa las dos clases. <code class="docutils literal notranslate"><span class="pre">Nótese</span> <span class="pre">que</span> <span class="pre">en</span> <span class="pre">este</span> <span class="pre">caso</span> <span class="pre">se</span> <span class="pre">está</span> <span class="pre">realizando</span> <span class="pre">un</span> <span class="pre">mapeo</span> <span class="pre">de</span> <span class="pre">características</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">Hilbert,</span> <span class="pre">de</span> <span class="pre">mayor</span> <span class="pre">dimensión</span></code>, donde se linealiza nuestro problema de clasificación.</p></li>
<li><p>Aquí <code class="docutils literal notranslate"><span class="pre">rstride,</span> <span class="pre">cstride</span></code> son utilizados para realizar divisiones uniformes en el plano <code class="docutils literal notranslate"><span class="pre">XY</span></code>, <code class="docutils literal notranslate"><span class="pre">elev</span></code> y <code class="docutils literal notranslate"><span class="pre">azim</span></code> son ángulos de rotación en grados que giran la imagen en <code class="docutils literal notranslate"><span class="pre">XY</span></code> y <code class="docutils literal notranslate"><span class="pre">Z</span></code> respectivamente, . Para mas información acerca de como usar <code class="docutils literal notranslate"><span class="pre">plot_surface</span></code> (ver <a class="reference external" href="https://matplotlib.org/stable/gallery/mplot3d/surface3d.html">3D surface (colormap)</a>). Es <code class="docutils literal notranslate"><span class="pre">tarea</span> <span class="pre">del</span> <span class="pre">estudiante</span> <span class="pre">profundizar</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">uso</span> <span class="pre">de</span> <span class="pre">cada</span> <span class="pre">parámetro</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">figure</span><span class="p">,</span> <span class="n">elev</span><span class="o">=-</span><span class="mi">152</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">26</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">XX</span><span class="p">,</span> <span class="n">YY</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">)</span>
<span class="n">ZZ</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">XX</span> <span class="o">+</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">YY</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="o">-</span><span class="n">coef</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="c1">#ecuación del plano</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">ZZ</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span> <span class="c1">#rstride/cstride: control row/column stride</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;feature0&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;feature1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;feature0 ** 2&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/693fe5a15c5e3ce36a262bfed4a79ffdede23ede653983a90c020a5d5fea8558.png" src="_images/693fe5a15c5e3ce36a262bfed4a79ffdede23ede653983a90c020a5d5fea8558.png" />
</div>
</div>
<ul class="simple">
<li><p>Como función de las características originales, el modelo <code class="docutils literal notranslate"><span class="pre">SVM</span> <span class="pre">lineal</span> <span class="pre">ya</span> <span class="pre">no</span> <span class="pre">es</span> <span class="pre">lineal</span></code>. No es una línea, sino <code class="docutils literal notranslate"><span class="pre">más</span> <span class="pre">bien</span> <span class="pre">una</span> <span class="pre">elipse</span></code>, como se puede ver en el gráfico creado aquí. Para dibujar la función de decisión utilizamos la clase <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> de <code class="docutils literal notranslate"><span class="pre">LinearSVC()</span></code>.</p></li>
<li><p>La función <code class="docutils literal notranslate"><span class="pre">ravel(),</span> <span class="pre">devuelve</span> <span class="pre">un</span> <span class="pre">array</span> <span class="pre">1-D</span> <span class="pre">que</span> <span class="pre">contiene</span> <span class="pre">los</span> <span class="pre">elementos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">entrada</span></code>. Se hace una copia sólo si se necesita. Para conocer más métodos que pueden ser utilizados a partir de la clase <code class="docutils literal notranslate"><span class="pre">SVC</span></code> (ver <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">sklearn.svm.SVC</a>). Aquí <code class="docutils literal notranslate"><span class="pre">levels</span> <span class="pre">indica</span> <span class="pre">niveles</span> <span class="pre">específicos</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">que</span> <span class="pre">las</span> <span class="pre">curvas</span> <span class="pre">de</span> <span class="pre">nivel</span> <span class="pre">serán</span> <span class="pre">dibujadas</span></code>, los valores deben estar en orden creciente. Para mas información acerca de dibujos de contorno (ver <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html">matplotlib.pyplot.contourf</a>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ZZ</span> <span class="o">=</span> <span class="n">YY</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">dec</span> <span class="o">=</span> <span class="n">linear_svm_3d</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">XX</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">YY</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">ZZ</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">dec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">XX</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="n">dec</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dec</span><span class="o">.</span><span class="n">max</span><span class="p">()],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fa421c7badea6801ce3926f13661d0fd5d02c76c0e30434c821720dc6c1ef4c5.png" src="_images/fa421c7badea6801ce3926f13661d0fd5d02c76c0e30434c821720dc6c1ef4c5.png" />
</div>
</div>
</section>
<section id="el-kernel-trick">
<h2>El Kernel Trick<a class="headerlink" href="#el-kernel-trick" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Durante el entrenamiento, <code class="docutils literal notranslate"><span class="pre">SVM</span> <span class="pre">aprende</span> <span class="pre">cuan</span> <span class="pre">importante</span> <span class="pre">son</span> <span class="pre">cada</span> <span class="pre">uno</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">para</span> <span class="pre">representar</span> <span class="pre">el</span> <span class="pre">límite</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">entre</span> <span class="pre">las</span> <span class="pre">dos</span> <span class="pre">clases</span></code>. Recuerde que, <code class="docutils literal notranslate"><span class="pre">sólo</span> <span class="pre">un</span> <span class="pre">subconjunto</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">es</span> <span class="pre">importante</span> <span class="pre">para</span> <span class="pre">definir</span> <span class="pre">la</span> <span class="pre">frontera</span> <span class="pre">de</span> <span class="pre">decisión</span></code>, los que se encuentran en la frontera entre las clases (<code class="docutils literal notranslate"><span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span></code>).</p></li>
<li><p>Para hacer una predicción de un nuevo punto, se mide la distancia a cada uno de los vectores de soporte. <code class="docutils literal notranslate"><span class="pre">Se</span> <span class="pre">toma</span> <span class="pre">una</span> <span class="pre">decisión</span> <span class="pre">de</span> <span class="pre">clasificación</span> <span class="pre">basada</span> <span class="pre">en</span> <span class="pre">las</span> <span class="pre">distancias</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span> <span class="pre">y</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">importancia</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte,</span> <span class="pre">aprendida</span> <span class="pre">durante</span> <span class="pre">el</span> <span class="pre">entrenamiento</span></code> (almacenado en el atributo <code class="docutils literal notranslate"><span class="pre">dual_coef_</span></code> de <code class="docutils literal notranslate"><span class="pre">SVC</span></code>). La distancia entre los puntos de datos se mide, por ejemplo, mediante el <code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">Gaussiano</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
k_{\text{rbf}}=\exp(\gamma\|x_{1}-x_{2}\|^2).
\]</div>
<ul class="simple">
<li><p>Aquí, <span class="math notranslate nohighlight">\(x_{1}\)</span> y <span class="math notranslate nohighlight">\(x_{2}\)</span> son puntos de datos, <span class="math notranslate nohighlight">\(\|x_{1} - x_{2}\|\)</span> denota la distancia euclidiana, y <span class="math notranslate nohighlight">\(\gamma\)</span> es un parámetro que controla el ancho del kernel gaussiano. <span class="math notranslate nohighlight">\(\gamma\)</span><code class="docutils literal notranslate"><span class="pre">=gamma</span></code> define un <code class="docutils literal notranslate"><span class="pre">factor</span> <span class="pre">de</span> <span class="pre">escala</span> <span class="pre">general</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">noción</span> <span class="pre">de</span> <span class="pre">distancia</span> <span class="pre">entre</span> <span class="pre">dos</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">SVM</span></code>; esto, a su vez, define <code class="docutils literal notranslate"><span class="pre">cómo</span> <span class="pre">un</span> <span class="pre">vector</span> <span class="pre">de</span> <span class="pre">apoyo</span> <span class="pre">da</span> <span class="pre">forma</span> <span class="pre">al</span> <span class="pre">límite</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">en</span> <span class="pre">su</span> <span class="pre">vecindad</span> <span class="pre">cercana</span></code>. La siguiente figura muestra el resultado del entrenamiento de una máquina de vectores de soporte en un conjunto de datos bidimensional de dos clases. <code class="docutils literal notranslate"><span class="pre">El</span> <span class="pre">límite</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">se</span> <span class="pre">muestra</span> <span class="pre">en</span> <span class="pre">negro,</span> <span class="pre">y</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span> <span class="pre">son</span> <span class="pre">puntos</span> <span class="pre">más</span> <span class="pre">grandes</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">contorno</span> <span class="pre">ancho</span></code>. El siguiente código crea este gráfico entrenando una <code class="docutils literal notranslate"><span class="pre">SVM</span></code> en el conjunto de datos <code class="docutils literal notranslate"><span class="pre">forge</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">imp</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">make_handcrafted_dataset</span><span class="p">()</span>
<span class="n">svm</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X.shape =&quot;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;; y.shape&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X.shape = (26, 2) ; y.shape (26,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Dibujamos los vectores de soporte. <code class="docutils literal notranslate"><span class="pre">Las</span> <span class="pre">etiquetas</span> <span class="pre">de</span> <span class="pre">clase</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">soporte</span> <span class="pre">vienen</span> <span class="pre">dadas</span> <span class="pre">por</span> <span class="pre">el</span> <span class="pre">signo</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">coeficientes</span> <span class="pre">duales</span></code> obtenidos en Python por medio de la función <code class="docutils literal notranslate"><span class="pre">dual_coef_</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">svm</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">sv</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">support_vectors_</span>
<span class="n">sv_labels</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">dual_coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">sv</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sv</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">sv_labels</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">markeredgewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b94a1f5e1838509fc8071bbf1c66cbd7652bd4fcc228023abff3230398efe654.png" src="_images/b94a1f5e1838509fc8071bbf1c66cbd7652bd4fcc228023abff3230398efe654.png" />
</div>
</div>
<ul class="simple">
<li><p>En este caso, <code class="docutils literal notranslate"><span class="pre">SVM</span> <span class="pre">produce</span> <span class="pre">un</span> <span class="pre">límite</span> <span class="pre">muy</span> <span class="pre">suave</span> <span class="pre">y</span> <span class="pre">no</span> <span class="pre">lineal</span></code>. Aquí ajustamos dos parámetros: el parámetro <code class="docutils literal notranslate"><span class="pre">C</span></code> y el parámetro <code class="docutils literal notranslate"><span class="pre">gamma</span></code>, que ahora discutiremos en detalle. Puede <code class="docutils literal notranslate"><span class="pre">hacer</span> <span class="pre">uso</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">validación</span> <span class="pre">cruzada</span> <span class="pre">y</span> <span class="pre">pipeline</span></code> estudiados en la sección de evaluación de modelos.</p></li>
</ul>
</section>
<section id="ajuste-de-los-parametros-de-svm">
<h2>Ajuste de los parámetros de SVM<a class="headerlink" href="#ajuste-de-los-parametros-de-svm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>El <code class="docutils literal notranslate"><span class="pre">parámetro</span> <span class="pre">gamma</span></code> es el que se muestra en la fórmula dada en la sección anterior, que <code class="docutils literal notranslate"><span class="pre">controla</span> <span class="pre">la</span> <span class="pre">anchura</span> <span class="pre">del</span> <span class="pre">kernel</span> <span class="pre">Gaussiano</span></code>. Determina la <code class="docutils literal notranslate"><span class="pre">escala</span> <span class="pre">de</span> <span class="pre">lo</span> <span class="pre">que</span> <span class="pre">significa</span> <span class="pre">que</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">estén</span> <span class="pre">próximos</span> <span class="pre">entre</span> <span class="pre">sí</span></code>. El parámetro <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">es</span> <span class="pre">un</span> <span class="pre">parámetro</span> <span class="pre">de</span> <span class="pre">regularización,</span> <span class="pre">similar</span> <span class="pre">al</span> <span class="pre">utilizado</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">modelos</span> <span class="pre">lineales</span></code>. Limita la importancia de cada punto (o más precisamente, su <code class="docutils literal notranslate"><span class="pre">dual_coef_</span></code>). Veamos qué ocurre cuando variamos estos parámetros:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">C</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">gamma</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)):</span>
        <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_svm</span><span class="p">(</span><span class="n">log_C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">log_gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">a</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;class 0&quot;</span><span class="p">,</span> <span class="s2">&quot;class 1&quot;</span><span class="p">,</span> <span class="s2">&quot;sv class 0&quot;</span><span class="p">,</span> <span class="s2">&quot;sv class 1&quot;</span><span class="p">],</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/898da292eefd7f095f83abb7555ad98f6bcc19937a1e1f027443d8ae1c0ba666.png" src="_images/898da292eefd7f095f83abb7555ad98f6bcc19937a1e1f027443d8ae1c0ba666.png" />
</div>
</div>
<ul class="simple">
<li><p>De izquierda a derecha, aumentamos el valor del <code class="docutils literal notranslate"><span class="pre">parámetro</span> <span class="pre">gamma</span> <span class="pre">de</span> <span class="pre">0.1</span> <span class="pre">a</span> <span class="pre">10</span></code>. Un <code class="docutils literal notranslate"><span class="pre">gamma</span> <span class="pre">pequeño</span> <span class="pre">significa</span> <span class="pre">un</span> <span class="pre">radio</span> <span class="pre">grande</span> <span class="pre">para</span> <span class="pre">el</span> <span class="pre">kernel</span> <span class="pre">gaussiano,</span> <span class="pre">lo</span> <span class="pre">que</span> <span class="pre">significa</span> <span class="pre">que</span> <span class="pre">muchos</span> <span class="pre">puntos</span> <span class="pre">se</span> <span class="pre">consideran</span> <span class="pre">cercanos</span></code>. Esto se refleja en unos <code class="docutils literal notranslate"><span class="pre">límites</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">muy</span> <span class="pre">suaves</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">izquierda,</span> <span class="pre">y</span> <span class="pre">límites</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">centran</span> <span class="pre">más</span> <span class="pre">en</span> <span class="pre">puntos</span> <span class="pre">individuales</span> <span class="pre">más</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">derecha</span></code>. Un valor bajo de <code class="docutils literal notranslate"><span class="pre">gamma</span></code> significa que el límite de decisión variará lentamente, lo que produce un modelo de baja complejidad, mientras que <code class="docutils literal notranslate"><span class="pre">un</span> <span class="pre">valor</span> <span class="pre">alto</span> <span class="pre">de</span> <span class="pre">gamma</span> <span class="pre">da</span> <span class="pre">lugar</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">modelo</span> <span class="pre">más</span> <span class="pre">complejo</span></code>.</p></li>
<li><p>De arriba a abajo, aumentamos el parámetro <code class="docutils literal notranslate"><span class="pre">C</span></code> de 0.1 a 1000. Al igual que con los modelos lineales, un valor de <code class="docutils literal notranslate"><span class="pre">C</span></code> pequeño <code class="docutils literal notranslate"><span class="pre">corresponde</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">modelo</span> <span class="pre">muy</span> <span class="pre">restringido,</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">que</span> <span class="pre">cada</span> <span class="pre">punto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">sólo</span> <span class="pre">puede</span> <span class="pre">tener</span> <span class="pre">una</span> <span class="pre">influencia</span> <span class="pre">muy</span> <span class="pre">limitada</span></code>. Se puede ver que en la parte superior izquierda el límite de decisión parece casi lineal, y los puntos mal clasificados apenas influyen en la línea.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Aumentar</span> <span class="pre">C,</span> <span class="pre">como</span> <span class="pre">se</span> <span class="pre">muestra</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">parte</span> <span class="pre">inferior</span> <span class="pre">derecha,</span> <span class="pre">permite</span> <span class="pre">que</span> <span class="pre">estos</span> <span class="pre">puntos</span> <span class="pre">tengan</span> <span class="pre">una</span> <span class="pre">influencia</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">modelo</span> <span class="pre">y</span> <span class="pre">hace</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">límite</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">se</span> <span class="pre">doble</span> <span class="pre">para</span> <span class="pre">clasificarlos</span> <span class="pre">correctamente</span></code>. Apliquemos <code class="docutils literal notranslate"><span class="pre">SVM</span></code> de núcleo <code class="docutils literal notranslate"><span class="pre">RBF</span></code> al conjunto de datos <strong><code class="docutils literal notranslate"><span class="pre">breast</span> <span class="pre">cancer</span></code></strong>. Por defecto, <code class="docutils literal notranslate"><span class="pre">C=1</span></code> y <code class="docutils literal notranslate"><span class="pre">gamma=1/n_features</span></code>. Queda como tarea para el estudiante hiperparametrizar cada uno de estos parámetros por medio del uso de <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> y <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.90
Accuracy on test set: 0.94
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Aunque las <code class="docutils literal notranslate"><span class="pre">SVM</span></code> suelen funcionar bastante bien, <code class="docutils literal notranslate"><span class="pre">son</span> <span class="pre">muy</span> <span class="pre">sensibles</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">ajustes</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">parámetros</span> <span class="pre">y</span> <span class="pre">al</span> <span class="pre">escalado</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>. En particular, requieren que <code class="docutils literal notranslate"><span class="pre">todas</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">varíen</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">escala</span> <span class="pre">similar</span></code>. Veamos los valores mínimos y máximos de cada característica, trazados en escala logarítmica</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature magnitude&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/700ecae98132d85939512b9d8006e2b717c837a045a32d3fc605631492b0b8f1.png" src="_images/700ecae98132d85939512b9d8006e2b717c837a045a32d3fc605631492b0b8f1.png" />
</div>
</div>
<ul class="simple">
<li><p>A partir de este gráfico podemos determinar que <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">características</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">cáncer</span> <span class="pre">de</span> <span class="pre">mama</span> <span class="pre">son</span> <span class="pre">de</span> <span class="pre">órdenes</span> <span class="pre">de</span> <span class="pre">magnitud</span> <span class="pre">completamente</span> <span class="pre">diferentes</span></code>. Esto puede ser un problema para otros modelos (como los modelos lineales), pero <code class="docutils literal notranslate"><span class="pre">tiene</span> <span class="pre">efectos</span> <span class="pre">devastadores</span> <span class="pre">para</span> <span class="pre">SVM</span> <span class="pre">con</span> <span class="pre">kernel</span></code>. Examinemos algunas formas de resolver este problema.</p></li>
</ul>
</section>
<section id="preprocesamiento-de-datos-para-svm">
<h2>Preprocesamiento de datos para SVM<a class="headerlink" href="#preprocesamiento-de-datos-para-svm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Una forma de resolver este problema es <code class="docutils literal notranslate"><span class="pre">reescalar</span> <span class="pre">cada</span> <span class="pre">característica</span> <span class="pre">para</span> <span class="pre">que</span> <span class="pre">todas</span> <span class="pre">estén</span> <span class="pre">aproximadamente</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">misma</span> <span class="pre">escala</span></code>. Un método común de reescalado para <code class="docutils literal notranslate"><span class="pre">SVMs</span></code> con kernel consiste en <code class="docutils literal notranslate"><span class="pre">escalar</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">manera</span> <span class="pre">que</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">estén</span> <span class="pre">entre</span> <span class="pre">0</span> <span class="pre">y</span> <span class="pre">1</span></code>. Veremos cómo hacer esto usando el método de preprocesamiento <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code>, <span class="math notranslate nohighlight">\(X_{\textsf{scaled}}=(X-X_{\textsf{min}})/(X_{\textsf{max}}-X_{\textsf{min}})\)</span>. Con el fin de comprender su funcionamiento, vamos a <code class="docutils literal notranslate"><span class="pre">implementar</span> <span class="pre">el</span> <span class="pre">preprocesamiento</span> <span class="pre">&quot;de</span> <span class="pre">forma</span> <span class="pre">manual&quot;</span></code></p></li>
</ul>
<ul class="simple">
<li><p>Calculamos el valor <code class="docutils literal notranslate"><span class="pre">mínimo</span> <span class="pre">por</span> <span class="pre">característica</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>. Dado que <code class="docutils literal notranslate"><span class="pre">X_train</span></code> contiene 30 columnas de características, el resultado va a ser un array de dimensión 30. Para obtenerlo usamos <code class="docutils literal notranslate"><span class="pre">min(axis=0)</span></code>, para que el mínimo sea buscado sobre las filas de cada columna.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(426, 30)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">min_on_training</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">min_on_training</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(30,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Calculamos el <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">de</span> <span class="pre">rangos</span> <span class="pre">para</span> <span class="pre">cada</span> <span class="pre">característica</span> <span class="pre">(max</span> <span class="pre">-</span> <span class="pre">min)</span></code> en el conjunto de entrenamiento. El <code class="docutils literal notranslate"><span class="pre">máximo</span> <span class="pre">sobre</span> <span class="pre">cada</span> <span class="pre">columna</span> <span class="pre">es</span> <span class="pre">calculado</span> <span class="pre">usando</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">max(axis=0)</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">range_on_training</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="n">min_on_training</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">range_on_training</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(30,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Restamos</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">mínimo,</span> <span class="pre">cada</span> <span class="pre">una</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">en</span> <span class="pre">X_train</span></code> y después dividimos por el <code class="docutils literal notranslate"><span class="pre">rango</span> <span class="pre">para</span> <span class="pre">cada</span> <span class="pre">característica</span></code> resultante</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="n">min_on_training</span><span class="p">)</span> <span class="o">/</span> <span class="n">range_on_training</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Minimum for each feature</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minimum for each feature
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Maximum for each feature</span><span class="se">\n</span><span class="s2"> </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Maximum for each feature
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1.]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Utilizamos</span> <span class="pre">la</span> <span class="pre">misma</span> <span class="pre">transformación</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">prueba</span></code>, utilizando el mínimo y el rango del conjunto de entrenamiento</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span> <span class="o">-</span> <span class="n">min_on_training</span><span class="p">)</span> <span class="o">/</span> <span class="n">range_on_training</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Utilizando ahora los <code class="docutils literal notranslate"><span class="pre">datos</span> <span class="pre">transformados,</span> <span class="pre">entrenamos</span> <span class="pre">el</span> <span class="pre">modelo</span> <span class="pre">SVC()</span></code> y verificamos valores de <code class="docutils literal notranslate"><span class="pre">accuracy</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.984
Accuracy on test set: 0.972
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Escalar los datos supuso una gran diferencia. Ahora estamos en un régimen de <code class="docutils literal notranslate"><span class="pre">overfitting</span> <span class="pre">donde</span> <span class="pre">el</span> <span class="pre">rendimiento</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">es</span> <span class="pre">ligeramente</span> <span class="pre">mayor</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">de</span> <span class="pre">prueba</span></code>, pero menos cercano al 100% de <code class="docutils literal notranslate"><span class="pre">accuracy</span></code>. A partir de aquí, podemos intentar disminuir <code class="docutils literal notranslate"><span class="pre">C</span></code> o <code class="docutils literal notranslate"><span class="pre">gamma</span></code> para ajustar un modelo regularizado. Por ejemplo:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc_C01</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">svc_C01</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C01</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C01</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.948
Accuracy on test set: 0.958
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Nótese que sucede cuando seguimos disminuyendo el valor de <span class="math notranslate nohighlight">\(C\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc_C001</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">svc_C001</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.643
Accuracy on test set: 0.636
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Si usamos un parámetro de regularización <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">demasiado</span> <span class="pre">extremos,</span> <span class="pre">podemos</span> <span class="pre">caer</span> <span class="pre">nuevamente</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">mayor</span> <span class="pre">overfitting</span></code>, o en un modelo demasiado simple, que entrega valores de <code class="docutils literal notranslate"><span class="pre">accuracy</span></code> bastantes bajos para los datos de entrenamiento y prueba, esto es, <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">modelo</span> <span class="pre">no</span> <span class="pre">será</span> <span class="pre">capaz</span> <span class="pre">de</span> <span class="pre">generalizarse</span> <span class="pre">correctamente,</span> <span class="pre">underfitting</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svc_C100</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">svc_C100</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C100</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">svc_C100</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 1.000
Accuracy on test set: 0.965
</pre></div>
</div>
</div>
</div>
<div class="admonition-puntos-fuertes-puntos-debiles-y-parametros admonition">
<p class="admonition-title">Puntos fuertes, puntos débiles y parámetros</p>
<ul class="simple">
<li><p>Las máquinas de vectores de soporte kernelizadas son modelos potentes y funcionan bien en una variedad de conjuntos de datos. <code class="docutils literal notranslate"><span class="pre">Las</span> <span class="pre">SVM</span> <span class="pre">permiten</span> <span class="pre">establecer</span> <span class="pre">límites</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">complejos,</span> <span class="pre">incluso</span> <span class="pre">si</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">sólo</span> <span class="pre">tienen</span>&#160; <span class="pre">unas</span> <span class="pre">pocas</span> <span class="pre">características</span></code>. Funcionan bien con datos de baja y alta dimensión (es decir, pocas y muchas características), pero no escalan muy bien con el número de muestras. Ejecutar SVM con datos de hasta 10.000 muestras puede funcionar bien, pero <code class="docutils literal notranslate"><span class="pre">trabajar</span> <span class="pre">con</span> <span class="pre">conjuntos</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">100.000</span> <span class="pre">o</span> <span class="pre">más</span> <span class="pre">puede</span> <span class="pre">ser</span> <span class="pre">un</span> <span class="pre">reto</span> <span class="pre">en</span> <span class="pre">términos</span> <span class="pre">de</span> <span class="pre">tiempo</span> <span class="pre">de</span> <span class="pre">ejecución</span> <span class="pre">y</span> <span class="pre">uso</span> <span class="pre">de</span> <span class="pre">memoria</span></code>.</p></li>
<li><p>Otra desventaja de SVM es que <code class="docutils literal notranslate"><span class="pre">requieren</span> <span class="pre">un</span> <span class="pre">cuidadoso</span> <span class="pre">preprocesamiento</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">y</span> <span class="pre">el</span> <span class="pre">ajuste</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">parámetros</span></code>. Por eso, hoy en día, algunos investigadores utilizan modelos basados en árboles, como los bosques aleatorios o el gradient boosting (que requieren poco o ningún preprocesamiento) en muchas aplicaciones. Además, los modelos <code class="docutils literal notranslate"><span class="pre">SVM</span></code> son difíciles de inspeccionar, <code class="docutils literal notranslate"><span class="pre">puede</span> <span class="pre">ser</span> <span class="pre">difícil</span> <span class="pre">entender</span> <span class="pre">por</span> <span class="pre">qué</span> <span class="pre">se</span> <span class="pre">ha</span> <span class="pre">hecho</span> <span class="pre">una</span> <span class="pre">predicción</span> <span class="pre">concreta,</span> <span class="pre">y</span> <span class="pre">puede</span> <span class="pre">ser</span> <span class="pre">difícil</span> <span class="pre">explicar</span> <span class="pre">el</span> <span class="pre">modelo</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">inexperto</span></code>. Aun así, puede valer la pena probar las <code class="docutils literal notranslate"><span class="pre">SVM</span></code>, sobre todo si todas las características representan medidas en unidades similares (por ejemplo, todas son intensidades de píxeles) y están en escalas similares.</p></li>
<li><p>Los <code class="docutils literal notranslate"><span class="pre">parámetros</span> <span class="pre">importantes</span> <span class="pre">en</span> <span class="pre">las</span> <span class="pre">SVM</span> <span class="pre">con</span> <span class="pre">kernel</span> <span class="pre">son</span> <span class="pre">el</span> <span class="pre">parámetro</span> <span class="pre">de</span> <span class="pre">regularización</span> <span class="pre">C,</span> <span class="pre">la</span> <span class="pre">elección</span> <span class="pre">del</span> <span class="pre">kernel,</span> <span class="pre">y</span> <span class="pre">los</span> <span class="pre">parámetros</span> <span class="pre">específicos</span> <span class="pre">del</span> <span class="pre">kernel</span></code>. Aunque principalmente nos centramos en el núcleo <code class="docutils literal notranslate"><span class="pre">RBF</span></code>, hay otras opciones disponibles en <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. El núcleo <code class="docutils literal notranslate"><span class="pre">RBF</span></code> tiene sólo un parámetro,<code class="docutils literal notranslate"> <span class="pre">gamma</span></code>, que es la inversa de la anchura del kernel de Gaus. <code class="docutils literal notranslate"><span class="pre">gamma</span></code> y <code class="docutils literal notranslate"><span class="pre">C</span></code> controlan la complejidad del modelo, <code class="docutils literal notranslate"><span class="pre">con</span> <span class="pre">valores</span> <span class="pre">grandes</span> <span class="pre">en</span> <span class="pre">cualquiera</span> <span class="pre">de</span> <span class="pre">ellos</span> <span class="pre">obtenemos</span> <span class="pre">un</span> <span class="pre">modelo</span> <span class="pre">más</span> <span class="pre">complejo</span></code>. Por lo tanto, los buenos ajustes para los dos parámetros suelen estar fuertemente correlacionados, y <code class="docutils literal notranslate"><span class="pre">C</span></code> y <code class="docutils literal notranslate"><span class="pre">gamma</span></code> deben ajustarse juntos.</p></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "ml_tf"
        },
        kernelOptions: {
            name: "ml_tf",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ml_tf'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="decisiontree_model.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Árboles de decisión</p>
      </div>
    </a>
    <a class="right-next"
       href="ann_model.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Redes Neuronales y Deep Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#espacios-de-hilbert-con-kernel-reproductor">Espacios de Hilbert con Kernel reproductor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#construccion-de-kernels">Construcción de kernels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teorema-de-representacion">Teorema de representación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge-con-kernel">Regresión ridge con Kernel</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-de-vectores-de-soporte">Regresión de vectores de soporte</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-optima-lineal-varepsilon-insensible">Regresión óptima lineal <span class="math notranslate nohighlight">\(\varepsilon\)</span>-insensible</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-kernel-ridge">Regresión Kernel Ridge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Máquinas de vectores de soporte</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clases-linealmente-separables-clasificador-de-maximo-margen">Clases linealmente separables: Clasificador de máximo margen</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clases-no-separables">Clases no separables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion">Aplicación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-lineales-y-caracteristicas-no-lineales">Modelos lineales y características no lineales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-kernel-trick">El Kernel Trick</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ajuste-de-los-parametros-de-svm">Ajuste de los parámetros de SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preprocesamiento-de-datos-para-svm">Preprocesamiento de datos para SVM</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lihki Rubio, Ph.D.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>Lihki Rubio, Ph.D. All rights reserved.</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>