
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2. \(k\)-vecinos más cercanos &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=530fe47d" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/custom-checkpoint.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'knn_model';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/custom.js?v=14184634"></script>
    <script src="_static/.ipynb_checkpoints/custom-checkpoint.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3. Regresión Ridge y Lasso" href="linear_model.html" />
    <link rel="prev" title="1. Aprendizaje supervisado" href="supervised_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fotolihki.jpg" class="logo__image only-light" alt="Machine Learning - Home"/>
    <script>document.write(`<img src="_static/fotolihki.jpg" class="logo__image only-dark" alt="Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Profesor: Dr. Lihki Rubio
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="supervised_intro.html">1. Aprendizaje supervisado</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">2. <span class="math notranslate nohighlight">\(k\)</span>-vecinos más cercanos</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_model.html">3. Regresión Ridge y Lasso</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes_model.html">4. Clasificador Bayesiano</a></li>
<li class="toctree-l1"><a class="reference internal" href="decisiontree_model.html">5. Random Forest y XGBoost</a></li>


<li class="toctree-l1"><a class="reference internal" href="svm_model.html">8. Máquinas de vectores de soporte</a></li>

<li class="toctree-l1"><a class="reference internal" href="ann_model.html">10. Redes Neuronales y Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="practical_pca.html">11. Análisis de Componentes Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">12. Evaluación de modelos</a></li>
<li class="toctree-l1"><a class="reference internal" href="chains_pipelines.html">13. Cadenas de Algoritmos y Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">14. Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercises_sols.html">15. Soluciones a ejercicios</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">16. Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/knn_model.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>k-vecinos más cercanos</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">2.1. Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ejercicios">2.2. Ejercicios</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">2.3. Implementación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-de-kneighborsclassifier">2.4. Análisis de <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-breast-cancer-dataset">2.5. Aplicación: Breast Cancer Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-exploratorio-de-datos">2.6. Análisis Exploratorio de Datos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multicolinealidad">2.6.1. Multicolinealidad</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-de-inflacion-de-la-varianza-vif">2.6.2. Factor de Inflación de la Varianza (VIF)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-del-modelo-k-nn">2.7. Selección del modelo K-NN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-por-k-vecinos">2.8. Regresión por <span class="math notranslate nohighlight">\(k\)</span>-vecinos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-de-kneighborsregressor">2.9. Análisis de <code class="docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-world-hydropower-generation">2.9.1. Aplicación: World Hydropower Generation</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="k-vecinos-mas-cercanos">
<h1><span class="section-number">2. </span><span class="math notranslate nohighlight">\(k\)</span>-vecinos más cercanos<a class="headerlink" href="#k-vecinos-mas-cercanos" title="Link to this heading">#</a></h1>
<div class="tip admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>El algoritmo <em><strong>vecino más cercano</strong></em> es considerado uno de los <em><strong>más simples dentro del aprendizaje automático</strong></em>. Su enfoque consiste en <em><strong>memorizar el conjunto de entrenamiento y luego predecir la etiqueta del vecino más cercano en dicho conjunto</strong></em>. Este método se basa en la idea de que las <em><strong>características utilizadas para describir los puntos en el dominio son relevantes para determinar sus etiquetas</strong></em>, de manera que es probable que <em><strong>puntos cercanos tengan la misma etiqueta</strong></em>. Incluso en situaciones donde el conjunto de entrenamiento es muy grande, es posible encontrar un vecino más cercano, como por ejemplo cuando el <em><strong>conjunto de entrenamiento abarca toda la Web y las distancias se basan en enlaces</strong></em>.</p></li>
<li><p>A diferencia de otros paradigmas algorítmicos que requieren hipótesis predefinidas, el método del <em><strong>vecino más cercano calcula una etiqueta para cualquier punto de prueba sin buscar un predictor dentro de una clase de funciones predefinida</strong></em>. En este capítulo, se describen los métodos del <em><strong>vecino más cercano</strong></em> tanto para problemas de <em><strong>clasificación</strong></em> como de <em><strong>regresión</strong></em>, se analiza su rendimiento en clasificación binaria y se discute la eficacia de su aplicación.</p></li>
</ul>
</div>
<section id="analisis">
<h2><span class="section-number">2.1. </span>Análisis<a class="headerlink" href="#analisis" title="Link to this heading">#</a></h2>
<div class="proof definition admonition" id="def_knn">
<p class="admonition-title"><span class="caption-number">Definition 2.1 </span> (<span class="math notranslate nohighlight">\(k\)</span>-NN (Clasificación))</p>
<section class="definition-content" id="proof-content">
<ul class="simple">
<li><p>Sea <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> nuestro <em><strong>dominio de instancia</strong></em> (<em><strong>conjunto de objetos que se desea etiquetar</strong></em>), dotado con una métrica <span class="math notranslate nohighlight">\(\rho\)</span>. Es decir, <span class="math notranslate nohighlight">\(\rho:\mathcal{X}\times\mathcal{X}\longrightarrow\mathbb{R}\)</span> es una  función que retorna la <em><strong>distancia entre cualquier par de elementos</strong></em> de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Por ejemplo, si <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}^{d}\)</span>, entonces <span class="math notranslate nohighlight">\(\rho\)</span> puede ser la <em><strong>distancia Euclidiana</strong></em>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\rho(\boldsymbol{x}, \boldsymbol{x}')=\|\boldsymbol{x}-\boldsymbol{x}'\|=\sqrt{\sum_{i=1}^{d}(x_{i}-x_{i}')^{2}}.
\]</div>
<ul class="simple">
<li><p>Sea <span class="math notranslate nohighlight">\(S=(\boldsymbol{x}_{1}, y_{1}),\dots,(\boldsymbol{x}_{m}, y_{m})\)</span> una secuencia de <em><strong>ejemplos de entrenamiento</strong></em> y <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, el <em><strong>conjunto de etiquetas</strong></em>, usualmente: <span class="math notranslate nohighlight">\(\{0, 1\},~\{-1, +1\}\)</span> o <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Para cada <span class="math notranslate nohighlight">\(\boldsymbol{x}\in\mathcal{X}\)</span>, sea <span class="math notranslate nohighlight">\(\pi_{1}(\boldsymbol{x}),\dots,\pi_{m}(\boldsymbol{x})\)</span> una <em><strong>reordenación</strong></em> de <span class="math notranslate nohighlight">\(\{1,\dots,m\}\)</span> en función de su distancia a <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, <span class="math notranslate nohighlight">\(\rho(\boldsymbol{x}, \boldsymbol{x}_{i})\)</span>. Esto es, para todo <span class="math notranslate nohighlight">\(i&lt;m\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\rho(\boldsymbol{x}, \boldsymbol{x}_{\pi_{i}(\boldsymbol{x})})\leq\rho(\boldsymbol{x}, \boldsymbol{x}_{\pi_{i+1}(\boldsymbol{x})}).
\]</div>
<ul class="simple">
<li><p>Para un número <span class="math notranslate nohighlight">\(k\)</span>, la regla <span class="math notranslate nohighlight">\(k\)</span>-NN para la <em><strong>clasificación binaria</strong></em> se define del siguiente modo:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">input</span></code>: muestra de <em><strong>entrenamiento</strong></em> <span class="math notranslate nohighlight">\(S=(\boldsymbol{x}_{1}, y_{1}),\dots,(\boldsymbol{x}_{m}, y_{m})\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output</span></code>: <span class="math notranslate nohighlight">\(\forall~\boldsymbol{x}\in\mathcal{X}\)</span>, <em><strong>etiqueta mayoritaria</strong></em> en <span class="math notranslate nohighlight">\(\{y_{\pi_{i}(\boldsymbol{x})}:~i\leq k\}\)</span></p></li>
</ul>
</li>
</ul>
</section>
</div><figure class="align-center" id="knn-classification-fig">
<a class="reference internal image-reference" href="_images/knn_classification.png"><img alt="_images/knn_classification.png" src="_images/knn_classification.png" style="width: 531.9px; height: 463.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.1 </span><span class="caption-text">Clasificación usando <span class="math notranslate nohighlight">\(k\)</span>-NN para <span class="math notranslate nohighlight">\(k=3\)</span>. Source: <a class="reference external" href="http://kdnuggets.com">kdnuggets.com</a>.</span><a class="headerlink" href="#knn-classification-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Desempate</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">clasificación</span></code> <span class="math notranslate nohighlight">\(k\)</span>-NN</strong></em>: En ocasiones ocurren <em>empates a la hora de seleccionar la clase mayoritaria</em>. Existen varias formas de abordar este problema, las siguientes son las más frecuentes.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title"><em><strong>1. Elija un <em><strong><span class="math notranslate nohighlight">\(k\)</span></strong></em> diferente</strong></em></p>
<p>Nótese que, aunque un método de clasificación de tres vecinos más cercanos soluciona el problema de la selección de vecinos en las <a class="reference internal" href="#knn-ties"><span class="std std-numref">Fig. 2.2</span></a> i) y ii), el problema de la <a class="reference internal" href="#knn-ties"><span class="std std-numref">Fig. 2.2</span></a> iii) no es resuelto cuando se consideran por ejemplo cuatro. Elegir un <span class="math notranslate nohighlight">\(k\)</span> diferente, no siempre es la mejor solución.</p>
</div>
<figure class="align-center" id="knn-ties">
<a class="reference internal image-reference" href="_images/knn_ties.png"><img alt="_images/knn_ties.png" src="_images/knn_ties.png" style="width: 702.0px; height: 197.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.2 </span><span class="caption-text">k-NN: Empates respecto a distancia, en la selección de clase con mayor frecuencia.</span><a class="headerlink" href="#knn-ties" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="stretched-link admonition">
<p class="admonition-title"><em><strong>2. Elija al azar entre los valores empatados</strong></em></p>
<ul class="simple">
<li><p>Aplicando este enfoque a la <a class="reference internal" href="#knn-ties"><span class="std std-numref">Fig. 2.2</span></a> ii), cada una de las tres observaciones tendría la misma probabilidad de ser seleccionada como uno de los vecinos más cercanos.</p></li>
<li><p>En la <a class="reference internal" href="#knn-ties"><span class="std std-numref">Fig. 2.2</span></a> iii), se seleccionaría <span class="math notranslate nohighlight">\(A\)</span> como el vecino más cercano a <span class="math notranslate nohighlight">\(N\)</span>, y uno de los tres puntos observados restantes se seleccionaría al azar.</p></li>
<li><p>Dependiendo de cuáles de los valores empatados se permitan en el modelo, podría tener un efecto significativo en cómo se clasifica <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
</ul>
</div>
<div class="tip admonition">
<p class="admonition-title"><em><strong>3. Permitir observaciones hasta el punto de parada natural</strong></em></p>
<p>La idea aquí es <strong>elegir el valor de <span class="math notranslate nohighlight">\(k\geq 2\)</span> más pequeño tal que, no existan empates</strong></p>
<ul class="simple">
<li><p>Para la <a class="reference internal" href="#knn-ties"><span class="std std-numref">Fig. 2.2</span></a> i), se seleccionarían las dos observaciones más cercanas, nótese que, para <span class="math notranslate nohighlight">\(k=3\)</span> se tiene un empate. Para la <a class="reference internal" href="#knn-ties"><span class="std std-numref">Fig. 2.2</span></a> ii), como hay un empate para <span class="math notranslate nohighlight">\(k=3\)</span>, se consideran los tres vecinos más cercanos. La restricción de solo dos vecinos se elimina para esta observación en particular.</p></li>
<li><p>Del mismo modo, en la <a class="reference internal" href="#knn-ties"><span class="std std-numref">Fig. 2.2</span></a> iii) se seleccionan las cuatro observaciones. Observe que <em><strong>este método elegiría tantos valores como fuera necesario para evitar el empate</strong></em>. <em>Si en la <a class="reference internal" href="#knn-ties"><span class="std std-numref">Fig. 2.2</span></a> ii) hubiera ocho puntos equidistantes, se incluirían los ocho en el modelo</em>.</p></li>
</ul>
</div>
<div class="stretched-link admonition">
<p class="admonition-title"><strong><strong>4. Con base en vecinos seleccionados</strong></strong></p>
<p>Supongamos que <em><strong>nos hemos decidido por el tercer método</strong></em> y que ya tenemos nuestros vecinos seleccionados</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Elegir</span> <span class="pre">un</span></code></strong> <span class="math notranslate nohighlight">\(k\)</span> <strong><code class="docutils literal notranslate"><span class="pre">impar</span></code></strong>: Algunos sugieren simplemente <em>elegir un valor impar para</em> <span class="math notranslate nohighlight">\(k\)</span>. Este método no siempre funciona, ya que los estados de clasificación también podrían ser impares <span class="math notranslate nohighlight">\((A, B, C)\)</span>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Elegir</span> <span class="pre">al</span> <span class="pre">azar</span> <span class="pre">entre</span> <span class="pre">vecinos</span> <span class="pre">empatados</span></code></strong>. Con este método, <span class="math notranslate nohighlight">\(L\)</span> (<em>considerando <span class="math notranslate nohighlight">\(k=2\)</span></em>) y <span class="math notranslate nohighlight">\(N\)</span> (<em>considerando <span class="math notranslate nohighlight">\(k=4\)</span></em>) tienen la misma probabilidad de ser clasificados como <span class="math notranslate nohighlight">\(A\)</span> o <span class="math notranslate nohighlight">\(B\)</span>. Pero, ¿es justo el azar? ¿Tiene sentido que <span class="math notranslate nohighlight">\(N\)</span>, que está muy cerca de <span class="math notranslate nohighlight">\(A\)</span>, tenga la misma probabilidad de ser <span class="math notranslate nohighlight">\(B\)</span>?</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Ponderación</span> <span class="pre">por</span> <span class="pre">distancia</span></code></strong>. Para resolver el problema planteado por el método anterior, es posible <em><strong>ponderar los vecinos de modo que los más cercanos al punto no observado tengan un “mayor voto”</strong></em>. Este método daría lugar a que tanto <span class="math notranslate nohighlight">\(L\)</span> como <span class="math notranslate nohighlight">\(N\)</span> se clasificaran como <span class="math notranslate nohighlight">\(A\)</span>, ya que los vecinos <span class="math notranslate nohighlight">\(A\)</span> están más cerca que los demás en comparación.</p></li>
</ul>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Predictor</span></code></strong>: Definamos por <span class="math notranslate nohighlight">\(h_{S}(\boldsymbol{x})\)</span> la <em><strong>regla de predicción</strong></em> (el subíndice <span class="math notranslate nohighlight">\(S\)</span> hace hincapié en el hecho de que <em><strong>el predictor de salida depende de</strong></em> <span class="math notranslate nohighlight">\(S\)</span>). Esta función también se denomina <em><strong>predictor, hipótesis o clasificador</strong></em>. El <em><strong>predictor</strong></em> puede utilizarse para predecir la etiqueta de nuevos puntos de dominio. Cuando <span class="math notranslate nohighlight">\(k=1\)</span> (ver <a class="reference internal" href="#def_knn">Definition 2.1</a>), tenemos la regla <span class="math notranslate nohighlight">\(1\)</span>-NN:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h_{S}(\boldsymbol{x})=y_{\pi_{1}(\boldsymbol{x})}.
\]</div>
<ul class="simple">
<li><p>Nótese que hemos considerado la <em><strong>métrica Euclideana</strong></em> en esta definición, pero, <em><strong>dependiendo del conjunto de datos, podría ser mas adecuado utilizar una métrica diferente</strong></em> (ver <a class="reference external" href="https://es.wikipedia.org/wiki/Geometr%C3%ADa_del_taxista">Manhatan</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Minkowski_distance">Minkowski</a>). Esto es, una métrica alternativa, podría <em><strong>mejorar los errores cometidos por el clasificador</strong></em>.</p></li>
</ul>
<div class="proof definition admonition" id="def_knn_reg">
<p class="admonition-title"><span class="caption-number">Definition 2.2 </span> (<span class="math notranslate nohighlight">\(k\)</span>-NN (Regresión))</p>
<section class="definition-content" id="proof-content">
<ul class="simple">
<li><p>Para <em><strong>problemas de regresión</strong></em>, a saber, <span class="math notranslate nohighlight">\(\mathcal{Y}=\mathbb{R}\)</span>, se puede definir la <em><strong>regla de predicción</strong></em> como la <em><strong>media objetivo</strong></em> de los <span class="math notranslate nohighlight">\(k\)</span> vecinos más cercanos. Esto es,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h_{S}(\boldsymbol{x})=\frac{1}{k}\sum_{i=1}^{k}y_{\pi_{i}(\boldsymbol{x})}.
\]</div>
<ul class="simple">
<li><p>Más en general, para alguna función <span class="math notranslate nohighlight">\(\phi:~(\mathcal{X}, \mathcal{Y})^{k}\rightarrow\mathcal{Y}\)</span>, la regla <span class="math notranslate nohighlight">\(k\)</span>-NN con respecto a <span class="math notranslate nohighlight">\(\phi\)</span> es:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-knn-regeq">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-knn-regeq" title="Link to this equation">#</a></span>\[
h_{S}(\boldsymbol{x})=\phi((\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})}, y_{\pi_{1}(\boldsymbol{x})}),\dots,(\boldsymbol{x}_{\pi_{k}(\boldsymbol{x})}, y_{\pi_{k}(\boldsymbol{x})})).
\]</div>
<ul class="simple">
<li><p>Se puede verificar que podemos lanzar la predicción por mayoría de etiquetas (<em><strong>clasificación</strong></em>) o por el objetivo promediado (<em><strong>regresión</strong></em>) como en la Ecuación <a class="reference internal" href="#equation-knn-regeq">(2.1)</a> mediante una <em><strong>elección adecuada de</strong></em> <span class="math notranslate nohighlight">\(\phi\)</span>.</p></li>
<li><p>La generalidad puede llevar a otras reglas, por ejemplo, si <span class="math notranslate nohighlight">\(\mathcal{Y}=\mathbb{R}\)</span>, podemos tomar una <em><strong>media ponderada de los objetivos según la distancia a</strong></em> <span class="math notranslate nohighlight">\(x\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h_{S}(\boldsymbol{x})=\sum_{i=1}^{k}\frac{\rho(\boldsymbol{x}, \boldsymbol{x}_{\pi_{i}(\boldsymbol{x})})}{\sum_{j=1}^{k}\rho(\boldsymbol{x}, \boldsymbol{x}_{\pi_{j}(\boldsymbol{x})})}y_{\pi_{i}(\boldsymbol{x})}.
\]</div>
</section>
</div><div class="proof definition admonition" id="def_emp_error">
<p class="admonition-title"><span class="caption-number">Definition 2.3 </span> (Error empírico y verdadero revisado)</p>
<section class="definition-content" id="proof-content">
<ul class="simple">
<li><p>Para una <em><strong>distribución de probabilidad</strong></em>, <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, sobre <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span>, se puede medir la probabilidad de que <span class="math notranslate nohighlight">\(h\)</span> <em><strong>cometa un error</strong></em> cuando los puntos etiquetados se extraen aleatoriamente según <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. Redefinimos el <em><strong>error verdadero (o riesgo)</strong></em> de una regla de predicción <span class="math notranslate nohighlight">\(h\)</span> como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-real-error">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-real-error" title="Link to this equation">#</a></span>\[
L_{\mathcal{D}}(h):=\underset{(\boldsymbol{x},y)\sim\mathcal{D}}{\mathbb{P}}[h(\boldsymbol{x})\neq y]:=\mathcal{D}(\{(\boldsymbol{x},y):~h(\boldsymbol{x})\neq y\}).
\]</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Minimización</span> <span class="pre">Empírica</span> <span class="pre">de</span> <span class="pre">Riesgo</span> <span class="pre">(ERM)</span></code></strong>. Nos gustaría encontrar un <em><strong>predictor</strong></em>, <span class="math notranslate nohighlight">\(h\)</span>, para el que se <em><strong>minimizara el error</strong></em> <span class="math notranslate nohighlight">\(L_{\mathcal{D}}(h)\)</span> sobre una <em><strong>clase de hipótesis</strong></em> denota por <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, esto es: <span class="math notranslate nohighlight">\(\text{ERM}_{\mathcal{H}}(S)\in\underset{h\in\mathcal{H}}{\text{argmin}}~L_{S}(h)\)</span>. Cada <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> es una función de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> a <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>.</p></li>
<li><p>Sin embargo, no se conocen los datos que generan <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. A lo que sí se tiene acceso es a los <em><strong>datos de entrenamiento</strong></em>, <span class="math notranslate nohighlight">\(S\)</span>. Una noción útil de error que puede calcular es el <em><strong>error de entrenamiento (riesgo empírico)</strong></em>, es decir, el <em><strong>error en el que incurre el clasificador sobre la muestra de entrenamiento</strong></em>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L_{S}(h):=\frac{|\{i\in\left[m\right]:~h(\boldsymbol{x}_{i})\neq y_{i}\}|}{m},~\left[m\right]=\{1,\dots,m\}.
\]</div>
<ul class="simple">
<li><p>Dado <span class="math notranslate nohighlight">\(S\)</span>, y un <em><strong>aprendizaje</strong></em>, se puede calcular <span class="math notranslate nohighlight">\(L_{S}(h)\)</span> para cualquier función <span class="math notranslate nohighlight">\(h:X\rightarrow\{0,1\}\)</span>.</p></li>
<li><p>Deseamos encontrar alguna hipótesis, <span class="math notranslate nohighlight">\(h : X\rightarrow Y\)</span>, que (probablemente de forma aproximada) <em><strong>minimice el riesgo verdadero</strong></em>,<span class="math notranslate nohighlight">\(L_{D}(h)\)</span>.</p></li>
</ul>
</section>
</div><ul class="simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Predictor</span> <span class="pre">óptimo</span> <span class="pre">de</span> <span class="pre">Bayes</span></code></strong></em>: Dada cualquier distribución de probabilidad <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> sobre <span class="math notranslate nohighlight">\(\mathcal{X}\times\{0, 1\}\)</span>, la <em><strong>mejor función de predicción de etiquetas</strong></em> de <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> a <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> será</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-bayes-opt-pred">
<span class="eqno">(2.3)<a class="headerlink" href="#equation-bayes-opt-pred" title="Link to this equation">#</a></span>\[\begin{split}
f_{\mathcal{D}}(x)=
\begin{cases}
1,&amp;\text{si}~\mathbb{P}[y=1|x]\geq1/2\\
0,&amp;\text{otro caso}
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>Se puede verificar que <em><strong>para toda distribución de probabilidad</strong></em> <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, <em><strong>el predictor óptimo de Bayes</strong></em> <span class="math notranslate nohighlight">\(f_{D}\)</span> <em><strong>es óptimo</strong></em>, en el sentido de que <em><strong>ningún otro clasificador</strong></em>, <span class="math notranslate nohighlight">\(g : \mathcal \rightarrow\{0, 1\}\)</span>, <em><strong>tiene un error menor</strong></em>. Es decir, para cada clasificador <span class="math notranslate nohighlight">\(g, L_{\mathcal{D}}(f_{\mathcal{D}}) \leq L_{\mathcal{D}}(g)\)</span>.</p></li>
</ul>
<div class="proof definition admonition" id="gen_loss_fun">
<p class="admonition-title"><span class="caption-number">Definition 2.4 </span> (Funciones de pérdidas generalizadas)</p>
<section class="definition-content" id="proof-content">
<ul class="simple">
<li><p>Dado cualquier conjunto <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> (que desempeña el papel de nuestras <em><strong>hipótesis, o predictor</strong></em>) y algún <em><strong>dominio</strong></em> <span class="math notranslate nohighlight">\(Z\)</span> sea <span class="math notranslate nohighlight">\(\ell\)</span> cualquier función de <span class="math notranslate nohighlight">\(\mathcal{H}\times Z\)</span> al conjunto de los números reales no negativos, <span class="math notranslate nohighlight">\(\ell: \mathcal{H}\times Z\rightarrow\mathbb{R}^{+}\)</span>. Llamamos a estas funciones, <em><strong>funciones de pérdida</strong></em>.</p></li>
<li><p>Nótese que para los problemas de predicción, tenemos que <span class="math notranslate nohighlight">\(Z = \mathcal{X}\times\mathcal{Y}\)</span>. Sin embargo, nuestra noción de la <em><strong>función de pérdida</strong></em> se generaliza más allá de las tareas de predicción y, por tanto, permite que <span class="math notranslate nohighlight">\(Z\)</span> sea <em><strong>cualquier dominio de ejemplos</strong></em>.</p></li>
<li><p>Definimos ahora la <em><strong>función de riesgo</strong></em> como la <em><strong>pérdida esperada de un clasificador</strong></em>, <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>, con respecto a la distribución de probabilidad <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> sobre <span class="math notranslate nohighlight">\(Z\)</span>, a saber,</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-risk-function">
<span class="eqno">(2.4)<a class="headerlink" href="#equation-risk-function" title="Link to this equation">#</a></span>\[
L_{\mathcal{D}}(h):=\underset{z\sim\mathcal{D}}{\mathbb{E}}[\ell(h, z)].
\]</div>
<ul class="simple">
<li><p>Es decir, consideramos la <em><strong>esperanza de la pérdida de</strong></em> <span class="math notranslate nohighlight">\(h\)</span> <em><strong>sobre los objetos</strong></em> <span class="math notranslate nohighlight">\(z\)</span> <em><strong>elegidos aleatoriamente de acuerdo con</strong></em> <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. Del mismo modo, definimos el <em><strong>riesgo empírico</strong></em> como la pérdida esperada sobre una muestra dada <span class="math notranslate nohighlight">\(S=(z_{1},\dots,z_{m})\in Z^{m}\)</span>, a saber</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
L_{S}(h):=\frac{1}{m}\sum_{i=1}^{m}\ell(h, z_{i}).
\]</div>
<ul class="simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Ejemplos</span></code></strong></em>: Dada una variable aleatoria <span class="math notranslate nohighlight">\(z\)</span> que abarca el conjunto de pares <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span>, definimos las siguientes <em><strong>funciones de pérdida</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\text{0-1 Loss}:&amp;~\ell_{\text{0-1}}(h, (x,y)):=
\begin{cases}
0, &amp; \text{si}~ h(x)=y\\[2mm]
1, &amp; \text{si}~ h(x)\neq y
\end{cases}\\
\text{Square Loss}:&amp;~\ell_{\text{sq}}(h, (x,y)):=(h(x)-y)^{2}.
\end{align}
\end{split}\]</div>
<ul>
<li><p><span class="math notranslate nohighlight">\(\ell_{\text{0-1}}(h, (x,y))\)</span> se utiliza en <em><strong>problemas de clasificación binaria o multiclase</strong></em>. Hay que tener en cuenta que, para una variable aleatoria, <span class="math notranslate nohighlight">\(\alpha\)</span>, que toma los valores <span class="math notranslate nohighlight">\(\{0,1\}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{\alpha\sim\mathcal{D}}[\alpha] = \mathbb{P}_{\alpha\sim\mathcal{D}}[\alpha = 1].\]</div>
<p>Por lo tanto, las funciones de pérdida definidas en Eq. <a class="reference internal" href="#equation-risk-function">(2.4)</a> y Eq. <a class="reference internal" href="#equation-real-error">(2.2)</a> son equivalentes.</p>
</li>
</ul>
</section>
</div><div class="proof observation admonition" id="knn_analysis">
<p class="admonition-title"><span class="caption-number">Observation 2.1 </span> (<span class="math notranslate nohighlight">\(1\)</span>-NN)</p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>Dado que las reglas NN son <em><strong>métodos de aprendizaje tan naturales</strong></em>, sus propiedades de generalización han sido ampliamente estudiadas. La mayoría de los resultados anteriores son resultados de <em><strong>consistencia asintótica</strong></em>, que analizan el rendimiento de las reglas NN cuando el tamaño de la muestra, <span class="math notranslate nohighlight">\(m\)</span>, tiende a <span class="math notranslate nohighlight">\(\infty\)</span>, y la <em><strong>tasa de convergencia depende de la distribución subyacente</strong></em>.</p></li>
<li><p>Estamos interesados en <em><strong>aprendizajes de muestras de entrenamiento finitas</strong></em> y entender el <em><strong>rendimiento de generalización</strong></em> en función del <em><strong>tamaño de esos conjuntos de entrenamiento finitos</strong></em> y de <em><strong>supuestos previos claros sobre la distribución de los datos</strong></em>. Por lo tanto, presentamos un análisis de <em><strong>muestras finitas de la regla 1-NN</strong></em>.</p></li>
</ul>
</section>
</div><div class="admonition-generalizacion-de-la-regla-1-nn admonition">
<p class="admonition-title">Generalización de la regla <span class="math notranslate nohighlight">\(1\)</span>-NN</p>
<ul class="simple">
<li><p>Ahora analizamos el <em><strong>error verdadero</strong></em> de la regla <span class="math notranslate nohighlight">\(1\)</span>-NN para la <em><strong>clasificación binaria</strong></em> con <em><strong>función pérdida</strong></em> <span class="math notranslate nohighlight">\(0-1\)</span> a saber, <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0, 1\}\)</span> y <span class="math notranslate nohighlight">\(\ell(h, (\boldsymbol{x}, y))=\mathbb{1}_{[h(\boldsymbol{x})\neq y]}\)</span>. También supondremos en todo el análisis que <span class="math notranslate nohighlight">\(\mathcal{X} = [0,1]^{d}\)</span> y <span class="math notranslate nohighlight">\(\rho\)</span> es la <em><strong>distancia euclidiana</strong></em>.</p></li>
<li><p>Empezaremos introduciendo algunas notaciones. Sea <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> una distribución sobre <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span>. Denotemos por <span class="math notranslate nohighlight">\(\mathcal{D}_{\mathcal{X}}\)</span> la <em><strong>distribución marginal inducida por</strong></em> <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> y sea <span class="math notranslate nohighlight">\(\eta:\mathbb{R}^{d}\rightarrow\mathbb{R}\)</span> la <em><strong>probabilidad condicional sobre las etiquetas</strong></em>, esto es,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\eta(\boldsymbol{x})=\mathbb{P}[y=1|\boldsymbol{x}].
\]</div>
<ul class="simple">
<li><p>La <em><strong>regla óptima de Bayes</strong></em> (es decir, la hipótesis que minimiza <span class="math notranslate nohighlight">\(L_{\mathcal{D}}(h)\)</span> sobre todas las funciones <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>, ver Eq. <a class="reference internal" href="#equation-bayes-opt-pred">(2.3)</a>) es</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
h^{\star}(\boldsymbol{x})=\mathbb{1}_{\left[\eta(\boldsymbol{x})&gt;1/2\right]}.
\]</div>
<ul class="simple">
<li><p>Suponemos que la función de <em><strong>probabilidad condicional</strong></em> <span class="math notranslate nohighlight">\(\eta\)</span> es <span class="math notranslate nohighlight">\(c\)</span>-Lipschitz (ver <a class="reference external" href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz continuity</a>) para algún valor de <span class="math notranslate nohighlight">\(c&gt;0\)</span>. Es decir, <span class="math notranslate nohighlight">\(\forall~\boldsymbol{x}, \boldsymbol{x}'\in\mathcal{X},~|\eta(\boldsymbol{x})-\eta(\boldsymbol{x}')|\leq c\|\boldsymbol{x}-\boldsymbol{x}'\|\)</span>. En otras palabras, esto significa que <em><strong>si dos vectores están próximos, es probable que sus etiquetas sean las mismas</strong></em>.</p></li>
</ul>
</div>
<div class="tip admonition">
<p class="admonition-title">Nota</p>
<p>El siguiente Lema aplica la <em><strong>Lipschitzness</strong></em> de la función de <em><strong>probabilidad condicional</strong></em> para acotar el <em><strong>error verdadero</strong></em> de la regla <em><strong>1-NN</strong></em> en función de la distancia entre cada <em><strong>instancia de prueba</strong></em> y su <em><strong>vecino más cercano en el conjunto de entrenamiento</strong></em>.</p>
</div>
<div class="proof lemma admonition" id="knn_lemma1">
<p class="admonition-title"><span class="caption-number">Lemma 2.1 </span></p>
<section class="lemma-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(\mathcal{X}=[0,1]^{d},~ \mathcal{Y}=\{0,1\}\)</span>, y <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> una distribución sobre <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span>, para la cual, la <em><strong>función de probabilidad condicional</strong></em>, <span class="math notranslate nohighlight">\(\eta\)</span>, es una <em><strong>función</strong></em> <span class="math notranslate nohighlight">\(c\)</span><em><strong>-Lipschitz</strong></em>. Sea <span class="math notranslate nohighlight">\(S=(\boldsymbol{x}_{1}, y_{1}),\dots,(\boldsymbol{x}_{m}, y_{m})\)</span> una muestra <em><strong>iid</strong></em> y sea <span class="math notranslate nohighlight">\(h_{S}\)</span> su correspondiente <em><strong>hipótesis</strong></em> <span class="math notranslate nohighlight">\(1\)</span>-NN. Sea <span class="math notranslate nohighlight">\(h^{\star}\)</span> la <em><strong>regla óptima de Bayes</strong></em> para <span class="math notranslate nohighlight">\(\eta\)</span>. Entonces,</p>
<div class="math notranslate nohighlight">
\[
\underset{S\sim\mathcal{D}^{m}}{\mathbb{E}}[L_{\mathcal{D}}(h_{S})]\leq 2 L_{\mathcal{D}}(h^{\star})+c\underset{S\sim\mathcal{D}^{m}, \boldsymbol{x}\sim\mathcal{D}}{\mathbb{E}}[\|\boldsymbol{x}-\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})}\|].
\]</div>
</section>
</div><p><strong><code class="docutils literal notranslate"><span class="pre">Demostración</span></code></strong></p>
<ul class="simple">
<li><p>Dado que <span class="math notranslate nohighlight">\(L_{\mathcal{D}}(h_{S}):=\mathbb{E}_{(\boldsymbol{x}, y)\sim\mathcal{D}}(\ell(h, (\boldsymbol{x}, y)))=\mathbb{E}_{(\boldsymbol{x}, y)\sim\mathcal{D}}(\mathbb{1}_{[h(\boldsymbol{x})\neq y]})\)</span>, entonces <span class="math notranslate nohighlight">\(\mathbb{E}(L_{\mathcal{D}}(h_{S})))\)</span> es la <em><strong>probabilidad de muestrear un conjunto de entrenamiento</strong></em> <span class="math notranslate nohighlight">\(S\)</span> y un <em><strong>ejemplo adicional</strong></em> <span class="math notranslate nohighlight">\((\boldsymbol{x}, y)\)</span> tal que, la etiqueta de <span class="math notranslate nohighlight">\(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})}\)</span> es diferente de <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>En otras palabras, podemos primero <em><strong>muestrear</strong></em> <span class="math notranslate nohighlight">\(m\)</span> <em><strong>ejemplos no etiquetados</strong></em>, <span class="math notranslate nohighlight">\(S_{x}=(x_{1}, x_{2},\dots,x_{m})\)</span>, de acuerdo a <span class="math notranslate nohighlight">\(\mathcal{D}_{x}\)</span>, <em><strong>y un nuevo ejemplo no etiquetado adicional</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{x}\sim\mathcal{D}_{x}\)</span>. Luego encontramos <span class="math notranslate nohighlight">\(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})}\)</span> que es el <em><strong>vecino mas cercano a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> en <span class="math notranslate nohighlight">\(S_{\boldsymbol{x}}\)</span>, y finalmente muestrear: <span class="math notranslate nohighlight">\(y\sim\eta(\boldsymbol{x})\)</span> y <span class="math notranslate nohighlight">\(y_{\pi_{1}(\boldsymbol{x})}\sim\eta(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})})\)</span>, donde <span class="math notranslate nohighlight">\(\eta\)</span> es la <em><strong>probabilidad condicional asociada a la distribución marginal</strong></em> sobre <span class="math notranslate nohighlight">\(\mathcal{X},~\mathcal{D}_{x}\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>Entonces</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{E}_{S\sim\mathcal{D}^{m}}(L_{\mathcal{D}}(h_{S}))&amp;=\underset{S_{x}\sim\mathcal{D}_{x}^{m}, x\sim\mathcal{D}_{x}, y\sim\eta(x), y_{\pi_{1}(x)}\sim\eta(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})})}{\mathbb{E}}(\mathbb{1}_{[y\neq y_{\pi_{1}(\boldsymbol{x})}]})\\
&amp;=\underset{S_{x}\sim\mathcal{D}_{x}^{m}, x\sim\mathcal{D}_{x}}{\mathbb{E}}\left(\underset{y\sim\eta(x), y_{\pi_{1}(x)}\sim\eta(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})})}{\mathbb{E}}(\mathbb{1}_{[y\neq y_{\pi_{1}(\boldsymbol{x})}]})\right)\\
&amp;=\underset{S_{x}\sim\mathcal{D}_{x}^{m}, x\sim\mathcal{D}_{x}}{\mathbb{E}}\left(\underset{y\sim\eta(x), y_{\pi_{1}(x)}\sim\eta(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})})}{\mathbb{P}}(y\neq y_{\pi_{1}(\boldsymbol{x})})\right)\tag{$\star$}
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p><em><strong>Acotamos superiormente</strong></em> a: <span class="math notranslate nohighlight">\(\underset{y\sim\eta(x), y_{\pi_{1}(x)}\sim\eta(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})})}{\mathbb{P}}(y\neq y_{\pi_{1}(\boldsymbol{x})}),~\forall~\boldsymbol{x}, \pi_{1}(\boldsymbol{x})\in\mathcal{D}\)</span></p></li>
</ul>
<ul class="simple">
<li><p>Definamos <span class="math notranslate nohighlight">\(y':=y_{\pi_{1}(\boldsymbol{x})},~x':=\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})}\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\underset{y\sim\eta(x), y_{\pi_{1}(x)}\sim\eta(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})})}{\mathbb{P}}(y\neq y_{\pi_{1}(\boldsymbol{x})})&amp;=\mathbb{P}(y'=1|\boldsymbol{x}')\mathbb{P}(y=0|\boldsymbol{x})+\mathbb{P}(y'=0|\boldsymbol{x}')\mathbb{P}(y=1|\boldsymbol{x})\\
&amp;=\mathbb{P}(y'=1|\boldsymbol{x}')(1-\mathbb{P}(y=1|\boldsymbol{x}))+(1-\mathbb{P}(y'=1|\boldsymbol{x}'))\mathbb{P}(y=1|\boldsymbol{x})\\[3mm]
&amp;=\eta(\boldsymbol{x}')(1-\eta(\boldsymbol{x}))+(1-\eta(\boldsymbol{x}')\eta(\boldsymbol{x}))\\[3mm]
&amp;=(\underbrace{\textcolor{red}{\eta(\boldsymbol{x})}-\eta(\boldsymbol{x}})+\eta(\boldsymbol{x}'))(\textcolor{red}{1-\eta(\boldsymbol{x})})+(\textcolor{red}{1\underbrace{-\eta(\boldsymbol{x})+\eta(\boldsymbol{x})}}-\eta(\boldsymbol{x}'))\textcolor{red}{\eta(\boldsymbol{x})}\\[3mm]
&amp;=2\eta(\boldsymbol{x})(1-\eta(\boldsymbol{x}))+(\eta(\boldsymbol{x})-\eta(\boldsymbol{x}'))(2\eta(\boldsymbol{x})-1)
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Dado que <span class="math notranslate nohighlight">\(\eta(\boldsymbol{x})=\mathbb{P}(y=1|\boldsymbol{x})\in[0, 1]\)</span>, entonces <span class="math notranslate nohighlight">\(|2\eta(\boldsymbol{x})-1|\leq 1\)</span>. En efecto:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
0\leq\eta(\boldsymbol{x})\leq 1\leq\Leftrightarrow 0\leq 2\eta(\boldsymbol{x})\leq 2\Leftrightarrow-1\leq 2\eta(\boldsymbol{x})-1\leq 1\Leftrightarrow|2\eta(\boldsymbol{x})-1|\leq1.
\]</div>
<ul class="simple">
<li><p>Dado que <span class="math notranslate nohighlight">\(\eta(\boldsymbol{x})\)</span> es <span class="math notranslate nohighlight">\(c\)</span><em><strong>-Lipschitz</strong></em>, entonces, <span class="math notranslate nohighlight">\(\forall~\boldsymbol{x},\boldsymbol{x}'\in\mathcal{X},~ |\eta(\boldsymbol{x})-\eta(\boldsymbol{x}')|\leq c\|\boldsymbol{x}-\boldsymbol{x}'\|\)</span>. Por lo tanto,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\underset{y\sim\eta(x), y_{\pi_{1}(x)}\sim\eta(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})})}{\mathbb{P}}(y\neq y_{\pi_{1}(\boldsymbol{x})})&amp;=|2\eta(\boldsymbol{x})(1-\eta(\boldsymbol{x}))+(\eta(\boldsymbol{x})-\eta(\boldsymbol{x}'))(2\eta(\boldsymbol{x})-1)|\\
&amp;\leq|2\eta(\boldsymbol{x})(1-\eta(\boldsymbol{x}))|+|(\eta(\boldsymbol{x})-\eta(\boldsymbol{x}'))(2\eta(\boldsymbol{x})-1)|\\[3mm]
&amp;\leq 2\eta(\boldsymbol{x})(1-\eta(\boldsymbol{x}))+|\eta(\boldsymbol{x})-\eta(\boldsymbol{x}')|\textcolor{red}{|2\eta(\boldsymbol{x})-1|}\\[3mm]
&amp;\leq 2\eta(\boldsymbol{x})(1-\eta(\boldsymbol{x}))+c\|\boldsymbol{x}-\boldsymbol{x}'\|
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Reemplazando esta última desigualdad en (<span class="math notranslate nohighlight">\(\star\)</span>) se tiene que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\underset{S_{x}\sim\mathcal{D}_{x}^{m}, x\sim\mathcal{D}_{x}}{\mathbb{E}}\left(\underset{y\sim\eta(x), y_{\pi_{1}(x)}\sim\eta(\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})})}{\mathbb{P}}(y\neq y_{\pi_{1}(\boldsymbol{x})})\right)&amp;\leq\underset{\mathcal{X}\sim\mathcal{D}_{x}}{\mathbb{E}}(2\eta(\boldsymbol{x})(1-\eta(\boldsymbol{x})))\\
&amp;+c\underset{\boldsymbol{x}\sim\mathcal{D}_{x},~\mathcal{D}_{x}\sim\mathcal{D}_{x}^{m}}{\mathbb{E}}(\|\boldsymbol{x}-\boldsymbol{x}'\|)
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Entonces</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{S}(L_{\mathcal{D}}(h_{S}))\leq\underset{\mathcal{X}\sim\mathcal{D}_{x}}{\mathbb{E}}(2\eta(\boldsymbol{x})(1-\eta(\boldsymbol{x})))+c\underset{\boldsymbol{x}\sim\mathcal{D}_{x},~\mathcal{D}_{x}\sim\mathcal{D}_{x}^{m}}{\mathbb{E}}(\|\boldsymbol{x}-\boldsymbol{x}'\|)
\]</div>
<ul class="simple">
<li><p>Dado que <span class="math notranslate nohighlight">\(h^{\star}\)</span> es la <em><strong>regla óptima de Bayes</strong></em> para <span class="math notranslate nohighlight">\(\eta\)</span>, entonces <span class="math notranslate nohighlight">\(h^{\star}\)</span> minimiza la pérdida esperada del clasificador, esto es:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{x}(2\eta(\boldsymbol{x})(1-\eta(\boldsymbol{x})))\leq 2\mathbb{E}_{\boldsymbol{x}}(\min\{\eta(\boldsymbol{x}), 1-\eta(\boldsymbol{x})\})=2 L_{\mathcal{D}}(h^{\star})
\]</div>
<ul class="simple">
<li><p>Nótese que la desigualdad anterior, que involucra el mínimo <span class="math notranslate nohighlight">\(\min\{\eta(\boldsymbol{x}), 1-\eta(\boldsymbol{x})\}\)</span>, es valida. En efecto, si <span class="math notranslate nohighlight">\(\eta(\boldsymbol{x})\in[0, 1]\)</span> y <span class="math notranslate nohighlight">\(\min\{\eta(\boldsymbol{x}), 1-\eta(\boldsymbol{x})\}=\eta(\boldsymbol{x})\)</span>, entonces <span class="math notranslate nohighlight">\(\eta(\boldsymbol{x})\leq 1-\eta(\boldsymbol{x})\)</span>, por lo tanto:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
2\eta(\boldsymbol{x})(1-\eta(\boldsymbol{x}))\leq2(1-\eta(\boldsymbol{x}))^{2}\leq 2\min\{\eta(\boldsymbol{x}), 1-\eta(\boldsymbol{x})\}
\]</div>
<ul class="simple">
<li><p>Por lo tanto:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\underset{S\sim\mathcal{D}^{m}}{\mathbb{E}}[L_{\mathcal{D}}(h_{S})]\leq 2 L_{\mathcal{D}}(h^{\star})+c\underset{\boldsymbol{x}\sim\mathcal{D}_{x},~\mathcal{D}_{x}\sim\mathcal{D}_{x}^{m}}{\mathbb{E}}(\|\boldsymbol{x}-\boldsymbol{x}'\|).
\]</div>
<div class="tip admonition">
<p class="admonition-title">Observation</p>
<p>El siguiente paso es <em><strong>acotar la distancia entre una variable aleatoria</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> <em><strong>y su vecino mas cercano en</strong></em> <span class="math notranslate nohighlight">\(S\)</span>. Primero necesitaremos el siguiente Teorema, el cual <em><strong>acota el peso probabilístico de subconjuntos que no son alcanzados por una muestra aleatoria, como una función del tamaño de la muestra</strong></em>.</p>
</div>
<div class="proof lemma admonition" id="knn_lemma2">
<p class="admonition-title"><span class="caption-number">Lemma 2.2 </span></p>
<section class="lemma-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(C_{1}, C_{2},\dots,C_{r}\)</span> una colección de <em><strong>subconjuntos de algún dominio</strong></em> <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Sea <span class="math notranslate nohighlight">\(S\)</span> una sucesión de <span class="math notranslate nohighlight">\(m\)</span> <em><strong>puntos iid muestreados de acuerdo a una distribución de probabilidad</strong></em> <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> sobre <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Entonces:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{S\sim\mathcal{D}^{m}}\left(\sum_{i:C_{i}\cap S=\emptyset}\mathbb{P}(C_{i})\right)\leq\frac{r}{me}.
\]</div>
</section>
</div><p><strong><code class="docutils literal notranslate"><span class="pre">Demostración</span></code></strong></p>
<ul class="simple">
<li><p>Por <em><strong>linealidad del valor esperado</strong></em> se tiene que:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{E}_{S}\left(\sum_{i:C_{i}\cap S=\emptyset}\mathbb{P}(C_{i})\right)&amp;=\sum_{i:C_{i}\cap S=\emptyset}\mathbb{E}_{S}(\mathbb{P}(C_{i}))\\
&amp;=\sum_{i=1}^{r}\mathbb{E}_{S}(\mathbb{P}(C_{i})\mathbb{1}_{C_{i}\cap S=\emptyset})\\
&amp;=\sum_{i=1}^{r}\mathbb{P}(C_{i})\mathbb{E}_{S}(\mathbb{1}_{C_{i}\cap S=\emptyset})
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Entonces para cada <span class="math notranslate nohighlight">\(i\)</span>, usando <em><strong>propiedades de independencia y complemento</strong></em>, dado que <span class="math notranslate nohighlight">\(S\)</span> es una <em><strong>sucesión de</strong></em> <span class="math notranslate nohighlight">\(m\)</span> <em><strong>puntos iid</strong></em> tenemos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{S}(\mathbb{1}_{C_{i}\cap S=\emptyset})=\mathbb{P}_{S}(C_{i}\cap S=\emptyset)=\prod_{S}\mathbb{P}(C_{i}\setminus S)=(1-\mathbb{P}(C_{i}))^{m}.
\]</div>
<ul>
<li><p>Definamos <span class="math notranslate nohighlight">\(\varepsilon=-\mathbb{P}(C_{i})\)</span>. <em><strong>Verifiquemos que</strong></em> <span class="math notranslate nohighlight">\(e^{\varepsilon}\ge 1+\varepsilon,~\forall\varepsilon\in\mathbb{R}\)</span>. En efecto. Si <span class="math notranslate nohighlight">\(~\varepsilon\leq-1\Rightarrow e^{\varepsilon}&gt;0~\)</span> y <span class="math notranslate nohighlight">\(~1+\varepsilon\leq 0\Rightarrow 1+\varepsilon\leq0&lt;e^{\varepsilon}\Rightarrow1+\varepsilon\leq e^{\varepsilon}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    \begin{align*}
    \text{Si}~\varepsilon&gt;-1\Rightarrow e^{\varepsilon}&amp;=\lim_{n\rightarrow\infty}\left(1+\frac{\varepsilon}{n}\right)^{n}\underset{\text{Bernoulli}}{\geq}\lim_{n\rightarrow\infty}1+n\frac{\varepsilon}{n}=1+\varepsilon.
    \end{align*}
    \]</div>
<p>Entonces <span class="math notranslate nohighlight">\(1+\varepsilon\leq e^{\varepsilon},~\forall\varepsilon\in\mathbb{R}\)</span>.</p>
</li>
</ul>
<div class="proof definition admonition" id="des_bernoulli">
<p class="admonition-title"><span class="caption-number">Definition 2.5 </span> (Desigualdad de Bernoulli)</p>
<section class="definition-content" id="proof-content">
<p><em><strong>Desigualdad de Bernoulli</strong></em>: Sea <span class="math notranslate nohighlight">\(x\in\mathbb{R}\)</span> tal que <span class="math notranslate nohighlight">\(x&gt;-1\)</span> y <span class="math notranslate nohighlight">\(n\in\mathbb{Z}^{+}\)</span>, entonces, <span class="math notranslate nohighlight">\((1+x)^{n}\geq1+nx\)</span>.</p>
</section>
</div><ul class="simple">
<li><p>Dado que <span class="math notranslate nohighlight">\(\varepsilon=-\mathbb{P}(C_{i})\)</span>, entonces</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
(1-\mathbb{P}(C_{i}))^{m}\leq e^{-m \mathbb{P}(C_{i})}\Rightarrow\mathbb{E}_{S}(\mathbb{1}_{C_{i}\cap S=\emptyset})\leq e^{-m \mathbb{P}(C_{i})}
\]</div>
<ul class="simple">
<li><p>De este modo,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{E}_{S}\left(\sum_{i:C_{i}\cap S=\emptyset}\mathbb{P}(C_{i})\right)&amp;=\sum_{i=1}^{r}\mathbb{P}(C_{i})\mathbb{E}_{S}(\mathbb{1}_{C_{i}\cap S=\emptyset})\\
&amp;\leq\sum_{i=1}^{r}\mathbb{P}(C_{i})e^{-\mathbb{P}(C_{i})m}\\[3mm]
&amp;\leq r\max_{i}\mathbb{P}(C_{i})e^{-\mathbb{P}(C_{i})m},\quad f'(a)=0, a=\mathbb{P}(C_{i})\\
&amp;=r\frac{1}{m}e^{-m(1/m)}\\
&amp;=\frac{r}{me}
\end{align*}
\end{split}\]</div>
<div class="proof theorem admonition" id="knn_theorem1">
<p class="admonition-title"><span class="caption-number">Theorem 2.1 </span></p>
<section class="theorem-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(\mathcal{X}=[0, 1]^{d},~\mathcal{Y}=\{0, 1\}\)</span> y <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> una distribución sobre <span class="math notranslate nohighlight">\(\mathcal{X}\times\mathcal{Y}\)</span> para la cual la <em><strong>función de probabilidad condicional</strong></em>, <span class="math notranslate nohighlight">\(\eta\)</span>, es una función <span class="math notranslate nohighlight">\(c\)</span><em><strong>-Lipschitz</strong></em>. Denotamos con <span class="math notranslate nohighlight">\(h_{S}\)</span> el resultado de aplicar la regla <span class="math notranslate nohighlight">\(1\)</span>-NN a una muestra <span class="math notranslate nohighlight">\(S\sim\mathcal{D}^{m}\)</span>. Entonces</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{S\sim\mathcal{D}^{m}}(L_{\mathcal{D}}(h_{S}))\leq 2 L_{\mathcal{D}}(h^{\star})+4 c\sqrt{d} m^{-1/(d+1)}.
\]</div>
</section>
</div><p><strong><code class="docutils literal notranslate"><span class="pre">Demostración</span></code></strong></p>
<figure class="align-center" id="box-neigh-knn-fig">
<a class="reference internal image-reference" href="_images/box_neigh_knn.png"><img alt="_images/box_neigh_knn.png" src="_images/box_neigh_knn.png" style="width: 227.70000000000002px; height: 234.9px;" /></a>
</figure>
<ul class="simple">
<li><p>Fijemos <span class="math notranslate nohighlight">\(\varepsilon=1/T\)</span>, para algún entero <span class="math notranslate nohighlight">\(T\)</span>, sea <span class="math notranslate nohighlight">\(r=T^{d}\)</span>, y sea <span class="math notranslate nohighlight">\(C_{1}, C_{2}, \dots, C_{m}\)</span> un <em><strong>cubrimiento del conjunto</strong></em> <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> usando <em><strong>cajas de longitud</strong></em> <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Es decir, para cada <span class="math notranslate nohighlight">\((\alpha_{1}, \alpha_{2},\dots,\alpha_{d})\in [T]^{d}\)</span>, existe un conjunto <span class="math notranslate nohighlight">\(C_{i}\)</span> de la forma</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathcal{B}_{x}=\{\boldsymbol{x}:~\forall~j,~ x_{j}\in [(\alpha_{j}-1)/T, \alpha_{j}/T]\}
\]</div>
<ul class="simple">
<li><p>Entonces, <span class="math notranslate nohighlight">\(\forall~\boldsymbol{x}, \boldsymbol{x}'\)</span> en la <em><strong>misma caja</strong></em> <span class="math notranslate nohighlight">\(\mathcal{B}_{x}\)</span>, se tiene que, <span class="math notranslate nohighlight">\(x_{i}-x_{i}'\leq 1/T=\varepsilon\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\|\boldsymbol{x}-\boldsymbol{x}'\|=\left(\sum_{i=1}^{d}(x_{i}-x_{i}')^{2}\right)^{1/2}\hspace{-4mm}\leq\left(\sum_{i=1}^{d}\varepsilon^{2}\right)^{1/2}\hspace{-4mm}=\sqrt{d\varepsilon^{2}}=\sqrt{d}\varepsilon.
\]</div>
<ul class="simple">
<li><p>Si <span class="math notranslate nohighlight">\(\boldsymbol{x}, \boldsymbol{x}'\)</span> <em><strong>no caen en la misma caja</strong></em> <span class="math notranslate nohighlight">\(\mathcal{B}_{x}\)</span> entonces <span class="math notranslate nohighlight">\(x_{i}-x_{i}'\leq 1,~\forall~i\)</span>, entonces <span class="math notranslate nohighlight">\(\|\boldsymbol{x}-\boldsymbol{x}'\|\leq\sqrt{d}\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>Entonces, dado que <span class="math notranslate nohighlight">\(\mathbb{P}\left(\bigcup_{C_{i}\cap S\neq\emptyset}C_{i}\right)\leq1\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{x}_{\pi_{1}(x)}\in S\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{E}_{\boldsymbol{x}, S}(\|\boldsymbol{x}-\boldsymbol{x}'\|)&amp;=\mathbb{E}_{S}(\mathbb{E}_{\boldsymbol{x}}(\|\boldsymbol{x}-\boldsymbol{x}'\|))\\[3mm]
&amp;=\mathbb{E}_{S}\left(\mathbb{E}_{\underset{\boldsymbol{x}\in C_{i}}{i:C_{i}\cap S=\emptyset}}(\|\boldsymbol{x}-\boldsymbol{x}_{\pi_{1}(x)}\|)+\mathbb{E}_{\underset{\boldsymbol{x}\in C_{i}}{i:C_{i}\cap S\neq\emptyset}}(\|\boldsymbol{x}-\boldsymbol{x}_{\pi_{1}(x)}\|)\right)\\
&amp;\leq\mathbb{E}_{S}\left(P\left(\bigcup_{C_{i}\cap S=\emptyset}C_{i}\right)\sqrt{d}+P\left(\bigcup_{C_{i}\cap S\neq\emptyset}C_{i}\right)\sqrt{d}\varepsilon\right)\\
&amp;\leq\mathbb{E}_{S}\left(\frac{r}{me}\sqrt{d}+1\sqrt{d}\varepsilon\right)\\[3mm]
&amp;=\sqrt{d}\left(\frac{r}{me}+\varepsilon\right)
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Dado que <span class="math notranslate nohighlight">\(r=T^{d}=(1/\varepsilon)^{d}=1/\varepsilon^{d}\)</span>, entonces:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{\boldsymbol{x}, S}(\|\boldsymbol{x}-\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})}\|)\leq\sqrt{d}\left(\frac{1/\varepsilon^{d}}{me}+\varepsilon\right)=\sqrt{d}\left(\frac{\varepsilon^{-d}}{me}+\varepsilon\right)\leq\sqrt{d}\left(\frac{2^{d}\varepsilon^{-d}}{me}+\varepsilon\right).
\]</div>
<ul class="simple">
<li><p>Usando el <a class="reference internal" href="#knn_lemma1">Lemma 2.1</a> se tiene que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\underset{S\sim\mathcal{D}^{m}}{\mathbb{E}}[L_{\mathcal{D}}(h_{S})]&amp;\leq 2 L_{\mathcal{D}}(h^{\star})+c\underset{S\sim\mathcal{D}^{m}, \boldsymbol{x}\sim\mathcal{D}}{\mathbb{E}}[\|\boldsymbol{x}-\boldsymbol{x}_{\pi_{1}(\boldsymbol{x})}\|]\\
&amp;\leq 2 L_{\mathcal{D}}(h^{\star})+c\sqrt{d}\left(\frac{2^{d}\varepsilon^{-d}}{me}+\varepsilon\right)
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Finalmente, fijando <span class="math notranslate nohighlight">\(\varepsilon=2m^{-1/(d+1)}\)</span> se tiene que:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{2^{d}\varepsilon^{-d}}{me}+\varepsilon&amp;=\frac{2^{d}2^{-d}m^{d/(d+1)}}{me}+2m^{-1/(d+1)}\\
&amp;=m^{[d/(d+1)]-1}(1/e)+2m^{-1/(d+1)}\\
&amp;=m^{-1/(d+1)}(1/e+2)\\
&amp;\leq4m^{-1/(d+1)}
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Entonces</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\begin{align*}
\underset{S\sim\mathcal{D}^{m}}{\mathbb{E}}[L_{\mathcal{D}}(h_{S})]&amp;\leq 2 L_{\mathcal{D}}(h^{\star})+4c\sqrt{d}m^{-1/(d+1)}.
\end{align*}
\]</div>
<div class="proof observation admonition" id="observation-9">
<p class="admonition-title"><span class="caption-number">Observation 2.2 </span></p>
<section class="observation-content" id="proof-content">
<p>El Teorema implica que si <em><strong>primero fijamos la distribución generadora de datos y luego hacemos tender</strong></em> <span class="math notranslate nohighlight">\(m\)</span> <em><strong>a infinito</strong></em>, entonces <em><strong>el error de la regla</strong></em> <span class="math notranslate nohighlight">\(1\)</span>-NN <em><strong>converge al doble del error de Bayes</strong></em>. El análisis puede generalizarse a valores mayores que <span class="math notranslate nohighlight">\(k\)</span>, demostrando que el error esperado de la regla <span class="math notranslate nohighlight">\(k\)</span>-NN converge a <span class="math notranslate nohighlight">\((1+\sqrt{8/k})\)</span> veces el error del <em><strong>clasificador de Bayes</strong></em>.</p>
</section>
</div></section>
<section id="ejercicios">
<h2><span class="section-number">2.2. </span>Ejercicios<a class="headerlink" href="#ejercicios" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>En este ejercicio, demostrará el siguiente teorema para la regla <span class="math notranslate nohighlight">\(k\)</span>-NN.</p></li>
</ul>
<div class="proof theorem admonition" id="first_theorem">
<p class="admonition-title"><span class="caption-number">Theorem 2.2 </span></p>
<section class="theorem-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(\mathcal{X} = [0,1]^d\)</span>, <span class="math notranslate nohighlight">\(\mathcal{Y} = \{0,1\}\)</span> y <span class="math notranslate nohighlight">\(D\)</span> una distribución sobre <span class="math notranslate nohighlight">\(\mathcal{X} \times \mathcal{Y}\)</span> tal que la función de probabilidad condicional <span class="math notranslate nohighlight">\(\eta\)</span> es una función <span class="math notranslate nohighlight">\(c\)</span>-Lipschitz. Sea <span class="math notranslate nohighlight">\(h_S\)</span> el resultado de aplicar la regla de los <span class="math notranslate nohighlight">\(k\)</span>-vecinos más cercanos (<span class="math notranslate nohighlight">\(k\)</span>-NN) a una muestra <span class="math notranslate nohighlight">\(S \sim \mathcal{D}^m\)</span>, donde <span class="math notranslate nohighlight">\(k \geq 10\)</span>. Sea <span class="math notranslate nohighlight">\(h^*\)</span> la hipótesis óptima de Bayes. Entonces,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_S\left[ L_\mathcal{D}(h_S) \right] \leq \left(1 + \sqrt{\frac{8}{k}}\right)L_D(h^*) + (6c\sqrt{d} + k)m^{-1/(d+1)}.
\]</div>
</section>
</div><ul class="simple">
<li><p><strong>Parte 1</strong>: Demuestre el siguiente Lema</p></li>
</ul>
<div class="proof lemma admonition" id="lemma_bound_exp_pci">
<p class="admonition-title"><span class="caption-number">Lemma 2.3 </span></p>
<section class="lemma-content" id="proof-content">
<ul>
<li><p>Sea <span class="math notranslate nohighlight">\(C_1, \dots, C_r\)</span> una colección de subconjuntos de un conjunto dominio <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Sea <span class="math notranslate nohighlight">\(S\)</span> una secuencia de <span class="math notranslate nohighlight">\(m\)</span> puntos muestreados de manera independiente e idénticamente distribuida (i.i.d.) según alguna distribución de probabilidad <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> sobre <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. Entonces, para todo <span class="math notranslate nohighlight">\(k \geq 2\)</span>, se cumple que:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}_{S \sim \mathcal{D}^m} \left[ \sum_{i : |C_i \cap S| &lt; k} \mathbb{P}[C_i] \right] \leq \frac{2rk}{m}.
    \]</div>
</li>
</ul>
</section>
</div><ul>
<li><p><strong>Sugerencia</strong></p>
<ul class="simple">
<li><p>Demostrar que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}\left[\sum_{i:|C_i \cap S| &lt; k} \mathbb{P}[C_i]\right] = \sum_{i=1}^r \mathbb{P}[C_i] \mathbb{P}_S[|C_i \cap S| &lt; k].
    \]</div>
<ul class="simple">
<li><p>Fijar algún <span class="math notranslate nohighlight">\(i\)</span> y suponer que <span class="math notranslate nohighlight">\(k &lt; \mathbb{P}[C_i] m/2\)</span>. Usar el <a class="reference internal" href="#chernoff_lemma">Lemma 2.5</a> para demostrar que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathbb{P}_S[|C_i \cap S| &lt; k] \leq \mathbb{P}_S\left[|C_i \cap S| &lt; \mathbb{P}[C_i] m/2\right] \leq e^{-\mathbb{P}[C_i]m/8}.
    \]</div>
<ul class="simple">
<li><p>Usar la desigualdad <span class="math notranslate nohighlight">\(\max_a a e^{-ma} \leq 1/me\)</span> para demostrar que, para tal <span class="math notranslate nohighlight">\(i\)</span>, tenemos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathbb{P}[C_i] \mathbb{P}_S[|C_i \cap S| &lt; k] \leq \mathbb{P}[C_i] e^{-\mathbb{P}[C_i]m/8} \leq \frac{8}{me}.
    \]</div>
<ul class="simple">
<li><p>Concluir la demostración utilizando el hecho de que, en el caso de <span class="math notranslate nohighlight">\(k \geq \mathbb{P}[C_i]m/2\)</span>, claramente tenemos:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \mathbb{P}[C_i] \mathbb{P}_S[|C_i \cap S| &lt; k] \leq \mathbb{P}[C_i] \leq \frac{2k}{m}.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p><strong>Parte 2</strong>: Utilizamos la notación <span class="math notranslate nohighlight">\(y \sim p\)</span> como una abreviatura para indicar que “<span class="math notranslate nohighlight">\(y\)</span> es una variable aleatoria de Bernoulli con valor esperado <span class="math notranslate nohighlight">\(p\)</span>”. Demuestre el siguiente lema:</p></li>
</ul>
<div class="proof lemma admonition" id="bernoulli_bound_lemma">
<p class="admonition-title"><span class="caption-number">Lemma 2.4 </span></p>
<section class="lemma-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(k \geq 10\)</span> y sean <span class="math notranslate nohighlight">\(Z_1, \dots, Z_k\)</span> variables aleatorias independientes de Bernoulli con <span class="math notranslate nohighlight">\(\mathbb{P}[Z_i = 1] = p_i\)</span>. Denotemos por <span class="math notranslate nohighlight">\(p = \frac{1}{k} \sum_{i} p_i\)</span> y <span class="math notranslate nohighlight">\(p' = \frac{1}{k} \sum_{i=1}^k Z_i\)</span>. Se debe demostrar que:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{Z_1, \dots, Z_k} \left[ \mathbb{P}_{y \sim p} \left[ y \neq \mathbb{1}_{[p' &gt; 1/2]} \right] \right] \leq \left( 1 + \sqrt{\frac{8}{k}} \right) \mathbb{P}_{y \sim p} \left[ y \neq \mathbb{1}_{[p &gt; 1/2]}\right].
\]</div>
</section>
</div><p><strong>Sugerencia</strong></p>
<ul class="simple">
<li><p>Sin pérdida de generalidad, supongamos que <span class="math notranslate nohighlight">\(p \leq 1/2\)</span>. Entonces, tenemos que <span class="math notranslate nohighlight">\(\mathbb{P}_{y \sim p} [y \neq \mathbb{1}_{[p &gt; 1/2]}] = p\)</span>. Definimos <span class="math notranslate nohighlight">\(y^* = \mathbb{1}_{[p' &gt; 1/2]}\)</span>.</p></li>
<li><p>A continuación, demuestre que:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{Z_1, \dots, Z_k} \mathbb{P}_{y \sim p} [y \neq y^*] - p = \mathbb{P}_{Z_1, \dots, Z_k} [p' &gt; 1/2] (1 - 2p).
\]</div>
<ul>
<li><p>Utilizando el <a class="reference internal" href="#chernoff_lemma">Lemma 2.5</a>, podemos demostrar que:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{P}[p' &gt; 1/2] \leq e^{-k p h\left(\frac{1}{2p} - 1\right)},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(h(a) = (1 + a) \log(1 + a) - a\)</span>.</p>
</li>
<li><p>Para concluir la demostración del lema, podemos apoyarnos en la siguiente desigualdad (sin necesidad de demostrarla): Para todo <span class="math notranslate nohighlight">\(p \in [0, 1/2]\)</span> y <span class="math notranslate nohighlight">\(k \geq 10\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
(1 - 2p) e^{-k p + \frac{k}{2}(\log(2p) + 1)} \leq \sqrt{\frac{8}{k}}p.
\]</div>
<ul class="simple">
<li><p><strong>Parte 3</strong></p></li>
</ul>
<ul class="simple">
<li><p>Sea <span class="math notranslate nohighlight">\(p, p' \in [0, 1]\)</span> y <span class="math notranslate nohighlight">\(y' \in \{0, 1\}\)</span>. Demuestre que:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{P}_{y \sim p}[y \neq y'] \leq \mathbb{P}_{y \sim p'}[y \neq y'] + |p - p'|.
\]</div>
<ul class="simple">
<li><p><strong>Parte 4</strong></p></li>
</ul>
<ul>
<li><p>Concluya la demostración del teorema de acuerdo con los siguientes pasos:</p></li>
<li><p>Tal como en la demostración del <a class="reference internal" href="#knn_theorem1">Theorem 2.1</a>, sea <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> y sea <span class="math notranslate nohighlight">\(\{C_1, \dots, C_r\}\)</span> la cubierta del conjunto <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> utilizando cajas de longitud <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Para cada par de puntos <span class="math notranslate nohighlight">\(x, x' \in C_i\)</span>, tenemos que <span class="math notranslate nohighlight">\(\|x - x'\| \leq \sqrt{d} \varepsilon\)</span>. En caso contrario, se cumple que <span class="math notranslate nohighlight">\(\|x - x'\| \leq 2\sqrt{d}\)</span>.</p></li>
<li><p>Demostrar que</p>
<div class="math notranslate nohighlight" id="equation-expect-ld-hs-eq">
<span class="eqno">(2.5)<a class="headerlink" href="#equation-expect-ld-hs-eq" title="Link to this equation">#</a></span>\[\begin{split}
  \begin{align*}
  \mathbb{E}_S[L_{\mathcal{D}}(h_S)] &amp;\leq \mathbb{E}_S\left[\sum_{i : |C_i \cap S| &lt; k} \mathbb{P}[C_i]\right] \\
  &amp;+ \max_i \mathbb{P}_S, (x, y)\left( h_S(x) \neq y \mid \forall j \in [k], \|x - x_{\pi_j(x)}\| \leq \varepsilon \sqrt{d} \right).
  \end{align*}
  \end{split}\]</div>
</li>
</ul>
<ul class="simple">
<li><p>Acotemos el primer sumando usando el <a class="reference internal" href="#lemma_bound_exp_pci">Lemma 2.3</a>.</p></li>
<li><p>Para acotar el segundo sumando, fijemos <span class="math notranslate nohighlight">\(S|x\)</span> y <span class="math notranslate nohighlight">\(x\)</span> de manera que todos los <span class="math notranslate nohighlight">\(k\)</span> vecinos de <span class="math notranslate nohighlight">\(x\)</span> en <span class="math notranslate nohighlight">\(S|x\)</span> estén a una distancia de a lo sumo <span class="math notranslate nohighlight">\(\varepsilon\sqrt{d}\)</span> de <span class="math notranslate nohighlight">\(x\)</span>. Sin pérdida de generalidad, asumamos que los <span class="math notranslate nohighlight">\(k\)</span> vecinos más cercanos son <span class="math notranslate nohighlight">\(x_1, \dots, x_k\)</span>. Denotemos <span class="math notranslate nohighlight">\(p_i = \eta(x_i)\)</span> y definamos <span class="math notranslate nohighlight">\(p = \frac{1}{k} \sum_{i} p_i\)</span>. Utilizando el Ejercicio <a class="reference internal" href="#equation-expect-ld-hs-eq">(2.5)</a>, mostramos que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{y_1, \dots, y_j} \mathbb{P}_{y \sim \eta(x)} [h_S(x) \neq y] \leq \mathbb{E}_{y_1, \dots, y_j} \mathbb{P}_{y \sim p} [h_S(x) \neq y] + |p - \eta(x)|.
\]</div>
<ul class="simple">
<li><p>Sin pérdida de generalidad, asumamos que <span class="math notranslate nohighlight">\(p \leq 1/2\)</span>. Ahora, utilizamos el <a class="reference internal" href="#bernoulli_bound_lemma">Lemma 2.4</a> para demostrar que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{P}_{y_1, \dots, y_j} \mathbb{P}_{y \sim p} [h_S(x) \neq y] \leq \left( 1 + \sqrt{\frac{8}{k}} \right) \mathbb{P}_{y \sim p} [\mathbb{1}_{[p &gt; 1/2]} \neq y].
\]</div>
<ul class="simple">
<li><p>Además, se puede demostrar que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{P}_{y \sim p} [\mathbb{1}_{[p &gt; 1/2]} \neq y] = p = \min\{p, 1 - p\} \leq \min\{\eta(x), 1 - \eta(x)\} + |p - \eta(x)|.
\]</div>
<ul class="simple">
<li><p>Combinando todos los resultados anteriores, obtenemos que el segundo sumando en la Ecuación <a class="reference internal" href="#equation-expect-ld-hs-eq">(2.5)</a> está acotado por</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\left( 1 + \sqrt{\frac{8}{k}}\right) L_{\mathcal{D}}(h^\star) + 3c\varepsilon\sqrt{d}.
\]</div>
<ul class="simple">
<li><p>Usando <span class="math notranslate nohighlight">\(r = (2/\epsilon)^{d}\)</span>, obtenemos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_S [L_{\mathcal{D}}(h_S)] \leq \left( 1 + \sqrt{\frac{8}{k}} \right) L_{\mathcal{D}}(h^\star) + 3c\varepsilon\sqrt{d} + \frac{2 (2/\epsilon)^d k}{m}.
\]</div>
<ul>
<li><p>Finalmente, fijando <span class="math notranslate nohighlight">\(\epsilon = 2m^{-1/(d+1)}\)</span> y utilizando</p>
<div class="math notranslate nohighlight">
\[
    6c m^{-1/(d+1)} \sqrt{d} + \frac{2k}{e} m^{-1/(d+1)} \leq \left( 6c \sqrt{d} + k \right) m^{-1/(d+1)},
    \]</div>
<p>se concluye la demostración.</p>
</li>
</ul>
<div class="proof lemma admonition" id="chernoff_lemma">
<p class="admonition-title"><span class="caption-number">Lemma 2.5 </span> (Límite de Chernoff)</p>
<section class="lemma-content" id="proof-content">
<p>Sea <span class="math notranslate nohighlight">\(Z_1, \dots, Z_m\)</span> un conjunto de variables de Bernoulli independientes, donde para cada <span class="math notranslate nohighlight">\(i\)</span> se cumple que</p>
<div class="math notranslate nohighlight">
\[
P(Z_i = 1) = p_i \quad \text{y} \quad P(Z_i = 0) = 1 - p_i.
\]</div>
<p>Definimos <span class="math notranslate nohighlight">\(p = \sum_{i=1}^{m} p_i\)</span> y <span class="math notranslate nohighlight">\(Z = \sum_{i=1}^{m} Z_i\)</span>. Entonces, para cualquier <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span>, se tiene que</p>
<div class="math notranslate nohighlight">
\[
P(Z &gt; (1+\delta)p) \leq e^{-h(\delta) p},
\]</div>
<p>donde</p>
<div class="math notranslate nohighlight">
\[
h(\delta) = (1+\delta) \log(1+\delta) - \delta.
\]</div>
</section>
</div></section>
<section id="implementacion">
<h2><span class="section-number">2.3. </span>Implementación<a class="headerlink" href="#implementacion" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Clasificación</span> <span class="pre">k-vecinos</span></code></strong>. Como ya se estudió anteriormente, en su versión más sencilla, el algoritmo <em><strong>k-NN</strong></em> solo considera exactamente un vecino más cercano, que es el <em><strong>dato de entrenamiento más cercano al punto para el que queremos hacer una predicción</strong></em>. La predicción es entonces simplemente la etiqueta conocida para este punto de entrenamiento.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_knn_classification</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d564a6b7753f7f36c8e6c67a37b2f4bc5cae4a4cbaa8c6a7b924346d1d38142e.png" src="_images/d564a6b7753f7f36c8e6c67a37b2f4bc5cae4a4cbaa8c6a7b924346d1d38142e.png" />
</div>
</div>
<ul class="simple">
<li><p>Se han añadido <em><strong>tres nuevos puntos de datos, mostrados como estrellas</strong></em>. Para cada uno de ellos, marcamos el punto más cercano en el conjunto de entrenamiento. La predicción del algoritmo del <em><strong>vecino más cercano es la etiqueta de ese punto (mostrada por el color de la cruz)</strong></em>.</p></li>
<li><p>En lugar de considerar sólo al vecino más cercano, también <em><strong>podemos considerar un número arbitrario</strong></em> <span class="math notranslate nohighlight">\(k\)</span>, de vecinos. De ahí viene el nombre del algoritmo <span class="math notranslate nohighlight">\(k\)</span>-vecinos más cercanos. Cuando se considera <em><strong>más de un vecino, se utiliza la votación para asignar una etiqueta</strong></em>. Esto significa que, para cada punto de prueba, contamos <em><strong>cuántos vecinos pertenecen a clase 0 y cuántos vecinos pertenecen a la clase 1</strong></em>. A continuación, <em><strong>asignamos la clase que es más frecuente</strong></em>: es decir, la clase mayoritaria entre los <span class="math notranslate nohighlight">\(k\)</span> <em><strong>vecinos más cercanos</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_knn_classification</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/75e1fb3468a9dec8c2d121d4bd3be69ce732659d3c711e88b74fdd0fc39a1cc2.png" src="_images/75e1fb3468a9dec8c2d121d4bd3be69ce732659d3c711e88b74fdd0fc39a1cc2.png" />
</div>
</div>
<ul class="simple">
<li><p>Aunque esta ilustración se refiere a un <em><strong>problema de clasificación binaria</strong></em>, este método <em><strong>puede aplicarse a conjuntos de datos con cualquier número de clases</strong></em>. Para más clases, <em><strong>contamos cuántos vecinos pertenecen a cada clase y volvemos a predecir la clase más común</strong></em>. Ahora veamos cómo podemos aplicar el algoritmo de los <em><strong>vecinos más cercanos</strong></em> utilizando <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p></li>
<li><p>En primer lugar, dividimos nuestros datos en un conjunto de <em><strong>entrenamiento</strong></em> y otro de <em><strong>prueba</strong></em> para poder evaluar el rendimiento de la <em><strong>generalización</strong></em>. <code class="docutils literal notranslate"><span class="pre">random_state=0</span></code> nos asegura que obtendremos los <em><strong>mismos conjuntos de training y test para diferentes ejecuciones</strong></em>. Para mas información sobre <code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.train_test_split</span></code> (ver documentación <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">train_test_split</a>). Por defecto, cuando no es suministrado el porcentaje de entrenamiento <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> considera este porcentaje para el test como el 25%, esto es <code class="docutils literal notranslate"><span class="pre">test_size=0.25</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_forge</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>A continuación, <em><strong>importamos e instanciamos</strong></em> la clase <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> de <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> encargada de la tarea de clasificación. Aquí es cuando podemos <em><strong>establecer parámetros, como el número de vecinos a utilizar</strong></em>, el cual consideramos como 3 para este ejemplo <code class="docutils literal notranslate"><span class="pre">n_neighbors=3</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Ahora, <em><strong>ajustamos el clasificador utilizando el conjunto de entrenamiento</strong></em>. Para la clase <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> esto significa <em><strong>almacenar el conjunto de datos, para poder calcular los vecinos durante la predicción</strong></em>, teniendo en cuenta la asignación de la clase más frecuente, explicada anteriormente</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;KNeighborsClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">?<span>Documentation for KNeighborsClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div> </div></div></div></div></div></div>
</div>
<ul class="simple">
<li><p>Para hacer <em><strong>predicciones sobre los datos de prueba</strong></em>, llamamos al método de predicción <code class="docutils literal notranslate"><span class="pre">predict()</span></code>. Para cada punto de datos en el conjunto de prueba, éste <em><strong>calcula sus vecinos más cercanos en el conjunto de entrenamiento y encuentra la clase más común entre ellos</strong></em>. Para mas información sobre el cálculo del <code class="docutils literal notranslate"><span class="pre">score</span></code> (ver documentación <a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score">accuracy score</a>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[11.54155807,  5.21116083],
       [10.06393839,  0.99078055],
       [ 9.49123469,  4.33224792],
       [ 8.18378052,  1.29564214],
       [ 8.30988863,  4.80623966],
       [10.24028948,  2.45544401],
       [ 8.34468785,  1.63824349]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set predictions: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test set predictions: [1 0 1 0 1 0 0]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test set: [1 0 1 0 1 1 0]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Para evaluar <em><strong>lo bien que generaliza</strong></em> nuestro modelo, podemos llamar al método <code class="docutils literal notranslate"><span class="pre">score()</span></code> con los datos de prueba junto con las etiquetas de prueba <code class="docutils literal notranslate"><span class="pre">X_text,</span> <span class="pre">y_test</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set accuracy: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test set accuracy: 0.86
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Vemos que nuestro modelo tiene una <em><strong>precisión</strong></em> del 86%, lo que significa que <em><strong>el modelo predijo la clase correctamente para el 86% de las muestras del conjunto de datos de prueba</strong></em>.</p></li>
</ul>
</section>
<section id="analisis-de-kneighborsclassifier">
<h2><span class="section-number">2.4. </span>Análisis de <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code><a class="headerlink" href="#analisis-de-kneighborsclassifier" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Para los conjuntos de datos bidimensionales, también podemos ilustrar la <em><strong>predicción para todos los posibles puntos de prueba en el plano</strong></em> <span class="math notranslate nohighlight">\(xy\)</span>. Coloreamos el plano según la clase que se asignaría a un punto en esta región. Esto nos permite ver el <em><strong>límite de decisión (decision boundary)</strong></em> (ver <a class="reference external" href="https://github.com/amueller/mglearn/blob/master/mglearn/plot_2d_separator.py">plot_2d_separator</a>), el cual está <em><strong>entre el lugar donde el algoritmo asigna la clase 0 y el lugar donde asigna la clase 1</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">n_neighbors</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.4</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> neighbor(s)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;feature 0&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;feature 1&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dd0454462238fb3d76b0a191d4eb76da21368a1f3f47083f51ca0210fe84d3d8.png" src="_images/dd0454462238fb3d76b0a191d4eb76da21368a1f3f47083f51ca0210fe84d3d8.png" />
</div>
</div>
<ul class="simple">
<li><p>Como se puede ver en la figura de la izquierda, <em><strong>el uso de un solo vecino da como resultado una decisión que sigue de cerca los datos de entrenamiento</strong></em>. La consideración de <em><strong>más vecinos conduce a un límite de decisión más suave</strong></em>. Un límite más suave corresponde a un <em><strong>modelo más sencillo</strong></em>. En otras palabras, <em><strong>el uso de pocos vecinos corresponde a una alta complejidad del modelo</strong></em> (como se muestra en el lado derecho), y <em><strong>el uso de muchos vecinos corresponde a una baja complejidad del modelo</strong></em>.</p></li>
<li><p>Si se considera el caso extremo en el que el <em><strong>número de vecinos es el número de todos los puntos de datos del conjunto de entrenamiento</strong></em>, cada punto de prueba tendría exactamente los mismos vecinos (todos puntos de entrenamiento) y todas las predicciones serían las mismas: <em><strong>la clase más frecuente en el conjunto de entrenamiento</strong></em>.</p></li>
</ul>
</section>
<section id="aplicacion-breast-cancer-dataset">
<h2><span class="section-number">2.5. </span>Aplicación: Breast Cancer Dataset<a class="headerlink" href="#aplicacion-breast-cancer-dataset" title="Link to this heading">#</a></h2>
<figure class="align-center" id="breast-cancer-knn-fig">
<a class="reference internal image-reference" href="_images/breast_cancer_knn.png"><img alt="_images/breast_cancer_knn.png" src="_images/breast_cancer_knn.png" style="width: 616.5px; height: 511.2px;" /></a>
</figure>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Breast</span> <span class="pre">Cancer</span> <span class="pre">Dataset</span></code></strong>. Investiguemos si podemos confirmar la <em><strong>conexión entre la complejidad del modelo y la generalización</strong></em> que hemos discutido antes. Lo haremos con el conjunto de datos de cáncer de mama <em><strong>(Breast Cancer)</strong></em> del mundo real.</p></li>
</ul>
</section>
<section id="analisis-exploratorio-de-datos">
<h2><span class="section-number">2.6. </span>Análisis Exploratorio de Datos<a class="headerlink" href="#analisis-exploratorio-de-datos" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Procedemos a realizar un <em><strong>análisis exploratorio de datos</strong></em> para investigar y resumir principales características de nuestros datos. Primero cargamos nuestro dataset usando la función <code class="docutils literal notranslate"><span class="pre">load_breast_cancer()</span></code> de <code class="docutils literal notranslate"><span class="pre">sklearn.datasets</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">target_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;malignant&#39;, &#39;benign&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;mean radius&#39;,
 &#39;mean texture&#39;,
 &#39;mean perimeter&#39;,
 &#39;mean area&#39;,
 &#39;mean smoothness&#39;,
 &#39;mean compactness&#39;,
 &#39;mean concavity&#39;,
 &#39;mean concave points&#39;,
 &#39;mean symmetry&#39;,
 &#39;mean fractal dimension&#39;,
 &#39;radius error&#39;,
 &#39;texture error&#39;,
 &#39;perimeter error&#39;,
 &#39;area error&#39;,
 &#39;smoothness error&#39;,
 &#39;compactness error&#39;,
 &#39;concavity error&#39;,
 &#39;concave points error&#39;,
 &#39;symmetry error&#39;,
 &#39;fractal dimension error&#39;,
 &#39;worst radius&#39;,
 &#39;worst texture&#39;,
 &#39;worst perimeter&#39;,
 &#39;worst area&#39;,
 &#39;worst smoothness&#39;,
 &#39;worst compactness&#39;,
 &#39;worst concavity&#39;,
 &#39;worst concave points&#39;,
 &#39;worst symmetry&#39;,
 &#39;worst fractal dimension&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(569, 30)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(569,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;diagnosis&#39;</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>diagnosis</th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>...</th>
      <th>worst radius</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>...</td>
      <td>25.38</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>...</td>
      <td>24.99</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>...</td>
      <td>23.57</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.2414</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>...</td>
      <td>14.91</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.2098</td>
      <td>0.8663</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.1980</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>...</td>
      <td>22.54</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.1374</td>
      <td>0.2050</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_copy</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Aquí <code class="docutils literal notranslate"><span class="pre">0=malignant,</span> <span class="pre">1=benign</span></code>. Verifiquemos que tipos de datos contiene el dataset. La función <code class="docutils literal notranslate"><span class="pre">info()</span></code> proporciona información sobre los <em><strong>tipos de datos, columnas, recuento de valores nulos, uso de memoria</strong></em>, etc.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 569 entries, 0 to 568
Data columns (total 31 columns):
 #   Column                   Non-Null Count  Dtype  
---  ------                   --------------  -----  
 0   diagnosis                569 non-null    int64  
 1   mean radius              569 non-null    float64
 2   mean texture             569 non-null    float64
 3   mean perimeter           569 non-null    float64
 4   mean area                569 non-null    float64
 5   mean smoothness          569 non-null    float64
 6   mean compactness         569 non-null    float64
 7   mean concavity           569 non-null    float64
 8   mean concave points      569 non-null    float64
 9   mean symmetry            569 non-null    float64
 10  mean fractal dimension   569 non-null    float64
 11  radius error             569 non-null    float64
 12  texture error            569 non-null    float64
 13  perimeter error          569 non-null    float64
 14  area error               569 non-null    float64
 15  smoothness error         569 non-null    float64
 16  compactness error        569 non-null    float64
 17  concavity error          569 non-null    float64
 18  concave points error     569 non-null    float64
 19  symmetry error           569 non-null    float64
 20  fractal dimension error  569 non-null    float64
 21  worst radius             569 non-null    float64
 22  worst texture            569 non-null    float64
 23  worst perimeter          569 non-null    float64
 24  worst area               569 non-null    float64
 25  worst smoothness         569 non-null    float64
 26  worst compactness        569 non-null    float64
 27  worst concavity          569 non-null    float64
 28  worst concave points     569 non-null    float64
 29  worst symmetry           569 non-null    float64
 30  worst fractal dimension  569 non-null    float64
dtypes: float64(30), int64(1)
memory usage: 137.9 KB
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>El método <code class="docutils literal notranslate"><span class="pre">DataFrame.describe()</span></code> genera <em><strong>estadísticos descriptivos</strong></em> que resumen <em><strong>tendencia central, dispersión</strong></em> y la <em><strong>forma de la distribución de un conjunto de datos</strong></em>, excluyendo los valores <code class="docutils literal notranslate"><span class="pre">NaN</span></code>. Una cosa importante es que el método <em><strong>describe() sólo trabaja con valores numéricos</strong></em>. Por lo tanto, si hay algún valor categórico en una columna, el método describe() lo ignorará y mostrará el resumen de las demás columnas, a menos que se pase el <code class="docutils literal notranslate"><span class="pre">parámetro</span> <span class="pre">include=&quot;all&quot;</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>diagnosis</th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>...</th>
      <th>worst radius</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>...</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
      <td>569.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.627417</td>
      <td>14.127292</td>
      <td>19.289649</td>
      <td>91.969033</td>
      <td>654.889104</td>
      <td>0.096360</td>
      <td>0.104341</td>
      <td>0.088799</td>
      <td>0.048919</td>
      <td>0.181162</td>
      <td>...</td>
      <td>16.269190</td>
      <td>25.677223</td>
      <td>107.261213</td>
      <td>880.583128</td>
      <td>0.132369</td>
      <td>0.254265</td>
      <td>0.272188</td>
      <td>0.114606</td>
      <td>0.290076</td>
      <td>0.083946</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.483918</td>
      <td>3.524049</td>
      <td>4.301036</td>
      <td>24.298981</td>
      <td>351.914129</td>
      <td>0.014064</td>
      <td>0.052813</td>
      <td>0.079720</td>
      <td>0.038803</td>
      <td>0.027414</td>
      <td>...</td>
      <td>4.833242</td>
      <td>6.146258</td>
      <td>33.602542</td>
      <td>569.356993</td>
      <td>0.022832</td>
      <td>0.157336</td>
      <td>0.208624</td>
      <td>0.065732</td>
      <td>0.061867</td>
      <td>0.018061</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>6.981000</td>
      <td>9.710000</td>
      <td>43.790000</td>
      <td>143.500000</td>
      <td>0.052630</td>
      <td>0.019380</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.106000</td>
      <td>...</td>
      <td>7.930000</td>
      <td>12.020000</td>
      <td>50.410000</td>
      <td>185.200000</td>
      <td>0.071170</td>
      <td>0.027290</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.156500</td>
      <td>0.055040</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>11.700000</td>
      <td>16.170000</td>
      <td>75.170000</td>
      <td>420.300000</td>
      <td>0.086370</td>
      <td>0.064920</td>
      <td>0.029560</td>
      <td>0.020310</td>
      <td>0.161900</td>
      <td>...</td>
      <td>13.010000</td>
      <td>21.080000</td>
      <td>84.110000</td>
      <td>515.300000</td>
      <td>0.116600</td>
      <td>0.147200</td>
      <td>0.114500</td>
      <td>0.064930</td>
      <td>0.250400</td>
      <td>0.071460</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.000000</td>
      <td>13.370000</td>
      <td>18.840000</td>
      <td>86.240000</td>
      <td>551.100000</td>
      <td>0.095870</td>
      <td>0.092630</td>
      <td>0.061540</td>
      <td>0.033500</td>
      <td>0.179200</td>
      <td>...</td>
      <td>14.970000</td>
      <td>25.410000</td>
      <td>97.660000</td>
      <td>686.500000</td>
      <td>0.131300</td>
      <td>0.211900</td>
      <td>0.226700</td>
      <td>0.099930</td>
      <td>0.282200</td>
      <td>0.080040</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>15.780000</td>
      <td>21.800000</td>
      <td>104.100000</td>
      <td>782.700000</td>
      <td>0.105300</td>
      <td>0.130400</td>
      <td>0.130700</td>
      <td>0.074000</td>
      <td>0.195700</td>
      <td>...</td>
      <td>18.790000</td>
      <td>29.720000</td>
      <td>125.400000</td>
      <td>1084.000000</td>
      <td>0.146000</td>
      <td>0.339100</td>
      <td>0.382900</td>
      <td>0.161400</td>
      <td>0.317900</td>
      <td>0.092080</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>28.110000</td>
      <td>39.280000</td>
      <td>188.500000</td>
      <td>2501.000000</td>
      <td>0.163400</td>
      <td>0.345400</td>
      <td>0.426800</td>
      <td>0.201200</td>
      <td>0.304000</td>
      <td>...</td>
      <td>36.040000</td>
      <td>49.540000</td>
      <td>251.200000</td>
      <td>4254.000000</td>
      <td>0.222600</td>
      <td>1.058000</td>
      <td>1.252000</td>
      <td>0.291000</td>
      <td>0.663800</td>
      <td>0.207500</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 31 columns</p>
</div></div></div>
</div>
<ul class="simple">
<li><p>Realicemos <em><strong>tabla de frecuencias y diagrama de barras</strong></em> para nuestra variable respuesta</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">diagnosis</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>diagnosis
1    357
0    212
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Count of cancer type&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">diagnosis</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Diagnosis&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d1b6feaacdb74155d33460d5fe09409e3cb7251074cb8f36ac082a076466f4a9.png" src="_images/d1b6feaacdb74155d33460d5fe09409e3cb7251074cb8f36ac082a076466f4a9.png" />
</div>
</div>
<ul class="simple">
<li><p>Nótese que nuestro dataset está <em><strong>desbalanceado</strong></em>. Existen técnicas como <em><strong>SMOTE</strong></em> y <em><strong>Stratified sampling</strong></em>, que pueden utilizarse para <em><strong>mejorar el score de clasificación</strong></em> de datos desbalanceados. <em><strong>Utilícela en su proyecto final de investigación</strong></em>.  Verifiquemos además si existen <em><strong>datos faltantes</strong></em> en nuestro <code class="docutils literal notranslate"><span class="pre">Dataframe</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>diagnosis                  0
mean radius                0
mean texture               0
mean perimeter             0
mean area                  0
mean smoothness            0
mean compactness           0
mean concavity             0
mean concave points        0
mean symmetry              0
mean fractal dimension     0
radius error               0
texture error              0
perimeter error            0
area error                 0
smoothness error           0
compactness error          0
concavity error            0
concave points error       0
symmetry error             0
fractal dimension error    0
worst radius               0
worst texture              0
worst perimeter            0
worst area                 0
worst smoothness           0
worst compactness          0
worst concavity            0
worst concave points       0
worst symmetry             0
worst fractal dimension    0
dtype: int64
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Seleccionamos aquellas variables que están correlacionadas con la variable respuesta usando: <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pointbiserialr.html">The Point Biserial Coefficient of Correlation</a>. Para que este método sea efectivo, requiere que las variables explicativas se distribuyan normalmente. Usamos una transformación en el caso que sea necesario</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">pointbiserialr</span><span class="p">,</span> <span class="n">kstest</span><span class="p">,</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler_params</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">significant_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;diagnosis&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
    <span class="n">D</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">kstest</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()))</span>
    
    <span class="k">if</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> 
        <span class="n">scaler_params</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">mean_</span><span class="p">,</span> <span class="n">scaler</span><span class="o">.</span><span class="n">scale_</span><span class="p">)</span>  <span class="c1"># Guardar la media y la desviación estándar</span>
        <span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="n">corr</span><span class="p">,</span> <span class="n">p_value</span> <span class="o">=</span> <span class="n">pointbiserialr</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;diagnosis&#39;</span><span class="p">])</span>
    
    <span class="k">if</span> <span class="n">p_value</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">:</span>
        <span class="n">significant_columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">column</span><span class="p">)</span>

<span class="n">df_significant</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">significant_columns</span><span class="p">]</span>

<span class="k">for</span> <span class="n">column</span><span class="p">,</span> <span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span> <span class="ow">in</span> <span class="n">scaler_params</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">df_significant</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>  <span class="c1"># Solo revertir las columnas que están en df_significant</span>
        <span class="n">df_significant</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_significant</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">+</span> <span class="n">mean</span>  <span class="c1"># Revertir la normalización</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">DataFrame con las columnas significativas y revertidas a su escala original:&quot;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df_significant</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>DataFrame con las columnas significativas y revertidas a su escala original:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>diagnosis</th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>...</th>
      <th>worst radius</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.80</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>...</td>
      <td>25.38</td>
      <td>17.33</td>
      <td>184.60</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.90</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>...</td>
      <td>24.99</td>
      <td>23.41</td>
      <td>158.80</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.00</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>...</td>
      <td>23.57</td>
      <td>25.53</td>
      <td>152.50</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>11.42</td>
      <td>20.38</td>
      <td>77.58</td>
      <td>386.1</td>
      <td>0.14250</td>
      <td>0.28390</td>
      <td>0.2414</td>
      <td>0.10520</td>
      <td>0.2597</td>
      <td>...</td>
      <td>14.91</td>
      <td>26.50</td>
      <td>98.87</td>
      <td>567.7</td>
      <td>0.2098</td>
      <td>0.8663</td>
      <td>0.6869</td>
      <td>0.2575</td>
      <td>0.6638</td>
      <td>0.17300</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>20.29</td>
      <td>14.34</td>
      <td>135.10</td>
      <td>1297.0</td>
      <td>0.10030</td>
      <td>0.13280</td>
      <td>0.1980</td>
      <td>0.10430</td>
      <td>0.1809</td>
      <td>...</td>
      <td>22.54</td>
      <td>16.67</td>
      <td>152.20</td>
      <td>1575.0</td>
      <td>0.1374</td>
      <td>0.2050</td>
      <td>0.4000</td>
      <td>0.1625</td>
      <td>0.2364</td>
      <td>0.07678</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 26 columns</p>
</div></div></div>
</div>
<ul class="simple">
<li><p>Pasamos a verficar si existe <em><strong>correlación entre las características</strong></em>. En caso positivo, removemos multicolinealidad usando el <a class="reference external" href="https://www.sciencedirect.com/topics/mathematics/variance-inflation-factor">Variance Inflation Factor</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corr</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">corr</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(25, 25)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">corr</span><span class="p">)</span>
<span class="n">mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">mask</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;.1f&#39;</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;size&#39;</span><span class="p">:</span><span class="mi">15</span><span class="p">},</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Reds&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/db32f6b9c7b96175d66d3078279bc71e4f5fe12f1e9092ab7fdbd1dcb4d88b11.png" src="_images/db32f6b9c7b96175d66d3078279bc71e4f5fe12f1e9092ab7fdbd1dcb4d88b11.png" />
</div>
</div>
<ul class="simple">
<li><p>Trazamos <em><strong>histogramas de cada característica</strong></em> en nuestro dataset</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">melted_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">id_vars</span> <span class="o">=</span> <span class="s2">&quot;diagnosis&quot;</span><span class="p">,</span><span class="n">value_vars</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mean radius&#39;</span><span class="p">,</span> <span class="s1">&#39;mean texture&#39;</span><span class="p">,</span> 
                                                              <span class="s1">&#39;mean perimeter&#39;</span><span class="p">])</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s2">&quot;variable&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;diagnosis&quot;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span> <span class="n">melted_data</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4e8f2b4ed2088dd2c26204bc7ec20e68de4bdb87a7dbe87043896e845fd0b88f.png" src="_images/4e8f2b4ed2088dd2c26204bc7ec20e68de4bdb87a7dbe87043896e845fd0b88f.png" />
</div>
</div>
<ul class="simple">
<li><p>Diagrama de <em><strong>densidad de distribución KDE</strong></em> y distribución mediante el diagrama de dispersión <code class="docutils literal notranslate"><span class="pre">stripplot()</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matplotlib</span><span class="o">.</span><span class="n">rc_file_defaults</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;diagnosis&quot;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">,</span> <span class="s2">&quot;mean radius&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">add_legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/14800506e97eacbd5dab32a5513cd4669d5da19f5b510f37722caa2fb66ad4a1.png" src="_images/14800506e97eacbd5dab32a5513cd4669d5da19f5b510f37722caa2fb66ad4a1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">stripplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;diagnosis&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;mean radius&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">jitter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1bc3fe92f03a2ed283185c220b54523c38e0352a73a48dba28e2b0758a63879b.png" src="_images/1bc3fe92f03a2ed283185c220b54523c38e0352a73a48dba28e2b0758a63879b.png" />
</div>
</div>
<div class="warning admonition">
<p class="admonition-title">¿Debo siempre eliminar variables explicativas no correlacionadas con mi variable respuesta?</p>
<ul class="simple">
<li><p>No es obligatorio eliminar variables sin correlación con la variable respuesta. <em><strong>La correlación mide solo relaciones lineales, y algunas variables pueden ser útiles en modelos no lineales</strong></em>. Además, algoritmos como <em><strong>Lasso, Random Forest o Gradient Boosting, SVM, ANNs manejan variables irrelevantes automáticamente</strong></em>.</p></li>
<li><p>Eliminar prematuramente puede descartar información valiosa. Sin embargo, un análisis de importancia de características sigue siendo recomendable para mejorar eficiencia y evitar ruido innecesario. En su lugar, <em><strong>es mejor aplicar técnicas como SHAP, Permutation Importance, RFE o transformaciones como PCA para evaluar la relevancia</strong></em> de las variables antes de eliminarlas.</p></li>
</ul>
</div>
<section id="multicolinealidad">
<h3><span class="section-number">2.6.1. </span>Multicolinealidad<a class="headerlink" href="#multicolinealidad" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>La <em><strong>multicolinealidad</strong></em> es un problema ya que, <em><strong>reduce la importancia de las variables independientes</strong></em>. A fin de solucionar este problema,  se <em><strong>eliminan los predictores altamente correlacionados</strong></em>. Podemos comprobar la presencia de multicolinealidad entre algunas de las variables. Por ejemplo, la columna <em><strong>mean radius</strong></em> tiene una <em><strong>correlación de 1.0</strong></em> con las columnas <em><strong>mean perimeter</strong></em> y <em><strong>mean área</strong></em>, respectivamente. Esto se debe a que <em><strong>las tres columnas contienen esencialmente la misma información</strong></em>, que es el tamaño físico de la observación.</p></li>
<li><p>Por lo tanto, <em><strong>sólo debemos elegir una de las tres columnas cuando pasemos al análisis posterior</strong></em>. Otro lugar donde la <em><strong>multicolinealidad</strong></em> es evidente, es entre las columnas <code class="docutils literal notranslate"><span class="pre">&quot;mean&quot;</span></code> y <code class="docutils literal notranslate"><span class="pre">&quot;worst&quot;</span></code>. Por ejemplo, la columna <code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">radius</span></code> tiene una <em><strong>correlación de 1.0</strong></em> con la columna <code class="docutils literal notranslate"><span class="pre">worst</span> <span class="pre">radius</span></code>. También hay multicolinealidad entre los atributos <code class="docutils literal notranslate"><span class="pre">compactness,</span> <span class="pre">concavity</span></code> y <code class="docutils literal notranslate"><span class="pre">concave</span> <span class="pre">points</span></code>. Así que podemos elegir solo uno de estos, por ejemplo <code class="docutils literal notranslate"><span class="pre">compactness</span></code>. De la matriz de correlación sabemos que estas columnas están altamente correlacionadas con las columnas <code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">radius</span></code>, <code class="docutils literal notranslate"><span class="pre">perimeter</span></code>, <code class="docutils literal notranslate"><span class="pre">area</span></code>.</p></li>
</ul>
</section>
<section id="factor-de-inflacion-de-la-varianza-vif">
<h3><span class="section-number">2.6.2. </span>Factor de Inflación de la Varianza (VIF)<a class="headerlink" href="#factor-de-inflacion-de-la-varianza-vif" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Para abordar la multicolinealidad, se pueden aplicar técnicas como la regularización o la selección de características para, seleccionar un subconjunto de variables independientes que no estén altamente correlacionadas entre sí. En esta sección, nos centraremos en la más común: <em><strong>VIF (Factores de Inflación de la Varianza)</strong></em>.</p></li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">VIF</span></code> determina la <em><strong>fuerza de la correlación entre las variables independientes</strong></em>. Se pronostica tomando una variable y comparándola con todas las demás. La puntuación <code class="docutils literal notranslate"><span class="pre">VIF</span></code> de una variable independiente representa hasta qué punto la variable se explica por otras variables independientes.</p></li>
<li><p>El valor <span class="math notranslate nohighlight">\(R^2\)</span> se determina para averiguar hasta qué punto una variable independiente es descrita por las demás variables independientes. Un valor alto de <span class="math notranslate nohighlight">\(R^{2}\)</span> significa que la variable está muy correlacionada con las demás variables. Esto se refleja en el <code class="docutils literal notranslate"><span class="pre">VIF</span></code>, que se indica a continuación:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{VIF}=\frac{1}{1-R^{2}}
\]</div>
<ul class="simple">
<li><p>Así, cuanto más se acerque el valor de <span class="math notranslate nohighlight">\(R^2\)</span> a 1, mayor será el valor de <code class="docutils literal notranslate"><span class="pre">VIF</span></code> y mayor la multicolinealidad con la variable independiente concreta.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p><em><strong>Un valor VIF de 1</strong></em>: Sin multicolinealidad (variable perfectamente independiente).</p></li>
<li><p><em><strong>Un valor VIF entre 1 y 5</strong></em>: Multicolinealidad baja a moderada (no se considera problemática).</p></li>
<li><p><em><strong>Un valor VIF entre 5 y 10</strong></em>: Multicolinealidad moderada a alta (considerada problemática).</p></li>
<li><p><em><strong>Un valor VIF superior a 10</strong></em>: Multicolinealidad alta (preocupación grave, requiere medidas).</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">VIF_calculation</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">VIF</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">VIF</span><span class="p">[</span><span class="s2">&quot;variable&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span>
    <span class="n">VIF</span><span class="p">[</span><span class="s2">&quot;VIF&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
    <span class="n">VIF</span> <span class="o">=</span> <span class="n">VIF</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;VIF&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">VIF</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">delete_multicollinearity</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">target_name</span><span class="p">,</span> <span class="n">VIF_threshold</span><span class="p">):</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">target_name</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">VIF_mat</span> <span class="o">=</span> <span class="n">VIF_calculation</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="n">n_VIF</span> <span class="o">=</span> <span class="n">VIF_mat</span><span class="p">[</span><span class="s2">&quot;VIF&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">n_VIF</span> <span class="o">&lt;=</span> <span class="n">VIF_threshold</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;There is no multicollinearity!&quot;</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">n_VIF</span> <span class="o">&gt;</span> <span class="n">VIF_threshold</span><span class="p">):</span>
      <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">VIF_mat</span><span class="p">[</span><span class="s2">&quot;variable&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">VIF_mat</span> <span class="o">=</span> <span class="n">VIF_calculation</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
      <span class="n">n_VIF</span> <span class="o">=</span> <span class="n">VIF_mat</span><span class="p">[</span><span class="s2">&quot;VIF&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">display</span><span class="p">(</span><span class="n">VIF_mat</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_copy</span> <span class="o">=</span> <span class="n">delete_multicollinearity</span><span class="p">(</span><span class="n">df_copy</span><span class="p">,</span> <span class="s2">&quot;diagnosis&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>variable</th>
      <th>VIF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>symmetry error</td>
      <td>8.648328</td>
    </tr>
    <tr>
      <th>1</th>
      <td>smoothness error</td>
      <td>8.347757</td>
    </tr>
    <tr>
      <th>2</th>
      <td>fractal dimension error</td>
      <td>7.644681</td>
    </tr>
    <tr>
      <th>3</th>
      <td>texture error</td>
      <td>7.103564</td>
    </tr>
    <tr>
      <th>4</th>
      <td>concavity error</td>
      <td>6.666504</td>
    </tr>
    <tr>
      <th>5</th>
      <td>worst concavity</td>
      <td>4.620458</td>
    </tr>
    <tr>
      <th>6</th>
      <td>area error</td>
      <td>2.190762</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_copy</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>texture error</th>
      <th>area error</th>
      <th>smoothness error</th>
      <th>concavity error</th>
      <th>symmetry error</th>
      <th>fractal dimension error</th>
      <th>worst concavity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.9053</td>
      <td>153.40</td>
      <td>0.006399</td>
      <td>0.05373</td>
      <td>0.03003</td>
      <td>0.006193</td>
      <td>0.7119</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.7339</td>
      <td>74.08</td>
      <td>0.005225</td>
      <td>0.01860</td>
      <td>0.01389</td>
      <td>0.003532</td>
      <td>0.2416</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.7869</td>
      <td>94.03</td>
      <td>0.006150</td>
      <td>0.03832</td>
      <td>0.02250</td>
      <td>0.004571</td>
      <td>0.4504</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.1560</td>
      <td>27.23</td>
      <td>0.009110</td>
      <td>0.05661</td>
      <td>0.05963</td>
      <td>0.009208</td>
      <td>0.6869</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.7813</td>
      <td>94.44</td>
      <td>0.011490</td>
      <td>0.05688</td>
      <td>0.01756</td>
      <td>0.005115</td>
      <td>0.4000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corr</span> <span class="o">=</span> <span class="n">df_copy</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">diverging_palette</span><span class="p">(</span><span class="mi">220</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">corr</span><span class="p">)</span>
<span class="n">mask</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">triu_indices_from</span><span class="p">(</span><span class="n">mask</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.8</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span>
            <span class="n">cbar_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;shrink&quot;</span><span class="p">:</span> <span class="mf">.5</span><span class="p">},</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/99c07c570d5e8aada0244befef924a93216e0a3750b92155e5e8b210baf9b665.png" src="_images/99c07c570d5e8aada0244befef924a93216e0a3750b92155e5e8b210baf9b665.png" />
</div>
</div>
<div class="warning admonition">
<p class="admonition-title">¿Debo siempre eliminar variables que presentan multicolinealidad?</p>
<p>La multicolinealidad <em><strong>afecta modelos lineales e interpretativos</strong></em>, pero <em><strong>no es un problema en modelos más complejos</strong></em></p>
<ol class="arabic simple">
<li><p><strong>Debe eliminarse (afecta estabilidad e interpretación)</strong>:</p>
<ul class="simple">
<li><p>Regresión Lineal y Logística → Inestabilidad en coeficientes, detectada con VIF.</p></li>
<li><p>Linear Discriminant Analysis (LDA) → Depende de matrices de covarianza, afectadas por multicolinealidad.</p></li>
<li><p>Ridge y Lasso → Ridge la mitiga, Lasso selecciona algunas variables.</p></li>
</ul>
</li>
<li><p><strong>No es un problema grave (modelos robustos)</strong>:</p>
<ul class="simple">
<li><p>Árboles de Decisión (Random Forest, XGBoost) → Seleccionan variables jerárquicamente.</p></li>
<li><p>SVM (kernels no lineales) → Puede aumentar costo computacional, pero no afecta resultados.</p></li>
<li><p>Redes Neuronales → No dependen de relaciones lineales, pero pueden sobreajustar con muchas variables redundantes.</p></li>
<li><p>KNN → No usa coeficientes, multicolinealidad irrelevante.</p></li>
</ul>
</li>
</ol>
</div>
</section>
</section>
<section id="seleccion-del-modelo-k-nn">
<h2><span class="section-number">2.7. </span>Selección del modelo K-NN<a class="headerlink" href="#seleccion-del-modelo-k-nn" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><em><strong>División en entrenamiento (train) y prueba (test)</strong></em>. Dividimos la variable objetivo y las variables independientes. A continuación, evaluamos el <em><strong>rendimiento del conjunto de entrenamiento y de prueba con diferentes números de vecinos</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Utilizaremos en el presente problema las columnas obtenidas luego de aplicar el <em><strong>Factor de Inflación de la Varianza (VIF)</strong></em>. Pude verificar que sucede cuando utiliza el dataset inicial, en el cual se retiraron de manera manual varias características, con base en la matriz de correlación</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df_copy</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;diagnosis&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">66</span><span class="p">)</span>

<span class="n">training_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">neighbors_settings</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n_neighbors</span> <span class="ow">in</span> <span class="n">neighbors_settings</span><span class="p">:</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">training_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="n">test_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matplotlib</span><span class="o">.</span><span class="n">rc_file_defaults</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors_settings</span><span class="p">,</span> <span class="n">training_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">neighbors_settings</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Test accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;n_neighbors&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/47aa373551a23172376e9454d7afa42926be96fb464350e8251a558eda238129.png" src="_images/47aa373551a23172376e9454d7afa42926be96fb464350e8251a558eda238129.png" />
</div>
</div>
<ul class="simple">
<li><p>El gráfico muestra <em><strong>accuracy para los conjuntos de entrenamiento y de prueba en el eje</strong></em> <span class="math notranslate nohighlight">\(y\)</span> contra el ajuste de <code class="docutils literal notranslate"><span class="pre">n_vecinos</span></code> en el eje <span class="math notranslate nohighlight">\(x\)</span>. Aunque los gráficos del mundo real no suelen ser muy suaves, podemos reconocer algunas de las características del sobreajuste <em><strong>(overfitting)</strong></em> y del subajuste <em><strong>(underfitting)</strong></em>. Si se considera <em><strong>un solo vecino más cercano, la predicción en el conjunto de entrenamiento es perfecta</strong></em>. Pero <em><strong>cuando se consideran más vecinos, el modelo se simplifica y la precisión del entrenamiento disminuye</strong></em>.</p></li>
<li><p>La <em><strong>precisión del conjunto de prueba cuando se utiliza un solo vecino es menor que cuando se utilizan más vecinos</strong></em>, lo que indica que <em><strong>el uso de un solo vecino más cercano conduce a una mayor precisión en el conjunto de entrenamiento (modelo demasiado complejo)</strong></em>. Pero cuando se consideran más vecinos, el modelo se simplifica y la precisión del entrenamiento disminuye.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-2 {
  color: var(--sklearn-color-text);
}

#sk-container-id-2 pre {
  padding: 0;
}

#sk-container-id-2 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-2 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-2 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-2 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-2 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-2 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-2 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-2 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-2 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-2 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-2 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-2 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-2 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-2 div.sk-label label.sk-toggleable__label,
#sk-container-id-2 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-2 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-2 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-2 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-2 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-2 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-2 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-2 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-2 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier(n_neighbors=9)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;KNeighborsClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">?<span>Documentation for KNeighborsClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>KNeighborsClassifier(n_neighbors=9)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8755868544600939 0.8881118881118881
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Queda como ejercicio para el lector aplicar las técnicas de <em><strong>validación cruzada y grid search</strong></em> para obtener el mejor modelo <span class="math notranslate nohighlight">\(k\)</span>-NN para este problema de aplicación.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_score</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">n_neighbors</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span>        
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> 
    <span class="n">score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">score</span> <span class="o">&gt;</span> <span class="n">best_score</span><span class="p">:</span>
        <span class="n">best_score</span> <span class="o">=</span> <span class="n">score</span>
        <span class="n">best_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="n">n_neighbors</span><span class="p">}</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best cross-validation accuracy: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best_score</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters: &quot;</span><span class="p">,</span> <span class="n">best_parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best cross-validation accuracy: 0.94
Best parameters:  {&#39;n_neighbors&#39;: 5}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">best_parameters</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-3 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-3 {
  color: var(--sklearn-color-text);
}

#sk-container-id-3 pre {
  padding: 0;
}

#sk-container-id-3 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-3 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-3 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-3 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-3 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-3 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-3 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-3 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-3 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-3 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-3 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-3 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-3 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-3 div.sk-label label.sk-toggleable__label,
#sk-container-id-3 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-3 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-3 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-3 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-3 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-3 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-3 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-3 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-3 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;KNeighborsClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">?<span>Documentation for KNeighborsClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>KNeighborsClassifier()</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train set accuracy:&quot;</span><span class="p">,</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="s2">&quot;; Test set accuracy:&quot;</span><span class="p">,</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train set accuracy: 0.9460093896713615 ; Test set accuracy: 0.9230769230769231
</pre></div>
</div>
</div>
</div>
</section>
<section id="regresion-por-k-vecinos">
<h2><span class="section-number">2.8. </span>Regresión por <span class="math notranslate nohighlight">\(k\)</span>-vecinos<a class="headerlink" href="#regresion-por-k-vecinos" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>También existe una <em><strong>variante de regresión del algoritmo</strong></em> <span class="math notranslate nohighlight">\(k\)</span><em><strong>-vecinos más cercanos</strong></em>. Una vez más, vamos a empezar utilizando el <em><strong>vecino más cercano simple</strong></em>, esta vez utilizando el conjunto de datos <code class="docutils literal notranslate"><span class="pre">wave</span></code>. Hemos añadido tres puntos de datos de prueba como estrellas verdes en el eje <span class="math notranslate nohighlight">\(x\)</span>. <em><strong>La predicción utilizando un solo vecino es sólo el valor objetivo del vecino más cercano</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_knn_regression</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/05bb075ea381e794f5abaf2c8af15fc7ed7323dec1b49fe691c4aacf38a1d4cf.png" src="_images/05bb075ea381e794f5abaf2c8af15fc7ed7323dec1b49fe691c4aacf38a1d4cf.png" />
</div>
</div>
<ul class="simple">
<li><p>De nuevo, podemos utilizar más que el único vecino más cercano para la regresión. <em><strong>Cuando se utilizan varios vecinos más cercanos, la predicción es el promedio, o la media, de los vecinos</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_knn_regression</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/926826ed0a405ee4b4c701da812b3b992cc47a0c961d2db79fc6fa30b42ed324.png" src="_images/926826ed0a405ee4b4c701da812b3b992cc47a0c961d2db79fc6fa30b42ed324.png" />
</div>
</div>
<ul class="simple">
<li><p>El algoritmo de <span class="math notranslate nohighlight">\(k\)</span>-vecinos más cercanos para la <em><strong>regresión</strong></em> se implementa en la clase <code class="docutils literal notranslate"><span class="pre">KNeighbors</span> <span class="pre">Regressor</span></code> en <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. Se utiliza de forma similar a <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_wave</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-4 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-4 {
  color: var(--sklearn-color-text);
}

#sk-container-id-4 pre {
  padding: 0;
}

#sk-container-id-4 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-4 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-4 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-4 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-4 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-4 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-4 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-4 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-4 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-4 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-4 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-4 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-4 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-4 div.sk-label label.sk-toggleable__label,
#sk-container-id-4 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-4 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-4 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-4 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-4 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-4 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-4 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-4 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-4 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsRegressor(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" checked><label for="sk-estimator-id-4" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;KNeighborsRegressor<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsRegressor.html">?<span>Documentation for KNeighborsRegressor</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>KNeighborsRegressor(n_neighbors=3)</pre></div> </div></div></div></div></div></div>
</div>
<ul class="simple">
<li><p>Ahora podemos hacer predicciones sobre el conjunto de prueba</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set predictions:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test set predictions:
[-0.05396539  0.35686046  1.13671923 -1.89415682 -1.13881398 -1.63113382
  0.35686046  0.91241374 -0.44680446 -1.13881398]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>También podemos <em><strong>evaluar el modelo utilizando el método score</strong></em>, que para los regresores devuelve la puntuación <span class="math notranslate nohighlight">\(R^2\)</span>. La puntuación <span class="math notranslate nohighlight">\(R^2\)</span>, también conocida como <em><strong>coeficiente de determinación</strong></em>, es una medida de predicción de un modelo de regresión, y arroja una puntuación entre 0 y 1. <em><strong>Un valor de 1 corresponde a una predicción perfecta, y un valor de 0 corresponde a un modelo constante que sólo predice la media de las respuestas del conjunto de entrenamiento</strong></em>, <code class="docutils literal notranslate"><span class="pre">y_train</span></code>. Aquí, el <code class="docutils literal notranslate"><span class="pre">score</span></code> es de 0.83, lo que indica un ajuste del modelo relativamente bueno.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set R^2: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test set R^2: 0.83
</pre></div>
</div>
</div>
</div>
</section>
<section id="analisis-de-kneighborsregressor">
<h2><span class="section-number">2.9. </span>Análisis de <code class="docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code><a class="headerlink" href="#analisis-de-kneighborsregressor" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Para nuestro conjunto de datos unidimensional, podemos ver cómo son las predicciones para todos los valores posibles de las características. Para ello, creamos un conjunto de datos de prueba compuesto por muchos puntos de la línea</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n_neighbors</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span>
    <span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">line</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> neighbor(s)</span><span class="se">\n</span><span class="s2"> train score: </span><span class="si">{:.2f}</span><span class="s2"> test score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">,</span> 
                                                                                  <span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
                                                                                  <span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Target&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Model predictions&quot;</span><span class="p">,</span> <span class="s2">&quot;Training data/target&quot;</span><span class="p">,</span> <span class="s2">&quot;Test data/target&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/31dea8befdcd165cb97d4b7e19533dcfe7729517ce34286b5a9900e0103649ef.png" src="_images/31dea8befdcd165cb97d4b7e19533dcfe7729517ce34286b5a9900e0103649ef.png" />
</div>
</div>
<ul class="simple">
<li><p>Como podemos ver en el gráfico, <em><strong>al utilizar un solo vecino</strong></em>, cada punto del conjunto de entrenamiento tiene una influencia obvia en las predicciones, y <em><strong>los valores predichos pasan por todos los puntos de datos</strong></em>. Esto conduce a una predicción muy inestable. <em><strong>Tener en cuenta más vecinos conduce a predicciones más suaves</strong></em>, pero éstas no se ajustan tan bien a los datos de entrenamiento.</p></li>
</ul>
<section id="aplicacion-world-hydropower-generation">
<h3><span class="section-number">2.9.1. </span>Aplicación: World Hydropower Generation<a class="headerlink" href="#aplicacion-world-hydropower-generation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">World</span> <span class="pre">Hydropower</span> <span class="pre">Generation</span></code></strong>: Esta sección se centra en el uso del modelo <span class="math notranslate nohighlight">\(k\)</span>-NN y sus hiperparamétros, para garantizar una mejor comprensión de este algoritmo. <em><strong>No se utilizan otros recursos como la ingeniería de características, la reducción de dimensionalidad</strong></em>. El conjunto de datos utilizado es una recopilación de la <em><strong>generación de energía de varios países europeos</strong></em>, medida en <em><strong>THh entre 2000 y 2019</strong></em>. Su contenido fue extraído de <em><strong>World in Data</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>En primer lugar, se importan las librerías básicas: <code class="docutils literal notranslate"><span class="pre">pandas,</span> <span class="pre">matplotlib</span></code> y <code class="docutils literal notranslate"><span class="pre">numpy</span></code> para <code class="docutils literal notranslate"><span class="pre">Dataframes,</span> <span class="pre">Gráficos</span></code> y <em><strong>operaciones numéricas</strong></em>;<code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> para normalizar nuestros datos entre 0 y 1, <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> para ayudar a dividir el conjunto de datos (normalmente 70% entrenamiento/ 30% prueba) y <code class="docutils literal notranslate"><span class="pre">neighbors</span></code> para generar nuestros modelos usando kNN</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_log_error</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">explained_variance_score</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Para medir la eficacia de nuestros modelos generados, utilizamoslas siguientes métricas:</p>
<ul>
<li><p><em><strong>mean_absolute_error (MAE)</strong></em>: medida de los errores entre observaciones emparejadas que expresan el mismo fenómeno; <span class="math notranslate nohighlight">\(\text{MAE}=\frac{1}{n}\sum_{j=1}^{n}|y_{j}-\hat{y}_{j}|\)</span></p></li>
<li><p><em><strong>mean_squared_error (root - RMSE)</strong></em>: la desviación estándar de los residuos (errores de predicción); <span class="math notranslate nohighlight">\(\text{RMSE}=\sqrt{\frac{1}{n}\sum_{j=1}^{n}(y_{j}-\hat{y}_{j})^{2}}\)</span></p></li>
<li><p><em><strong>mean_squared_log_error (root - RMSLE)</strong></em>: mide la relación entre lo real y lo predicho; <span class="math notranslate nohighlight">\(\text{RMSLE}=\sqrt{\frac{1}{n}\sum_{j=1}^{n}(\log(y_{j}+1)-\log(\hat{y}_{j}+1))^{2}}\)</span></p></li>
<li><p><em><strong>r2_score (R2):</strong></em>: coeficiente de determinación, la proporción de la varianza en la variable dependiente que es predecible a partir de la(s) variable(s) independiente(s); <span class="math notranslate nohighlight">\(R^{2}=1-\sum_{j=1}^{n}(y_{j}-\hat{y}_{j})^{2}/\sum_{j=1}^{n}(y_{j}-\overline{y}_{j})^{2}\)</span></p></li>
<li><p><em><strong>explained_variance_score (EVS)</strong></em>: mide la discrepancia entre un modelo y los datos reales <span class="math notranslate nohighlight">\(\text{EVS}=1-\text{Var}(y-\hat{y})/\text{Var}(y).\)</span></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Preprocesamiento</span> <span class="pre">de</span> <span class="pre">datos</span></code></strong>: Inicialmente, los datos son importados, y se excluyen las columnas categóricas (en este caso, sólo <code class="docutils literal notranslate"><span class="pre">&quot;Country&quot;</span></code>). Para transformar el conjunto de datos y mantenerlo como un <code class="docutils literal notranslate"><span class="pre">Dataframe</span></code>, se utiliza la librería <code class="docutils literal notranslate"><span class="pre">scaler</span></code> dentro de la librería <em><strong>Dataframe, normalizando los datos entre 0 y 1, y manteniendo las propiedades del dataframe</strong></em>. Después de esto, se llama a la función <code class="docutils literal notranslate"><span class="pre">describe</span></code>, para mostrar algunos datos importantes sobre el <code class="docutils literal notranslate"><span class="pre">dataframe</span></code>, como <code class="docutils literal notranslate"><span class="pre">mean,</span> <span class="pre">max,</span> <span class="pre">min,</span> <span class="pre">std</span></code> y otros.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">float_format</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{:.4f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">df_power</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/lihkir/Data/main/Hydropower_Consumption.csv&#39;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_power</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 153 entries, 0 to 152
Data columns (total 21 columns):
 #   Column   Non-Null Count  Dtype 
---  ------   --------------  ----- 
 0   Country  153 non-null    object
 1   2000     153 non-null    int64 
 2   2001     153 non-null    int64 
 3   2002     153 non-null    int64 
 4   2003     153 non-null    int64 
 5   2004     153 non-null    int64 
 6   2005     153 non-null    int64 
 7   2006     153 non-null    int64 
 8   2007     153 non-null    int64 
 9   2008     153 non-null    int64 
 10  2009     153 non-null    int64 
 11  2010     153 non-null    int64 
 12  2011     153 non-null    int64 
 13  2012     153 non-null    int64 
 14  2013     153 non-null    int64 
 15  2014     153 non-null    int64 
 16  2015     153 non-null    int64 
 17  2016     153 non-null    int64 
 18  2017     153 non-null    int64 
 19  2018     153 non-null    int64 
 20  2019     153 non-null    int64 
dtypes: int64(20), object(1)
memory usage: 25.2+ KB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_power</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Country</th>
      <th>2000</th>
      <th>2001</th>
      <th>2002</th>
      <th>2003</th>
      <th>2004</th>
      <th>2005</th>
      <th>2006</th>
      <th>2007</th>
      <th>2008</th>
      <th>...</th>
      <th>2010</th>
      <th>2011</th>
      <th>2012</th>
      <th>2013</th>
      <th>2014</th>
      <th>2015</th>
      <th>2016</th>
      <th>2017</th>
      <th>2018</th>
      <th>2019</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Afghanistan</td>
      <td>312</td>
      <td>498</td>
      <td>555</td>
      <td>63</td>
      <td>565</td>
      <td>59</td>
      <td>637</td>
      <td>748</td>
      <td>542</td>
      <td>...</td>
      <td>751</td>
      <td>595</td>
      <td>71</td>
      <td>804</td>
      <td>895</td>
      <td>989</td>
      <td>1025</td>
      <td>105</td>
      <td>105</td>
      <td>107</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Africa</td>
      <td>75246</td>
      <td>80864</td>
      <td>85181</td>
      <td>82873</td>
      <td>87405</td>
      <td>89066</td>
      <td>92241</td>
      <td>95341</td>
      <td>97157</td>
      <td>...</td>
      <td>107427</td>
      <td>110445</td>
      <td>110952</td>
      <td>117673</td>
      <td>123727</td>
      <td>115801</td>
      <td>123816</td>
      <td>130388</td>
      <td>132735</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Albania</td>
      <td>4548</td>
      <td>3519</td>
      <td>3477</td>
      <td>5117</td>
      <td>5411</td>
      <td>5319</td>
      <td>4951</td>
      <td>276</td>
      <td>3759</td>
      <td>...</td>
      <td>7673</td>
      <td>4036</td>
      <td>4725</td>
      <td>6959</td>
      <td>4726</td>
      <td>5866</td>
      <td>7136</td>
      <td>448</td>
      <td>448</td>
      <td>4018</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Algeria</td>
      <td>54</td>
      <td>69</td>
      <td>57</td>
      <td>265</td>
      <td>251</td>
      <td>555</td>
      <td>218</td>
      <td>226</td>
      <td>283</td>
      <td>...</td>
      <td>173</td>
      <td>378</td>
      <td>389</td>
      <td>99</td>
      <td>193</td>
      <td>145</td>
      <td>72</td>
      <td>56</td>
      <td>117</td>
      <td>152</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Angola</td>
      <td>903</td>
      <td>1007</td>
      <td>1132</td>
      <td>1229</td>
      <td>1733</td>
      <td>2197</td>
      <td>2638</td>
      <td>2472</td>
      <td>3103</td>
      <td>...</td>
      <td>3666</td>
      <td>3967</td>
      <td>3734</td>
      <td>4719</td>
      <td>4991</td>
      <td>5037</td>
      <td>5757</td>
      <td>7576</td>
      <td>7576</td>
      <td>8422</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">countries</span> <span class="o">=</span> <span class="n">df_power</span><span class="o">.</span><span class="n">Country</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_power</span> <span class="o">=</span> <span class="n">df_power</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Country&quot;</span><span class="p">])</span>
<span class="n">df_power</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_power</span><span class="p">),</span> 
                        <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;2000&#39;</span><span class="p">,</span><span class="s1">&#39;2001&#39;</span><span class="p">,</span><span class="s1">&#39;2002&#39;</span><span class="p">,</span><span class="s1">&#39;2003&#39;</span><span class="p">,</span><span class="s1">&#39;2004&#39;</span><span class="p">,</span><span class="s1">&#39;2005&#39;</span><span class="p">,</span>
                                 <span class="s1">&#39;2006&#39;</span><span class="p">,</span><span class="s1">&#39;2007&#39;</span><span class="p">,</span><span class="s1">&#39;2008&#39;</span><span class="p">,</span><span class="s1">&#39;2009&#39;</span><span class="p">,</span><span class="s1">&#39;2010&#39;</span><span class="p">,</span><span class="s1">&#39;2011&#39;</span><span class="p">,</span>
                                 <span class="s1">&#39;2012&#39;</span><span class="p">,</span><span class="s1">&#39;2013&#39;</span><span class="p">,</span><span class="s1">&#39;2014&#39;</span><span class="p">,</span><span class="s1">&#39;2015&#39;</span><span class="p">,</span><span class="s1">&#39;2016&#39;</span><span class="p">,</span><span class="s1">&#39;2017&#39;</span><span class="p">,</span>
                                 <span class="s1">&#39;2018&#39;</span><span class="p">,</span><span class="s1">&#39;2019&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_power</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>2000</th>
      <th>2001</th>
      <th>2002</th>
      <th>2003</th>
      <th>2004</th>
      <th>2005</th>
      <th>2006</th>
      <th>2007</th>
      <th>2008</th>
      <th>2009</th>
      <th>2010</th>
      <th>2011</th>
      <th>2012</th>
      <th>2013</th>
      <th>2014</th>
      <th>2015</th>
      <th>2016</th>
      <th>2017</th>
      <th>2018</th>
      <th>2019</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
      <td>153.0000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.0327</td>
      <td>0.0353</td>
      <td>0.0355</td>
      <td>0.0328</td>
      <td>0.0419</td>
      <td>0.0390</td>
      <td>0.0415</td>
      <td>0.0434</td>
      <td>0.0371</td>
      <td>0.0445</td>
      <td>0.0328</td>
      <td>0.0433</td>
      <td>0.0352</td>
      <td>0.0290</td>
      <td>0.0282</td>
      <td>0.0265</td>
      <td>0.0306</td>
      <td>0.0296</td>
      <td>0.0388</td>
      <td>0.0259</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.1229</td>
      <td>0.1262</td>
      <td>0.1278</td>
      <td>0.1249</td>
      <td>0.1372</td>
      <td>0.1364</td>
      <td>0.1366</td>
      <td>0.1440</td>
      <td>0.1293</td>
      <td>0.1493</td>
      <td>0.1208</td>
      <td>0.1459</td>
      <td>0.1294</td>
      <td>0.1121</td>
      <td>0.1062</td>
      <td>0.1031</td>
      <td>0.1117</td>
      <td>0.1110</td>
      <td>0.1317</td>
      <td>0.1043</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.0002</td>
      <td>0.0003</td>
      <td>0.0003</td>
      <td>0.0003</td>
      <td>0.0003</td>
      <td>0.0004</td>
      <td>0.0003</td>
      <td>0.0004</td>
      <td>0.0004</td>
      <td>0.0003</td>
      <td>0.0004</td>
      <td>0.0002</td>
      <td>0.0004</td>
      <td>0.0002</td>
      <td>0.0003</td>
      <td>0.0002</td>
      <td>0.0002</td>
      <td>0.0002</td>
      <td>0.0003</td>
      <td>0.0002</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.0027</td>
      <td>0.0030</td>
      <td>0.0026</td>
      <td>0.0027</td>
      <td>0.0037</td>
      <td>0.0028</td>
      <td>0.0031</td>
      <td>0.0039</td>
      <td>0.0032</td>
      <td>0.0035</td>
      <td>0.0043</td>
      <td>0.0040</td>
      <td>0.0034</td>
      <td>0.0025</td>
      <td>0.0024</td>
      <td>0.0019</td>
      <td>0.0025</td>
      <td>0.0020</td>
      <td>0.0029</td>
      <td>0.0019</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.0116</td>
      <td>0.0128</td>
      <td>0.0119</td>
      <td>0.0118</td>
      <td>0.0155</td>
      <td>0.0107</td>
      <td>0.0149</td>
      <td>0.0151</td>
      <td>0.0147</td>
      <td>0.0152</td>
      <td>0.0145</td>
      <td>0.0156</td>
      <td>0.0140</td>
      <td>0.0096</td>
      <td>0.0121</td>
      <td>0.0086</td>
      <td>0.0117</td>
      <td>0.0113</td>
      <td>0.0171</td>
      <td>0.0097</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
      <td>1.0000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Ahora, el objetivo es crear un <em><strong>modelo que prediga la generación de energía para 2019, basándose en los 18 años anteriores (2000 - 2018)</strong></em> con <em><strong>al menos un 75% de precisión</strong></em>. Para ello, el conjunto de datos se separó en <code class="docutils literal notranslate"><span class="pre">X</span></code> e <code class="docutils literal notranslate"><span class="pre">y</span></code>, siendo <code class="docutils literal notranslate"><span class="pre">X</span></code> datos de predicción e <code class="docutils literal notranslate"><span class="pre">y</span></code> lo que se pretende predecir. Para ello, se dividen en <em><strong>entrenamiento (70%)</strong></em> y <em><strong>prueba (30%)</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df_power</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;2019&quot;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_power</span><span class="p">[</span><span class="s2">&quot;2019&quot;</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Como ya tenemos seleccionados los datos de entrenamiento y de prueba, es necesario <em><strong>encontrar el factor</strong></em> <span class="math notranslate nohighlight">\(k\)</span> que genere los <em><strong>mejores resultados para el algoritmo</strong></em>. Una de las formas de encontrar este factor <span class="math notranslate nohighlight">\(k\)</span> es <em><strong>realizar una prueba con varios valores y medir los resultados porcentuales</strong></em>. Será necesario agotar un gran número de posibilidades de <span class="math notranslate nohighlight">\(k\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rmsle_val</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">best_rmsle</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">knn</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> 
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">rmsle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_log_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">rmsle</span> <span class="o">&lt;</span> <span class="n">best_rmsle</span><span class="p">):</span>
        <span class="n">best_rmsle</span> <span class="o">=</span> <span class="n">rmsle</span>
        <span class="n">best_k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="n">rmsle_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rmsle</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;RMSLE value for k= &#39;</span> <span class="p">,</span> <span class="n">k</span> <span class="p">,</span> <span class="s1">&#39;is:&#39;</span><span class="p">,</span> <span class="n">rmsle</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best RMSLE: </span><span class="si">{</span><span class="n">best_rmsle</span><span class="si">}</span><span class="s2">, Best k: </span><span class="si">{</span><span class="n">best_k</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSLE value for k=  1 is: 0.05771085849033799
RMSLE value for k=  2 is: 0.046287824938614094
RMSLE value for k=  3 is: 0.05245290672301628
RMSLE value for k=  4 is: 0.039043708745309796
RMSLE value for k=  5 is: 0.04413545109970342
RMSLE value for k=  6 is: 0.049397885066996064
RMSLE value for k=  7 is: 0.05368842644105438
RMSLE value for k=  8 is: 0.057205532019402504
RMSLE value for k=  9 is: 0.05970971610365409
RMSLE value for k=  10 is: 0.06264220706736036
RMSLE value for k=  11 is: 0.06519555508343818
RMSLE value for k=  12 is: 0.06683376633168203
RMSLE value for k=  13 is: 0.06835697276199447
RMSLE value for k=  14 is: 0.07009875902703569
RMSLE value for k=  15 is: 0.0714815908074978
RMSLE value for k=  16 is: 0.07276108145829546
RMSLE value for k=  17 is: 0.07381745094856672
RMSLE value for k=  18 is: 0.07488933459172763
RMSLE value for k=  19 is: 0.07579673979403485
RMSLE value for k=  20 is: 0.07667076282857414
Best RMSLE: 0.039043708745309796, Best k: 4
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Nuestra métrica indica que <em><strong>el menor error se produce cuando tenemos</strong></em> <span class="math notranslate nohighlight">\(k = 4\)</span> <em><strong>(RMSLE de 0.0390)</strong></em>, lo que indica un error relativo entre los valores predichos y los actuales del 3,90%. Por lo tanto, <em><strong>presentaremos todos los valores en un gráfico, que nos mostrará visualmente los resultados obtenidos</strong></em>. Esta función se conoce como <em><strong>“función codo”</strong></em>, dada la variación porcentual que se produce entre los valores de <span class="math notranslate nohighlight">\(k\)</span>, primero hacia abajo y luego hacia arriba, cuando <span class="math notranslate nohighlight">\(k\)</span> encuentra su mejor valor. Estamos trazando los valores <em><strong>RMSLE</strong></em> frente a los valores de <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">curve</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">rmsle_val</span><span class="p">)</span>
<span class="n">curve</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f006c598ae66e262e0a4d4106eadffa87bdeb385d282694f198f43f35798b76e.png" src="_images/f006c598ae66e262e0a4d4106eadffa87bdeb385d282694f198f43f35798b76e.png" />
</div>
</div>
<ul class="simple">
<li><p>El gráfico muestra una <em><strong>caída brusca del RMSLE</strong></em> a medida que <span class="math notranslate nohighlight">\(k\)</span> avanza hasta 4, momento en el que empieza a aumentar indefinidamente, lo que nos lleva a la conclusión de que <em><strong>4 es el mejor resultado para</strong></em> <span class="math notranslate nohighlight">\(k\)</span>. Una vez encontrado el mejor valor para <span class="math notranslate nohighlight">\(k\)</span>, es hora de entrenar el modelo y predecir los resultados. También haremos uso de la función de `score***, que nos permitirá ver la <em><strong>tasa de precisión de nuestro modelo (80,16%)</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">knn</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8016285342188538
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Una vez se ha construido el modelo y se prueban algunas predicciones, podemos aplicar las <em><strong>métricas</strong></em> y <em><strong>analizar los resultados</strong></em>. Aquí se <em><strong>comparan el conjunto de prueba con las predicciones</strong></em> usando las métricas <code class="docutils literal notranslate"><span class="pre">R2,</span> <span class="pre">EVS,</span> <span class="pre">MAE,</span> <span class="pre">RMSE,</span> <span class="pre">RMSLE</span></code>, para verificar cuáles son los resultados de cada una de ellas.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r2_valid</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">mae_valid</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">evs_valid</span> <span class="o">=</span> <span class="n">explained_variance_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;uniform_average&#39;</span><span class="p">)</span>
<span class="n">rmse_valid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="n">rmsle_valid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_log_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R2 Valid:&#39;</span><span class="p">,</span><span class="n">r2_valid</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;EVS Valid:&#39;</span><span class="p">,</span> <span class="n">evs_valid</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE Valid:&#39;</span><span class="p">,</span> <span class="n">mae_valid</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;RMSE Valid:&#39;</span><span class="p">,</span><span class="n">rmse_valid</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;RMSLE Valid:&#39;</span><span class="p">,</span> <span class="n">rmsle_valid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R2 Valid: 0.8016285342188538
EVS Valid: 0.8131526313306373
MAE Valid: 0.017960057840249434
RMSE Valid: 0.04999229884993785
RMSLE Valid: 0.039043708745309796
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Los resultados muestran un <code class="docutils literal notranslate"><span class="pre">R2</span> <span class="pre">del</span> <span class="pre">80,16%</span></code> y un <code class="docutils literal notranslate"><span class="pre">EVS</span> <span class="pre">del</span> <span class="pre">81,31%</span></code>, lo que indica que nuestro modelo tiene un <em><strong>gran ajuste a su muestra (R2)</strong></em> y una <em><strong>fuerte asociación entre el modelo y sus datos actuales (EVS)</strong></em>. En cuanto al <em><strong>RMSLE</strong></em>, sólo tiene en cuenta el <em><strong>error relativo entre el valor previsto y el real, y la escala del error no es significativa</strong></em>. Por otro lado, el valor <em><strong>RMSE aumenta en magnitud si aumenta la escala del error</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>Una vez realizada la predicción y comprobado el modelo, <em><strong>organizamos los resultados uno al lado del otro para poder hacer una comparación</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">country_test</span> <span class="o">=</span> <span class="n">countries</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">countries</span><span class="p">)</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">):]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_prediction</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">))</span>
<span class="n">data_prediction</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data_prediction</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span><span class="s1">&#39;Prediction&#39;</span><span class="p">])</span>
<span class="n">data_prediction</span> <span class="o">=</span> <span class="n">data_prediction</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">country_test</span><span class="p">)</span>
<span class="n">data_prediction</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Test</th>
      <th>Prediction</th>
    </tr>
    <tr>
      <th>Country</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Papua New Guinea</th>
      <td>0.0406</td>
      <td>0.0268</td>
    </tr>
    <tr>
      <th>Paraguay</th>
      <td>0.0044</td>
      <td>0.0039</td>
    </tr>
    <tr>
      <th>Peru</th>
      <td>0.0134</td>
      <td>0.0096</td>
    </tr>
    <tr>
      <th>Phillipines</th>
      <td>0.0280</td>
      <td>0.0128</td>
    </tr>
    <tr>
      <th>Poland</th>
      <td>0.0164</td>
      <td>0.0125</td>
    </tr>
    <tr>
      <th>Portugal</th>
      <td>0.0321</td>
      <td>0.0155</td>
    </tr>
    <tr>
      <th>Puerto Rico</th>
      <td>0.4982</td>
      <td>0.3417</td>
    </tr>
    <tr>
      <th>Reunion</th>
      <td>0.5331</td>
      <td>0.3417</td>
    </tr>
    <tr>
      <th>Romania</th>
      <td>0.0007</td>
      <td>0.0075</td>
    </tr>
    <tr>
      <th>Russia</th>
      <td>0.0032</td>
      <td>0.0039</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;font.size&#39;</span><span class="p">:</span> <span class="mi">18</span><span class="p">});</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">));</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data_prediction</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">data_prediction</span><span class="o">.</span><span class="n">Test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Test&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data_prediction</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">data_prediction</span><span class="o">.</span><span class="n">Prediction</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Prediction&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Country&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Hydropower Generation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eba61bbb4067d9a18d308227990a850cad5ca2bcd79030ecd14b636505378381.png" src="_images/eba61bbb4067d9a18d308227990a850cad5ca2bcd79030ecd14b636505378381.png" />
</div>
</div>
<ul class="simple">
<li><p>Nótese que el conjunto de prueba para este ejemplo fue tomado como, el <em><strong>último 30% de nuestras observaciones</strong></em>, lo cual corresponde a los <em><strong>últimos 46 países</strong></em> (observaciones) existentes en nuestro dataset. En la sección dedicada a la <em><strong>evaluación de modelos</strong></em>, se abordarán técnicas especificas para que <em><strong>pliegues de prueba sean repartidos proporcionalmente en todas las posiciones posibles en nuestro data set</strong></em>, usando la librería <code class="docutils literal notranslate"><span class="pre">KFold</span></code>, así como también hiperparametrización usando <code class="docutils literal notranslate"><span class="pre">GridSearch</span></code>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Puntos fuertes, puntos débiles y parámetros</p>
<ul class="simple">
<li><p>En principio, hay <em><strong>dos parámetros importantes</strong></em> en el clasificador <code class="docutils literal notranslate"><span class="pre">KNeighbors</span></code>: el <em><strong>número de vecinos</strong></em> y <em><strong>cómo se mide la distancia entre los puntos de datos</strong></em>. En la práctica, utilizar <em><strong>un número pequeño de vecinos, como tres o cinco, suele funcionar bien, pero se debería ajustar este parámetro</strong></em>.</p></li>
<li><p>La elección de la <em><strong>medida de distancia correcta es también crucial</strong></em>. Por defecto, <code class="docutils literal notranslate"><span class="pre">KNeighbors</span></code> utiliza la <em><strong>distancia euclidiana</strong></em>, que funciona bien en muchos casos. Uno de los puntos fuertes de <span class="math notranslate nohighlight">\(k\)</span>-NN es que <em><strong>el modelo es muy fácil de entender</strong></em>, y a menudo da un rendimiento razonable sin necesidad de muchos ajustes. El uso de este algoritmo es un buen <em><strong>método de referencia para probar, antes de considerar técnicas más avanzadas</strong></em>.</p></li>
<li><p>La construcción del modelo de vecinos más cercanos suele ser muy rápida, pero <em><strong>cuando el conjunto de entrenamiento es muy grande (ya sea en número de características o en número de muestras) la predicción puede ser lenta</strong></em>. Cuando se utiliza el algoritmo <span class="math notranslate nohighlight">\(k\)</span>-NN, es importante <em><strong>pre-procesar los datos</strong></em>, tema que revisaremos en secciones posteriores. Este enfoque <em><strong>no suele funcionar bien en conjuntos de datos con muchas características (cientos o más)</strong></em>, y <em><strong>lo hace especialmente mal con conjuntos de datos en los que la mayoría de las características son 0 la mayor parte del tiempo (los llamados conjuntos de datos dispersos</strong></em>).</p></li>
<li><p>Por lo tanto, aunque el algoritmo de <span class="math notranslate nohighlight">\(k\)</span><em><strong>-vecinos más cercanos</strong></em> es fácil de entender, <em><strong>no se utiliza a menudo en la práctica, debido a que la predicción es lenta y a su incapacidad para manejar muchas características</strong></em>.</p></li>
</ul>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "tf"
        },
        kernelOptions: {
            name: "tf",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'tf'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="supervised_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1. </span>Aprendizaje supervisado</p>
      </div>
    </a>
    <a class="right-next"
       href="linear_model.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Regresión Ridge y Lasso</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">2.1. Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ejercicios">2.2. Ejercicios</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">2.3. Implementación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-de-kneighborsclassifier">2.4. Análisis de <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-breast-cancer-dataset">2.5. Aplicación: Breast Cancer Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-exploratorio-de-datos">2.6. Análisis Exploratorio de Datos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multicolinealidad">2.6.1. Multicolinealidad</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#factor-de-inflacion-de-la-varianza-vif">2.6.2. Factor de Inflación de la Varianza (VIF)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-del-modelo-k-nn">2.7. Selección del modelo K-NN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-por-k-vecinos">2.8. Regresión por <span class="math notranslate nohighlight">\(k\)</span>-vecinos</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-de-kneighborsregressor">2.9. Análisis de <code class="docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-world-hydropower-generation">2.9.1. Aplicación: World Hydropower Generation</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lihki Rubio, Ph.D.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>Lihki Rubio, Ph.D. All rights reserved.</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>