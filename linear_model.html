
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3. Regresión Ridge y Lasso &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=530fe47d" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/custom-checkpoint.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'linear_model';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/custom.js?v=14184634"></script>
    <script src="_static/.ipynb_checkpoints/custom-checkpoint.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. Clasificador Bayesiano" href="bayes_model.html" />
    <link rel="prev" title="2. \(k\)-vecinos más cercanos" href="knn_model.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fotolihki.jpg" class="logo__image only-light" alt="Machine Learning - Home"/>
    <script>document.write(`<img src="_static/fotolihki.jpg" class="logo__image only-dark" alt="Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Profesor: Dr. Lihki Rubio
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="supervised_intro.html">1. Aprendizaje supervisado</a></li>
<li class="toctree-l1"><a class="reference internal" href="knn_model.html">2. <span class="math notranslate nohighlight">\(k\)</span>-vecinos más cercanos</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Regresión Ridge y Lasso</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes_model.html">4. Clasificador Bayesiano</a></li>
<li class="toctree-l1"><a class="reference internal" href="decisiontree_model.html">5. Random Forest y XGBoost</a></li>

<li class="toctree-l1"><a class="reference internal" href="svm_model.html">7. Máquinas de vectores de soporte</a></li>
<li class="toctree-l1"><a class="reference internal" href="ann_model.html">8. Redes Neuronales y Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl_computer_vision.html">9. Deep Learning para Visión por Computadora</a></li>




<li class="toctree-l1"><a class="reference internal" href="practical_pca.html">14. Análisis de Componentes Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">15. Evaluación de modelos</a></li>
<li class="toctree-l1"><a class="reference internal" href="chains_pipelines.html">16. Cadenas de Algoritmos y Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">17. Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">18. Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/linear_model.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Regresión Ridge y Lasso</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-de-regresion-lineal">3.1. Modelo de regresión lineal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimos-cuadrados-ordinarios">3.2. Mínimos Cuadrados Ordinarios</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-regresion-ridge-y-ols">3.3. Aplicación: Regresión Ridge y OLS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-exploratorio-de-datos">3.3.1. Análisis Exploratorio de Datos</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge">3.4. Regresión ridge</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">3.4.1. Análisis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">3.4.2. Implementación</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso">3.5. Lasso</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-regresion-lasso">3.6. Aplicación: Regresión Lasso</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-lineales-para-clasificacion">3.7. Modelos lineales para clasificación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-lineales-para-la-clasificacion-multiclase">3.8. Modelos lineales para la clasificación multiclase</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="regresion-ridge-y-lasso">
<h1><span class="section-number">3. </span>Regresión Ridge y Lasso<a class="headerlink" href="#regresion-ridge-y-lasso" title="Link to this heading">#</a></h1>
<div class="tip admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Los modelos de <em><strong>regresión lineal</strong></em> son esenciales en estadística y aprendizaje automático para <em><strong>analizar la relación entre variables dependientes e independientes</strong></em>. Su simplicidad permite interpretar dichas relaciones, siendo útiles en la <em><strong>predicción de valores futuros, evaluación del impacto de variables y detección de tendencias</strong></em>.</p></li>
<li><p>Además, forman la <em><strong>base de métodos avanzados como la regresión logística y redes neuronales</strong></em>, siendo clave en economía, ciencias sociales, biología e ingeniería.</p></li>
</ul>
</div>
<section id="modelo-de-regresion-lineal">
<h2><span class="section-number">3.1. </span>Modelo de regresión lineal<a class="headerlink" href="#modelo-de-regresion-lineal" title="Link to this heading">#</a></h2>
<p><strong><code class="docutils literal notranslate"><span class="pre">Formulación</span></code></strong>: El modelo</p>
<div class="math notranslate nohighlight" id="equation-linear-reg">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-linear-reg" title="Link to this equation">#</a></span>\[
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon},
\]</div>
<p>se denomina, <em><strong>modelo de regresión lineal</strong></em> clásico, si se cumplen los siguientes supuestos:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}(\boldsymbol{\varepsilon})=\boldsymbol{0}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Cov}(\boldsymbol{\varepsilon})=\mathbb{E}(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}^{T})=\sigma^{2}\boldsymbol{I}\)</span></p></li>
<li><p>La matriz de diseño <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> tiene <em><strong>rango completo</strong></em>, es decir <span class="math notranslate nohighlight">\(\textrm{rk}(\boldsymbol{X})=p+1\)</span></p></li>
<li><p>El <em><strong>modelo de regresión normal</strong></em> clasico es obtenido si adicionalmente  se tiene que <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\sim N(\boldsymbol{0}, \sigma^{2}\boldsymbol{I})\)</span>.</p></li>
<li><p>El <em><strong>modelo de regresión lineal</strong></em> <a class="reference internal" href="#equation-linear-reg">(3.1)</a> puede escribirse en la siguiente forma</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-linear-reg-mat">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-linear-reg-mat" title="Link to this equation">#</a></span>\[\begin{split}
\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{i}\\
\vdots\\
y_{n}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\
1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
1 &amp; x_{i1} &amp; x_{i2} &amp; \cdots &amp; x_{ip}\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{pmatrix}
\begin{pmatrix}
\beta_{0}\\[2mm]
\beta_{1}\\[2mm]
\beta_{2}\\
\vdots\\[2mm]
\beta_{p}
\end{pmatrix}
+
\begin{pmatrix}
\varepsilon_{1}\\
\varepsilon_{2}\\
\vdots\\
\varepsilon_{i}\\
\vdots\\
\varepsilon_{n}
\end{pmatrix}
\end{split}\]</div>
<ul class="simple">
<li><p>A partir del sistema <a class="reference internal" href="#equation-linear-reg-mat">(3.2)</a>, se puede observar que la <span class="math notranslate nohighlight">\(i\)</span>-esima <em><strong>predicción para un modelo lineal</strong></em> es la siguiente:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}\cdot x_{i1}+\hat{\beta}_{2}\cdot x_{i2}+\cdots+\hat{\beta}_{p}\cdot x_{ip}=\boldsymbol{\hat{\beta}}^{T}\boldsymbol{x}_{i},~i = 1,2,\dots, n.
\]</div>
<ul class="simple">
<li><p>Aquí, <span class="math notranslate nohighlight">\(x_{i1},\dots, x_{ip}\)</span> denotan las <em><strong>variables predictoras o características</strong></em> (en este ejemplo, el número de características es <span class="math notranslate nohighlight">\(p\)</span>). Los valores, <span class="math notranslate nohighlight">\(\hat{\beta}_{i},~i=0,1,\dots,p\)</span>, son los <em><strong>parámetros aprendidos por el modelo</strong></em> y <span class="math notranslate nohighlight">\(\hat{y}_{i}\)</span> es la <em><strong>predicción obtenida por el modelo</strong></em>. Por ejemplo, para un conjunto de datos con una sola característica, se tiene que:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y}_{i} = \hat{\beta}_{0}+\hat{\beta}_{1}\cdot x_{i1},~i=1,2,\dots,n.
\]</div>
<ul class="simple">
<li><p>Aquí, <span class="math notranslate nohighlight">\(\hat{\beta}_{1}\)</span> es la <em><strong>pendiente</strong></em> y <span class="math notranslate nohighlight">\(\hat{\beta}_{0}\)</span> es el <em><strong>desplazamiento en el eje</strong></em> <span class="math notranslate nohighlight">\(y\)</span>. Para más características, <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> contiene las <em><strong>pendientes a lo largo de cada eje de características</strong></em>. Alternativamente, se puede pensar en la <em><strong>respuesta predicha</strong></em> como una <em><strong>suma ponderada de las características de entrada</strong></em>, con pesos (que pueden ser negativos) dados por las entradas de <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span>.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Ilustración</span></code></strong></p>
<ul class="simple">
<li><p>Considere el ejemplo de intentar <em><strong>aprender los parámetros</strong></em> <span class="math notranslate nohighlight">\(\hat{\beta}_{1}:=w[0]\)</span> y <span class="math notranslate nohighlight">\(\hat{\beta}_{0}:=b\)</span> en nuestro <em><strong>conjunto de datos de ondas unidimensionales (wave)</strong></em> usando <code class="docutils literal notranslate"><span class="pre">plot_linear_regression_wave()</span></code>. En este caso, la función para la regresión lineal de ondas unidimensionales, usa la librería <code class="docutils literal notranslate"><span class="pre">LinearRegression()</span></code>, para ajustar la recta de regresión, basada en mínimos cuadrados.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_linear_regression_wave</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w[0]: 0.393906  b: -0.031804
</pre></div>
</div>
<img alt="_images/a8410b8c3495ed199573c9f331e8da7d4b878b233eb2be2c9c6897437a0cc4e1.png" src="_images/a8410b8c3495ed199573c9f331e8da7d4b878b233eb2be2c9c6897437a0cc4e1.png" />
</div>
</div>
<ul class="simple">
<li><p>Si observamos <span class="math notranslate nohighlight">\(\hat{\beta}_{1}\)</span>, vemos que <em><strong>la pendiente debería estar en torno a 0.4</strong></em>, lo que podemos confirmar visualmente en el gráfico. <em><strong>El intercepto</strong></em>, punto en el que la línea de predicción debería cruzar el eje <span class="math notranslate nohighlight">\(y\)</span>, está <em><strong>ligeramente por debajo de cero</strong></em>, lo que también se puede confirmar en la imagen.</p></li>
<li><p>Los <em><strong>modelos de regresión lineal</strong></em> pueden caracterizarse como <em><strong>modelos de regresión</strong></em> en los que <em><strong>la predicción es una línea para una sola característica, un plano cuando se utilizan dos características, o un hiperplano en dimensiones más altas</strong></em> (es decir, cuando se utilizan más características).</p></li>
<li><p>Si se comparan las <em><strong>predicciones realizadas por la línea recta</strong></em> con las <em><strong>realizadas por el modelo KNeighborsRegressor</strong></em>, <em><strong>usar una línea recta para hacer predicciones parece muy restrictivo. Parece que se pierden todos los detalles finos de los datos</strong></em>. En cierto sentido, esto es cierto. Es una suposición fuerte (y algo irreal) que nuestro objetivo <span class="math notranslate nohighlight">\(y\)</span> es una combinación lineal de las características. Observar los datos de forma unidimensional da una perspectiva algo sesgada.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Para los <em><strong>conjuntos de datos con muchas características, los modelos lineales pueden ser muy potentes</strong></em>. En particular, si tiene <em><strong>más características que puntos de datos de entrenamiento, cualquier objetivo</strong></em> <span class="math notranslate nohighlight">\(y\)</span> <em><strong>puede modelarse perfectamente (en el conjunto de entrenamiento) como una función lineal</strong></em>.</p></li>
<li><p>Hay muchos modelos lineales diferentes para la regresión. <em><strong>La diferencia entre estos modelos radica en cómo se aprenden los parámetros del modelo</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{1}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{0}\)</span> a partir de los datos de entrenamiento, y en <em><strong>cómo se puede controlar la complejidad del modelo</strong></em>. A continuación veremos los modelos lineales más populares.</p></li>
</ul>
</div>
</section>
<section id="minimos-cuadrados-ordinarios">
<h2><span class="section-number">3.2. </span>Mínimos Cuadrados Ordinarios<a class="headerlink" href="#minimos-cuadrados-ordinarios" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><em><strong>La regresión lineal, o mínimos cuadrados ordinarios (Ordinary Least Squares (OLS))</strong></em>, es el modelo lineal <em><strong>más sencillo y clásico para la regresión</strong></em>. La regresión lineal <em><strong>encuentra el vector de parámetros</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> <em><strong>que minimiza el error cuadrático medio entre las predicciones y los verdaderos objetivos de la regresión</strong></em>, <span class="math notranslate nohighlight">\(y\)</span>, <em><strong>en el conjunto de entrenamiento</strong></em>.</p></li>
<li><p>El error cuadrático medio es la suma de las diferencias al cuadrado entre las predicciones y los valores reales. La regresión lineal no tiene parámetros, lo cual es una ventaja, pero, <em><strong>no tiene forma de controlar la complejidad del modelo</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_wave</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>El número <code class="docutils literal notranslate"><span class="pre">42</span></code> es normalmente elegido en la literatura relacionadas con <code class="docutils literal notranslate"><span class="pre">AI</span></code>, como homenaje al libro de la <code class="docutils literal notranslate"><span class="pre">Douglas</span> <span class="pre">Adams</span></code> “<a class="reference external" href="https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#The_number_42">The Hitchhiker’s Guide to the Galaxy</a>”, una serie de cómic de ciencia ficción creada por <code class="docutils literal notranslate"><span class="pre">Douglas</span> <span class="pre">Adams</span></code> que se ha hecho popular entre los aficionados al género y los <em><strong>miembros de la comunidad científica</strong></em>.</p></li>
<li><p>El número 42 corresponde a la respuesta a la gran pregunta <code class="docutils literal notranslate"><span class="pre">&quot;Life,</span> <span class="pre">the</span> <span class="pre">universe,</span> <span class="pre">and</span> <span class="pre">everything&quot;</span></code>, calculada por un ordenador (llamado <code class="docutils literal notranslate"><span class="pre">&quot;Deep</span> <span class="pre">Thought&quot;</span></code>) creado específicamente para resolverla <code class="docutils literal notranslate"><span class="pre">:)</span></code>. Supuestamente, el pensamiento profundo (Deep Thought) tarda <span class="math notranslate nohighlight">\(7\frac{1}{2}\)</span> millones de años en calcular y comprobar la respuesta, que resulta ser 42.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Sin embargo, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> puede ser cualquier número entero, mas aún, podemos realizar un <code class="docutils literal notranslate"><span class="pre">grid</span> <span class="pre">search</span></code> para conseguir aquel parámetro <code class="docutils literal notranslate"><span class="pre">random_state</span></code> que nos entrega el mejor score. Mas adelante abordaremos el uso de <code class="docutils literal notranslate"><span class="pre">GridSearch</span></code>. Si no establece <code class="docutils literal notranslate"><span class="pre">random_state</span></code> en <code class="docutils literal notranslate"><span class="pre">42</span></code> o cualquier otro entero positivo, cada vez que ejecute su código de nuevo, generará un conjunto de pruebas diferente.</p></li>
</ul>
<ul class="simple">
<li><p>Los <em><strong>parámetros de “pendiente”</strong></em> (<span class="math notranslate nohighlight">\(\hat{\beta}_{i},~i=1,2,\dots,p\)</span>), también llamados <em><strong>pesos o coeficientes, se almacenan en el atributo coef_</strong></em>, mientras que el <em><strong>desplazamiento o intercepto</strong></em> (<span class="math notranslate nohighlight">\(\hat{\beta}_{0}\)</span>) se almacena en el atributo <code class="docutils literal notranslate"><span class="pre">intercept_</span></code>:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lr.coef_: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lr.intercept_: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>lr.coef_: [0.39390555]
lr.intercept_: -0.031804343026759746
</pre></div>
</div>
</div>
</div>
<div class="proof observation admonition" id="observation_lm1">
<p class="admonition-title"><span class="caption-number">Observation 3.1 </span></p>
<section class="observation-content" id="proof-content">
<p>El extraño <em><strong>guion bajo</strong></em> al final de <code class="docutils literal notranslate"><span class="pre">coef_</span></code> e <code class="docutils literal notranslate"><span class="pre">intercept</span></code>_ es usado a menudo por <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> para almacenar cualquier <em><strong>objeto que se deriva de los datos de entrenamiento</strong></em>, usando atributos que terminan con un guion bajo al final. Esto es <em><strong>para separarlos de los parámetros que son establecidos por el usuario</strong></em>.</p>
</section>
</div><ul class="simple">
<li><p>El atributo <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> es siempre un <em><strong>único número flotante</strong></em>, mientras que el atributo <em><strong>coef_ es una matriz NumPy con una entrada por característica</strong></em>. Como solo tenemos una característica de entrada en el conjunto de datos <code class="docutils literal notranslate"><span class="pre">wave</span></code>, <code class="docutils literal notranslate"><span class="pre">lr.coef_</span></code> solo tiene una entrada. Veamos el rendimiento del conjunto de entrenamiento y del conjunto de prueba</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.67
Test set score: 0.66
</pre></div>
</div>
</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">¿Cual sería un buen score?</p>
<p>Definir un <em><strong>buen score</strong></em> en machine learning es un tema subjetivo, y bastante ligado a los datos. Pero, <em><strong>de forma coherente con los estándares de la industria, cualquier score superior al 70% es un gran rendimiento del modelo</strong></em>. De hecho, <em><strong>una medida de precisión de entre el 70% y el 90% no sólo es ideal, sino que es realista</strong></em>.</p>
</div>
<ul class="simple">
<li><p><em><strong>Un</strong></em> <span class="math notranslate nohighlight">\(R^2\)</span> <em><strong>en torno a 0.66 no es muy bueno</strong></em>, pero podemos ver que los <em><strong>score en los conjuntos de entrenamiento y de prueba están muy cerca</strong></em>. Esto significa que probablemente estemos <em><strong>subajustando (underfitting)</strong></em>, no <em><strong>sobreajustando (overfitting)</strong></em> nuestro modelo de regresión lineal. Para este conjunto de datos unidimensional, hay poco peligro de overfitting, ya que el modelo es muy simple (o restringido).</p></li>
<li><p><em><strong>En este tipo de casos, optamos por complejizar el modelo para obtener un modelo menos simple</strong></em>. Sin embargo, <em><strong>con conjuntos de datos de mayor dimensión</strong></em> (i.e dataset con un gran número de características), <em><strong>los modelos lineales son más potentes, y hay más posibilidades de que se ajusten en exceso</strong></em>.</p></li>
</ul>
</section>
<section id="aplicacion-regresion-ridge-y-ols">
<h2><span class="section-number">3.3. </span>Aplicación: Regresión Ridge y OLS<a class="headerlink" href="#aplicacion-regresion-ridge-y-ols" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Veamos cómo se comporta <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> en un conjunto de datos más complejo, como el conjunto de datos de <em><strong>viviendas de Boston</strong></em>. Recordemos que este conjunto de datos tiene <em><strong>506 muestras y 105 características</strong></em> derivadas. En primer lugar, <em><strong>cargamos el conjunto de datos</strong></em> y lo <em><strong>dividimos en un conjunto de entrenamiento y otro de prueba</strong></em>. A continuación, construimos el modelo de regresión lineal como antes. Iniciemos realizando un <em><strong>análisis exploratorio de datos</strong></em>.</p></li>
</ul>
<figure class="align-center" id="boston-houses-ds-numref">
<a class="reference internal image-reference" href="_images/boston_houses_ds.jpeg"><img alt="_images/boston_houses_ds.jpeg" src="_images/boston_houses_ds.jpeg" style="width: 672.0px; height: 448.7px;" /></a>
</figure>
<section id="analisis-exploratorio-de-datos">
<h3><span class="section-number">3.3.1. </span>Análisis Exploratorio de Datos<a class="headerlink" href="#analisis-exploratorio-de-datos" title="Link to this heading">#</a></h3>
<p><strong><code class="docutils literal notranslate"><span class="pre">1.</span> <span class="pre">Importar</span> <span class="pre">librerías</span> <span class="pre">PythonImportar</span> <span class="pre">librerías</span> <span class="pre">Python</span></code></strong></p>
<ul class="simple">
<li><p>Se importan las <em><strong>librerías esenciales</strong></em> para facilitar el análisis que abarca la <em><strong>carga de datos, la evaluación estadística, la visualización, la transformación de datos, la fusión y la unión</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<p><strong><code class="docutils literal notranslate"><span class="pre">2.</span> <span class="pre">Lectura</span> <span class="pre">de</span> <span class="pre">datos</span></code></strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/lihkir/Data/main/boston.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Análisis</span> <span class="pre">de</span> <span class="pre">Datos</span></code></strong>: El objetivo principal de la <em><strong>comprensión de datos</strong></em> es obtener información general sobre los datos, que abarca el <em><strong>número de filas y columnas, valores y tipos de datos así como también datos faltantes.</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>crim</th>
      <th>zn</th>
      <th>indus</th>
      <th>chas</th>
      <th>nox</th>
      <th>rm</th>
      <th>age</th>
      <th>dis</th>
      <th>rad</th>
      <th>tax</th>
      <th>ptratio</th>
      <th>black</th>
      <th>lstat</th>
      <th>medv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1</td>
      <td>296</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>crim</th>
      <th>zn</th>
      <th>indus</th>
      <th>chas</th>
      <th>nox</th>
      <th>rm</th>
      <th>age</th>
      <th>dis</th>
      <th>rad</th>
      <th>tax</th>
      <th>ptratio</th>
      <th>black</th>
      <th>lstat</th>
      <th>medv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>502</th>
      <td>0.06263</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0</td>
      <td>0.573</td>
      <td>6.593</td>
      <td>69.1</td>
      <td>2.4786</td>
      <td>1</td>
      <td>273</td>
      <td>21.0</td>
      <td>391.99</td>
      <td>9.67</td>
      <td>22.4</td>
    </tr>
    <tr>
      <th>503</th>
      <td>0.04527</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0</td>
      <td>0.573</td>
      <td>6.120</td>
      <td>76.7</td>
      <td>2.2875</td>
      <td>1</td>
      <td>273</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>9.08</td>
      <td>20.6</td>
    </tr>
    <tr>
      <th>504</th>
      <td>0.06076</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0</td>
      <td>0.573</td>
      <td>6.976</td>
      <td>91.0</td>
      <td>2.1675</td>
      <td>1</td>
      <td>273</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>5.64</td>
      <td>23.9</td>
    </tr>
    <tr>
      <th>505</th>
      <td>0.10959</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0</td>
      <td>0.573</td>
      <td>6.794</td>
      <td>89.3</td>
      <td>2.3889</td>
      <td>1</td>
      <td>273</td>
      <td>21.0</td>
      <td>393.45</td>
      <td>6.48</td>
      <td>22.0</td>
    </tr>
    <tr>
      <th>506</th>
      <td>0.04741</td>
      <td>0.0</td>
      <td>11.93</td>
      <td>0</td>
      <td>0.573</td>
      <td>6.030</td>
      <td>80.8</td>
      <td>2.5050</td>
      <td>1</td>
      <td>273</td>
      <td>21.0</td>
      <td>396.90</td>
      <td>7.88</td>
      <td>11.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Index: 506 entries, 1 to 506
Data columns (total 14 columns):
 #   Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   crim     506 non-null    float64
 1   zn       506 non-null    float64
 2   indus    506 non-null    float64
 3   chas     506 non-null    int64  
 4   nox      506 non-null    float64
 5   rm       506 non-null    float64
 6   age      506 non-null    float64
 7   dis      506 non-null    float64
 8   rad      506 non-null    int64  
 9   tax      506 non-null    int64  
 10  ptratio  506 non-null    float64
 11  black    506 non-null    float64
 12  lstat    506 non-null    float64
 13  medv     506 non-null    float64
dtypes: float64(11), int64(3)
memory usage: 59.3 KB
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><em><strong>Información de atributos (por orden)</strong></em>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">CRIM</span></code>: tasa de criminalidad per cápita por ciudad</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ZN</span></code>: proporción de suelo residencial para parcelas de más de 25.000 pies cuadrados</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INDUS</span></code>: proporción de acres comerciales no minoristas por ciudad</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CHAS</span></code>: Variable dummy del Río Charles (= 1 si el tramo limita con el río; 0 en caso contrario)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NOX</span></code>: concentración de óxidos nítricos (partes por 10 millones)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RM</span></code>: número medio de habitaciones por vivienda</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">AGE</span></code>: proporción de unidades ocupadas por sus propietarios construidas antes de 1940</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DIS</span></code>: distancias ponderadas a cinco centros de empleo de Boston</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RAD</span></code>: índice de accesibilidad a autopistas radiales</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TAX</span></code>: tipo del impuesto sobre bienes inmuebles de valor íntegro por 10.000 dólares</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PTRATIO</span></code>: relación alumnos-profesor por ciudad</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">B</span></code>: 1000(Bk - 0,63)^2 donde Bk es la proporción de negros por ciudad</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LSTAT</span></code>: % más bajo de la población</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MEDV</span></code>: Valor medio de las viviendas ocupadas por sus propietarios en $1000’s.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Comprobación</span> <span class="pre">de</span> <span class="pre">duplicados</span></code></strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">nunique</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>crim       504
zn          26
indus       76
chas         2
nox         81
rm         446
age        356
dis        412
rad          9
tax         66
ptratio     46
black      357
lstat      455
medv       229
dtype: int64
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Conteo</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">faltantes</span> <span class="pre">y</span> <span class="pre">porcentaje</span></code></strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>crim       0
zn         0
indus      0
chas       0
nox        0
rm         0
age        0
dis        0
rad        0
tax        0
ptratio    0
black      0
lstat      0
medv       0
dtype: int64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span><span class="o">*</span><span class="mi">100</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>crim       0.0
zn         0.0
indus      0.0
chas       0.0
nox        0.0
rm         0.0
age        0.0
dis        0.0
rad        0.0
tax        0.0
ptratio    0.0
black      0.0
lstat      0.0
medv       0.0
dtype: float64
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Operaciones relacionadas con <em><strong>Feature Engineering, Reducción y Gestión</strong></em> pueden llevarse a cabo de ser necesarias.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">3.</span> <span class="pre">EDA</span> <span class="pre">Análisis</span> <span class="pre">exploratorio</span> <span class="pre">de</span> <span class="pre">datos</span></code></strong></p>
<ul class="simple">
<li><p>El <em><strong>análisis exploratorio de datos</strong></em> se refiere al proceso crucial de realizar investigaciones iniciales sobre los datos para <em><strong>descubrir patrones y comprobar supuestos</strong></em> con la ayuda de <em><strong>estadísticas resumidas y representaciones gráficas</strong></em>.</p>
<ul>
<li><p>El <em><strong>EDA</strong></em> puede utilizarse para buscar <em><strong>valores atípicos, patrones y tendencias en los datos</strong></em>.</p></li>
<li><p><em><strong>EDA</strong></em> ayuda a encontrar <em><strong>patrones significativos en los datos</strong></em>.</p></li>
<li><p><em><strong>EDA</strong></em> proporciona una <em><strong>visión en profundidad de los conjuntos de datos</strong></em> para resolver nuestros problemas de negocio.</p></li>
<li><p><em><strong>EDA</strong></em> proporciona una pista para <em><strong>imputar los valores que faltan en el conjunto de datos</strong></em>.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Resumen</span> <span class="pre">estadístico</span></code></strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;chas&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;chas&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">object</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>crim</th>
      <td>506.0</td>
      <td>3.613524</td>
      <td>8.601545</td>
      <td>0.00632</td>
      <td>0.082045</td>
      <td>0.25651</td>
      <td>3.677083</td>
      <td>88.9762</td>
    </tr>
    <tr>
      <th>zn</th>
      <td>506.0</td>
      <td>11.363636</td>
      <td>23.322453</td>
      <td>0.00000</td>
      <td>0.000000</td>
      <td>0.00000</td>
      <td>12.500000</td>
      <td>100.0000</td>
    </tr>
    <tr>
      <th>indus</th>
      <td>506.0</td>
      <td>11.136779</td>
      <td>6.860353</td>
      <td>0.46000</td>
      <td>5.190000</td>
      <td>9.69000</td>
      <td>18.100000</td>
      <td>27.7400</td>
    </tr>
    <tr>
      <th>nox</th>
      <td>506.0</td>
      <td>0.554695</td>
      <td>0.115878</td>
      <td>0.38500</td>
      <td>0.449000</td>
      <td>0.53800</td>
      <td>0.624000</td>
      <td>0.8710</td>
    </tr>
    <tr>
      <th>rm</th>
      <td>506.0</td>
      <td>6.284634</td>
      <td>0.702617</td>
      <td>3.56100</td>
      <td>5.885500</td>
      <td>6.20850</td>
      <td>6.623500</td>
      <td>8.7800</td>
    </tr>
    <tr>
      <th>age</th>
      <td>506.0</td>
      <td>68.574901</td>
      <td>28.148861</td>
      <td>2.90000</td>
      <td>45.025000</td>
      <td>77.50000</td>
      <td>94.075000</td>
      <td>100.0000</td>
    </tr>
    <tr>
      <th>dis</th>
      <td>506.0</td>
      <td>3.795043</td>
      <td>2.105710</td>
      <td>1.12960</td>
      <td>2.100175</td>
      <td>3.20745</td>
      <td>5.188425</td>
      <td>12.1265</td>
    </tr>
    <tr>
      <th>rad</th>
      <td>506.0</td>
      <td>9.549407</td>
      <td>8.707259</td>
      <td>1.00000</td>
      <td>4.000000</td>
      <td>5.00000</td>
      <td>24.000000</td>
      <td>24.0000</td>
    </tr>
    <tr>
      <th>tax</th>
      <td>506.0</td>
      <td>408.237154</td>
      <td>168.537116</td>
      <td>187.00000</td>
      <td>279.000000</td>
      <td>330.00000</td>
      <td>666.000000</td>
      <td>711.0000</td>
    </tr>
    <tr>
      <th>ptratio</th>
      <td>506.0</td>
      <td>18.455534</td>
      <td>2.164946</td>
      <td>12.60000</td>
      <td>17.400000</td>
      <td>19.05000</td>
      <td>20.200000</td>
      <td>22.0000</td>
    </tr>
    <tr>
      <th>black</th>
      <td>506.0</td>
      <td>356.674032</td>
      <td>91.294864</td>
      <td>0.32000</td>
      <td>375.377500</td>
      <td>391.44000</td>
      <td>396.225000</td>
      <td>396.9000</td>
    </tr>
    <tr>
      <th>lstat</th>
      <td>506.0</td>
      <td>12.653063</td>
      <td>7.141062</td>
      <td>1.73000</td>
      <td>6.950000</td>
      <td>11.36000</td>
      <td>16.955000</td>
      <td>37.9700</td>
    </tr>
    <tr>
      <th>medv</th>
      <td>506.0</td>
      <td>22.532806</td>
      <td>9.197104</td>
      <td>5.00000</td>
      <td>17.025000</td>
      <td>21.20000</td>
      <td>25.000000</td>
      <td>50.0000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cat_cols</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;object&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span>
<span class="n">num_cols</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">number</span><span class="p">)</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Categorical Variables:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cat_cols</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Numerical Variables:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">num_cols</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Categorical Variables:
Index([&#39;chas&#39;], dtype=&#39;object&#39;)
Numerical Variables:
[&#39;crim&#39;, &#39;zn&#39;, &#39;indus&#39;, &#39;nox&#39;, &#39;rm&#39;, &#39;age&#39;, &#39;dis&#39;, &#39;rad&#39;, &#39;tax&#39;, &#39;ptratio&#39;, &#39;black&#39;, &#39;lstat&#39;, &#39;medv&#39;]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Análisis</span> <span class="pre">univariado</span></code></strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">kurtosis</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Asimetría</span></code></strong></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">skew</span> <span class="pre">=</span> <span class="pre">0</span></code>: Distribución <em><strong>simétrica</strong></em> (valores aceptables <em><strong>skew</strong></em><span class="math notranslate nohighlight">\(\in(-1, 1)\)</span>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">skew</span> <span class="pre">&gt;</span> <span class="pre">0</span></code>: Mayor peso en la cola izquierda de la distribución (<em><strong>sesgo positivo</strong></em>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">skew</span> <span class="pre">&lt;</span> <span class="pre">0</span></code>: Mayor peso en la cola derecha de la distribución (<em><strong>sesgo negativo</strong></em>).</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Kurtosis</span></code></strong>: Determina si una distribución tiene <em><strong>colas gruesas con respecto a la distribución normal</strong></em>. Proporciona información sobre la <em><strong>forma de una distribución de frecuencias</strong></em>.</p>
<ul>
<li><p><em><strong>kurtosis=3</strong></em>: se denomina <em><strong>mesocúrtica</strong></em> (distribución normal).</p></li>
<li><p><em><strong>kurtosis&lt;3</strong></em>: se denomina <em><strong>platicúrtica (distribución con colas menos gruesas que la normal)</strong></em>.</p></li>
<li><p><em><strong>kurtosis&gt;3</strong></em>: se denomina <em><strong>leptocúrtica (distribución con colas más gruesas que la normal)</strong></em> y significa que trata de <em><strong>producir más valores atípicos que la distribución normal</strong></em>.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Variables</span> <span class="pre">numéricas</span></code></strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">num_cols</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Column: &#39;</span><span class="p">,</span> <span class="n">col</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Skew:&#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">skew</span><span class="p">(),</span> <span class="mi">2</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Kurtosis: &#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">kurtosis</span><span class="p">(),</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">data</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">grid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  crim
Skew: 5.22
Kurtosis:  37.13
</pre></div>
</div>
<img alt="_images/f57c99ee4a80524dedb3a827eb0532b4e366dfd68a2be5c5214770fc9b90312b.png" src="_images/f57c99ee4a80524dedb3a827eb0532b4e366dfd68a2be5c5214770fc9b90312b.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  zn
Skew: 2.23
Kurtosis:  4.03
</pre></div>
</div>
<img alt="_images/9499ec2afb11f86a40dc65542451823c436abc067741adef4b2a244c197aedfa.png" src="_images/9499ec2afb11f86a40dc65542451823c436abc067741adef4b2a244c197aedfa.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  indus
Skew: 0.3
Kurtosis:  -1.23
</pre></div>
</div>
<img alt="_images/7ca7c15defa0649036a8c0ff4d1ab74c702f3bbaafbb23a32c337b99b8b10b26.png" src="_images/7ca7c15defa0649036a8c0ff4d1ab74c702f3bbaafbb23a32c337b99b8b10b26.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  nox
Skew: 0.73
Kurtosis:  -0.06
</pre></div>
</div>
<img alt="_images/0da124f08b23a3c2e6cc04c40fdd442269c13f7c94e712a4e0100c35f64b7402.png" src="_images/0da124f08b23a3c2e6cc04c40fdd442269c13f7c94e712a4e0100c35f64b7402.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  rm
Skew: 0.4
Kurtosis:  1.89
</pre></div>
</div>
<img alt="_images/812d13e56f6f6a0f903e1c5d6b6a6d7145963961d9319d2795d0795def9d7fec.png" src="_images/812d13e56f6f6a0f903e1c5d6b6a6d7145963961d9319d2795d0795def9d7fec.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  age
Skew: -0.6
Kurtosis:  -0.97
</pre></div>
</div>
<img alt="_images/d55ff077079f55879b161dc9abbc9ea9f6d260fe6608bcd51c97015289983075.png" src="_images/d55ff077079f55879b161dc9abbc9ea9f6d260fe6608bcd51c97015289983075.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  dis
Skew: 1.01
Kurtosis:  0.49
</pre></div>
</div>
<img alt="_images/2a6d245d4d3bddf63e52a95342cd20be35c2e32afb8ed1679472a73e069c3f10.png" src="_images/2a6d245d4d3bddf63e52a95342cd20be35c2e32afb8ed1679472a73e069c3f10.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  rad
Skew: 1.0
Kurtosis:  -0.87
</pre></div>
</div>
<img alt="_images/731121365781bb51f0cf6812901e58acfe8907eb041e7e28241858ceba84dcef.png" src="_images/731121365781bb51f0cf6812901e58acfe8907eb041e7e28241858ceba84dcef.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  tax
Skew: 0.67
Kurtosis:  -1.14
</pre></div>
</div>
<img alt="_images/f41d5814a3421c3843565a9e4069e7b43cddddcca5cb9af6ba80edf5aef9d0e1.png" src="_images/f41d5814a3421c3843565a9e4069e7b43cddddcca5cb9af6ba80edf5aef9d0e1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  ptratio
Skew: -0.8
Kurtosis:  -0.29
</pre></div>
</div>
<img alt="_images/0e6d3a644c0956dfd7b22c1a697cffa924e42bfab94628a918b4208f2f1faa12.png" src="_images/0e6d3a644c0956dfd7b22c1a697cffa924e42bfab94628a918b4208f2f1faa12.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  black
Skew: -2.89
Kurtosis:  7.23
</pre></div>
</div>
<img alt="_images/afee426106bd44c84670f53600b445dc94d322a8302f11895e73e7637912d4b5.png" src="_images/afee426106bd44c84670f53600b445dc94d322a8302f11895e73e7637912d4b5.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  lstat
Skew: 0.91
Kurtosis:  0.49
</pre></div>
</div>
<img alt="_images/3218b467d2832e6ba1cec1a843d55b036b773f2deda325d436d280d7c4a64f88.png" src="_images/3218b467d2832e6ba1cec1a843d55b036b773f2deda325d436d280d7c4a64f88.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Column:  medv
Skew: 1.11
Kurtosis:  1.5
</pre></div>
</div>
<img alt="_images/a8beaafdc8ed5afab0b5460dc07cfbba478b95a3b2c841cd7490523800a95719.png" src="_images/a8beaafdc8ed5afab0b5460dc07cfbba478b95a3b2c841cd7490523800a95719.png" />
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Variables</span> <span class="pre">categóricas</span></code></strong>: Usando <em><strong>diagramas de barras</strong></em> representamos la variable dummy del <em><strong>Río Charles (= 1 si el tramo limita con el río; 0 en caso contrario)</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matplotlib</span><span class="o">.</span><span class="n">rc_file_defaults</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="s1">&#39;chas&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">order</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;chas&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/931c70e626e8ec7040cbdc7dbd99699da391300e56057543797443924ba3ba8a.png" src="_images/931c70e626e8ec7040cbdc7dbd99699da391300e56057543797443924ba3ba8a.png" />
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Transformación</span> <span class="pre">de</span> <span class="pre">datos:</span></code></strong> Las variables por ejemplo <code class="docutils literal notranslate"><span class="pre">crime</span></code> y <code class="docutils literal notranslate"><span class="pre">black</span></code>, por ejemplo, están muy sesgadas y en una escala mayor. Hagamos una <em><strong>transformación logarítmica</strong></em>. La transformación logarítmica <em><strong>puede ayudar en la normalización</strong></em>, por lo que esta variable puede <em><strong>mantener la escala estándar con otras variables</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_transform</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">col</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">colname</span> <span class="ow">in</span> <span class="n">col</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">colname</span><span class="p">]</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="n">data</span><span class="p">[</span><span class="n">colname</span> <span class="o">+</span> <span class="s1">&#39;_log&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">colname</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data</span><span class="p">[</span><span class="n">colname</span> <span class="o">+</span> <span class="s1">&#39;_log&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">colname</span><span class="p">])</span>
    <span class="n">data</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_transform</span><span class="p">(</span><span class="n">data</span><span class="p">,[</span><span class="s1">&#39;crim&#39;</span><span class="p">,</span><span class="s1">&#39;black&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Index: 506 entries, 1 to 506
Data columns (total 16 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   crim       506 non-null    float64
 1   zn         506 non-null    float64
 2   indus      506 non-null    float64
 3   chas       506 non-null    object 
 4   nox        506 non-null    float64
 5   rm         506 non-null    float64
 6   age        506 non-null    float64
 7   dis        506 non-null    float64
 8   rad        506 non-null    int64  
 9   tax        506 non-null    int64  
 10  ptratio    506 non-null    float64
 11  black      506 non-null    float64
 12  lstat      506 non-null    float64
 13  medv       506 non-null    float64
 14  crim_log   506 non-null    float64
 15  black_log  506 non-null    float64
dtypes: float64(13), int64(2), object(1)
memory usage: 67.2+ KB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;crim_log&quot;</span><span class="p">],</span> <span class="n">axlabel</span><span class="o">=</span><span class="s2">&quot;crim_log&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/83ac1b8f312931b484bdcb074af6e3e085583d2bcb7dc7d8e3f1946a014b8059.png" src="_images/83ac1b8f312931b484bdcb074af6e3e085583d2bcb7dc7d8e3f1946a014b8059.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;black_log&quot;</span><span class="p">],</span> <span class="n">axlabel</span><span class="o">=</span><span class="s2">&quot;black_log&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ce3af9636303f02522d389721c5c21a35eb22cf3540b8720cb55c478086747a8.png" src="_images/ce3af9636303f02522d389721c5c21a35eb22cf3540b8720cb55c478086747a8.png" />
</div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Análisis</span> <span class="pre">bivariado</span></code></strong>: Pasemos ahora al análisis bivariado. El <em><strong>análisis bivariado ayuda a comprender cómo se relacionan las variables entre sí</strong></em> y la <em><strong>relación entre las variables dependientes e independientes</strong></em> presentes en el conjunto de datos. Puede utilizar el siguiente comando para <em><strong>visualizar todos los scatter plots, para las posibles relaciones</strong></em>:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;chas&#39;</span><span class="p">,</span> <span class="s1">&#39;black_log&#39;</span><span class="p">,</span> <span class="s1">&#39;crim_log&#39;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">));</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scatter_regplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">strx</span><span class="p">,</span> <span class="n">stry</span><span class="p">):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.4</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">strx</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">stry</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">strx</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">stry</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Relación entre </span><span class="si">%s</span><span class="s1"> y medv&#39;</span><span class="o">%</span><span class="k">col</span>)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_cols</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;medv&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">num_cols</span><span class="p">:</span>
    <span class="n">scatter_regplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="s1">&#39;medv&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f2f6ae9426c22426869e817e2a8a015be7d8906643de63ee294095f82222da54.png" src="_images/f2f6ae9426c22426869e817e2a8a015be7d8906643de63ee294095f82222da54.png" />
<img alt="_images/db8db4f5421d068509548ba05838f6678fc5a992a6d43c33d58880cbd57cd7a5.png" src="_images/db8db4f5421d068509548ba05838f6678fc5a992a6d43c33d58880cbd57cd7a5.png" />
<img alt="_images/fb6da0e82a3d4efc2e9fa5f7918c649e8e7624d22c5e51e9b8dcf316a1cb4e64.png" src="_images/fb6da0e82a3d4efc2e9fa5f7918c649e8e7624d22c5e51e9b8dcf316a1cb4e64.png" />
<img alt="_images/31f1f32d5042d09a8871e595494ee00e3d10fe0ea239025a535b75a3d4caa5bd.png" src="_images/31f1f32d5042d09a8871e595494ee00e3d10fe0ea239025a535b75a3d4caa5bd.png" />
<img alt="_images/f2368441040e09ff4e79c28ac901c9d1205ce67552c802b8df7faae0ffce84da.png" src="_images/f2368441040e09ff4e79c28ac901c9d1205ce67552c802b8df7faae0ffce84da.png" />
<img alt="_images/1fb5d5f281920b7b452516ba0a3dad644496a2cb83b674164165913b7571a33d.png" src="_images/1fb5d5f281920b7b452516ba0a3dad644496a2cb83b674164165913b7571a33d.png" />
<img alt="_images/3e114e52ade7a9a75ed44ffbb99e76f81d5b179718c64fd669c1cc2fad8134ce.png" src="_images/3e114e52ade7a9a75ed44ffbb99e76f81d5b179718c64fd669c1cc2fad8134ce.png" />
<img alt="_images/4816d28fcd9ae45fa24ea6c8a16f6b8733464b666f10e2de4a5e3b63280bf248.png" src="_images/4816d28fcd9ae45fa24ea6c8a16f6b8733464b666f10e2de4a5e3b63280bf248.png" />
<img alt="_images/f137b6e624f72ae42815d5d3ed4fe7c5ca502b7274c12257dc87e77e451b972d.png" src="_images/f137b6e624f72ae42815d5d3ed4fe7c5ca502b7274c12257dc87e77e451b972d.png" />
<img alt="_images/4106ede7a36477fa48159d2a242222b1c289e7729e3f465e04f5315bcafb3e32.png" src="_images/4106ede7a36477fa48159d2a242222b1c289e7729e3f465e04f5315bcafb3e32.png" />
<img alt="_images/63d65c9c114a5de3d74d08d5caf6a3d46433d632f703e6f5af28378cda70af10.png" src="_images/63d65c9c114a5de3d74d08d5caf6a3d46433d632f703e6f5af28378cda70af10.png" />
<img alt="_images/5d60f49a92f8747b444b1f0141c3ab3c101fc5648728e0d406307ab82cf7c0e5.png" src="_images/5d60f49a92f8747b444b1f0141c3ab3c101fc5648728e0d406307ab82cf7c0e5.png" />
</div>
</div>
<ul class="simple">
<li><p>Un <em><strong>mapa de calor</strong></em> se utiliza ampliamente para este tipo de análisis. El mapa de calor <em><strong>muestra la correlación entre las variables, ya sea positiva o negativa</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">matplotlib</span><span class="o">.</span><span class="n">rc_file_defaults</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;chas&#39;</span><span class="p">,</span> <span class="s1">&#39;black_log&#39;</span><span class="p">,</span> <span class="s1">&#39;crim_log&#39;</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">annot</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">vmin</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/aa5a722f974aa3e4c2fe2006930b18cb2be4c827a528987eb5d1db6c8bd332e8.png" src="_images/aa5a722f974aa3e4c2fe2006930b18cb2be4c827a528987eb5d1db6c8bd332e8.png" />
</div>
</div>
<ul class="simple">
<li><p>A manera de ejemplo, <em><strong>utilizaremos el dataset mglearn.datasets.load_extended_boston() de las 104 características</strong></em> resultantes de las 13 características originales junto con las <em><strong>91 combinaciones posibles de dos características dentro de esas 13</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_extended_boston</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Al comparar los <code class="docutils literal notranslate"><span class="pre">score</span></code> de los conjuntos de entrenamiento y de prueba, comprobamos que <em><strong>predecimos con mucha precisión en el conjunto de entrenamiento, pero el</strong></em> <span class="math notranslate nohighlight">\(R^2\)</span> <em><strong>en el conjunto de prueba es mucho peor</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.95
Test set score: 0.61
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Esta <em><strong>discrepancia entre el rendimiento en el conjunto de entrenamiento y el conjunto de prueba</strong></em> es un claro signo de <em><strong>overfitting</strong></em>, y por lo tanto, debemos tratar de <em><strong>encontrar un modelo que nos permita controlar la complejidad</strong></em>. Usualmente, en este tipo de casos utilizamos <em><strong>técnicas de regularización</strong></em>. Una de las alternativas más utilizadas a la regresión lineal estándar es la <em><strong>regresión ridge</strong></em>, que estudiaremos a continuación.</p></li>
</ul>
</section>
</section>
<section id="regresion-ridge">
<h2><span class="section-number">3.4. </span>Regresión ridge<a class="headerlink" href="#regresion-ridge" title="Link to this heading">#</a></h2>
<section id="analisis">
<h3><span class="section-number">3.4.1. </span>Análisis<a class="headerlink" href="#analisis" title="Link to this heading">#</a></h3>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>En la regresión ridge, los coeficientes <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> se eligen no sólo para que predigan bien en los datos de entrenamiento, sino que también, para que se ajusten a una <em><strong>restricción</strong></em> adicional. <em><strong>La regresión Ridge regulariza la regresión lineal imponiendo una penalización al tamaño de los coeficientes</strong></em>.</p></li>
<li><p><em><strong>La magnitud de los coeficientes se considera lo más pequeña posible</strong></em>; en otras palabras, todas las entradas de <span class="math notranslate nohighlight">\(\boldsymbol{\hat{\beta}}\)</span> deben ser cercanas a cero. Intuitivamente, esto significa que <em><strong>cada característica debe tener el menor efecto posible sobre el resultado</strong></em> (lo que se traduce en tener una pendiente pequeña), <em><strong>sin dejar de predecir bien</strong></em>. Esta restricción es un ejemplo de lo que se llama <em><strong>regularización</strong></em>.</p></li>
<li><p>La <em><strong>regularización</strong></em> consiste en <em><strong>restringir explícitamente un modelo para evitar el overfitting</strong></em>. El tipo particular utilizado por la <em><strong>regresión ridge</strong></em> se conoce como <em><strong>regularización</strong></em> <span class="math notranslate nohighlight">\(L^2\)</span>.</p></li>
</ul>
</div>
<figure class="align-default" id="ridge-lasso-ilustration-fig">
<a class="reference internal image-reference" href="_images/ridge_lasso_ilustration.jpg"><img alt="_images/ridge_lasso_ilustration.jpg" src="_images/ridge_lasso_ilustration.jpg" style="width: 623.5px; height: 259.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3.1 </span><span class="caption-text">Ilustration del score de predicción en el conjunto de test para <em><strong>linear regression, ridge</strong></em> y <em><strong>lasso</strong></em>.</span><a class="headerlink" href="#ridge-lasso-ilustration-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong><code class="docutils literal notranslate"><span class="pre">Formulación</span></code></strong></p>
<ul>
<li><p>Consideremos el <em><strong>modelo de regresión lineal</strong></em></p>
<div class="math notranslate nohighlight" id="equation-linear-model-comp">
<span class="eqno">(3.3)<a class="headerlink" href="#equation-linear-model-comp" title="Link to this equation">#</a></span>\[
    y_{i}=\beta_{0}+\beta_{1}\cdot x_{i1}+\beta_{2}\cdot x_{i2}+\cdots+\beta_{p}\cdot x_{ip}+\varepsilon_{i},~i = 1,2,\dots, n,
    \]</div>
<p>basado en los datos observados <span class="math notranslate nohighlight">\(\{(y_{i}, x_{i1}, x_{i2},\dots, x_{ip}):~i=1,2,\dots,n\}\)</span> para la variable respuesta <span class="math notranslate nohighlight">\(y\)</span> y <span class="math notranslate nohighlight">\(p\)</span> variables predictoras <span class="math notranslate nohighlight">\(\boldsymbol{x}=(x_{1}, x_{2},\dots,x_{p})\)</span>. La <em><strong>regresión ridge</strong></em> propuesta por <span id="id1">[<a class="reference internal" href="biblio.html#id63" title="Arthur E Hoerl and Robert W Kennard. Ridge regression: biased estimation for nonorthogonal problems. Technometrics, 12(1):55–67, 1970.">Hoerl and Kennard, 1970</a>]</span> es un método para <em><strong>evitar la inestabilidad de las estimaciones en los modelos de regresión lineal, causada por la multicolinealidad</strong></em>, esto es, correlación alta entre más de dos variables predictoras.</p>
</li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Problema de multicolinealidad</p>
<ul class="simple">
<li><p>Cuando las variables predictoras están correlacionadas, esto indica que los <em><strong>cambios en una variable están asociados a cambios en otra</strong></em>. Cuanto más fuerte sea la correlación, más difícil será cambiar una variable sin cambiar otra.</p></li>
<li><p><em><strong>Resulta difícil para el modelo estimar la relación entre cada variable predictora y la variable respuesta de forma independiente</strong></em> porque las variables predictoras tienden a cambiar al unísono.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Este método es una regularización, en la que la <em><strong>suma de cuadrados de los coeficientes de regresión, excluyendo el intercepto, es el término de penalización</strong></em>, y las estimaciones de coeficientes de regresión se obtienen de la siguiente manera.</p></li>
</ul>
<ul class="simple">
<li><p>En primer lugar, obtenemos la <em><strong>media</strong></em> <span class="math notranslate nohighlight">\(\bar x_{j}=n^{-1}\sum_{i=1}^{n}x_{ij}\)</span> y la <em><strong>varianza</strong></em> <span class="math notranslate nohighlight">\(s_{j}^2=n^{-1}\sum_{i=1}^{n}(x_{ij}-\bar x_{j})^{2}\)</span>, <span class="math notranslate nohighlight">\(j=1,2,\dots,p\)</span> de los datos para las variables predictoras y <em><strong>estandaricemos</strong></em> los datos de la siguiente manera</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-estand-ridge-reg">
<span class="eqno">(3.4)<a class="headerlink" href="#equation-estand-ridge-reg" title="Link to this equation">#</a></span>\[
z_{ij}=\frac{x_{ij}-\bar x_{j}}{s_{j}},~i=1,2,\dots,n,~j=1,2,\dots,p.
\]</div>
<div class="dropdown admonition">
<p class="admonition-title">¿Por qué estandarizar?</p>
<ul class="simple">
<li><p>La <em><strong>regresión ridge</strong></em> regulariza la <em><strong>regresión lineal</strong></em> imponiendo una <em><strong>penalización al tamaño de los coeficientes</strong></em>. Así, los coeficientes se contraen hacia cero y entre sí. Cuando esto ocurre, si las <em><strong>variables independientes no tienen la misma escala, la contracción no es justa</strong></em>.</p></li>
<li><p>Dos variables independientes con <em><strong>diferentes escalas tendrán diferentes contribuciones a los términos penalizados</strong></em>, porque el término penalizado es una suma de cuadrados de todos los coeficientes. Para evitar este tipo de problemas, muy a menudo, las <em><strong>variables independientes se centran y se escalan</strong></em> para que tengan varianza unitaria.</p></li>
</ul>
</div>
<ul>
<li><p>El modelo de <em><strong>regresión lineal basado en los datos estandarizados</strong></em> puede expresarse entonces como</p>
<div class="math notranslate nohighlight" id="equation-eq-reg-li-z">
<span class="eqno">(3.5)<a class="headerlink" href="#equation-eq-reg-li-z" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    y_{i} &amp;= \beta_{0} + \beta_{1}\bar x_{1} + \beta_{2}\bar x_{2} + \cdots + \beta_{p}\bar x_{p} + 
    \beta_{1}^{\star}z_{i1} + \beta_{2}^{\star}z_{i2} + \cdots + \beta_{p}^{\star}z_{ip} + \varepsilon_{i}\\
    &amp;=\beta_{0}^{\star} + \beta_{1}^{\star}z_{i1} + \beta_{2}^{\star}z_{i2} + \cdots + \beta_{p}^{\star}z_{ip} + \varepsilon_{i},~ i = 1,2,\dots,n,
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\beta_{0}^{\star}=\beta_{0}+\beta_{1}\bar x_{1}+\beta_{2}\bar x_{2}+\cdots+\beta_{p}\bar x_{p}~\text{y}~\beta_{j}^{\star}=s_{j}\beta_{j}\)</span>. Por lo tanto, podemos expresar el <em><strong>modelo de regresión lineal basados en los datos estandarizados</strong></em> para la variable predictora como</p>
<div class="math notranslate nohighlight" id="equation-reg-ridge-model">
<span class="eqno">(3.6)<a class="headerlink" href="#equation-reg-ridge-model" title="Link to this equation">#</a></span>\[
    \boldsymbol{y}=\beta_{0}^{\star}\boldsymbol{1}+Z\boldsymbol{\beta}_{s}+\boldsymbol{\varepsilon},
    \]</div>
<p>en <em><strong>forma matricial</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{pmatrix}
    y_{1}\\
    y_{2}\\
    \vdots\\
    y_{i}\\
    \vdots\\
    y_{n}
    \end{pmatrix}
    =
    \beta_{0}^{\star}
    \begin{pmatrix}
    1\\
    1\\
    \vdots\\
    1\\
    \vdots\\
    1
    \end{pmatrix}
    +
    \begin{pmatrix}
    z_{11} &amp; z_{12} &amp; \cdots &amp; z_{1p}\\
    z_{21} &amp; z_{22} &amp; \cdots &amp; z_{2p}\\
    \vdots &amp; \vdots &amp; &amp; \vdots\\
    z_{31} &amp; z_{32} &amp; \cdots &amp; z_{3p}\\
    \vdots &amp; \vdots &amp; &amp; \vdots\\
    z_{n1} &amp; z_{n2} &amp; \cdots &amp; z_{np}
    \end{pmatrix}
    \begin{pmatrix}
    \beta_{0}\\[2mm]
    \beta_{1}\\[2mm]
    \beta_{2}\\
    \vdots\\[2mm]
    \beta_{p}
    \end{pmatrix}
    +
    \begin{pmatrix}
    \varepsilon_{1}\\
    \varepsilon_{2}\\
    \vdots\\
    \varepsilon_{i}\\
    \vdots\\
    \varepsilon_{n}
    \end{pmatrix}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\boldsymbol{1}\)</span> es un vector <span class="math notranslate nohighlight">\(n\)</span>-dimensional de unos, <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{s}=(s_{1}\beta_{1}, s_{2}\beta_{2},\dots,s_{p}\beta_{p})^{T}\)</span>, y <span class="math notranslate nohighlight">\(Z\)</span> es una matriz de <span class="math notranslate nohighlight">\(n\times p\)</span> que contiene los datos estandarizados <span class="math notranslate nohighlight">\(z_{ij}=(x_{ij}-\bar x_{j})/s_{j},~ i=1,2,\dots,n; j=1,2,\dots,p\)</span> en su posición <span class="math notranslate nohighlight">\((i,j)\)</span>.</p>
</li>
</ul>
<ul>
<li><p>El estimador <em><strong>ridge</strong></em> para el vector de coeficientes esta dado entonces por <em><strong>minimización</strong></em> de:</p>
<div class="math notranslate nohighlight" id="equation-ridge-coeff-stand">
<span class="eqno">(3.7)<a class="headerlink" href="#equation-ridge-coeff-stand" title="Link to this equation">#</a></span>\[
    S_{\lambda}(\beta_{0}^{\star}, \boldsymbol{\beta}_{s})=(\boldsymbol{y}-\beta_{0}^{\star}\boldsymbol{1}-Z\boldsymbol{\beta}_{s})^{T}(\boldsymbol{y}-\beta_{0}^{\star}\boldsymbol{1}-Z\boldsymbol{\beta}_{s})+\lambda\boldsymbol{\beta}_{s}^{T}\boldsymbol{\beta}_{s}
    \]</div>
<p>donde el <em><strong>término de regularización</strong></em> <span class="math notranslate nohighlight">\(L^2\)</span>: (<span class="math notranslate nohighlight">\(\lambda\boldsymbol{\beta}_{s}^{T}\boldsymbol{\beta}_{s}\)</span>) con <em><strong>parámetro de regularización</strong></em> <span class="math notranslate nohighlight">\(\lambda\)</span> ha sido agregado al <em><strong>vector de coeficientes de regresión, excepto el intercepto</strong></em>. Este término es conocido como <code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">decay</span></code>. Veamos una simulación de la <em><strong>regularización ridge</strong></em> usando a la <a class="reference external" href="https://es.wikipedia.org/wiki/Matriz_de_Hilbert">matriz de Hilbert</a> como input y un <em><strong>vector unitario</strong></em>, como output.</p>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_alphas</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_alphas</span><span class="p">)</span>

<span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">coefs</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Ridge coefficients as a function of the regularization&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;tight&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f50d6aed9c507611edfa6c4cf66752632889d8ada237dbf33c3b87747ad6ee21.png" src="_images/f50d6aed9c507611edfa6c4cf66752632889d8ada237dbf33c3b87747ad6ee21.png" />
</div>
</div>
<p>La Eq. <a class="reference internal" href="#equation-ridge-coeff-stand">(3.7)</a> puede <em><strong>reescribirse</strong></em> de la siguiente forma al <em><strong>desarrollar los productos asociados y aplicar propiedades de la transpuesta</strong></em></p>
<div class="math notranslate nohighlight">
\[
S_{\lambda}(\beta_{0}^{\star}, \boldsymbol{\beta}_{s})=\boldsymbol{y}^{T}\boldsymbol{y}-2\boldsymbol{y}^{T}\beta_{0}^{\star}\boldsymbol{1}-2\boldsymbol{y}^{T}Z\boldsymbol{\beta}_{s}+2\beta_{0}\textcolor{red}{\boldsymbol{1}^{T}Z}\boldsymbol{\beta}_{s}+n\beta_{0}^{\star^{2}}+\boldsymbol{\beta}_{s}^{T}Z^{T}Z\boldsymbol{\beta}_{s}+\lambda\boldsymbol{\beta}_{s}^{T}\boldsymbol{\beta}_{s}
\]</div>
<p>Diferenciando con respecto a <span class="math notranslate nohighlight">\(\beta_{0}^{\star}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{s}\)</span> para resolver el <em><strong>problema de minimización</strong></em>, obtenemos las siguientes ecuaciones:</p>
<div class="math notranslate nohighlight" id="equation-eq-betaz">
<span class="eqno">(3.8)<a class="headerlink" href="#equation-eq-betaz" title="Link to this equation">#</a></span>\[
\begin{align*}
\frac{\partial S_{\lambda}(\beta_{0}^{\star}, \boldsymbol{\beta}_{s})}{\partial\beta_{0}^{\star}}&amp;=-2n\overline{y}+2n\beta_{0}^{\star}=0
\end{align*}
\]</div>
<p>Nótese que <span class="math notranslate nohighlight">\(Z^{T}\boldsymbol{1}=\boldsymbol{1}^{T}Z=0\)</span> <em><strong>(verifíquelo)</strong></em>. Para el caso de la derivada parcial con respecto a <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{s}\)</span> se requiere antes, estudiar la <em><strong>derivada de una forma cuadrática</strong></em> de la forma <span class="math notranslate nohighlight">\(\boldsymbol{x}^{T}A\boldsymbol{x}\)</span>. Nótese que</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{x}^{T}A\boldsymbol{x}&amp;=(x_{1}, x_{2},\dots, x_{p})
\begin{pmatrix}
A_{11} &amp; A_{12} &amp; \cdots &amp; A_{1p}\\
A_{21} &amp; A_{22} &amp; \cdots &amp; A_{2p}\\
\vdots\\
A_{31} &amp; A_{32} &amp; \cdots &amp; A_{3p}\\
\vdots\\
A_{p1} &amp; A_{p2} &amp; \cdots &amp; A_{pp}
\end{pmatrix}
\begin{pmatrix}
x_{1}\\
x_{2}\\
\vdots\\
x_{p}
\end{pmatrix}
% &amp;=\left(\sum_{i=1}^{p}x_{i}A_{i1}, \sum_{i=1}^{p}x_{i}A_{i2},\dots, \sum_{i=1}^{p}x_{i}A_{ip}\right)
% \begin{pmatrix}
% x_{1}\\
% x_{2}\\
% \vdots\\
% x_{p}
% \end{pmatrix}\\
=\sum_{j=1}^{p}\left(\sum_{i=1}^{p}x_{i}A_{ij}\right)x_{j}
\end{align*}
\end{split}\]</div>
<p>Derivando con respecto a <span class="math notranslate nohighlight">\(x_{k}\)</span> para obtener la <span class="math notranslate nohighlight">\(k\)</span><em><strong>-ésima componente del gradiente</strong></em> <span class="math notranslate nohighlight">\(\nabla_{\boldsymbol{x}}(\boldsymbol{x}^{T}A\boldsymbol{x})\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial(\boldsymbol{x}^{T}A\boldsymbol{x})}{\partial x_{k}}&amp;=
\frac{\partial}{\partial x_{k}}\left[\sum_{j=1}^{p}\left(\sum_{i=1}^{p}x_{i}A_{ij}\right)x_{j}\right]\\
&amp;=\sum_{j=1}^{p}\left(\sum_{i=1}^{p}\frac{\partial x_{i}}{\partial x_{k}}A_{ij}\right)x_{j}+
\sum_{j=1}^{p}\frac{\partial x_{j}}{\partial x_{k}}\left(\sum_{i=1}^{p}x_{i}A_{ij}\right)\\
&amp;=\sum_{j=1}^{p}A_{kj}x_{j}+\sum_{i=1}^{p}x_{i}A_{ik}
\end{align*}
\end{split}\]</div>
<p>Por lo tanto, para <span class="math notranslate nohighlight">\(k=1,2,\dots,p\)</span>, <em><strong>bajo el supuesto de simetría</strong></em> para <span class="math notranslate nohighlight">\(A\)</span> se tiene que</p>
<div class="math notranslate nohighlight" id="equation-quadratic-form-dedrivate">
<span class="eqno">(3.9)<a class="headerlink" href="#equation-quadratic-form-dedrivate" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\frac{\partial(\boldsymbol{x}^{T}A\boldsymbol{x})}{\partial\boldsymbol{x}}&amp;=
\begin{pmatrix}
\displaystyle{\sum_{j=1}^{p}A_{1j}x_{j}+\sum_{i=1}^{p}x_{i}A_{i1}}\\
\displaystyle{\sum_{j=1}^{p}A_{2j}x_{j}+\sum_{i=1}^{p}x_{i}A_{i2}}\\
\vdots\\
\displaystyle{\sum_{j=1}^{p}A_{pj}x_{j}+\sum_{i=1}^{p}x_{i}A_{ip}}
\end{pmatrix}\\
&amp;=A\boldsymbol{x}+A^{T}\boldsymbol{x}\\
&amp;=(A+A^{T})\boldsymbol{x}\\
&amp;=2A\boldsymbol{x}
\end{align*}
\end{split}\]</div>
<p><em><strong>Nótese que</strong></em> <span class="math notranslate nohighlight">\(A:=Z^{T}Z\)</span>, <em><strong>es simétrica</strong></em>, en efecto: <span class="math notranslate nohighlight">\(A^{T}=(Z^{T}Z)^{T}=Z^{T}(Z^{T})^{T}=Z^{T}Z=A\)</span>, entonces <span class="math notranslate nohighlight">\(\partial_{\boldsymbol{\beta}_{s}}(\boldsymbol{\beta}_{s}^{T}Z^{T}Z\boldsymbol{\beta}_{s})=2Z^{T}Z\boldsymbol{\beta}_{s}\)</span>, por lo tanto</p>
<div class="math notranslate nohighlight" id="equation-eq-betas">
<span class="eqno">(3.10)<a class="headerlink" href="#equation-eq-betas" title="Link to this equation">#</a></span>\[
\begin{align*}
\frac{\partial S_{\lambda}(\beta_{0}^{\star},\boldsymbol{\beta}_{s})}{\partial\boldsymbol{\beta}_{s}}=-2Z^{T}\boldsymbol{y}+2Z^{T}Z\boldsymbol{\beta}_{s}+2\lambda\boldsymbol{\beta}_{s}=\boldsymbol{0}
\end{align*}
\]</div>
<p><em><strong>Resolviendo las ecuaciones</strong></em> <a class="reference internal" href="#equation-eq-betaz">(3.8)</a> y <a class="reference internal" href="#equation-eq-betas">(3.10)</a> para <span class="math notranslate nohighlight">\(\beta_{0}^{\star}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{s}\)</span>, se tienen <em><strong>estimadores ridge</strong></em> para el modelo de regresión <a class="reference internal" href="#equation-eq-reg-li-z">(3.5)</a></p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_{0}^{\star}=\overline{y}\quad\text{y}\quad\hat{\boldsymbol{\beta}}_{s}=(Z^{T}Z+\lambda I_{p})^{-1}Z^{T}\boldsymbol{y}
\]</div>
<p>Dado que <span class="math notranslate nohighlight">\(\beta_{0}^{\star}=\beta_{0}+\beta_{1}\bar x_{1}+\beta_{2}\bar x_{2}+\cdots+\beta_{p}\bar x_{p}\)</span> <em><strong>usando la estimación obtenida</strong></em> <span class="math notranslate nohighlight">\(\hat{\beta}_{0}^{\star}\)</span> se tiene que</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_{0}=\overline{y}-\hat{\beta}_{1}\bar x_{1}-\hat{\beta}_{2}\bar x_{2}-\cdots-\hat{\beta}_{p}\bar x_{p}.
\]</div>
<p>Además, la <em><strong>estimación ridge del vector de coeficientes de regresión</strong></em>, está dada separadamente por la <em><strong>minimización de la función</strong></em></p>
<div class="math notranslate nohighlight">
\[
S_{\lambda}(\boldsymbol{\beta}_{s})=(\boldsymbol{y}-Z\boldsymbol{\beta}_{s})^{T}(\boldsymbol{y}-Z\boldsymbol{\beta}_{s})+\lambda\boldsymbol{\beta}_{s}^{T}\boldsymbol{\beta}_{s}.
\]</div>
<p><em><strong>En efecto</strong></em>, para obtener <em><strong>estimación ridge del vector de coeficientes</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_{s}=(\hat{\beta}_{1}, \hat{\beta}_{2},\dots, \hat{\beta}_{p})\)</span>, primero, nótese que al reemplazar <span class="math notranslate nohighlight">\(\hat{\beta}_{0}^{\star}=\overline{y}\)</span> en Eq. <a class="reference internal" href="#equation-reg-ridge-model">(3.6)</a> se tiene que <span class="math notranslate nohighlight">\(y=\overline{y}\boldsymbol{1}+Z\boldsymbol{\beta}_{s}+\varepsilon\)</span>, entonces <span class="math notranslate nohighlight">\(\boldsymbol{y}-\overline{y}\boldsymbol{1}\)</span> esta <em><strong>centrando los datos en relación a la variable respuesta</strong></em>.</p>
<p><em><strong>Estandarizando las variables predictoras y respuesta</strong></em> en nuestro modelo inicial Ecuación <a class="reference internal" href="#equation-linear-model-comp">(3.3)</a>, mediante <span class="math notranslate nohighlight">\(y_{i}-\overline{y}\)</span> y <span class="math notranslate nohighlight">\((x_{ij}-\bar x_{j})/s_{j}\)</span> se tienen las siguientes igualdades, <em><strong>verifiquelas</strong></em></p>
<div class="math notranslate nohighlight" id="equation-ridge-normalization">
<span class="eqno">(3.11)<a class="headerlink" href="#equation-ridge-normalization" title="Link to this equation">#</a></span>\[
\sum_{i=1}^{n}y_{i}=0,\quad\sum_{i=1}^{n}x_{ij}=0,~j=1,2,\dots,p,\quad\sum_{i=1}^{n}x_{ij}^{2}=n
\]</div>
<p>Entonces,</p>
<div class="math notranslate nohighlight">
\[
\beta_{0}^{\star}=\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_{i}=0.
\]</div>
<p><em><strong>En virtud de la implicación</strong></em> de estas igualdades con respecto al parámetro <span class="math notranslate nohighlight">\(\beta_{0}^{\star}\)</span> y la Ecuación <a class="reference internal" href="#equation-eq-reg-li-z">(3.5)</a>, podemos <em><strong>considerar sin perdida de generalidad</strong></em>, el modelo de regresión</p>
<div class="math notranslate nohighlight">
\[
y=X\boldsymbol{\beta}+\boldsymbol{\varepsilon},
\]</div>
<p>donde <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{n\times p},~\boldsymbol{\beta}\in\mathbb{R}^{p},~E(\boldsymbol{\varepsilon})=0\)</span> y <span class="math notranslate nohighlight">\(\textrm{cov}(\boldsymbol{\varepsilon})=\sigma^2\boldsymbol{I}\)</span>.</p>
<p>Por lo tanto <em><strong>minimizando el operador</strong></em> <span class="math notranslate nohighlight">\(S_{\lambda}(\boldsymbol{\beta})=(y-X\boldsymbol{\beta})^{T}(y-X\boldsymbol{\beta})+\lambda\boldsymbol{\beta}^{T}\boldsymbol{\beta}\)</span>, de <em><strong>forma análoga al procedimiento de optimización para</strong></em> Eq. <a class="reference internal" href="#equation-ridge-coeff-stand">(3.7)</a>, obtenemos el estimador de ridge:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}}_{R}=(X^{T}X+\lambda\boldsymbol{I}_{p})^{-1}X^{T}\boldsymbol{y}.
\]</div>
<div class="admonition-propiedades-del-estimador-ridge admonition" id="prop-ridge-estimador">
<p class="admonition-title">Propiedades del estimador ridge</p>
<p>El <em><strong>estimador ridge</strong></em> satisface las siguientes propiedades:</p>
<div class="math notranslate nohighlight" id="equation-ridge-props">
<span class="eqno">(3.12)<a class="headerlink" href="#equation-ridge-props" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align}
\text{E}(\hat{\boldsymbol{\beta}}_{R})&amp;=(X^{T}X+\lambda\boldsymbol{I}_{p})^{-1}X^{T}X\boldsymbol{\beta}\\
\text{Cov}(\boldsymbol{\beta}_{R})&amp;=\sigma^{2}(X^{T}X+\lambda\boldsymbol{I}_{p})^{-1}X^{T}X(X^{T}X+\lambda\boldsymbol{I}_{p})^{-1}\\
\text{E}(\hat{\boldsymbol{\beta}}_{R}-\boldsymbol{\beta})&amp;=-\lambda(X^{T}X+\lambda\boldsymbol{I}_{p})^{-1}\boldsymbol{\beta}\\
\text{E}[(\hat{\boldsymbol{\beta}}_{R}-\boldsymbol{\beta})^{T}(\hat{\boldsymbol{\beta}}_{R}-\boldsymbol{\beta})]&amp;=\displaystyle{\sigma^{2}\sum_{j=1}^{p}\frac{l_{j}}{(l_{j}+\lambda)^{2}}+\lambda^{2}\boldsymbol{\beta}^{T}(X^{T}X+\lambda\boldsymbol{I}_{p})^{-2}\boldsymbol{\beta}},
\end{align}
\end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(l_{1}, l_{2},\dots, l_{p}\)</span> son los <em><strong>autovalores ordenados</strong></em> de <span class="math notranslate nohighlight">\(X^{T}X\)</span>. El primer término del lado derecho de la última ecuación en <a class="reference internal" href="#equation-ridge-props">(3.12)</a> representa la suma de las <em><strong>varianzas de los componentes del estimador ridge</strong></em>, y el segundo término es el <em><strong>cuadrado del sesgo</strong></em>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Ejercicio para el lector</p>
<p>Queda como <em><strong>ejercicio para el lector verificar las Propiedades</strong></em> <a class="reference internal" href="#equation-ridge-props">(3.12)</a> del <em><strong>estimador ridge</strong></em>. <em>Sugerencia: Revisar el texto de Sadanori Konishi</em>, <em><strong>Introduction to Multivariate Analysis: Linear and Nonlinear Modeling</strong></em> <span id="id2">[<a class="reference internal" href="biblio.html#id20" title="S. Konishi. Introduction to Multivariate Analysis: Linear and Nonlinear Modeling. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis, 2014. ISBN 9781466567283. URL: https://books.google.com.co/books?id=fcuuAwAAQBAJ.">Konishi, 2014</a>]</span>.</p>
</div>
</section>
<section id="implementacion">
<h3><span class="section-number">3.4.2. </span>Implementación<a class="headerlink" href="#implementacion" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>La <em><strong>regresión ridge</strong></em> se implementa en <code class="docutils literal notranslate"><span class="pre">linear_model.Ridge</span></code>. Veamos qué tal lo hace en el conjunto de datos ampliado de <code class="docutils literal notranslate"><span class="pre">Boston</span> <span class="pre">Housing</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.89
Test set score: 0.75
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Como puede ver, <em><strong>el score en el conjunto de entrenamiento de Ridge es menor que el de la regresión lineal</strong></em>, cuyos scores fueron: <strong>Training set score: 0.95 and Test set score: 0.61</strong>. Además, <em><strong>la puntuación en el conjunto de prueba es mayor</strong></em>. En este caso, <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, usa <code class="docutils literal notranslate"><span class="pre">alpha=1.0</span></code> como parámetro por default (ver <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">sklearn.linear_model.Ridge</a>). Esto es coherente con nuestras expectativas.</p></li>
<li><p>Con la regresión, nos ajustamos demasiado a los datos. <em><strong>Ridge es un modelo más restringido, por lo que existe menos probabilidad de overfitting</strong></em>. Un modelo <em><strong>menos complejo</strong></em> significa un <em><strong>peor rendimiento en el conjunto de de entrenamiento, pero una mejor generalización</strong></em>. Como sólo nos interesa el rendimiento de la generalización, deberíamos elegir el modelo Ridge en lugar del modelo de regresión lineal.</p></li>
</ul>
<ul class="simple">
<li><p>El modelo <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> hace un <em><strong>balance entre la simplicidad del modelo (coeficientes casi nulos) y su rendimiento en el conjunto de entrenamiento</strong></em>. La importancia que el modelo da a la simplicidad frente al rendimiento del conjunto de entrenamiento, puede ser <em><strong>especificada por el usuario</strong></em>, utilizando el parámetro <code class="docutils literal notranslate"><span class="pre">alpha</span></code>.</p></li>
<li><p>En el ejemplo anterior, hemos utilizado el parámetro por defecto <code class="docutils literal notranslate"><span class="pre">alpha=1.0</span></code>. Sin embargo, no hay ninguna razón por la que este nos dió la mejor compensación. El ajuste óptimo de <code class="docutils literal notranslate"><span class="pre">alpha</span></code> depende del conjunto de datos concreto que estemos utilizando.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Aumentar <code class="docutils literal notranslate"><span class="pre">alpha</span></code> obliga a los coeficientes a acercarse más a cero, lo que <em><strong>disminuye el rendimiento del conjunto de entrenamiento, pero puede ayudar a mejorar la generalización</strong></em>.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge10</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge10</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge10</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.79
Test set score: 0.64
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>La disminución de <code class="docutils literal notranslate"><span class="pre">alpha</span></code> permite que los coeficientes estén menos restringidos. <em><strong>Para valores muy pequeños de alpha, los coeficientes apenas están restringidos, y terminamos con un modelo que se parece a LinearRegression</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge01</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge01</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ridge01</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.93
Test set score: 0.77
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Observe cómo el parámetro <code class="docutils literal notranslate"><span class="pre">alpha</span></code> se corresponde con la complejidad del modelo. Discutiremos los <em><strong>métodos para seleccionar adecuadamente los parámetros</strong></em> en el capítulo de <em><strong>evaluación de modelos</strong></em>. También podemos obtener una visión más cualitativa de cómo el parámetro <code class="docutils literal notranslate"><span class="pre">alpha</span></code> cambia el modelo, inspeccionando el atributo <code class="docutils literal notranslate"><span class="pre">coef_</span></code> de los modelos con diferentes valores de <code class="docutils literal notranslate"><span class="pre">alpha</span></code>.</p></li>
<li><p>Un <code class="docutils literal notranslate"><span class="pre">alpha</span></code> más alto significa un modelo más restringido, por lo que esperamos que las entradas de <code class="docutils literal notranslate"><span class="pre">coef_</span></code> tengan una magnitud menor.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ridge alpha=1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge10</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ridge alpha=10&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge01</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ridge alpha=0.1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;LinearRegression&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Coefficient index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Coefficient magnitude&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6201dcc7554169547eeb452d97f5da5b94b893384a9f7ad0e4a9d472bf882f44.png" src="_images/6201dcc7554169547eeb452d97f5da5b94b893384a9f7ad0e4a9d472bf882f44.png" />
</div>
</div>
<ul class="simple">
<li><p>Aquí, el eje <span class="math notranslate nohighlight">\(x\)</span> enumera las entradas de <code class="docutils literal notranslate"><span class="pre">coef_</span></code>. <span class="math notranslate nohighlight">\(x=0\)</span> muestra el <em><strong>coeficiente asociado a la primera característica</strong></em>, <span class="math notranslate nohighlight">\(x=1\)</span> el <em><strong>coeficiente asociado a la segunda característica</strong></em>, y así sucesivamente hasta <span class="math notranslate nohighlight">\(x=100\)</span>. El eje <span class="math notranslate nohighlight">\(y\)</span> muestra los <em><strong>valores numéricos de los valores correspondientes de los coeficientes</strong></em>. La principal conclusión es que para <code class="docutils literal notranslate"><span class="pre">alpha=10</span></code>, los coeficientes se sitúan en su mayoría entre -3 y 3.</p></li>
<li><p>Los coeficientes del modelo <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> con <code class="docutils literal notranslate"><span class="pre">alpha=1</span></code> son algo mayores. Los puntos correspondientes a <code class="docutils literal notranslate"><span class="pre">alpha=0,1</span></code> tienen una magnitud aún mayor, y muchos de los puntos correspondientes a la <em><strong>regresión lineal sin ninguna regularización</strong></em> (que sería <code class="docutils literal notranslate"><span class="pre">alpha=0</span></code>), son tan grandes que quedan fuera del gráfico.</p></li>
<li><p>Otra forma de entender la influencia de la regularización es fijar un valor de <code class="docutils literal notranslate"><span class="pre">alpha</span></code> pero variando la cantidad de datos de entrenamiento disponibles. Si submuestreamos el conjunto de datos de <code class="docutils literal notranslate"><span class="pre">Boston</span> <span class="pre">Housing</span></code> y evaluamos <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> y <code class="docutils literal notranslate"><span class="pre">Ridge(alpha=1)</span></code> en subconjuntos de tamaño creciente, obtenemos la siguiente <em><strong>curva de aprendizaje</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_ridge_n_samples</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/28eb4daa31d5fe05ea1ca41537ec212aeaa2df773b983131c17905ab35438690.png" src="_images/28eb4daa31d5fe05ea1ca41537ec212aeaa2df773b983131c17905ab35438690.png" />
</div>
</div>
<ul class="simple">
<li><p>Como era de esperarse, la <em><strong>puntuación de entrenamiento es mayor que la de prueba para todos los tamaños de conjuntos de datos</strong></em>, tanto para la <em><strong>regresión lineal</strong></em> como para la <em><strong>ridge</strong></em>. Debido a que la <em><strong>regresión ridge está regularizada, la puntuación de entrenamiento es inferior a la de la regresión lineal en todos los casos</strong></em>.</p></li>
<li><p>La puntuación de la prueba de la regresión ridge es mejor, en particular, para los subconjuntos pequeños de datos. <em><strong>Para menos de 400 puntos de datos, la regresión lineal no es capaz de aprender nada</strong></em>. <em><strong>A medida que el modelo dispone de más datos, ambos modelos mejoran, y la regresión lineal alcanza a la ridge.</strong></em></p></li>
</ul>
</section>
</section>
<section id="lasso">
<h2><span class="section-number">3.5. </span>Lasso<a class="headerlink" href="#lasso" title="Link to this heading">#</a></h2>
<p><strong><code class="docutils literal notranslate"><span class="pre">Observación</span></code></strong></p>
<ul class="simple">
<li><p>Una alternativa a la <em><strong>regresión ridge</strong></em> para regularizar la <em><strong>regresión lineal</strong></em> es la <em><strong>regresión lasso</strong></em>. Al igual que con la regresión <em><strong>ridge</strong></em>, el uso de <em><strong>lasso</strong></em> también <em><strong>restringe los coeficientes para que sean cercanos a cero, pero de una forma ligeramente diferente, llamada regularización</strong></em> <span class="math notranslate nohighlight">\(L^1\)</span>.</p></li>
<li><p>La consecuencia de la regularización <span class="math notranslate nohighlight">\(L^1\)</span> es que <em><strong>cuando se utiliza lasso, algunos coeficientes son exactamente cero</strong></em>. Esto significa que, <em><strong>algunas características son totalmente ignoradas por el modelo</strong></em>. Esto puede verse como una forma de <strong><code class="docutils literal notranslate"><span class="pre">selección</span> <span class="pre">automática</span> <span class="pre">de</span> <span class="pre">características</span></code></strong>.</p></li>
<li><p>El hecho de que algunos coeficientes sean exactamente cero a menudo hace que un <em><strong>modelo sea más fácil de interpretar, y puede revelar las características más importantes de un modelo</strong></em>.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Formulación</span></code></strong></p>
<p>El método <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> es un método de estimación de parámetros, mediante la <em><strong>minimización de la siguiente función objetivo</strong></em>, que impone la suma de valores absolutos (normas <span class="math notranslate nohighlight">\(L^{1}\)</span>) de los coeficientes de regresión como una <em><strong>restricción (penalización) a la suma de errores al cuadrado</strong></em>:</p>
<div class="math notranslate nohighlight">
\[
S_{\lambda}(\boldsymbol{\beta})=(y-X\boldsymbol{\beta})^{T}(y-X\boldsymbol{\beta})+\lambda\sum_{i=1}^{p}|\beta_{j}|,
\]</div>
<p>donde los datos observados están normalizados como en la Ecuación <a class="reference internal" href="#equation-ridge-normalization">(3.11)</a>. A diferencia de la contracción de los coeficientes de regresión hacia cero, que se produce en la regresión de ridge, <em><strong>lasso da lugar a una estimación exactamente igual a cero para algunos de los coeficientes.</strong></em></p>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Una ventaja de la regresión <em><strong>ridge</strong></em> es que si <span class="math notranslate nohighlight">\(p &lt; n\)</span> (<em><strong>número de variables predictoras menor que el número de observaciones</strong></em>), entonces con una selección adecuada del parámetro de regularización <span class="math notranslate nohighlight">\(\lambda\)</span>, <em><strong>es posible obtener estimaciones estables de los coeficientes de regresión, incluso en casos que impliquen multicolinealidad entre las variables predictoras</strong></em> o en los que <span class="math notranslate nohighlight">\(X^{T}X\)</span> es aproximadamente singular para la matriz de diseño <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>Sin embargo, debido a que, a diferencia de <em><strong>lasso</strong></em>, la regresión <em><strong>ridge no puede producir estimaciones exactamente iguales a cero</strong></em>, entonces, la regresión <em><strong>ridge no puede utilizarse como método de selección de variables</strong></em>.</p></li>
</ul>
</div>
<figure class="align-center" id="fig-ridge-lasso">
<a class="reference internal image-reference" href="_images/ridge_lasso.png"><img alt="_images/ridge_lasso.png" src="_images/ridge_lasso.png" style="width: 618.75px; height: 377.7px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3.2 </span><span class="caption-text">Estimación ridge (izquierda) y estimación lasso (derecha).</span><a class="headerlink" href="#fig-ridge-lasso" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>La diferencia entre la estimación lasso y la estimación ridge puede demostrarse, para simplificar, para el caso de sólo dos variables predictoras <span class="math notranslate nohighlight">\(x_{1}\)</span> y <span class="math notranslate nohighlight">\(x_{2}\)</span>. <em><strong>En la estimación ridge, la solución se basa en la restricción</strong></em> <span class="math notranslate nohighlight">\(\beta_{1}^{2}+\beta_{2}^{2}\leq c_{1}\)</span> de minimizar</p>
<div class="math notranslate nohighlight">
\[
S(\beta_{1}, \beta_{2})=\sum_{i=1}^{n}\left(y_{i}-\sum_{j=1}^{2}\beta_{j}x_{ij}\right)^{2},
\]</div>
<p>para datos centrados, mientras que <em><strong>la estimación lasso se basa en la restricción</strong></em> <span class="math notranslate nohighlight">\(|\beta_{1}|+|\beta_{2}|\leq c_{2}\)</span>.</p>
<ul class="simple">
<li><p>Dado que la <em><strong>estimación por mínimos cuadrados es la solución que minimiza</strong></em> <span class="math notranslate nohighlight">\(S(\beta_{1}, \beta_{2})\)</span>, <em><strong>esta se produce en el centro de una elipse</strong></em>. Sin embargo, como se muestra en la <a class="reference internal" href="#fig-ridge-lasso"><span class="std std-numref">Fig. 3.2</span></a>, las soluciones que satisfacen las restricciones en las <em><strong>estimaciones ridge se encuentran en regiones diferentes de las que satisfacen las restricciones en las estimaciones lasso</strong></em>.</p></li>
<li><p>La diferencia esencial entre la estimación ridge y la estimación lasso como se muestra en la <a class="reference internal" href="#fig-ridge-lasso"><span class="std std-numref">Fig. 3.2</span></a>, es que <em><strong>la estimación ridge reduce todas las estimaciones del coeficiente de regresión hacia, pero no exactamente cero</strong></em>, en relación con las correspondientes a los mínimos cuadrados, mientras que <em><strong>la estimación lasso localiza algunas de las estimaciones del coeficiente de regresión exactamente iguales a cero</strong></em>.</p></li>
<li><p>Debido a su característica de reducir algunos coeficientes a exactamente cero, <em><strong>lasso también puede utilizarse para la selección de variables en modelos a gran escala con muchas variables predictoras</strong></em>, para las que el parámetro de regularización <span class="math notranslate nohighlight">\(\lambda\)</span> afecta al grado de de esparcimiento de la solución.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Ejercicio para el lector</p>
<p>Queda como <em><strong>ejercicio para el lector, encontrar parámetros de estimación lasso</strong></em>, tal como se realizó en el caso de la regresión ridge. Se sugiere investigar sobre el <em><strong>algoritmo LARS (Least Angle Regression) de Efron et al. (2004)</strong></em>.</p>
</div>
</section>
<section id="aplicacion-regresion-lasso">
<h2><span class="section-number">3.6. </span>Aplicación: Regresión Lasso<a class="headerlink" href="#aplicacion-regresion-lasso" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Apliquemos <code class="docutils literal notranslate"><span class="pre">lasso</span></code> al conjunto de datos ampliado de <code class="docutils literal notranslate"><span class="pre">Boston</span> <span class="pre">Housing</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of features used: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.29
Test set score: 0.21
Number of features used: 4
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Como se puede ver, <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> lo hace bastante mal, tanto en el conjunto de entrenamiento como en el de prueba. Esto indica <em><strong>underfitting</strong></em>, pero, nótese que sólo utilizó 4 de las 105 características (<em><strong>feature selection</strong></em>). De forma similar a <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> también tiene un parámetro de regularización, <code class="docutils literal notranslate"><span class="pre">alpha</span></code>, que <em><strong>controla la fuerza con la que los coeficientes son empujados hacia cero</strong></em>.</p></li>
<li><p>En el ejemplo anterior, utilizamos el valor por defecto de <code class="docutils literal notranslate"><span class="pre">alpha=1.0</span></code>. Para reducir <em><strong>underfitting</strong></em>, intentemos disminuir <code class="docutils literal notranslate"><span class="pre">alpha</span></code>. Cuando hacemos esto, también necesitamos aumentar el ajuste por defecto de <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> (<em><strong>número máximo de iteraciones a ejecutar</strong></em>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lasso001</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of features used: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lasso001</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.90
Test set score: 0.77
Number of features used: 33
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><em><strong>Un alpha más bajo nos permitió ajustar un modelo más complejo</strong></em>, que funcionó mejor en los datos de entrenamiento y de prueba. El rendimiento es ligeramente mejor que utilizando <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, y <em><strong>estamos utilizando solo 33 de las 105 características</strong></em>.</p></li>
<li><p>Esto hace que este modelo sea potencialmente más fácil de entender. Sin embargo, <em><strong>si fijamos alpha demasiado bajo, volvemos a eliminar el efecto de la regularización y acabamos en overfitting</strong></em>, con un resultado similar al de <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lasso00001</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso00001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lasso00001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of features used: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lasso00001</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.95
Test set score: 0.64
Number of features used: 96
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Una vez más, podemos <em><strong>trazar los coeficientes de los diferentes modelos</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Lasso alpha=1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso001</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Lasso alpha=0.01&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso00001</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Lasso alpha=0.0001&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge01</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ridge alpha=0.1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Coefficient index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Coefficient magnitude&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c0c7e6b3fd341734dee6870102148608e29ddf4070208f0631699d16d1fed149.png" src="_images/c0c7e6b3fd341734dee6870102148608e29ddf4070208f0631699d16d1fed149.png" />
</div>
</div>
<ul class="simple">
<li><p>Para <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">1</span></code>, no sólo vemos que la mayoría de los coeficientes son cero (algo que ya sabíamos), sino que los coeficientes restantes también son de pequeña magnitud. Disminuyendo <em><strong>alpha a 0.01 , obtenemos la solución mostrada en triángulos salmones, que hace que la mayoría de las características sean exactamente cero</strong></em>.</p></li>
<li><p>Utilizando <em><strong>alpha = 0.0001, obtenemos un modelo bastante poco regularizado, con la mayoría de los coeficientes no nulos y de gran magnitud</strong></em>. A modo de comparación, la mejor solución <code class="docutils literal notranslate"><span class="pre">Ridge</span></code> se muestra con puntos rojos. El modelo <em><strong>Ridge con alpha = 0.1 tiene un rendimiento predictivo similar al del modelo lasso con alpha = 0.01, pero utilizando Ridge, todos los coeficientes son distintos de cero</strong></em>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>En la práctica, la regresión <em><strong>Ridge suele ser la primera opción entre estos dos modelos</strong></em>. Sin embargo, <em><strong>si tiene una gran cantidad de características y espera que sólo unas pocas sean importantes, Lasso podría ser una mejor opción</strong></em>. Del mismo modo, si desea tener un modelo que es fácil de interpretar, <em><strong>Lasso proporcionará un modelo que es más fácil de entender, ya que seleccionará sólo un subconjunto de las características de entrada</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> también proporciona la clase <code class="docutils literal notranslate"><span class="pre">ElasticNet</span></code>, que combina las penalizaciones de <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> y <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>. En la práctica, esta combinación funciona mejor, aunque al precio de tener dos parámetros que ajustar: uno para la regularización <span class="math notranslate nohighlight">\(L^1\)</span>, y otro para la regularización <span class="math notranslate nohighlight">\(L^2\)</span>.</p></li>
</ul>
</section>
<section id="modelos-lineales-para-clasificacion">
<h2><span class="section-number">3.7. </span>Modelos lineales para clasificación<a class="headerlink" href="#modelos-lineales-para-clasificacion" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><em><strong>Los modelos lineales también se utilizan ampliamente para la clasificación</strong></em>. Veamos primero la <em><strong>clasificación binaria</strong></em>. En este caso, la predicción se realiza mediante la siguiente fórmula</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-linear-class">
<span class="eqno">(3.13)<a class="headerlink" href="#equation-linear-class" title="Link to this equation">#</a></span>\[
\hat{y}=\beta_{0}+\beta_{1}\cdot x_{1}+\beta_{2}\cdot x_{2}+\cdots+\beta_{p}\cdot x_{p}
\]</div>
<ul class="simple">
<li><p>La fórmula es muy similar a la de la regresión lineal, pero <em><strong>en lugar de devolver simplemente la suma ponderada de las características, ponemos un umbral al valor predicho en cero</strong></em>. Si la función es menor que cero, predecimos la clase -1; si es mayor que cero, predecimos la clase +1. Esta regla de predicción es común a todos los modelos lineales de clasificación. De nuevo, hay muchas formas diferentes de encontrar los coeficientes <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p></li>
<li><p>En los modelos lineales de regresión, la salida, <span class="math notranslate nohighlight">\(\hat{y}\)</span>, <em><strong>es una función lineal de las características</strong></em>: una <em><strong>línea</strong></em>, un <em><strong>plano</strong></em> o un <em><strong>hiperplano</strong></em>. En los modelos lineales de clasificación, la frontera de decisión es una función lineal de la entrada. En otras palabras, un <em><strong>clasificador lineal (binario) es un clasificador que separa dos clases utilizando una línea, un plano o un hiperplano</strong></em>.</p></li>
<li><p>Hay muchos algoritmos para aprender modelos lineales. Todos estos algoritmos difieren en los dos aspectos siguientes:</p>
<ul>
<li><p>La forma en que miden <em><strong>que tan bien una combinación particular de coeficientes se ajusta a los datos de entrenamiento.</strong></em></p></li>
<li><p><em><strong>Si utilizan regularización, de que tipo utilizan</strong></em></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Los distintos algoritmos eligen diferentes formas de medir lo que significa <em><strong>“ajustarse bien al conjunto de entrenamiento”</strong></em>. Los dos algoritmos de clasificación lineal más comunes son la <em><strong>regresión logística</strong></em>, implementada en <code class="docutils literal notranslate"><span class="pre">linear_model.LogisticRegression</span></code>, y las <em><strong>máquinas de vectores de soporte lineales (SVMs lineales)</strong></em>, implementadas en <em><strong>svm.LinearSVC (SVC significa clasificador de vectores de soporte)</strong></em>.</p></li>
<li><p>A pesar de su nombre, <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> es un algoritmo de clasificación y no de regresión, por lo tanto no debe confundirse con <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>. Podemos aplicar los modelos <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> y <code class="docutils literal notranslate"><span class="pre">LinearSVC</span></code> al conjunto de datos <code class="docutils literal notranslate"><span class="pre">forge</span></code> y visualizar la frontera de decisión encontrada por los modelos lineales.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_forge</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">LinearSVC</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">()],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.7</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/638e795a884c760097d79b40c7bd23d5034bb0cc3fab5bc02ded072c85a97aa9.png" src="_images/638e795a884c760097d79b40c7bd23d5034bb0cc3fab5bc02ded072c85a97aa9.png" />
</div>
</div>
<ul class="simple">
<li><p>En esta figura, tenemos la primera característica del conjunto de datos <code class="docutils literal notranslate"><span class="pre">forge</span></code> en el eje <span class="math notranslate nohighlight">\(x\)</span> y la segunda característica en el eje <span class="math notranslate nohighlight">\(y\)</span>, como antes. Se muestran las <em><strong>fronteras de decisión encontrados por LinearSVC y LogisticRegression</strong></em> respectivamente como <em><strong>líneas rectas, separando el área clasificada como clase 1 en la parte superior, del área clasificada como clase 0 en la parte inferior</strong></em>.</p></li>
<li><p>En otras palabras, cualquier nuevo punto de datos que se encuentre por encima de la línea negra será clasificado como clase 1 por el clasificador respectivo, mientras que cualquier punto que se encuentre por debajo de la línea negra será clasificado como clase 0. Los dos modelos entregan fronteras de decisión similares. <em><strong>Obsérvese que ambos clasifican erróneamente dos de los puntos</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p><em><strong>Por defecto, ambos modelos aplican una regularización</strong></em> <span class="math notranslate nohighlight">\(L^{2}\)</span>, <em><strong>de la misma manera que lo hace Ridge para la regresión</strong></em>. Para <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> y <code class="docutils literal notranslate"><span class="pre">LinearSVC</span></code> el parámetro de compensación que determina la fuerza de la regularización se llama <code class="docutils literal notranslate"><span class="pre">C</span></code>, y <em><strong>los valores más altos de C corresponden a menor regularización</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>Hay otro aspecto interesante de cómo actúa el parámetro <code class="docutils literal notranslate"><span class="pre">C</span></code>. El uso de <em><strong>valores bajos de C harán que los algoritmos traten de ajustarse a la “mayoría” de los puntos de datos</strong></em>, mientras que <em><strong>el uso de valores más altos de C enfatiza la importancia de que cada punto de datos individual sea clasificado correctamente</strong></em>. Veamos una ilustración utilizando <code class="docutils literal notranslate"><span class="pre">LinearSVC</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>En otras palabras, cuando se utiliza un valor alto para el parámetro <code class="docutils literal notranslate"><span class="pre">C</span></code>, <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> y <code class="docutils literal notranslate"><span class="pre">LinearSVC</span></code> intentan ajustarse al conjunto de entrenamiento lo mejor posible, mientras que con valores bajos del parámetro <code class="docutils literal notranslate"><span class="pre">C</span></code>, los modelos ponen más énfasis en encontrar un vector de coeficientes <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> que se acerque a cero.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_linear_svc_regularization</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f1e9bfee555c8ef83f9f65bc2ce154b22607fd77531a97fb21d1c1671b96d5d3.png" src="_images/f1e9bfee555c8ef83f9f65bc2ce154b22607fd77531a97fb21d1c1671b96d5d3.png" />
</div>
</div>
<ul class="simple">
<li><p>En el lado izquierdo, tenemos un <em><strong>valor de</strong></em> <span class="math notranslate nohighlight">\(C\)</span> <em><strong>muy pequeño</strong></em> (<span class="math notranslate nohighlight">\(C=0.01\)</span>) <em><strong>que corresponde a una gran regularización</strong></em>. La mayoría de los puntos de la clase 0 están en la parte superior, y la mayoría de los puntos de la clase 1 están en la parte inferior. <em><strong>El modelo fuertemente regularizado elige una línea relativamente horizontal, clasificando erróneamente dos puntos</strong></em>.</p></li>
<li><p>En el gráfico central, <span class="math notranslate nohighlight">\(C\)</span> es ligeramente más alto (<span class="math notranslate nohighlight">\(C=10\)</span>), y <em><strong>el modelo se centra más en las dos muestras mal clasificadas, inclinando el límite de decisión</strong></em>. Por último, en el lado derecho, correspondiente al <em><strong>valor mas alto de</strong></em> <span class="math notranslate nohighlight">\(C\)</span> (<span class="math notranslate nohighlight">\(C=1000\)</span>), <em><strong>el modelo inclina mucho mas la frontera de decisión, clasificando ahora correctamente todos los puntos de la clase 0</strong></em>. Solo <em><strong>uno de los puntos de la clase 1 sigue estando mal clasificado</strong></em>, ya que no es posible clasificar correctamente todos los puntos de este conjunto de datos utilizando una línea recta.</p></li>
<li><p>El modelo ilustrado en la parte derecha se esfuerza por clasificar correctamente todos los puntos, pero puede que no capte bien la disposición general de las clases. En otras palabras, <em><strong>es probable que el modelo ilustrado en la derecha presente overfitting</strong></em>. Al igual que en el caso de la regresión, los modelos lineales de clasificación pueden parecer muy restrictivos en espacios de baja dimensión, ya que sólo permiten límites de decisión que sean líneas rectas o planos.</p></li>
<li><p>De nuevo, <em><strong>en dimensiones altas, los modelos lineales de clasificación se vuelven muy potentes, y la protección contra el overfitting es cada vez más importante cuando se consideran más características</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>Analicemos <code class="docutils literal notranslate"><span class="pre">LinearLogistic</span></code> con más detalle en el conjunto de datos de <em><strong>cáncer de mama</strong></em>:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> 
                                                    <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> 
                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.955
Test set score: 0.951
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>El valor por defecto de <span class="math notranslate nohighlight">\(C=1\)</span> proporciona un rendimiento bastante bueno, con una <em><strong>precisión del 95% tanto en el conjunto de entrenamiento como en el de prueba</strong></em>. Si intentamos aumentar <span class="math notranslate nohighlight">\(C\)</span> obtenemos un modelo más complejo</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logreg100</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1000000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg100</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg100</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.958
Test set score: 0.958
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>El uso de <span class="math notranslate nohighlight">\(C=1000000\)</span> da lugar a una <em><strong>mayor precisión en el conjunto de entrenamiento</strong></em>. También podemos investigar qué ocurre si utilizamos un modelo aún más regularizado que el predeterminado de <span class="math notranslate nohighlight">\(C=1\)</span>, estableciendo <span class="math notranslate nohighlight">\(C=0.01\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logreg001</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logreg001</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set score: 0.934
Test set score: 0.930
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Cuando se desplaza más hacia la izquierda en la escala mostrada en la <a class="reference internal" href="supervised_intro.html#fig-sweet-spot"><span class="std std-numref">Fig. 1.3</span></a> e obtiene un <em><strong>modelo subjustado, tanto la precisión del conjunto de entrenamiento como la de la prueba disminuyen en relación con los parámetros por defecto</strong></em>. Por último, veamos los coeficientes aprendidos por los modelos con las tres configuraciones diferentes de los parámetros de regularización <span class="math notranslate nohighlight">\(C\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logreg</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;C=1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logreg100</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;C=100&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logreg001</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;C=0.001&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Coefficient index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Coefficient magnitude&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e35d06c0c4064f1468ae32a2a70dc9ea8257e050f69e231be5eb6853b35d70b5.png" src="_images/e35d06c0c4064f1468ae32a2a70dc9ea8257e050f69e231be5eb6853b35d70b5.png" />
</div>
</div>
<ul class="simple">
<li><p><em><strong>Si deseamos un modelo más interpretable, el uso de la regularización</strong></em> <span class="math notranslate nohighlight">\(L^{1}\)</span> <em><strong>podría ayudar, ya que limita el modelo a utilizar sólo unas pocas características</strong></em>. El siguiente es el gráfico de coeficientes y las precisiones de clasificación para la regularización <span class="math notranslate nohighlight">\(L^{1}\)</span></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">C</span><span class="p">,</span> <span class="n">marker</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">]):</span>
    <span class="n">lr_l1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> 
                               <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> 
                               <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training accuracy of l1 logreg with C=</span><span class="si">{:.3f}</span><span class="s2">: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">C</span><span class="p">,</span> <span class="n">lr_l1</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy of l1 logreg with C=</span><span class="si">{:.3f}</span><span class="s2">: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">C</span><span class="p">,</span> <span class="n">lr_l1</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lr_l1</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">marker</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;C=</span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">C</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Coefficient index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Coefficient magnitude&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">3</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training accuracy of l1 logreg with C=0.001: 0.91
Test accuracy of l1 logreg with C=0.001: 0.92
Training accuracy of l1 logreg with C=1.000: 0.96
Test accuracy of l1 logreg with C=1.000: 0.96
Training accuracy of l1 logreg with C=100.000: 0.99
Test accuracy of l1 logreg with C=100.000: 0.98
</pre></div>
</div>
<img alt="_images/59d1f7e7323ea6be6979c260fc0621f4ef698b29d41959e25ebd39abde03c9cb.png" src="_images/59d1f7e7323ea6be6979c260fc0621f4ef698b29d41959e25ebd39abde03c9cb.png" />
</div>
</div>
<ul class="simple">
<li><p>Como puede ver, <em><strong>hay mucha similitud entre los modelos lineales de clasificación binaria y los modelos lineales de regresión</strong></em>. Como en la regresión, la principal diferencia entre los modelos es el parámetro de penalización, que influye en la regularización, <em><strong>en si el modelo utilizará todas las características disponibles o seleccionará sólo un subconjunto.</strong></em></p></li>
</ul>
</section>
<section id="modelos-lineales-para-la-clasificacion-multiclase">
<h2><span class="section-number">3.8. </span>Modelos lineales para la clasificación multiclase<a class="headerlink" href="#modelos-lineales-para-la-clasificacion-multiclase" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Muchos modelos de clasificación lineal sólo sirven para la clasificación binaria y no se extienden de forma natural al caso multiclase (con la excepción de la <em><strong>regresión logística</strong></em>). Una técnica común para extender un algoritmo de clasificación binaria a un <em><strong>algoritmo de clasificación multiclase</strong></em> es el enfoque <code class="docutils literal notranslate"><span class="pre">one-vs.-rest</span></code>. En el enfoque <code class="docutils literal notranslate"><span class="pre">one-vs.-rest</span></code>, <em><strong>se aprende un modelo binario para cada clase fija, el cual intenta separar esa clase de todas las demás, lo cual da lugar a tantos modelos binarios como clases exista</strong></em>.</p></li>
<li><p>Para hacer una predicción, se ejecutan todos los clasificadores binarios en un punto de prueba. <em><strong>El clasificador que tenga la mayor puntuación en su clase “gana”, y esta etiqueta de clase se devuelve como predicción</strong></em>. Al tener un clasificador binario por clase, se tiene un vector de coeficientes <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> para cada clase. La clase para la que el resultado de la fórmula de clasificación dada aquí, es la más alta, es la etiqueta de clase asignada:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\beta_{0}+\beta_{1}\cdot x_{1}+\beta_{2}\cdot x_{2}+\cdots+\beta_{p}\cdot x_{p}
\]</div>
<ul class="simple">
<li><p>Las matemáticas que subyacen a la <em><strong>regresión logística multiclase</strong></em> difieren en cierta medida del enfoque de una sola clase, pero también dan como resultado un <em><strong>vector de coeficientes y un intercepto por clase, y se aplica el mismo método para hacer una predicción</strong></em>. Apliquemos el método de <em><strong>one-vs.-rest</strong></em> a un conjunto de datos de clasificación de tres clases. Utilizamos un <em><strong>conjunto de datos bidimensional, en el que cada clase viene dada por datos muestreados de una distribución gaussiana.</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Class 0&quot;</span><span class="p">,</span> <span class="s2">&quot;Class 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Class 2&quot;</span><span class="p">]);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/53c04a4c01f06a4e7490bdb5135467da9e7500ef870d821ddf339e0665e5d7e9.png" src="_images/53c04a4c01f06a4e7490bdb5135467da9e7500ef870d821ddf339e0665e5d7e9.png" />
</div>
</div>
<ul class="simple">
<li><p>Ahora, entrenamos un clasificador <code class="docutils literal notranslate"><span class="pre">LinearSVC</span></code> en el conjunto de datos</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">linear_svm</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficient shape: &quot;</span><span class="p">,</span> <span class="n">linear_svm</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept shape: &quot;</span><span class="p">,</span> <span class="n">linear_svm</span><span class="o">.</span><span class="n">intercept_</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coefficient shape:  (3, 2)
Intercept shape:  (3,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Vemos que la dimensión (<code class="docutils literal notranslate"><span class="pre">shape</span></code>) de <code class="docutils literal notranslate"><span class="pre">coef_</span></code> es <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">2)</span></code>, lo que significa que <em><strong>cada fila de coef_ contiene el vector de coeficientes para cada una de las tres clases</strong></em> y <em><strong>cada columna contiene el valor del coeficiente para cada característica específica</strong></em> (hay dos en este conjunto de datos).</p></li>
<li><p>La matriz <em><strong>intercept_ es ahora una matriz unidimensional que almacena los interceptos de cada clase</strong></em>. Visualicemos las líneas dadas por los tres clasificadores binarios. En este caso <code class="docutils literal notranslate"><span class="pre">line=x</span></code>, para el clasificador separador <code class="docutils literal notranslate"><span class="pre">ax+by+c=0</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="k">for</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">linear_svm</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">linear_svm</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">line</span> <span class="o">*</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 2&#39;</span><span class="p">,</span> <span class="s1">&#39;Line class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Line class 1&#39;</span><span class="p">,</span> 
                <span class="s1">&#39;Line class 2&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/795867808ab653a76c2223f509dce5ec7a6e1d5a705aa47c4b65654160f50229.png" src="_images/795867808ab653a76c2223f509dce5ec7a6e1d5a705aa47c4b65654160f50229.png" />
</div>
</div>
<ul class="simple">
<li><p>Se puede ver que <em><strong>todos los puntos que pertenecen a la clase 0 en los datos de entrenamiento están por encima de la línea correspondiente a la clase 0</strong></em>, lo que significa que están en el lado de la “clase 0” de este clasificador binario. Además, <em><strong>los puntos de la clase 0 están por encima de la línea correspondiente a la clase 2, lo que significa que son clasificados como “resto” por el clasificador binario de la clase 2</strong></em>.</p></li>
<li><p>Los puntos que pertenecen a la clase 0 están a la izquierda de la línea correspondiente a la clase 1, lo que significa que el clasificador binario para la clase 1 también los clasifica como “resto”. Por tanto, cualquier punto de esta zona será clasificado como clase 0 por el clasificador final (<em><strong>el resultado de la fórmula de confianza de la clasificación para el clasificador 0 es mayor que cero, mientras que es menor que cero para las otras dos clases</strong></em>).</p></li>
<li><p>Pero, <em><strong>¿qué ocurre con el triángulo del centro del gráfico? Los tres clasificadores binarios clasifican los puntos allí como “resto”</strong></em>. <em><strong>¿A qué clase se asignaría un punto allí?</strong></em> La respuesta es, la que tiene el valor más alto de la fórmula de clasificación: <em><strong>la clase de la línea más cercana</strong></em>.</p></li>
<li><p>El siguiente ejemplo muestra las <em><strong>predicciones para todas las regiones del espacio 2D de la zona</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_classification</span><span class="p">(</span><span class="n">linear_svm</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.7</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="k">for</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">linear_svm</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">linear_svm</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">line</span> <span class="o">*</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Class 2&#39;</span><span class="p">,</span> <span class="s1">&#39;Line class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Line class 1&#39;</span><span class="p">,</span>
                <span class="s1">&#39;Line class 2&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="p">(</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a178fd1d0dea0829ff2c5530800ce0ad1e9518dda25836ffea1b5e5ccf1b6c87.png" src="_images/a178fd1d0dea0829ff2c5530800ce0ad1e9518dda25836ffea1b5e5ccf1b6c87.png" />
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Puntos fuertes, puntos débiles y parámetros</p>
<ul class="simple">
<li><p>El parámetro principal de los modelos lineales es el parámetro de regularización, llamado <code class="docutils literal notranslate"><span class="pre">alpha</span></code> en los modelos de regresión y <code class="docutils literal notranslate"><span class="pre">C</span></code> en <code class="docutils literal notranslate"><span class="pre">LinearSVC</span></code> y <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code>. Los <em><strong>valores grandes de alpha o valores pequeños de C están asociados con modelos simples</strong></em>. En particular, para los modelos de regresión, el ajuste de estos parámetros es bastante importante. Normalmente, <code class="docutils literal notranslate"><span class="pre">C</span></code> y <code class="docutils literal notranslate"><span class="pre">alpha</span></code> se buscan en una escala logarítmica.</p></li>
<li><p>La otra decisión que hay que tomar es si se quiere utilizar la regularización <span class="math notranslate nohighlight">\(L^1\)</span> o la regularización <span class="math notranslate nohighlight">\(L^2\)</span>. <em><strong>Si se supone que sólo unas pocas características son realmente importantes, se debería utilizar la regularización</strong></em> <span class="math notranslate nohighlight">\(L^1\)</span>, <em><strong>de lo contrario, debería utilizar</strong></em> <span class="math notranslate nohighlight">\(L^2\)</span> <em><strong>por defecto</strong></em>. <span class="math notranslate nohighlight">\(L^1\)</span> <em><strong>también puede ser útil si la interpretabilidad del modelo es importante</strong></em>. Como <span class="math notranslate nohighlight">\(L^1\)</span> utilizará sólo unas pocas características, es más fácil explicar qué características son importantes para el modelo, y cuáles son los efectos de esas características.</p></li>
<li><p>Los <em><strong>modelos lineales son muy rápidos de entrenar y de predecir</strong></em>. Se adaptan a conjuntos de datos muy grandes y funcionan bien con datos dispersos. Si sus datos constan de cientos de miles o millones de muestras, es posible que desee investigar el uso de la opción <code class="docutils literal notranslate"><span class="pre">solver='sag'</span></code> en <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> y <code class="docutils literal notranslate"><span class="pre">Ridge</span></code>, que puede ser más rápida que la predeterminada en grandes conjuntos de datos. Otras opciones son la clase <em><strong>SGDClassifier y la clase SGDRegressor que implementan versiones aún más escalables de los modelos lineales descritos aquí</strong></em>.</p></li>
<li><p>Otro punto fuerte de los modelos lineales es que <em><strong>permiten entender con relativa facilidad cómo se realiza una predicción, utilizando las fórmulas que vimos antes para la regresión y la clasificación</strong></em>. Por desgracia, a menudo, no está del todo claro por qué los coeficientes son como son. Esto es particularmente cierto si su conjunto de datos tiene características altamente correlacionadas; en estos casos, los coeficientes pueden ser difíciles de interpretar.</p></li>
<li><p><em><strong>Los modelos lineales suelen funcionar bien cuando el número de características es grande en comparación con el número de muestras</strong></em>. También se utilizan a menudo en conjuntos de datos muy grandes, simplemente porque no es factible entrenar otros modelos. Sin embargo, <em><strong>en espacios de menor dimensión otros modelos pueden ofrecer un mejor rendimiento de generalización</strong></em>. Veremos algunos ejemplos en los que los modelos lineales fallan cuando abordemos <strong><code class="docutils literal notranslate"><span class="pre">máquinas</span> <span class="pre">de</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span> <span class="pre">kernelizadas</span></code></strong></p></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "ml_tf"
        },
        kernelOptions: {
            name: "ml_tf",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ml_tf'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="knn_model.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span><span class="math notranslate nohighlight">\(k\)</span>-vecinos más cercanos</p>
      </div>
    </a>
    <a class="right-next"
       href="bayes_model.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Clasificador Bayesiano</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-de-regresion-lineal">3.1. Modelo de regresión lineal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimos-cuadrados-ordinarios">3.2. Mínimos Cuadrados Ordinarios</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-regresion-ridge-y-ols">3.3. Aplicación: Regresión Ridge y OLS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-exploratorio-de-datos">3.3.1. Análisis Exploratorio de Datos</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-ridge">3.4. Regresión ridge</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">3.4.1. Análisis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">3.4.2. Implementación</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso">3.5. Lasso</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-regresion-lasso">3.6. Aplicación: Regresión Lasso</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-lineales-para-clasificacion">3.7. Modelos lineales para clasificación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-lineales-para-la-clasificacion-multiclase">3.8. Modelos lineales para la clasificación multiclase</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lihki Rubio, Ph.D.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>Lihki Rubio, Ph.D. All rights reserved.</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>