
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Árboles de decisión &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=530fe47d" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/custom-checkpoint.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'decisiontree_model';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/custom.js?v=14184634"></script>
    <script src="_static/.ipynb_checkpoints/custom-checkpoint.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Máquinas de vectores de soporte" href="svm_model.html" />
    <link rel="prev" title="Clasificadores Naive Bayes" href="bayes_model.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fotolihki.jpg" class="logo__image only-light" alt="Machine Learning - Home"/>
    <script>document.write(`<img src="_static/fotolihki.jpg" class="logo__image only-dark" alt="Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Profesor: Dr. Lihki Rubio
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="supervised_intro.html">Aprendizaje supervisado</a></li>
<li class="toctree-l1"><a class="reference internal" href="knn_model.html"><span class="math notranslate nohighlight">\(k\)</span>-Vecinos más cercanos</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_model.html">Modelos lineales</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes_model.html">Clasificadores Naive Bayes</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Árboles de decisión</a></li>

<li class="toctree-l1"><a class="reference internal" href="svm_model.html">Máquinas de vectores de soporte</a></li>
<li class="toctree-l1"><a class="reference internal" href="ann_model.html">Redes Neuronales y Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="practical_pca.html">Análisis de Componentes Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">Evaluación de modelos</a></li>
<li class="toctree-l1"><a class="reference internal" href="chains_pipelines.html">Cadenas de Algoritmos y Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/decisiontree_model.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Árboles de decisión</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Árboles de decisión</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combinacion-de-clasificadores">Combinación de clasificadores</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#enfoque-boosting">Enfoque Boosting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-funcion-log-loss">La función Log-Loss</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#arboles-boosting">Arboles Boosting</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">Implementación</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-de-complejidad">Control de complejidad</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-de-los-arboles-de-decision">Análisis de los árboles de decisión</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#caracteristicas-importantes-en-los-arboles">Características importantes en los árboles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensamble-de-arboles-de-decision">Ensamble de árboles de decisión</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="arboles-de-decision">
<h1>Árboles de decisión<a class="headerlink" href="#arboles-de-decision" title="Link to this heading">#</a></h1>
<section id="analisis">
<h2>Análisis<a class="headerlink" href="#analisis" title="Link to this heading">#</a></h2>
<p><strong><code class="docutils literal notranslate"><span class="pre">Observación</span></code></strong></p>
<ul class="simple">
<li><p>Los árboles de clasificación <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">basan</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">idea</span> <span class="pre">simple,</span> <span class="pre">pero</span> <span class="pre">poderosa</span></code>, y se encuentran entre las <code class="docutils literal notranslate"><span class="pre">técnicas</span> <span class="pre">más</span> <span class="pre">populares</span> <span class="pre">de</span> <span class="pre">clasificación</span></code>. Son sistemas de varias etapas, y la clasificación de un patrón en una clase se realiza de forma secuencial. <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">través</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">serie</span> <span class="pre">de</span> <span class="pre">pruebas,</span> <span class="pre">las</span> <span class="pre">clases</span> <span class="pre">se</span> <span class="pre">rechazan</span> <span class="pre">de</span> <span class="pre">forma</span> <span class="pre">secuencial</span> <span class="pre">hasta</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">llega</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">decisión</span> <span class="pre">a</span> <span class="pre">favor</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">clase</span> <span class="pre">restante</span></code>.</p></li>
<li><p>Cada una de las pruebas, cuyo resultado decide qué clases se rechazan, es de <code class="docutils literal notranslate"><span class="pre">tipo</span> <span class="pre">binario</span> <span class="pre">&quot;Sí&quot;</span> <span class="pre">o</span> <span class="pre">&quot;No&quot;</span> <span class="pre">y</span> <span class="pre">se</span> <span class="pre">aplica</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">sola</span> <span class="pre">característica</span></code>. Nuestro objetivo es presentar la filosofía principal en torno a un tipo especial de árboles conocidos como <strong><code class="docutils literal notranslate"><span class="pre">árboles</span> <span class="pre">de</span> <span class="pre">clasificación</span> <span class="pre">binarios</span> <span class="pre">ordinarios</span> <span class="pre">(OBCT)</span></code></strong>. Estos, pertenecen a una clase más general de métodos que construyen árboles, tanto para la clasificación como para la regresión, conocidos como <code class="docutils literal notranslate"><span class="pre">árboles</span> <span class="pre">de</span> <span class="pre">clasificación</span> <span class="pre">y</span> <span class="pre">regresión</span> <span class="pre">(CART)</span></code>.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Formulación</span></code></strong></p>
<ul class="simple">
<li><p>La idea básica de los <code class="docutils literal notranslate"><span class="pre">OBCT</span></code> es <code class="docutils literal notranslate"><span class="pre">dividir</span> <span class="pre">el</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">características</span> <span class="pre">en</span> <span class="pre">(hiper)rectángulos</span></code>; es decir, el espacio se divide mediante hiperplanos, que son paralelos a los ejes. Esto se ilustra en la <a class="reference internal" href="#fig-decision-hypplanes-obct"><span class="std std-numref">Fig. 9</span></a></p></li>
</ul>
<figure class="align-center" id="fig-decision-hypplanes-obct">
<a class="reference internal image-reference" href="_images/decision_hypplanes_obct.png"><img alt="_images/decision_hypplanes_obct.png" src="_images/decision_hypplanes_obct.png" style="width: 400.8px; height: 413.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">Partición de espacio de características. Árbol de clasificación (OBCT). Fuente <span id="id1">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#fig-decision-hypplanes-obct" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La partición del espacio en <code class="docutils literal notranslate"><span class="pre">(hiper)rectángulos</span></code> se realiza mediante una serie de <code class="docutils literal notranslate"><span class="pre">&quot;preguntas&quot;</span></code> de esta forma: <code class="docutils literal notranslate"><span class="pre">¿es</span> <span class="pre">el</span> <span class="pre">valor</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">característica</span></code> <span class="math notranslate nohighlight">\(x_{i} &lt; a\)</span>?. Este también se conoce como el <code class="docutils literal notranslate"><span class="pre">criterio</span> <span class="pre">de</span> <span class="pre">división</span></code>. La secuencia de preguntas puede realizarse de forma agradable mediante el uso de un árbol. La <a class="reference internal" href="#fig-decision-tree-obct"><span class="std std-numref">Fig. 10</span></a> muestra el árbol correspondiente al caso ilustrado
en la <a class="reference internal" href="#fig-decision-hypplanes-obct"><span class="std std-numref">Fig. 9</span></a>.</p></li>
</ul>
<figure class="align-center" id="fig-decision-tree-obct">
<a class="reference internal image-reference" href="_images/decision_tree_obct.png"><img alt="_images/decision_tree_obct.png" src="_images/decision_tree_obct.png" style="width: 405.59999999999997px; height: 362.4px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Árbol de clasificación (OBCT). Partición del espacio para <a class="reference internal" href="#fig-decision-hypplanes-obct"><span class="std std-numref">Fig. 9</span></a>. Fuente <span id="id2">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#fig-decision-tree-obct" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<p>Cada nodo del árbol realiza una prueba contra una característica individual y, si este no es un <code class="docutils literal notranslate"><span class="pre">nodo</span> <span class="pre">hoja</span> <span class="pre">(sin</span> <span class="pre">división</span> <span class="pre">adicional)</span></code>, este es conectado a dos <code class="docutils literal notranslate"><span class="pre">nodos</span> <span class="pre">descendientes</span> <span class="pre">(nodo</span> <span class="pre">de</span> <span class="pre">decisión)</span></code>: uno está asociado a la <code class="docutils literal notranslate"><span class="pre">respuesta</span> <span class="pre">&quot;Yes&quot;</span></code> y el otro a la <code class="docutils literal notranslate"><span class="pre">respuesta</span> <span class="pre">&quot;No&quot;</span></code>.</p>
</div>
<ul class="simple">
<li><p>Partiendo del nodo raíz, se realiza un recorrido de decisiones sucesivas hasta llegar a un nodo hoja. <code class="docutils literal notranslate"><span class="pre">Cada</span> <span class="pre">nodo</span> <span class="pre">hoja</span> <span class="pre">está</span> <span class="pre">asociado</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">única</span> <span class="pre">clase</span></code>. La <code class="docutils literal notranslate"><span class="pre">asignación</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">punto</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">clase</span> <span class="pre">se</span> <span class="pre">realiza</span> <span class="pre">según</span> <span class="pre">la</span> <span class="pre">etiqueta</span> <span class="pre">del</span> <span class="pre">nodo</span> <span class="pre">hoja</span> <span class="pre">correspondiente</span></code>. Este tipo de clasificación es conceptualmente simple y fácil de interpretar. Por ejemplo, en un <code class="docutils literal notranslate"><span class="pre">sistema</span> <span class="pre">de</span> <span class="pre">diagnóstico</span> <span class="pre">médico</span></code>, se puede empezar con una pregunta: <code class="docutils literal notranslate"><span class="pre">¿La</span> <span class="pre">temperatura</span> <span class="pre">es</span> <span class="pre">alta?</span></code> si la respuesta es afirmativa, una segunda pregunta puede ser: <code class="docutils literal notranslate"><span class="pre">¿Presenta</span> <span class="pre">moquea?</span></code>. El proceso continúa hasta que se llega a una <code class="docutils literal notranslate"><span class="pre">decisión</span> <span class="pre">final</span> <span class="pre">sobre</span> <span class="pre">la</span> <span class="pre">enfermedad</span></code>.</p></li>
<li><p>Además, los árboles son útiles para <code class="docutils literal notranslate"><span class="pre">construir</span> <span class="pre">sistemas</span> <span class="pre">de</span> <span class="pre">razonamiento</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">inteligencia</span> <span class="pre">artificial</span></code>. Por ejemplo, la existencia de objetos específicos, que se deduce a través de una serie de preguntas relacionadas, basadas en los valores de ciertas características (de alto nivel), puede conducir al <code class="docutils literal notranslate"><span class="pre">reconocimiento</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">escena</span> <span class="pre">o</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">objeto</span> <span class="pre">representado</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">imagen</span></code>.</p></li>
<li><p>Una vez desarrollado el árbol, la clasificación es sencilla. El mayor reto consiste en <code class="docutils literal notranslate"><span class="pre">construir</span> <span class="pre">el</span> <span class="pre">árbol,</span> <span class="pre">explotando</span> <span class="pre">la</span> <span class="pre">información</span> <span class="pre">que</span> <span class="pre">reside</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>. Las principales preguntas a las que uno se enfrenta al diseñar un árbol, entre otras que se discutirán más adelante, son:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">¿Qué</span> <span class="pre">criterio</span> <span class="pre">de</span> <span class="pre">división</span> <span class="pre">debe</span> <span class="pre">adoptarse?</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">¿Cuándo</span> <span class="pre">se</span> <span class="pre">debe</span> <span class="pre">detener</span> <span class="pre">el</span> <span class="pre">crecimiento</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">y</span> <span class="pre">declarar</span> <span class="pre">un</span> <span class="pre">nodo</span> <span class="pre">como</span> <span class="pre">final?</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">¿Cómo</span> <span class="pre">se</span> <span class="pre">asocia</span> <span class="pre">un</span> <span class="pre">nodo</span> <span class="pre">hoja</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">clase</span> <span class="pre">concreta?</span></code></p></li>
</ul>
</li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Criterio</span> <span class="pre">de</span> <span class="pre">división</span></code></strong>: Ya hemos dicho que las preguntas que se hacen en cada nodo son del tipo</p>
<div class="math notranslate nohighlight">
\[
\text{¿es}~x_{i} &lt; a\text{?}
\]</div>
<ul class="simple">
<li><p>El objetivo es <code class="docutils literal notranslate"><span class="pre">seleccionar</span> <span class="pre">un</span> <span class="pre">valor</span> <span class="pre">adecuado</span> <span class="pre">para</span> <span class="pre">el</span> <span class="pre">umbral</span></code> <span class="math notranslate nohighlight">\(a\)</span>. Supongamos que, partiendo del nodo raíz, el árbol ha crecido hasta el <code class="docutils literal notranslate"><span class="pre">nodo</span> <span class="pre">actual</span></code> <span class="math notranslate nohighlight">\(t\)</span>. Cada nodo, <span class="math notranslate nohighlight">\(t\)</span>, está asociado a un subconjunto <span class="math notranslate nohighlight">\(X_{t}\subseteq X\)</span> del <code class="docutils literal notranslate"><span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>, <span class="math notranslate nohighlight">\(X\)</span>. Este es el <code class="docutils literal notranslate"><span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">que</span> <span class="pre">han</span> <span class="pre">sobrevivido</span> <span class="pre">a</span> <span class="pre">este</span> <span class="pre">nodo,</span> <span class="pre">después</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">pruebas</span> <span class="pre">que</span> <span class="pre">han</span> <span class="pre">tenido</span> <span class="pre">lugar</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">nodos</span> <span class="pre">anteriores</span> <span class="pre">del</span> <span class="pre">árbol</span></code>.</p></li>
<li><p>Por ejemplo, en la <a class="reference internal" href="#fig-decision-tree-obct"><span class="std std-numref">Fig. 10</span></a>, <code class="docutils literal notranslate"><span class="pre">un</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">puntos,</span> <span class="pre">que</span> <span class="pre">pertenecen,</span> <span class="pre">digamos,</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">clase</span></code> <span class="math notranslate nohighlight">\(\omega_{1}\)</span>, <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">participarán</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">nodo</span></code> <span class="math notranslate nohighlight">\(t_{1}\)</span> porque <code class="docutils literal notranslate"><span class="pre">ya</span> <span class="pre">han</span> <span class="pre">sido</span> <span class="pre">asignados</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">nodo</span> <span class="pre">hoja</span> <span class="pre">previamente</span> <span class="pre">etiquetado</span></code>. El propósito de un criterio de división es <code class="docutils literal notranslate"><span class="pre">dividir</span></code> <span class="math notranslate nohighlight">\(X_{t}\)</span> <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">dos</span> <span class="pre">subconjuntos</span> <span class="pre">disyuntos,</span> <span class="pre">digamos</span></code> <span class="math notranslate nohighlight">\(X_{tY}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">y</span></code> <span class="math notranslate nohighlight">\(X_{tN}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">dependiendo</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">respuesta</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">pregunta</span> <span class="pre">específica</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">nodo</span></code> <span class="math notranslate nohighlight">\(t\)</span>. Para cada división, se cumple lo siguiente:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
X_{tY}\cap X_{tN}&amp;=\emptyset\\
X_{tY}\cup X_{tN}&amp;=X_{t}
\end{align}
\end{split}\]</div>
<ul class="simple">
<li><p>El objetivo en cada nodo es seleccionar qué característica se va a probar y también cuál es el mejor valor del umbral <span class="math notranslate nohighlight">\(a\)</span>. <code class="docutils literal notranslate"><span class="pre">La</span> <span class="pre">filosofía</span> <span class="pre">adoptada</span> <span class="pre">es</span> <span class="pre">hacer</span> <span class="pre">la</span> <span class="pre">elección</span> <span class="pre">de</span> <span class="pre">manera</span> <span class="pre">que</span> <span class="pre">cada</span> <span class="pre">división</span> <span class="pre">genere</span> <span class="pre">conjuntos,</span></code> <span class="math notranslate nohighlight">\(X_{tY}\)</span>,  <span class="math notranslate nohighlight">\(X_{tN}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">que</span> <span class="pre">sean</span> <span class="pre">más</span> <span class="pre">homogéneos</span> <span class="pre">en</span> <span class="pre">cuanto</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">clase</span> <span class="pre">en</span> <span class="pre">comparación</span> <span class="pre">con</span></code> <span class="math notranslate nohighlight">\(X_{t}\)</span>. En otras palabras, los datos en cada uno de los dos conjuntos descendientes deben mostrar una mayor preferencia por clases específicas, en comparación con el conjunto antecesor.</p></li>
<li><p>En la terminología adoptada, los conjuntos <span class="math notranslate nohighlight">\(X_{tY}\)</span> y <span class="math notranslate nohighlight">\(X_{tN}\)</span> <code class="docutils literal notranslate"><span class="pre">deben</span> <span class="pre">ser</span> <span class="pre">más</span> <span class="pre">puros</span> <span class="pre">en</span> <span class="pre">comparación</span> <span class="pre">con</span></code> <span class="math notranslate nohighlight">\(X_{t}\)</span> . Así pues, primero debemos <code class="docutils literal notranslate"><span class="pre">seleccionar</span> <span class="pre">un</span> <span class="pre">criterio</span> <span class="pre">que</span> <span class="pre">mida</span> <span class="pre">la</span> <span class="pre">impureza</span></code> y, a continuación, <code class="docutils literal notranslate"><span class="pre">calcular</span> <span class="pre">el</span> <span class="pre">valor</span> <span class="pre">umbral</span> <span class="pre">y</span> <span class="pre">elegir</span> <span class="pre">la</span> <span class="pre">característica</span> <span class="pre">específica</span></code> (que se va a probar) para <code class="docutils literal notranslate"><span class="pre">maximizar</span> <span class="pre">la</span> <span class="pre">disminución</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">impureza</span> <span class="pre">del</span> <span class="pre">nodo</span></code>.</p></li>
<li><p>Por ejemplo, una <code class="docutils literal notranslate"><span class="pre">medida</span> <span class="pre">común</span> <span class="pre">para</span> <span class="pre">cuantificar</span> <span class="pre">la</span> <span class="pre">impureza</span> <span class="pre">del</span> <span class="pre">nodo,</span></code> <span class="math notranslate nohighlight">\(t\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">entropía</span></code>, definida como</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
I(t)=-\sum_{m=1}^{M}P(\omega_{m}|t)\log_{2}P(\omega_{m}|t)
\]</div>
<figure class="align-center" id="entropy-function-fig">
<a class="reference internal image-reference" href="_images/entropy_function.png"><img alt="_images/entropy_function.png" src="_images/entropy_function.png" style="width: 600.0px; height: 589.2px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">Función de entropía <span class="math notranslate nohighlight">\(I(t)\)</span> para clasificación binaria.</span><a class="headerlink" href="#entropy-function-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">El</span> <span class="pre">valor</span> <span class="pre">máximo</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(I(t)\)</span> <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">produce</span> <span class="pre">si</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">probabilidades</span> <span class="pre">son</span> <span class="pre">iguales</span> <span class="pre">(máxima</span> <span class="pre">impureza)</span></code>, y <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">valor</span> <span class="pre">más</span> <span class="pre">pequeño,</span> <span class="pre">que</span> <span class="pre">es</span> <span class="pre">igual</span> <span class="pre">a</span> <span class="pre">cero,</span> <span class="pre">cuando</span> <span class="pre">sólo</span> <span class="pre">uno</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">de</span> <span class="pre">probabilidad</span> <span class="pre">es</span> <span class="pre">uno</span> <span class="pre">y</span> <span class="pre">el</span> <span class="pre">resto</span> <span class="pre">es</span> <span class="pre">igual</span> <span class="pre">a</span> <span class="pre">cero</span></code>. Las probabilidades se aproximan como</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P(\omega_{m}|t)=\frac{N_{t}^{m}}{N_{t}},\quad m=1,2,\dots,M,
\]</div>
<ul>
<li><p><span class="math notranslate nohighlight">\(N_{t}^{m}\)</span> es el <code class="docutils literal notranslate"><span class="pre">número</span> <span class="pre">de</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">clase</span></code> <span class="math notranslate nohighlight">\(m\)</span> <code class="docutils literal notranslate"><span class="pre">en</span></code> <span class="math notranslate nohighlight">\(X_{t}\)</span>, y <span class="math notranslate nohighlight">\(N_{t}\)</span> el <code class="docutils literal notranslate"><span class="pre">número</span> <span class="pre">total</span> <span class="pre">de</span> <span class="pre">puntos</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\(X_{t}\)</span>. La <code class="docutils literal notranslate"><span class="pre">disminución</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">impureza</span> <span class="pre">del</span> <span class="pre">nodo</span></code>, tras dividir los datos en dos conjuntos, se define como</p>
<div class="math notranslate nohighlight">
\[
    \Delta I(t)=I(t)-\frac{N_{t_{Y}}}{N_{t}}I(t_{Y})-\frac{N_{t_{N}}}{N_{t}}I(t_{N}),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(I(t_{Y})\)</span> y <span class="math notranslate nohighlight">\(I(t_{N})\)</span> son las <code class="docutils literal notranslate"><span class="pre">impurezas</span> <span class="pre">asociadas</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">dos</span> <span class="pre">nuevos</span> <span class="pre">conjuntos</span></code>, respectivamente. Esto es, disminución de la impureza del nodo es la <code class="docutils literal notranslate"><span class="pre">diferencia</span> <span class="pre">entre</span> <span class="pre">la</span> <span class="pre">entropía</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">nodo</span> <span class="pre">padre</span> <span class="pre">y</span> <span class="pre">la</span> <span class="pre">suma</span> <span class="pre">ponderada</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">entropías</span> <span class="pre">de</span> <span class="pre">sus</span> <span class="pre">nodos</span> <span class="pre">hijos</span></code>. A esta última expresión <span class="math notranslate nohighlight">\(\Delta I(t)\)</span>, también se le conoce como <code class="docutils literal notranslate"><span class="pre">ganancia</span> <span class="pre">de</span> <span class="pre">información</span></code>.</p>
</li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Ejemplo: Cálculo de Entropía</p>
<p>Calculemos el <code class="docutils literal notranslate"><span class="pre">índice</span> <span class="pre">de</span> <span class="pre">entropía</span></code> <span class="math notranslate nohighlight">\(I(t)\)</span> para <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">casos</span> <span class="pre">diferentes</span></code> de un conjunto con <code class="docutils literal notranslate"><span class="pre">4</span> <span class="pre">balotas</span></code> de dos colores diferentes, <code class="docutils literal notranslate"><span class="pre">rojo</span> <span class="pre">y</span> <span class="pre">azul</span></code>:</p>
<ul class="simple">
<li><p>4 balotas rojas y 0 balotas azules:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
I(t)&amp;=-P(\text{rojo})\cdot\log_{2}(P(\text{rojo}))-P(\text{azul})\cdot\log_{2}(P(\text{azul}))\\
&amp;=-\frac{4}{4}\cdot\log_{2}\frac{4}{4}-\frac{0}{4}\cdot\log_{2}\frac{0}{4}=0
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>2 balotas rojas y 2 balotas azules:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
I(t)&amp;=-P(\text{rojo})\cdot\log_{2}(P(\text{rojo}))-P(\text{azul})\cdot\log_{2}(P(\text{azul}))\\
&amp;=-\frac{2}{4}\cdot\log_{2}\frac{2}{4}-\frac{2}{4}\cdot\log_{2}\frac{2}{4}=1
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>3 balotas rojas y 1 balota azul:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
I(t)&amp;=-P(\text{rojo})\cdot\log_{2}(P(\text{rojo}))-P(\text{azul})\cdot\log_{2}(P(\text{azul}))\\
&amp;=-\frac{3}{4}\cdot\log_{2}\frac{3}{4}-\frac{1}{4}\cdot\log_{2}\frac{1}{4}=0.811
\end{align*}
\end{split}\]</div>
</div>
<figure class="align-center" id="trees-entropy-fig">
<a class="reference internal image-reference" href="_images/entropy_decision_tree.png"><img alt="_images/entropy_decision_tree.png" src="_images/entropy_decision_tree.png" style="width: 624.8000000000001px; height: 754.8000000000001px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">Pureza de nodos en un árbol de decisión. Fuente <code class="docutils literal notranslate"><span class="pre">towardsdatascience</span></code>.</span><a class="headerlink" href="#trees-entropy-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>El objetivo ahora es <code class="docutils literal notranslate"><span class="pre">seleccionar</span> <span class="pre">la</span> <span class="pre">característica</span> <span class="pre">específica</span></code> <span class="math notranslate nohighlight">\(x_{i}\)</span> <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">el</span> <span class="pre">umbral</span></code> <span class="math notranslate nohighlight">\(a_{t}\)</span> tal que, <span class="math notranslate nohighlight">\(\Delta I(t)\)</span> sea máximo. Esto definirá ahora <code class="docutils literal notranslate"><span class="pre">dos</span> <span class="pre">nuevos</span> <span class="pre">nodos</span> <span class="pre">descendientes</span></code> de <span class="math notranslate nohighlight">\(t\)</span>, a saber, <span class="math notranslate nohighlight">\(t_{N}\)</span> y <span class="math notranslate nohighlight">\(t_{Y}\)</span>; así, el árbol crece con dos nuevos nodos. Una forma de <code class="docutils literal notranslate"><span class="pre">buscar</span> <span class="pre">distintos</span> <span class="pre">valores</span> <span class="pre">umbral</span></code> es la siguiente:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Para</span> <span class="pre">cada</span> <span class="pre">una</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">características</span></code> <span class="math notranslate nohighlight">\(x_{i},~i=1,2\dots,l\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">realice</span> <span class="pre">un</span> <span class="pre">ranking</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">valores</span></code> <span class="math notranslate nohighlight">\(x_{in},~n=1,2,\dots,N_{t}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">que</span> <span class="pre">toma</span> <span class="pre">esta</span> <span class="pre">característica</span> <span class="pre">entre</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\(X_{t}\)</span>.</p></li>
<li><p>A continuación, <code class="docutils literal notranslate"><span class="pre">defina</span> <span class="pre">una</span> <span class="pre">secuencia</span> <span class="pre">de</span> <span class="pre">valores</span> <span class="pre">umbral</span> <span class="pre">correspondientes</span></code>, <span class="math notranslate nohighlight">\(a_{in}\)</span> <code class="docutils literal notranslate"><span class="pre">que</span> <span class="pre">estén</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">medio,</span> <span class="pre">entre</span> <span class="pre">valores</span> <span class="pre">distintos</span> <span class="pre">consecutivos</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(x_{in}\)</span></p></li>
<li><p>Seguidamente, <code class="docutils literal notranslate"><span class="pre">compruebe</span> <span class="pre">el</span> <span class="pre">cambio</span> <span class="pre">de</span> <span class="pre">impureza</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">produce</span> <span class="pre">para</span> <span class="pre">cada</span> <span class="pre">uno</span> <span class="pre">de</span> <span class="pre">estos</span> <span class="pre">valores</span> <span class="pre">umbral</span></code> y quédese con el que consiga la <code class="docutils literal notranslate"><span class="pre">máxima</span> <span class="pre">disminución</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Repita</span> <span class="pre">el</span> <span class="pre">proceso</span> <span class="pre">para</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">características</span></code> y, por último, <code class="docutils literal notranslate"><span class="pre">quédese</span> <span class="pre">con</span> <span class="pre">la</span> <span class="pre">combinación</span> <span class="pre">que</span> <span class="pre">dé</span> <span class="pre">como</span> <span class="pre">resultado</span> <span class="pre">la</span> <span class="pre">mejor</span> <span class="pre">disminución</span> <span class="pre">máxima</span></code>.</p></li>
</ol>
</li>
</ul>
<ul class="simple">
<li><p>Además de la <code class="docutils literal notranslate"><span class="pre">entropía</span></code>, pueden utilizarse otros <code class="docutils literal notranslate"><span class="pre">índices</span> <span class="pre">de</span> <span class="pre">medición</span> <span class="pre">de</span> <span class="pre">impurezas</span></code>. Una alternativa popular, que da como resultado un <code class="docutils literal notranslate"><span class="pre">máximo</span> <span class="pre">ligeramente</span> <span class="pre">superior</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">de</span> <span class="pre">entropía</span></code>, es el llamado <code class="docutils literal notranslate"><span class="pre">índice</span> <span class="pre">de</span> <span class="pre">Gini</span></code>, definido como</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
I(t)=\sum_{m=1}^{M}P(\omega_{m}|t)(1-P(\omega_{m}|t)).
\]</div>
<ul class="simple">
<li><p>Este índice también es <code class="docutils literal notranslate"><span class="pre">cero</span> <span class="pre">si</span> <span class="pre">uno</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">de</span> <span class="pre">probabilidad</span> <span class="pre">es</span> <span class="pre">igual</span> <span class="pre">a</span> <span class="pre">1</span> <span class="pre">y</span> <span class="pre">el</span> <span class="pre">resto</span> <span class="pre">son</span> <span class="pre">cero</span></code>, y toma
su valor <code class="docutils literal notranslate"><span class="pre">máximo</span> <span class="pre">cuando</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">clases</span> <span class="pre">son</span> <span class="pre">equiprobables</span></code>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Regla de detención de división (stop-splitting)</p>
<ul class="simple">
<li><p>La pregunta obvia cuando crece un árbol es <code class="docutils literal notranslate"><span class="pre">cuándo</span> <span class="pre">dejar</span> <span class="pre">de</span> <span class="pre">cultivarlo</span></code>. Una forma posible es <code class="docutils literal notranslate"><span class="pre">adoptar</span> <span class="pre">un</span> <span class="pre">valor</span> <span class="pre">umbral,</span> </code><span class="math notranslate nohighlight">\(T\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">y</span> <span class="pre">dejar</span> <span class="pre">de</span> <span class="pre">dividir</span> <span class="pre">un</span> <span class="pre">nodo</span> <span class="pre">una</span> <span class="pre">vez</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">valor</span> <span class="pre">máximo</span></code> <span class="math notranslate nohighlight">\(\Delta I(t)\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">para</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">divisiones</span> <span class="pre">posibles,</span> <span class="pre">sea</span> <span class="pre">menor</span> <span class="pre">que</span></code> <span class="math notranslate nohighlight">\(T\)</span>.</p></li>
<li><p>Otra posibilidad es parar cuando la cardinalidad de <span class="math notranslate nohighlight">\(X_{t}\)</span> es menor que un número determinado o si el <code class="docutils literal notranslate"><span class="pre">nodo</span> <span class="pre">es</span> <span class="pre">puro,</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">sentido</span> <span class="pre">de</span> <span class="pre">que</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">que</span> <span class="pre">lo</span> <span class="pre">componen</span> <span class="pre">pertenecen</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">única</span> <span class="pre">clase</span></code>.</p></li>
</ul>
</div>
<div class="tip admonition">
<p class="admonition-title">Regla de asignación de clase</p>
<p>Una vez que se declara que un <code class="docutils literal notranslate"><span class="pre">nodo</span></code> <span class="math notranslate nohighlight">\(t\)</span><code class="docutils literal notranslate"> <span class="pre">es</span> <span class="pre">un</span> <span class="pre">nodo</span> <span class="pre">hoja,</span> <span class="pre">se</span> <span class="pre">le</span> <span class="pre">asigna</span> <span class="pre">una</span> <span class="pre">etiqueta</span> <span class="pre">de</span> <span class="pre">clase</span></code>; normalmente por mayoría. Es decir, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">le</span> <span class="pre">asigna</span> <span class="pre">la</span> <span class="pre">etiqueta</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">clase</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">que</span> <span class="pre">pertenecen</span> <span class="pre">la</span> <span class="pre">mayoría</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(X_{t}\)</span>.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Podado del árbol</p>
<ul class="simple">
<li><p>La experiencia ha demostrado que <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">crecimiento</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">y</span> <span class="pre">el</span> <span class="pre">uso</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">regla</span> <span class="pre">de</span> <span class="pre">parada</span> <span class="pre">no</span> <span class="pre">siempre</span> <span class="pre">funciona</span> <span class="pre">bien</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">práctica</span></code>; el crecimiento puede detenerse antes de tiempo o puede dar lugar a árboles de tamaño muy grande.</p></li>
<li><p>Una práctica común es <code class="docutils literal notranslate"><span class="pre">hacer</span> <span class="pre">crecer</span> <span class="pre">primero</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">hasta</span> <span class="pre">un</span> <span class="pre">tamaño</span> <span class="pre">grande</span> <span class="pre">y</span> <span class="pre">luego</span> <span class="pre">adoptar</span> <span class="pre">una</span> <span class="pre">técnica</span> <span class="pre">de</span> <span class="pre">poda</span> <span class="pre">para</span> <span class="pre">eliminar</span> <span class="pre">nodos</span></code>. Se pueden utilizar diferentes criterios de poda; uno muy popular es <code class="docutils literal notranslate"><span class="pre">combinar</span> <span class="pre">una</span> <span class="pre">estimación</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">probabilidad</span> <span class="pre">de</span> <span class="pre">error</span> <span class="pre">con</span> <span class="pre">un</span> <span class="pre">índice</span> <span class="pre">de</span> <span class="pre">medición</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">complejidad</span></code> <span id="id3">[<a class="reference internal" href="biblio.html#id2" title="L Breiman, J Friedman, R Olshen, and C Stone. Cart. Classification and Regression Trees, 1984.">Breiman <em>et al.</em>, 1984</a>, <a class="reference internal" href="biblio.html#id3" title="Brian D Ripley. Pattern recognition and neural networks. Cambridge university press, 2007.">Ripley, 2007</a>]</span>.</p></li>
</ul>
</div>
<div class="proof observation admonition" id="observation_dt3">
<p class="admonition-title"><span class="caption-number">Observation 4 </span></p>
<section class="observation-content" id="proof-content">
<ol class="arabic simple">
<li><p>Entre las notables ventajas de los árboles de decisión está el hecho de que <code class="docutils literal notranslate"><span class="pre">pueden</span> <span class="pre">tratar</span> <span class="pre">de</span> <span class="pre">forma</span> <span class="pre">natural</span> <span class="pre">mezclas</span> <span class="pre">de</span> <span class="pre">variables</span> <span class="pre">numéricas</span> <span class="pre">y</span> <span class="pre">categóricas</span></code>. Además, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">adaptan</span> <span class="pre">bien</span> <span class="pre">a</span> <span class="pre">grandes</span> <span class="pre">conjuntos</span> <span class="pre">de</span> <span class="pre">datos</span></code>. <code class="docutils literal notranslate"><span class="pre">Pueden</span> <span class="pre">tratar</span> <span class="pre">eficazmente</span> <span class="pre">datos</span> <span class="pre">faltantes</span></code>. En muchos dominios, no se conocen todos los valores de las características para cada patrón. Los valores pueden no haber sido registrados, o pueden ser demasiado costosos de obtener.</p></li>
<li><p>Debido a su simplicidad estructural, son <code class="docutils literal notranslate"><span class="pre">fácilmente</span> <span class="pre">interpretables</span></code>; en otras palabras, <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">posible</span> <span class="pre">que</span> <span class="pre">un</span> <span class="pre">humano</span> <span class="pre">entienda</span> <span class="pre">la</span> <span class="pre">razón</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">salida</span> <span class="pre">del</span> <span class="pre">algoritmo</span> <span class="pre">de</span> <span class="pre">aprendizaje</span></code>. En algunas, como en las decisiones financieras, esto es un requisito legal. Por otro lado, <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">rendimiento</span> <span class="pre">de</span> <span class="pre">predicción</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">clasificadores</span> <span class="pre">de</span> <span class="pre">árbol</span> <span class="pre">no</span> <span class="pre">es</span> <span class="pre">tan</span> <span class="pre">bueno</span> <span class="pre">como</span> <span class="pre">el</span> <span class="pre">de</span> <span class="pre">otros</span> <span class="pre">métodos</span></code>, como las <code class="docutils literal notranslate"><span class="pre">máquinas</span> <span class="pre">de</span> <span class="pre">soporte</span> <span class="pre">vectorial</span></code> y las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">neuronales</span></code>, que se tratarán en posteriores capítulos</p></li>
<li><p>Uno de los <code class="docutils literal notranslate"><span class="pre">principales</span> <span class="pre">inconvenientes</span> <span class="pre">asociados</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">clasificadores</span> <span class="pre">de</span> <span class="pre">árbol</span> <span class="pre">es</span> <span class="pre">que</span> <span class="pre">son</span> <span class="pre">inestables</span></code>. Es decir, un <code class="docutils literal notranslate"><span class="pre">pequeño</span> <span class="pre">cambio</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">puede</span> <span class="pre">dar</span> <span class="pre">lugar</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">muy</span> <span class="pre">diferente</span></code>. La razón de esto radica en la <code class="docutils literal notranslate"><span class="pre">naturaleza</span> <span class="pre">jerárquica</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">clasificadores</span> <span class="pre">de</span> <span class="pre">árbol</span></code>. Un error que se produce en un nodo en un nivel alto del árbol se propaga hasta las hojas inferiores.</p></li>
</ol>
</section>
</div><div class="tip admonition">
<p class="admonition-title">Bagging (Bootstrap Aggregating)</p>
<p><code class="docutils literal notranslate"><span class="pre">Bagging</span> <span class="pre">(Bootstrap</span> <span class="pre">Aggregating)</span></code> es una técnica que puede <code class="docutils literal notranslate"><span class="pre">reducir</span> <span class="pre">la</span> <span class="pre">varianza</span> <span class="pre">y</span> <span class="pre">mejorar</span> <span class="pre">el</span> <span class="pre">rendimiento</span> <span class="pre">del</span> <span class="pre">error</span> <span class="pre">de</span> <span class="pre">generalización</span></code>.</p>
<ul class="simple">
<li><p>La idea básica es <code class="docutils literal notranslate"><span class="pre">crear</span> <span class="pre">un</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">variantes</span></code> <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(~X_{1}, X_{2},\dots, X_{B}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento,</span></code> <span class="math notranslate nohighlight">\(X\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">utilizando</span> <span class="pre">técnicas</span> <span class="pre">bootstrap</span></code>, mediante un muestreo uniforme de <span class="math notranslate nohighlight">\(X\)</span> con reemplazo.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Para</span> <span class="pre">cada</span> <span class="pre">una</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">variantes</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento,</span></code> <span class="math notranslate nohighlight">\(X_{i}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">se</span> <span class="pre">construye</span> <span class="pre">un</span> <span class="pre">árbol,</span></code> <span class="math notranslate nohighlight">\(T_{i}\)</span>.</p></li>
<li><p>La decisión final para la clasificación de un punto dado es a favor de la <code class="docutils literal notranslate"><span class="pre">clase</span> <span class="pre">predicha</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">mayoría</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">arboles</span> <span class="pre">subclasificadores</span></code>, <span class="math notranslate nohighlight">\(T_{i}, i = 1, 2,\dots, B\)</span> <span id="id4">[<a class="reference internal" href="biblio.html#id4" title="Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.">Breiman, 1996</a>]</span>.</p></li>
</ul>
</div>
<div class="tip admonition">
<p class="admonition-title">Bosques Aleatorios (Random Forest)</p>
<p>Los <code class="docutils literal notranslate"><span class="pre">Bosques</span> <span class="pre">Aleatorios</span> <span class="pre">(Random</span> <span class="pre">Forest)</span> <span class="pre">utilizan</span> <span class="pre">la</span> <span class="pre">idea</span> <span class="pre">de</span> <span class="pre">bagging</span> <span class="pre">junto</span> <span class="pre">con</span> <span class="pre">la</span> <span class="pre">selección</span> <span class="pre">aleatoria</span> <span class="pre">de</span> <span class="pre">características</span></code> <span id="id5">[<a class="reference internal" href="biblio.html#id5" title="Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.">Breiman, 2001</a>]</span>. La diferencia con el bagging radica en la forma en que se construyen los árboles de decisión.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">La</span> <span class="pre">característica</span> <span class="pre">a</span> <span class="pre">dividir</span> <span class="pre">en</span> <span class="pre">cada</span> <span class="pre">nodo</span> <span class="pre">se</span> <span class="pre">selecciona</span> <span class="pre">como</span> <span class="pre">la</span> <span class="pre">mejor</span> <span class="pre">entre</span> <span class="pre">un</span> <span class="pre">conjunto</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(F\)</span> <code class="docutils literal notranslate"><span class="pre">características</span> <span class="pre">elegidas</span> <span class="pre">al</span> <span class="pre">azar</span></code>, donde <span class="math notranslate nohighlight">\(F\)</span> es un parámetro definido por el usuario.</p></li>
<li><p>Esta <code class="docutils literal notranslate"><span class="pre">aleatoriedad</span> <span class="pre">adicional</span> <span class="pre">introducida</span> <span class="pre">tiene</span> <span class="pre">un</span> <span class="pre">efecto</span> <span class="pre">sustancial</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">mejora</span> <span class="pre">del</span> <span class="pre">rendimiento</span></code>. Los bosques aleatorios suelen tener una <code class="docutils literal notranslate"><span class="pre">precisión</span> <span class="pre">predictiva</span> <span class="pre">muy</span> <span class="pre">buena</span></code> y se han utilizado en una serie de aplicaciones, como el <code class="docutils literal notranslate"><span class="pre">reconocimiento</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">postura</span> <span class="pre">del</span> <span class="pre">cuerpo</span> <span class="pre">en</span> <span class="pre">términos</span> <span class="pre">del</span> <span class="pre">popular</span> <span class="pre">sensor</span> <span class="pre">Kinect</span> <span class="pre">de</span> <span class="pre">Microsoft</span></code> <span id="id6">[<a class="reference internal" href="biblio.html#id6" title="Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard Moore, Alex Kipman, and Andrew Blake. Real-time human pose recognition in parts from single depth images. In CVPR 2011, 1297–1304. Ieee, 2011.">Shotton <em>et al.</em>, 2011</a>]</span>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Además de los métodos anteriores, recientemente, también se han sugerido <code class="docutils literal notranslate"><span class="pre">técnicas</span> <span class="pre">Bayesianas</span> <span class="pre">utilizadas</span> <span class="pre">para</span> <span class="pre">estabilizar</span> <span class="pre">el</span> <span class="pre">rendimiento</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">árboles</span></code>; véase <span id="id7">[<a class="reference internal" href="biblio.html#id7" title="Hugh A Chipman, Edward I George, and Robert E McCulloch. Bart: bayesian additive regression trees. The Annals of Applied Statistics, 4(1):266–298, 2010.">Chipman <em>et al.</em>, 2010</a>, <a class="reference internal" href="biblio.html#id8" title="Yuhong Wu, Håkon Tjelmeland, and Mike West. Bayesian cart: prior specification and posterior simulation. Journal of Computational and Graphical Statistics, 16(1):44–66, 2007.">Wu <em>et al.</em>, 2007</a>]</span>. Por supuesto, <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">efecto</span> <span class="pre">de</span> <span class="pre">utilizar</span> <span class="pre">múltiples</span> <span class="pre">árboles,</span> <span class="pre">es</span> <span class="pre">perder</span> <span class="pre">una</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">principales</span> <span class="pre">ventajas</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">árboles,</span> <span class="pre">su</span> <span class="pre">facilidad</span> <span class="pre">de</span> <span class="pre">interpretación.</span></code></p></li>
</ul>
</section>
<section id="combinacion-de-clasificadores">
<h2>Combinación de clasificadores<a class="headerlink" href="#combinacion-de-clasificadores" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Hasta ahora, hemos revisado en detalle una serie de clasificadores, y se presentarán más métodos en las siguientes secciones, relativos a las <code class="docutils literal notranslate"><span class="pre">máquinas</span> <span class="pre">de</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span></code> y las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">neuronales</span></code>. La pregunta obvia a la que se enfrenta un profesional/investigador sin experiencia es: <code class="docutils literal notranslate"><span class="pre">¿qué</span> <span class="pre">método</span> <span class="pre">uso</span> <span class="pre">entonces?</span> <span class="pre">Por</span> <span class="pre">desgracia,</span> <span class="pre">no</span> <span class="pre">hay</span> <span class="pre">una</span> <span class="pre">respuesta</span> <span class="pre">definitiva.</span></code></p></li>
</ul>
<div class="admonition-no-free-lunch-theorem admonition">
<p class="admonition-title">No free lunch theorem</p>
<ul class="simple">
<li><p>El objetivo del diseño de cualquier clasificador, y en general de cualquier esquema de aprendizaje es <code class="docutils literal notranslate"><span class="pre">proporcionar</span> <span class="pre">un</span> <span class="pre">buen</span> <span class="pre">rendimiento</span> <span class="pre">de</span> <span class="pre">generalización</span></code>. Sin embargo, no hay razones independientes del contexto o del uso para apoyar una técnica de aprendizaje en lugar de otra.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Cada</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">aprendizaje,</span> <span class="pre">representada</span> <span class="pre">por</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">disponible,</span> <span class="pre">mostrará</span> <span class="pre">una</span> <span class="pre">preferencia</span> <span class="pre">por</span> <span class="pre">un</span> <span class="pre">esquema</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">específico</span></code> que se ajuste a las especificidades del problema en cuestión.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Un</span> <span class="pre">algoritmo</span> <span class="pre">que</span> <span class="pre">obtiene</span> <span class="pre">la</span> <span class="pre">máxima</span> <span class="pre">puntuación</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">problema</span> <span class="pre">puede</span> <span class="pre">obtener</span> <span class="pre">una</span> <span class="pre">puntuación</span> <span class="pre">baja</span> <span class="pre">en</span> <span class="pre">otro</span></code>. Esto se resume a veces como el <strong><code class="docutils literal notranslate"><span class="pre">Teorema</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">gratuidad</span> <span class="pre">(No</span> <span class="pre">free</span> <span class="pre">lunch</span> <span class="pre">theorem)</span></code></strong></p></li>
</ul>
</div>
<ul class="simple">
<li><p>En la práctica, hay que <code class="docutils literal notranslate"><span class="pre">probar</span> <span class="pre">diferentes</span> <span class="pre">métodos</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">dentro</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">disponibles</span></code>, cada uno optimizado para la tarea específica, y <code class="docutils literal notranslate"><span class="pre">probar</span> <span class="pre">su</span> <span class="pre">rendimiento</span> <span class="pre">de</span> <span class="pre">generalización</span></code> con un conjunto de datos independiente distinto del utilizado para el entrenamiento, utilizando, por ejemplo, el método de exclusión o cualquiera de sus variantes.</p></li>
<li><p>A continuación, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">mantiene</span> <span class="pre">y</span> <span class="pre">se</span> <span class="pre">utiliza</span> <span class="pre">el</span> <span class="pre">método</span> <span class="pre">que</span> <span class="pre">ha</span> <span class="pre">obtenido</span> <span class="pre">la</span> <span class="pre">mejor</span> <span class="pre">puntuación</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">específica</span></code>. Con este fin, hay una serie de esfuerzos importantes para comparar diferentes clasificadores contra diferentes conjuntos de datos y <code class="docutils literal notranslate"><span class="pre">medir</span> <span class="pre">el</span> <span class="pre">rendimiento</span> <span class="pre">&quot;medio&quot;,</span> <span class="pre">mediante</span> <span class="pre">el</span> <span class="pre">uso</span> <span class="pre">de</span> <span class="pre">diferentes</span> <span class="pre">índices</span> <span class="pre">estadísticos</span> <span class="pre">para</span> <span class="pre">cuantificar</span> <span class="pre">el</span> <span class="pre">rendimiento</span> <span class="pre">global</span> <span class="pre">de</span> <span class="pre">cada</span> <span class="pre">clasificador</span></code> frente a los conjuntos de datos.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Esquemas</span> <span class="pre">de</span> <span class="pre">combinación</span> <span class="pre">de</span> <span class="pre">clasificadores</span></code></strong></p>
<ul class="simple">
<li><p>Una tendencia para <code class="docutils literal notranslate"><span class="pre">mejorar</span> <span class="pre">el</span> <span class="pre">rendimiento</span></code> es <code class="docutils literal notranslate"><span class="pre">combinar</span> <span class="pre">diferentes</span> <span class="pre">clasificadores</span> <span class="pre">y</span> <span class="pre">explotar</span> <span class="pre">sus</span> <span class="pre">ventajas</span> <span class="pre">individuales</span></code>. Una observación que justifica este enfoque es que, durante las pruebas, <code class="docutils literal notranslate"><span class="pre">hay</span> <span class="pre">patrones</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">que</span> <span class="pre">incluso</span> <span class="pre">el</span> <span class="pre">mejor</span> <span class="pre">clasificador</span> <span class="pre">para</span> <span class="pre">una</span> <span class="pre">tarea</span> <span class="pre">concreta</span> <span class="pre">no</span> <span class="pre">logra</span> <span class="pre">predecir</span> <span class="pre">su</span> <span class="pre">verdadera</span> <span class="pre">clase</span></code>. En cambio, <code class="docutils literal notranslate"><span class="pre">los</span> <span class="pre">mismos</span> <span class="pre">patrones</span> <span class="pre">pueden</span> <span class="pre">ser</span> <span class="pre">clasificados</span> <span class="pre">correctamente</span> <span class="pre">por</span> <span class="pre">otros</span> <span class="pre">clasificadores,</span> <span class="pre">con</span> <span class="pre">un</span> <span class="pre">rendimiento</span> <span class="pre">global</span> <span class="pre">inferior</span></code>.</p></li>
<li><p>Esto muestra que puede haber cierta <code class="docutils literal notranslate"><span class="pre">complementariedad</span> <span class="pre">entre</span> <span class="pre">los</span> <span class="pre">distintos</span> <span class="pre">clasificadores</span></code>, y la combinación puede conducir a un <code class="docutils literal notranslate"><span class="pre">mayor</span> <span class="pre">rendimiento</span> <span class="pre">en</span> <span class="pre">comparación</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">obtenido</span> <span class="pre">por</span> <span class="pre">el</span> <span class="pre">mejor</span> <span class="pre">(único)</span> <span class="pre">clasificador</span></code>. Recordemos que <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">bagging</span> <span class="pre">mencionado</span> <span class="pre">anteriormente,</span> <span class="pre">es</span> <span class="pre">un</span> <span class="pre">tipo</span> <span class="pre">de</span> <span class="pre">combinación</span> <span class="pre">de</span> <span class="pre">clasificadores</span></code>. La cuestión que se plantea ahora es seleccionar un <code class="docutils literal notranslate"><span class="pre">esquema</span> <span class="pre">de</span> <span class="pre">combinación</span></code>. Hay diferentes esquemas, y los resultados que proporcionan pueden ser diferentes. A continuación, <code class="docutils literal notranslate"><span class="pre">resumimos</span> <span class="pre">los</span> <span class="pre">esquemas</span> <span class="pre">de</span> <span class="pre">combinación</span> <span class="pre">más</span> <span class="pre">populares</span></code>.</p></li>
</ul>
<div class="admonition-regla-de-la-media-aritmetica admonition">
<p class="admonition-title">Regla de la media aritmética</p>
<p>Suponiendo que utilizamos <span class="math notranslate nohighlight">\(L\)</span> clasificadores, en los que <code class="docutils literal notranslate"><span class="pre">cada</span> <span class="pre">uno</span> <span class="pre">da</span> <span class="pre">un</span> <span class="pre">valor</span> <span class="pre">de</span> <span class="pre">probabilidad</span> <span class="pre">posterior</span></code>, <span class="math notranslate nohighlight">\(P_{j}(\omega_{i}|\boldsymbol{x}), i = 1,2,\dots,M,~j = 1, 2, . . . L\)</span>, la decisión sobre <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">asignación</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">clase</span> <span class="pre">se</span> <span class="pre">basa</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">siguiente</span> <span class="pre">regla</span></code>:</p>
<div class="math notranslate nohighlight">
\[
\text{Asignar}~\boldsymbol{x}~\text{a la clase}~\omega_{i}=\textrm{arg}\max_{k}\frac{1}{L}\sum_{j=1}^{L}P_{j}(\omega_{k}|\boldsymbol{x}),\quad k=1,2,\dots,M.
\]</div>
<p>Esta regla equivale a calcular la <code class="docutils literal notranslate"><span class="pre">probabilidad</span> <span class="pre">posterior</span> <span class="pre">&quot;final&quot;</span></code>, <span class="math notranslate nohighlight">\(P(\omega_{i}|\boldsymbol{x})\)</span>, por medio de <code class="docutils literal notranslate"><span class="pre">minimización</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">distancia</span> <span class="pre">media</span> <span class="pre">de</span> <span class="pre">Kullback-Leibler</span></code></p>
<div class="math notranslate nohighlight">
\[
D_{av}=\frac{1}{L}\sum_{j=1}^{L}D_{j},~\text{donde}~D_{j}=\sum_{i=1}^{M}P_{j}(\omega_{i}|\boldsymbol{x})\ln\frac{P_{j}(\omega_{i}|\boldsymbol{x})}{P(\omega_{i}|\boldsymbol{x})}.
\]</div>
</div>
<div class="admonition-regla-de-promedio-geometrica admonition">
<p class="admonition-title">Regla de promedio geométrica</p>
<p>Esta regla es el resultado de <code class="docutils literal notranslate"><span class="pre">minimizar</span> <span class="pre">la</span> <span class="pre">formulación</span> <span class="pre">alternativa</span> <span class="pre">de</span> <span class="pre">distancia</span> <span class="pre">de</span> <span class="pre">Kullback-Leibler</span></code> (nótese que esta distancia no es simétrica); en otras palabras</p>
<div class="math notranslate nohighlight">
\[
D_{j}=\sum_{i=1}^{M}P(\omega_{i}|\boldsymbol{x})\ln\frac{P(\omega_{i}|\boldsymbol{x})}{P_{j}(\omega_{i}|\boldsymbol{x})},
\]</div>
<p>lo que da lugar a</p>
<div class="math notranslate nohighlight">
\[
\text{Asignar}~\boldsymbol{x}~\text{a la clase}~\omega_{i}=\textrm{arg}\max_{k}\prod_{j=1}^{L} P_{j}(\omega_{k}|\boldsymbol{x}),\quad k=1,2,\dots,M.
\]</div>
</div>
<div class="admonition-apilamiento-stacking admonition">
<p class="admonition-title">Apilamiento (Stacking)</p>
<p>Una forma alternativa es utilizar una <code class="docutils literal notranslate"><span class="pre">media</span> <span class="pre">ponderada</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">salidas</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">clasificadores</span> <span class="pre">individuales</span></code>, donde <code class="docutils literal notranslate"><span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">combinación</span> <span class="pre">se</span> <span class="pre">obtienen</span> <span class="pre">de</span> <span class="pre">forma</span> <span class="pre">óptima</span> <span class="pre">utilizando</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>. Supongamos que la salida de cada <code class="docutils literal notranslate"><span class="pre">clasificador</span> <span class="pre">individual</span></code>, <span class="math notranslate nohighlight">\(f_{j}(x)\)</span>, es de tipo suave (<code class="docutils literal notranslate"><span class="pre">infinitamente</span> <span class="pre">diferenciable</span></code>); por ejemplo, una estimación de probabilidad posterior, como antes. Entonces, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">salida</span> <span class="pre">combinada</span> <span class="pre">viene</span> <span class="pre">dada</span> <span class="pre">por</span></code></p>
<div class="math notranslate nohighlight">
\[
f(\boldsymbol{x})=\sum_{j=1}^{L}\omega_{j}f_{j}(\boldsymbol{x}),
\]</div>
<p>donde <code class="docutils literal notranslate"><span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">son</span> <span class="pre">estimados</span> <span class="pre">vía</span> <span class="pre">la</span> <span class="pre">siguiente</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span></code>:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\omega}}=\textrm{arg}\min_{\boldsymbol{\omega}}\sum_{n=1}^{N}\mathcal{L}(y_{n}, f(\boldsymbol{x}_{n}))=\textrm{arg}\min_{\boldsymbol{\omega}}\sum_{n=1}^{N}\mathcal{L}\left(y_{n}, \sum_{j=1}^{L}\omega_{j}f_{j}(\boldsymbol{x}_{n})\right)
\]</div>
<p>donde, <span class="math notranslate nohighlight">\(\mathcal{L}(\cdot,\cdot)\)</span> es una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span></code>; por ejemplo, la del <code class="docutils literal notranslate"><span class="pre">error</span> <span class="pre">cuadrático</span></code>.</p>
</div>
<ul class="simple">
<li><p>Sin embargo, <code class="docutils literal notranslate"><span class="pre">adoptar</span> <span class="pre">la</span> <span class="pre">anterior</span> <span class="pre">optimización,</span> <span class="pre">basada</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento,</span> <span class="pre">puede</span> <span class="pre">conducir</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">sobreajuste</span></code>. De acuerdo con el apilamiento <span id="id8">[<a class="reference internal" href="biblio.html#id23" title="David H Wolpert. Stacked generalization. Neural networks, 5(2):241–259, 1992.">Wolpert, 1992</a>]</span> se adopta un <code class="docutils literal notranslate"><span class="pre">razonamiento</span> <span class="pre">de</span> <span class="pre">validación</span> <span class="pre">cruzada</span></code> y en lugar de <span class="math notranslate nohighlight">\(f_{j}(\boldsymbol{x}_{n})\)</span>, empleamos <span class="math notranslate nohighlight">\(f_{j}^{(-n)}(\boldsymbol{x}_{n})\)</span>, donde este último es la <code class="docutils literal notranslate"><span class="pre">salida</span> <span class="pre">del</span></code> <span class="math notranslate nohighlight">\(j\)</span><code class="docutils literal notranslate"><span class="pre">-ésimo</span> <span class="pre">clasificador</span> <span class="pre">entrenado</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">tras</span> <span class="pre">excluir</span> <span class="pre">el</span> <span class="pre">par</span></code> <span class="math notranslate nohighlight">\((y_{n}, x_{n})\)</span>. En otras palabras, los pesos se estiman mediante</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\omega}}=\text{argmin}_{\boldsymbol{\omega}}\sum_{n=1}^{N}\mathcal{L}\left(y_{n}, \sum_{j=1}^{L}\omega_{j}f_{j}^{(-n)}(\boldsymbol{x}_{n})\right).
\]</div>
<ul class="simple">
<li><p>A veces, <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">ponderaciones</span> <span class="pre">tienen</span> <span class="pre">que</span> <span class="pre">ser</span> <span class="pre">positivas</span> <span class="pre">y</span> <span class="pre">sumar</span> <span class="pre">uno</span></code>, lo que da lugar a una tarea de <code class="docutils literal notranslate"><span class="pre">optimización</span> <span class="pre">restringida</span></code>.</p></li>
</ul>
<div class="admonition-regla-de-votacion-por-mayoria admonition">
<p class="admonition-title">Regla de votación por mayoría</p>
<p>Los métodos anteriores pertenecen a la familia de reglas <code class="docutils literal notranslate"><span class="pre">soft-type</span></code>. Una alternativa popular es una regla <code class="docutils literal notranslate"><span class="pre">hard-type</span></code>, que se basa en un <code class="docutils literal notranslate"><span class="pre">esquema</span> <span class="pre">de</span> <span class="pre">votación</span></code>. <code class="docutils literal notranslate"><span class="pre">Se</span> <span class="pre">decide</span> <span class="pre">a</span> <span class="pre">favor</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">clase</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">que</span> <span class="pre">hay</span> <span class="pre">consenso</span> <span class="pre">o</span> <span class="pre">al</span> <span class="pre">menos</span></code> <span class="math notranslate nohighlight">\(l_{c}\)</span> <code class="docutils literal notranslate"><span class="pre">clasificadores</span> <span class="pre">están</span> <span class="pre">de</span> <span class="pre">acuerdo</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">etiqueta</span> <span class="pre">de</span> <span class="pre">clase</span></code>, donde</p>
<div class="math notranslate nohighlight">
\[\begin{split}
l_{c}=
\begin{cases}
\displaystyle{\frac{L}{2}+1}, &amp; L~\text{es par}\\
\displaystyle{\frac{L+1}{2}}, &amp; L~\text{es impar}
\end{cases}
\end{split}\]</div>
<p><code class="docutils literal notranslate"><span class="pre">En</span> <span class="pre">caso</span> <span class="pre">contrario,</span> <span class="pre">la</span> <span class="pre">decisión</span> <span class="pre">es</span> <span class="pre">de</span> <span class="pre">rechazo</span></code> (es decir, no se toma ninguna decisión).</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Reglas del Tipo: Hard-Soft</p>
<p>Una <code class="docutils literal notranslate"><span class="pre">regla</span> <span class="pre">de</span> <span class="pre">clasificación</span> <span class="pre">soft</span></code> generalmente <code class="docutils literal notranslate"><span class="pre">estima</span> <span class="pre">las</span> <span class="pre">probabilidades</span> <span class="pre">condicionales</span> <span class="pre">de</span> <span class="pre">clase</span> <span class="pre">de</span> <span class="pre">forma</span> <span class="pre">explícita</span> <span class="pre">y,</span> <span class="pre">a</span> <span class="pre">continuación,</span> <span class="pre">realiza</span> <span class="pre">la</span> <span class="pre">predicción</span> <span class="pre">de</span> <span class="pre">clase</span> <span class="pre">basada</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">mayor</span> <span class="pre">probabilidad</span> <span class="pre">estimada</span></code>. Por el contrario, la <code class="docutils literal notranslate"><span class="pre">clasificación</span> <span class="pre">hard</span></code> omite el requisito de estimar la probabilidad de la clase y <code class="docutils literal notranslate"><span class="pre">estima</span> <span class="pre">directamente</span> <span class="pre">el</span> <span class="pre">límite</span> <span class="pre">de</span> <span class="pre">clasificación</span></code>.</p>
</div>
<div class="admonition-regla-de-la-mediana admonition">
<p class="admonition-title">Regla de la mediana</p>
<p>Cuando <code class="docutils literal notranslate"><span class="pre">valores</span> <span class="pre">atípicos</span> <span class="pre">están</span> <span class="pre">presentes</span></code>, se puede utilizar en su lugar el valor de la <code class="docutils literal notranslate"><span class="pre">mediana</span></code>:</p>
<div class="math notranslate nohighlight">
\[
\text{Asignar}~\boldsymbol{x}~\text{a la clase}~\omega_{i}=\textrm{arg}\max_{k}\textrm{median}\{P_{j}(\omega_{k}|\boldsymbol{x})\},~k=1,2,\dots,M.
\]</div>
</div>
<ul class="simple">
<li><p>Ocurre que, el <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">free</span> <span class="pre">lunch</span> <span class="pre">theorem</span></code> también es válido para las reglas de combinación; <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">hay</span> <span class="pre">una</span> <span class="pre">regla</span> <span class="pre">universalmente</span> <span class="pre">óptima</span></code>. Todo depende de los datos de que se disponga; véase <span id="id9">[<a class="reference internal" href="biblio.html#id9" title="Anil K Jain, Robert P. W. Duin, and Jianchang Mao. Statistical pattern recognition: a review. IEEE Transactions on pattern analysis and machine intelligence, 22(1):4–37, 2000.">Jain <em>et al.</em>, 2000</a>]</span>. Hay otras cuestiones relacionadas con la teoría de la combinación de clasificadores; por ejemplo, <code class="docutils literal notranslate"><span class="pre">¿cómo</span> <span class="pre">se</span> <span class="pre">eligen</span> <span class="pre">los</span> <span class="pre">clasificadores</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">van</span> <span class="pre">a</span> <span class="pre">combinar?</span> <span class="pre">¿Deben</span> <span class="pre">ser</span> <span class="pre">dependientes</span> <span class="pre">o</span> <span class="pre">independientes?</span></code>.</p></li>
<li><p>Además, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">combinación</span> <span class="pre">no</span> <span class="pre">implica</span> <span class="pre">necesariamente</span> <span class="pre">una</span> <span class="pre">mejora</span> <span class="pre">del</span> <span class="pre">rendimiento,</span> <span class="pre">en</span> <span class="pre">algunos</span> <span class="pre">casos,</span> <span class="pre">se</span> <span class="pre">puede</span> <span class="pre">experimentar</span> <span class="pre">una</span> <span class="pre">pérdida</span> <span class="pre">de</span> <span class="pre">rendimiento</span> <span class="pre">(mayor</span> <span class="pre">tasa</span> <span class="pre">de</span> <span class="pre">error)</span> <span class="pre">en</span> <span class="pre">comparación</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">mejor</span> <span class="pre">clasificador</span> <span class="pre">(único)</span></code>. Por tanto, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">combinación</span> <span class="pre">debe</span> <span class="pre">realizarse</span> <span class="pre">con</span> <span class="pre">cuidado</span></code> <span id="id10">[<a class="reference internal" href="biblio.html#id19" title="Konstantinos Koutroumbas and Sergios Theodoridis. Pattern recognition. Academic Press, 2008.">Koutroumbas and Theodoridis, 2008</a>, <a class="reference internal" href="biblio.html#id24" title="Ludmila I Kuncheva. Combining pattern classifiers: methods and algorithms. John Wiley &amp; Sons, 2014.">Kuncheva, 2014</a>]</span>.</p></li>
</ul>
</section>
<section id="enfoque-boosting">
<h2>Enfoque Boosting<a class="headerlink" href="#enfoque-boosting" title="Link to this heading">#</a></h2>
<div class="proof observation admonition" id="observation_dt1">
<p class="admonition-title"><span class="caption-number">Observation 5 </span></p>
<section class="observation-content" id="proof-content">
<p>¿<strong>Un algoritmo de aprendizaje débil</strong>, es decir, uno que funciona ligeramente mejor que una adivinación aleatoria, <strong>puede convertirse en uno fuerte con un buen índice de rendimiento</strong>?.</p>
</section>
</div><ul class="simple">
<li><p>El <code class="docutils literal notranslate"><span class="pre">enfoque</span> <span class="pre">boosting</span></code> es un procedimiento para <code class="docutils literal notranslate"><span class="pre">combinar</span> <span class="pre">o</span> <span class="pre">&quot;reforzar</span> <span class="pre">(boost)&quot;</span> <span class="pre">el</span> <span class="pre">rendimiento</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">clasificadores</span> <span class="pre">débiles</span></code>(clasificadores cuyas estimaciones de parámetros suelen ser imprecisas y ofrecen un rendimiento deficiente) con el fin de <code class="docutils literal notranslate"><span class="pre">obtener</span> <span class="pre">un</span> <span class="pre">mejor</span> <span class="pre">clasificador</span></code>. Se diferencia del <code class="docutils literal notranslate"><span class="pre">bagging</span></code> en que es un <em>procedimiento determinista y genera conjuntos de entrenamiento y clasificadores secuencialmente, basándose en los resultados de la iteración anterior</em>. En cambio, el bagging genera los conjuntos de entrenamiento aleatoriamente y puede generar los clasificadores en paralelo.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p><strong>Boosting</strong> <em>asigna un peso a cada patrón en el conjunto de entrenamiento, reflejando su importancia</em>, y <em>construye un clasificador utilizando el conjunto de entrenamiento y el conjunto de pesos</em>. Por lo tanto, <strong>requiere un clasificador que pueda manejar pesos en las muestras de entrenamiento</strong> (algunos clasificadores pueden ser incapaces de admitir patrones ponderados).</p></li>
<li><p>En este caso, se puede <em>muestrear un subconjunto de los ejemplos de entrenamiento de acuerdo con la distribución de los pesos</em> (ponderación de muestras de entrenamiento) y <em>utilizar estos ejemplos para entrenar al clasificador en la siguiente etapa de la iteración</em>. El aprendiz final se obtiene mediante una <em>media ponderada de todos los aprendices de base (débiles) jerárquicamente diseñados</em>. Por lo tanto, el boosting también puede considerarse un <em>esquema para combinar diferentes aprendices</em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Dado un <em>número suficiente de iteraciones, se puede mejorar significativamente lo (pobre) del aprendiz débil</em>. Por ejemplo, en algunos casos de clasificación, el <em>error de entrenamiento puede tender a cero a medida que aumenta el número de iteraciones</em>. El entrenamiento de un clasificador <em>mediante una manipulación adecuada de los datos de entrenamiento</em> (de hecho, <em>el mecanismo de ponderación identifica las muestras difíciles, las que siguen fallando, y pone más énfasis en ellas</em>) <em>se puede obtener un clasificador fuerte</em>. Por supuesto, como veremos más adelante, el hecho de que el error de entrenamiento tienda a cero no significa necesariamente que el error de prueba llegue a cero.</p></li>
</ul>
<figure class="align-center" id="boosted-tree-numref">
<a class="reference internal image-reference" href="_images/boosted_tree.jpg"><img alt="_images/boosted_tree.jpg" src="_images/boosted_tree.jpg" style="width: 508.5px; height: 210.6px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">Ilustración del <code class="docutils literal notranslate"><span class="pre">Arbol</span> <span class="pre">Boosting</span></code>: Fuente <span id="id11">[<a class="reference internal" href="biblio.html#id26" title="Sonia Kahiomba Kiangala and Zenghui Wang. An effective adaptive customization framework for small manufacturing plants using extreme gradient boosting-xgboost and random forest ensemble learning algorithms in an industry 4.0 environment. Machine Learning with Applications, 4:100024, 2021.">Kiangala and Wang, 2021</a>]</span>.</span><a class="headerlink" href="#boosted-tree-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition-algoritmo-adaboost-adaptive-boosting admonition">
<p class="admonition-title">Algoritmo AdaBoost (Adaptive Boosting)</p>
<p>Considere una <em>tarea de clasificación de dos clases</em> y supongamos que se nos da un conjunto de <span class="math notranslate nohighlight">\(N\)</span> <em>observaciones de entrenamiento</em>, <span class="math notranslate nohighlight">\((y_{n}, \boldsymbol{x}_{n}),~n=1,2,\dots,N\)</span>, con <span class="math notranslate nohighlight">\(y_{n}\in\{-1, 1\}\)</span>. Nuestro objetivo es <em>diseñar un clasificador binario</em>,</p>
<div class="math notranslate nohighlight" id="equation-sign-fn-adaboost-eq">
<span class="eqno">(24)<a class="headerlink" href="#equation-sign-fn-adaboost-eq" title="Link to this equation">#</a></span>\[
f(\boldsymbol{x})=\text{sgn}\{F(\boldsymbol{x})\}
\]</div>
<p>donde</p>
<div class="math notranslate nohighlight" id="equation-adaboosting-model-eq">
<span class="eqno">(25)<a class="headerlink" href="#equation-adaboosting-model-eq" title="Link to this equation">#</a></span>\[
F(\boldsymbol{x}):=\sum_{k=1}^{K}a_{k}\phi(\boldsymbol{x};\boldsymbol{\theta}_{k}),
\]</div>
<p>y <span class="math notranslate nohighlight">\(\phi(\boldsymbol{x}; \boldsymbol{\theta}_{k})\in\{-1, 1\}\)</span>, es el <em>clasificador base en la iteración</em> <span class="math notranslate nohighlight">\(k\)</span>, definido en términos de un <em>conjunto de parámetros</em>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{k},~k=1,2,\dots, K\)</span> <em>a ser estimados</em>.</p>
<ul class="simple">
<li><p>El clasificador base se selecciona como uno binario. El conjunto de <em>parámetros desconocidos se obtienen mediante</em> <code class="docutils literal notranslate"><span class="pre">step-wise</span></code> y de forma <code class="docutils literal notranslate"><span class="pre">greedy</span></code>; es decir, en cada iteración <span class="math notranslate nohighlight">\(i\)</span>, <em>solo optimizamos con respecto a un único par</em>, <span class="math notranslate nohighlight">\((a_{i}, \boldsymbol{\theta}_{i})\)</span> <em>manteniendo los parámetros</em> <span class="math notranslate nohighlight">\(a_{k}, \boldsymbol{\theta}_{k},~k=1,2,\dots,i-1\)</span>, <em>obtenidos en los pasos anteriores, fijos</em>.</p></li>
<li><p>Nótese que <em>lo ideal sería optimizar respecto a todos los parámetros desconocidos</em>, <span class="math notranslate nohighlight">\(a_{k}\)</span>, <span class="math notranslate nohighlight">\(k=1,2,\dots,K\)</span>, <em>simultáneamente</em>, sin embargo, esto conduciría a una <em>tarea de optimización muy exigente, desde el punto de vista computacional</em>. Los algoritmos <code class="docutils literal notranslate"><span class="pre">greedy</span></code> son muy populares debido a su <em>simplicidad computacional</em>, y conducen a un muy <em>buen rendimiento en una amplia gama de tareas de aprendizaje</em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Supongamos que <em>nos encontramos en el paso de iteración</em> <span class="math notranslate nohighlight">\(i\)</span><em>-ésimo</em>; consideremos la <em>suma parcial</em> de términos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
F_{i}(\cdot)=\sum_{k=1}^{i}a_{k}\phi(\cdot; \boldsymbol{\theta}_{k}).
\]</div>
<ul>
<li><p>Entonces, podemos escribir la siguiente <em>recursion</em></p>
<div class="math notranslate nohighlight">
\[
    F_{i}(\cdot)=F_{i-1}(\cdot)+a_{i}\phi(\cdot; \boldsymbol{\theta}_{i}),\quad i=1,2,\dots,K,
    \]</div>
<p>partiendo de una <em>condición inicial</em>. Con base en el <em>razonamiento greedy</em>, <span class="math notranslate nohighlight">\(F_{i-1}(\cdot)\)</span> <em>se supone conocido</em> y el objetivo es <em>optimizar con respecto al conjunto de parámetros</em> <span class="math notranslate nohighlight">\(a_{i},~\boldsymbol{\theta}_{i}\)</span>.</p>
</li>
</ul>
<ul>
<li><p>Para la <em>tarea de optimización</em>, debe adoptarse una <em>función de pérdida</em>. Sin duda, <em>existen diferentes opciones, que dan distintos nombres al algoritmo derivado</em>. Una función de pérdida popular, utilizada para la clasificación, es la <strong>pérdida exponencial</strong>, definida como</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{L}(y, F(\boldsymbol{x}))=\exp(-yF(\boldsymbol{x})):\quad\text{exponential loss function},
    \]</div>
<p>y da lugar al <strong>algoritmo boosting adaptativo (AdaBoost)</strong>.</p>
</li>
</ul>
<figure class="align-center" id="loss-fn-adaboost-fig">
<a class="reference internal image-reference" href="_images/loss_fn_adaboost.png"><img alt="_images/loss_fn_adaboost.png" src="_images/loss_fn_adaboost.png" style="width: 430.40000000000003px; height: 262.40000000000003px;" />
</a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">Pérdida 0-1, exponencial, log-loss y error al cuadrado. Fuente <span id="id12">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#loss-fn-adaboost-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La función de <em>pérdida exponencial</em>, junto con la función de <em>pérdida 0-1</em>, se muestran en <a class="reference internal" href="#loss-fn-adaboost-fig"><span class="std std-numref">Fig. 14</span></a>. <em>La primera puede considerarse un límite superior (diferenciable) de la función de pérdida 0-1 (no diferenciable)</em>.</p></li>
<li><p>Observe que <strong>la pérdida exponencial coloca más peso sobre los errores de clasificación</strong> <span class="math notranslate nohighlight">\((yF(\boldsymbol{x}) &lt; 0)\)</span> <strong>en comparación con los correctamente identificados</strong> <span class="math notranslate nohighlight">\((yF(\boldsymbol{x}) &gt; 0)\)</span>. Empleando la función de <em>pérdida exponencial</em>, el conjunto <span class="math notranslate nohighlight">\(a_{i},\boldsymbol{\theta}_{i}\)</span> se obtiene mediante la respectiva <strong>función de coste empírica</strong>, de la siguiente manera:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-costfn-adaboost-eq">
<span class="eqno">(26)<a class="headerlink" href="#equation-costfn-adaboost-eq" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
(a_{i}, \boldsymbol{\theta}_{i})&amp;=\text{arg}\text{min}_{a,\boldsymbol{\theta}}\sum_{n=1}^{N}\mathcal{L}(y_{n}, F(\boldsymbol{x}_{n}))\\
&amp;=\text{arg}\text{min}_{a,\boldsymbol{\theta}}\sum_{n=1}^{N}\exp\left(-y_{n}(F_{i-1}(\boldsymbol{x}_{n})+a\phi(\boldsymbol{x}_{n};\boldsymbol{\theta}))\right).
\end{align*}
\end{split}\]</div>
<ul>
<li><p>Esta <em>tarea de optimización</em> también se realiza en dos pasos. En primer lugar, <span class="math notranslate nohighlight">\(a\)</span> <strong>se trata de forma fija y optimizamos con respecto a</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-weights-omega-eq">
<span class="eqno">(27)<a class="headerlink" href="#equation-weights-omega-eq" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \boldsymbol{\theta}_{i}&amp;=\text{arg}\text{min}_{a,\boldsymbol{\theta}}\sum_{n=1}^{N}\exp\left(-y_{n}(F_{i-1}(\boldsymbol{x}_{n})+a\phi(\boldsymbol{x}_{n};\boldsymbol{\theta}))\right)\\
    &amp;=\text{argmin}_{\boldsymbol{\theta}}\sum_{n=1}^{N}\omega_{n}^{(i)}\exp(-y_{n}a\phi(\boldsymbol{x}_{n};\boldsymbol{\theta})),
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\omega_{n}^{(i)}:=\exp(-y_{n}F_{i-1}(\boldsymbol{x}_{n})),~n=1,2,\dots,N.\)</span></p>
</li>
<li><p>Observe que <span class="math notranslate nohighlight">\(\omega_{n}^{(i)}\)</span> <em>no depende ni de</em> <span class="math notranslate nohighlight">\(a\)</span> <em>ni de</em> <span class="math notranslate nohighlight">\(\phi(x_{n}; \boldsymbol{\theta})\)</span>, por lo que puede considerarse un <strong>peso asociado a la muestra</strong> <span class="math notranslate nohighlight">\(n\)</span>. Además, <em>su valor depende por completo de los resultados obtenidos en las pruebas anteriores</em>.</p></li>
</ul>
<ul>
<li><p>Ahora nos centraremos en el coste de Ecuación <a class="reference internal" href="#equation-costfn-adaboost-eq">(26)</a>. La <strong>optimización depende de la forma específica del
clasificador base</strong>. Nótese, sin embargo, que la <em>función de pérdida es de forma exponencial</em>. Además, el <em>clasificador
es binario</em>, de modo que <span class="math notranslate nohighlight">\(\phi(\boldsymbol{x}, \boldsymbol{\theta})\in\{-1, 1\}\)</span>.</p></li>
<li><p>Si <em>suponemos que</em> <span class="math notranslate nohighlight">\(a &gt; 0\)</span> (volveremos a ello pronto) la optimización de Ecuación <a class="reference internal" href="#equation-costfn-adaboost-eq">(26)</a> se ve fácilmente que es equivalente a <em>optimizar el siguiente coste</em>:</p>
<div class="math notranslate nohighlight" id="equation-loss-funtion-01">
<span class="eqno">(28)<a class="headerlink" href="#equation-loss-funtion-01" title="Link to this equation">#</a></span>\[
    \boldsymbol{\theta}_{i}=\text{argmin}_{\theta}P_{i},~\text{donde}~P_{i}:=\sum_{n=1}^{N}\omega_{n}^{(i)}\chi_{(-\infty, 0]}(y_{n}\phi(\boldsymbol{x}_{n}, \boldsymbol{\theta})),
    \]</div>
<p>y <span class="math notranslate nohighlight">\(\chi_{(-\infty, 0]}\)</span> es la <em>función de pérdida 0-1</em>.</p>
</li>
</ul>
<ul class="simple">
<li><p>En otras palabras, <strong>solo contribuyen los puntos mal clasificados</strong> (<span class="math notranslate nohighlight">\(y_{n}\phi(\boldsymbol{x}_{n}; \boldsymbol{\theta}) &lt; 0\)</span>). Nótese que <span class="math notranslate nohighlight">\(P_{i}\)</span> es el <strong>error ponderado del clasificador empírico</strong>. Claramente, cuando se minimiza el error de clasificación en la Ecuación <a class="reference internal" href="#equation-loss-funtion-01">(28)</a>, el coste de Ecuación <a class="reference internal" href="#equation-costfn-adaboost-eq">(26)</a> <em>también se minimiza, porque la pérdida exponencial coloca mayor peso a los puntos mal clasificados</em> (cota superior para <span class="math notranslate nohighlight">\(\chi_{(-\infty, 0]}\)</span>).</p></li>
<li><p>Para garantizar que <span class="math notranslate nohighlight">\(P_{i}\)</span> permanece en el intervalo <span class="math notranslate nohighlight">\([0, 1]\)</span>, <em>los pesos son normalizados a la unidad dividiendo por la suma respectiva</em>; nótese que esto no afecta al proceso de optimización. En otras palabras, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i}\)</span> <strong>puede calcularse para minimizar el error de clasificación empírico, cometido por el clasificador base</strong>. Para clasificadores base de estructura muy simple, <strong>dicha minimización es computacionalmente factible</strong>.</p></li>
</ul>
<ul>
<li><p>Una vez calculado el <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i}\)</span> óptimo, a partir de las respectivas definiciones,</p>
<div class="math notranslate nohighlight" id="equation-pi-eq">
<span class="eqno">(29)<a class="headerlink" href="#equation-pi-eq" title="Link to this equation">#</a></span>\[
    \sum_{y_{n}\phi(\boldsymbol{x}_{n}; \boldsymbol{\theta}_{i})&lt;0}\omega_{n}^{(i)}=P_{i},~\text{y}~\sum_{y_{n}\phi(\boldsymbol{x}_{n}; \boldsymbol{\theta}_{i})&gt;0}\omega_{n}^{(i)}=1-P_{i}.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p><em>Combinando</em> las Ecuaciones <a class="reference internal" href="#equation-pi-eq">(29)</a>-<a class="reference internal" href="#equation-weights-omega-eq">(27)</a>, se demuestra fácilmente que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
a_{i}=\text{argmin}_{a}\{\exp(-a)(1-P_{i})+\exp(a)P_{i}\}.
\]</div>
<ul class="simple">
<li><p><em>Si se toma la derivada con respecto a</em> <span class="math notranslate nohighlight">\(a\)</span> <em>y se iguala a cero</em>, se obtiene</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial a_{i}}(\exp(-a_{i})(1-P_{i})+\exp(a_{i})P_{i})=0\Leftrightarrow a_{i}=\frac{1}{2}\ln\frac{1-P_{i}}{P_{i}}.
\]</div>
<ul>
<li><p><em>Nótese que si</em> <span class="math notranslate nohighlight">\(P_{i} &lt; 0.5\)</span><em>, entonces</em> <span class="math notranslate nohighlight">\(a_{i} &gt; 0\)</span><em>, que es lo que se espera en la práctica</em>. <strong>Una vez estimados</strong> <span class="math notranslate nohighlight">\(a_{i}\)</span> <strong>y</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i}\)</span> <strong>las ponderaciones para la siguiente iteración vienen dadas por</strong></p>
<div class="math notranslate nohighlight">
\[
    \omega_{n}^{(i+1)}=\frac{\exp(-y_{n}F_{i}(\boldsymbol{x}_{n}))}{Z_{i}}=\frac{\omega_{n}^{(i)}\exp(-y_{n}a_{i}\phi(\boldsymbol{x}_{n}, \boldsymbol{\theta}_{i}))}{Z_{i}},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(Z_{i}\)</span> es el <em><strong>factor normalizado</strong></em></p>
<div class="math notranslate nohighlight">
\[
    Z_{i}:=\sum_{n=1}^{N}\omega_{n}^{(i)}\exp(-y_{n}a_{i}\phi(\boldsymbol{x}_{n}; \boldsymbol{\theta}_{i})).
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Si se observa la forma en que se forman los pesos, se puede comprender uno de los principales secretos subyacentes al algoritmo <strong>AdaBoost: El peso asociado a una muestra de entrenamiento</strong> <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> <strong>aumenta (disminuye) con respecto a su valor en la iteración anterior, dependiendo de si el patrón ha fallado (tenido éxito) respecto a su valor en la iteración anterior</strong>.</p></li>
<li><p>Además, el <strong>porcentaje de disminución (aumento) depende del valor de</strong> <span class="math notranslate nohighlight">\(a_{i}\)</span>, <strong>que controla la importancia relativa en la construcción del clasificador final</strong>. Las <em>muestras difíciles, que siguen fallando en iteraciones sucesivas, ganan importancia en su participación en el valor de error empírico ponderado</em>.</p></li>
<li><p>Para el caso del <em>AdaBoost</em>, puede demostrarse que el error de entrenamiento tiende a cero exponencialmente rápido. El esquema se resume en el siguiente algoritmo, introducido por <em>Yoav Freund y Robert Schapire</em> quienes recibieron el prestigioso <em>premio Gödel</em> por este algoritmo en 2003.</p></li>
</ul>
<div class="proof algorithm admonition" id="adaboost_algo">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Algoritmo AdaBoost)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inicialización</strong></p>
<ol class="arabic simple">
<li><p>Inicializa: <span class="math notranslate nohighlight">\(\omega_{n}^{(1)}=1/N,~i=1,2,\dots,N\)</span></p></li>
<li><p>Inicializa: <span class="math notranslate nohighlight">\(i=1\)</span></p></li>
</ol>
<p><strong>Repeat</strong></p>
<ol class="arabic simple">
<li><p>Calcular el óptimo <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i}\)</span> en <span class="math notranslate nohighlight">\(\phi(\cdot; \boldsymbol{\theta}_{i})\)</span> minimizando <span class="math notranslate nohighlight">\(P_{i}\)</span></p></li>
<li><p>Calcular el óptimo <span class="math notranslate nohighlight">\(P_{i}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a_{i}=\displaystyle{1/2[\ln(1-P_{i})/P_{i}]}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Z_{i}=0\)</span></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span>, <strong>Do</strong>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\omega_{n}^{(i+1)}=\omega_{n}^{(i)}\exp(-y_{n}a_{i}\phi(\boldsymbol{x}_{n}; \boldsymbol{\theta}_{i}))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Z_{i}=Z_{i}+\omega_{n}^{(i+1)}\)</span></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span>, <strong>Do</strong>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\omega_{n}^{(i+1)}=\omega_{n}^{(i+1)}/Z_{i}\)</span></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(K=i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i=i+1\)</span></p></li>
<li><p><strong>Until</strong> Un criterio de parada se cumpla.</p></li>
<li><p><span class="math notranslate nohighlight">\(f(\cdot)=\text{sgn}\left(\sum_{k=1}^{K}a_{k}\phi(\cdot, \boldsymbol{\theta}_{k})\right)\)</span></p></li>
</ol>
</section>
</div><div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>El enfoque propuesto implica la partición del conjunto de datos en dos subconjuntos: uno de entrenamiento y otro de prueba. El primero se emplea para el entrenamiento del algoritmo, mientras que en cada iteración se evalúa la pérdida binaria 0-1 en el conjunto de prueba. Se observa una <strong>disminución garantizada de la pérdida exponencial en el conjunto de entrenamiento y, por lo general, también de la pérdida binaria 0-1</strong>.</p></li>
<li><p>Aunque los <strong>errores en el conjunto de prueba suelen disminuir al principio del algoritmo, llega un punto en el que comienzan a aumentar significativamente</strong>. Cuando esto sucede, revertimos el clasificador a la forma que dio el mejor error de prueba y descartamos cualquier cambio posterior (es decir, clasificadores débiles adicionales).</p></li>
</ul>
</div>
</section>
<section id="la-funcion-log-loss">
<h2>La función Log-Loss<a class="headerlink" href="#la-funcion-log-loss" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>En <code class="docutils literal notranslate"><span class="pre">AdaBoost</span></code>, se empleó la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span> <span class="pre">exponencial</span></code>. Desde un punto de vista teórico, esto puede
justificarse con el siguiente argumento. Consideremos el <code class="docutils literal notranslate"><span class="pre">valor</span> <span class="pre">medio</span> <span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">etiqueta</span> <span class="pre">binaria,</span></code> <span class="math notranslate nohighlight">\(y\)</span>, de
la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span> <span class="pre">exponencial</span></code></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-mean-exp-loss-fn-eq">
<span class="eqno">(30)<a class="headerlink" href="#equation-mean-exp-loss-fn-eq" title="Link to this equation">#</a></span>\[
\mathbb{E}(\exp(-yF(\boldsymbol{x})))=P(y=1)\exp(-F(\boldsymbol{x}))+P(y=-1)\exp(F(\boldsymbol{x}))
\]</div>
<ul class="simple">
<li><p>Tomando <code class="docutils literal notranslate"><span class="pre">derivada</span> <span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span></code> <span class="math notranslate nohighlight">\(F(\boldsymbol{x})\)</span> <code class="docutils literal notranslate"><span class="pre">e</span> <span class="pre">igualando</span> <span class="pre">a</span> <span class="pre">cero</span></code>, fácilmente obtenemos el mínimo de <a class="reference internal" href="#equation-mean-exp-loss-fn-eq">(30)</a> el cual ocurre en</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-min-exp-loss-fn-eq">
<span class="eqno">(31)<a class="headerlink" href="#equation-min-exp-loss-fn-eq" title="Link to this equation">#</a></span>\[
\frac{\partial}{\partial F(\boldsymbol{x})}(\mathbb{E}(\exp(-yF(\boldsymbol{x}))))=0\Leftrightarrow F_{\star}(\boldsymbol{x})=\text{argmin}_{f}\mathbb{E}[\exp(-yf)]=\frac{1}{2}\ln\frac{P(y=1|\boldsymbol{x})}{P(y=-1|\boldsymbol{x})}
\]</div>
<ul class="simple">
<li><p>El logaritmo de la proporción del lado derecho se conoce como la <code class="docutils literal notranslate"><span class="pre">proporción</span> <span class="pre">log-odds</span></code>. Por lo tanto, si se considera la función de minimización en Ecuación <a class="reference internal" href="#equation-costfn-adaboost-eq">(26)</a> como la aproximación empírica del valor medio en Ecuación <a class="reference internal" href="#equation-mean-exp-loss-fn-eq">(30)</a>, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">justifica</span> <span class="pre">plenamente</span> <span class="pre">considerar</span> <span class="pre">el</span> <span class="pre">signo</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">en</span></code> Ecuación <a class="reference internal" href="#equation-sign-fn-adaboost-eq">(24)</a> <code class="docutils literal notranslate"><span class="pre">como</span> <span class="pre">regla</span> <span class="pre">de</span> <span class="pre">clasificación</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>Un problema importante asociado a la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span> <span class="pre">exponencial</span></code>, como se ve fácilmente en la <a class="reference internal" href="#loss-fn-adaboost-fig"><span class="std std-numref">Fig. 14</span></a>, es que <code class="docutils literal notranslate"><span class="pre">pondera</span> <span class="pre">en</span> <span class="pre">gran</span> <span class="pre">medida</span> <span class="pre">las</span> <span class="pre">muestras</span> <span class="pre">clasificadas</span> <span class="pre">erróneamente</span></code>, dependiendo del valor del margen respectivo, definido como</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
m_{x}:=|yF(\boldsymbol{x})|.
\]</div>
<ul class="simple">
<li><p>Tenga en cuenta que <code class="docutils literal notranslate"><span class="pre">cuanto</span> <span class="pre">más</span> <span class="pre">lejos</span> <span class="pre">esté</span> <span class="pre">el</span> <span class="pre">punto</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">superficie</span> <span class="pre">de</span> <span class="pre">decisión</span></code> (<span class="math notranslate nohighlight">\(F(x) = 0\)</span>)<code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">mayor</span> <span class="pre">será</span> <span class="pre">el</span> <span class="pre">valor</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(|F(x)|\)</span>. Por lo tanto, los <code class="docutils literal notranslate"><span class="pre">puntos</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">encuentran</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">lado</span> <span class="pre">equivocado</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">superficie</span> <span class="pre">de</span> <span class="pre">decisión</span></code> (<span class="math notranslate nohighlight">\(yF(x) &lt; 0\)</span>) y lejos son (exponencialmente) grandes, y <code class="docutils literal notranslate"><span class="pre">su</span> <span class="pre">influencia</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">proceso</span> <span class="pre">de</span> <span class="pre">optimización</span> <span class="pre">es</span> <span class="pre">grande</span></code> en comparación con los demás puntos. Así pues, <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">presencia</span> <span class="pre">de</span> <span class="pre">valores</span> <span class="pre">atípicos,</span> <span class="pre">la</span> <span class="pre">pérdida</span> <span class="pre">exponencial</span> <span class="pre">no</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">más</span> <span class="pre">adecuada</span></code>. De hecho, en tales entornos, el <code class="docutils literal notranslate"><span class="pre">rendimiento</span> <span class="pre">de</span> <span class="pre">AdaBoost</span> <span class="pre">puede</span> <span class="pre">degradarse</span> <span class="pre">drásticamente</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>Una función de pérdida alternativa es la <code class="docutils literal notranslate"><span class="pre">log-loss</span> <span class="pre">o</span> <span class="pre">desviación</span> <span class="pre">binomial</span></code> (ver Figura <a class="reference internal" href="#loss-fn-adaboost-fig"><span class="std std-numref">Fig. 14</span></a>), definida como</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(y, F(\boldsymbol{x})):=\ln(1+\exp(-yF(\boldsymbol{x}))):\quad\text{log-loss function},
\]</div>
<ul class="simple">
<li><p>Nótese que su <code class="docutils literal notranslate"><span class="pre">incremento</span> <span class="pre">es</span> <span class="pre">casi</span> <span class="pre">lineal</span> <span class="pre">para</span> <span class="pre">valores</span> <span class="pre">negativos</span> <span class="pre">grandes</span></code>. Tal función conduce a una <code class="docutils literal notranslate"><span class="pre">influencia</span> <span class="pre">más</span> <span class="pre">equilibrada</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">pérdida</span> <span class="pre">entre</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">puntos</span></code>. Observe además que la función que minimiza la media de la pérdida logarítmica, con respecto a <span class="math notranslate nohighlight">\(y\)</span>, es la misma que la dada en Ecuación <a class="reference internal" href="#equation-min-exp-loss-fn-eq">(31)</a> (<code class="docutils literal notranslate"><span class="pre">verifíquelo</span></code>). Sin embargo, si se emplea la pérdida logarítmica en lugar de la exponencial, la tarea de optimización es más compleja, y hay que recurrir a esquemas de optimización de tipo <code class="docutils literal notranslate"><span class="pre">gradiente</span> <span class="pre">descendiente</span></code> o esquemas de <code class="docutils literal notranslate"><span class="pre">optimización</span> <span class="pre">de</span> <span class="pre">tipo</span> <span class="pre">Newton</span></code> (ver <span id="id13">[<a class="reference internal" href="biblio.html#id25" title="Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 1189–1232, 2001.">Friedman, 2001</a>]</span>).</p></li>
</ul>
</section>
<section id="arboles-boosting">
<h2>Arboles Boosting<a class="headerlink" href="#arboles-boosting" title="Link to this heading">#</a></h2>
<div class="proof observation admonition" id="observation_dt2">
<p class="admonition-title"><span class="caption-number">Observation 6 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>En la discusión sobre la comparación experimental de varios métodos, se afirmó que <strong>los árboles boosting se encuentran entre los esquemas de aprendizaje más potentes para la clasificación y la minería de datos</strong>. Por lo tanto, merece la pena dedicar más tiempo a este tipo especial de técnicas de boosting.</p></li>
</ul>
</section>
</div><ul>
<li><p>A partir de los conocimientos adquiridos hasta ahora, no es difícil ver que la <em>salida de un árbol puede escribirse de forma compacta como</em></p>
<div class="math notranslate nohighlight">
\[
    T(\boldsymbol{x}, \boldsymbol{\Theta})=\sum_{j=1}^{J}\hat{y}_{j}\chi_{R_{j}}(\boldsymbol{x}),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(J\)</span> es el <strong>número de nodos hoja</strong>, <span class="math notranslate nohighlight">\(R_{j}\)</span> es la <strong>región asociada a la hoja</strong> <span class="math notranslate nohighlight">\(j\)</span> tras la <em>partición espacial</em> impuesta por el árbol, <span class="math notranslate nohighlight">\(\hat{y}_{j}\)</span> es la <strong>etiqueta respectiva asociada a</strong> <span class="math notranslate nohighlight">\(R_{j}\)</span> (valor de salida/predicción para la regresión), y <span class="math notranslate nohighlight">\(\chi\)</span> es nuestra conocida <strong>función característica</strong>.</p>
</li>
<li><p>El conjunto de <em>parámetros</em>, <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span>, se compone de <span class="math notranslate nohighlight">\((\hat{y}_{j}, R_{j} ), j = 1, 2,\dots, J\)</span>, que <em>se estiman durante el entrenamiento</em>. Estos se pueden obtener mediante la <strong>selección de una función de coste adecuada</strong>. También se suelen emplear técnicas subóptimas, para construir un árbol, como las analizadas en la primera sección.</p></li>
</ul>
<ul class="simple">
<li><p>En un modelo de árbol boosting, el <strong>clasificador base está formado por un árbol</strong>. En la práctica, se pueden emplear <em>árboles cuyo tamaño (<strong>la profundidad máxima es la mayor longitud posible entre la raíz y una hoja</strong>) no debe ser muy grande, para acercarse más a un clasificador débil</em>. Normalmente, <strong>valores de</strong> <span class="math notranslate nohighlight">\(J\)</span> <strong>entre 4 y 8</strong> son aconsejables. El modelo de <strong>árbol boosting</strong> puede escribirse como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-bost-tree-model-eq">
<span class="eqno">(32)<a class="headerlink" href="#equation-bost-tree-model-eq" title="Link to this equation">#</a></span>\[
F(\boldsymbol{x})=\sum_{k=1}^{K}T(\boldsymbol{x}; \boldsymbol{\Theta}_{k}),\quad\text{donde}\quad T(\boldsymbol{x}; \boldsymbol{\Theta}_{k})=\sum_{j=1}^{J}\hat{y}_{kj}\chi_{R_{kj}}(\boldsymbol{x}).
\]</div>
<ul class="simple">
<li><p>La Ecuación <a class="reference internal" href="#equation-bost-tree-model-eq">(32)</a> es básicamente la misma que Ecuación <a class="reference internal" href="#equation-adaboosting-model-eq">(25)</a>, con el coeficiente <span class="math notranslate nohighlight">\(a_{k}\)</span> igual a uno. Hemos supuesto que el <em>tamaño de todos los árboles es el mismo</em>, aunque no tiene por qué ser así. Adoptando una <em>función de pérdida</em> <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> y la <code class="docutils literal notranslate"><span class="pre">lógica</span> <span class="pre">greedy</span></code> utilizada para el enfoque boosting más general, llegamos al siguiente esquema recursivo de optimización:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-rec-optimization-eq">
<span class="eqno">(33)<a class="headerlink" href="#equation-rec-optimization-eq" title="Link to this equation">#</a></span>\[
\boldsymbol{\Theta}_{i}=\text{argmin}_{\boldsymbol{\Theta}}\sum_{n=1}^{N}\mathcal{L}(y_{n}, F_{i-1}(\boldsymbol{x}_{n})+T(\boldsymbol{x}_{n}; \boldsymbol{\Theta})).
\]</div>
<ul class="simple">
<li><p>La <em>optimización</em> con respecto a <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span> tiene lugar en <em>dos pasos: uno con respecto a</em> <span class="math notranslate nohighlight">\(\hat{y}_{ij},~j=1,2,\dots,J\)</span>, dado <span class="math notranslate nohighlight">\(R_{ij}\)</span> , y luego <em>uno con respecto a las regiones</em> <span class="math notranslate nohighlight">\(R_{ij}\)</span>. Esta última es una tarea difícil y solo se simplifica en casos muy especiales. En la práctica, se pueden emplear varias aproximaciones. Nótese que en el caso de la <em>pérdida exponencial</em> y la <em>tarea de clasificación de dos clases</em>, lo anterior está directamente relacionado con el esquema <code class="docutils literal notranslate"><span class="pre">AdaBoost</span></code>.</p></li>
<li><p>En <code class="docutils literal notranslate"><span class="pre">Python</span></code> el modelo <code class="docutils literal notranslate"><span class="pre">XGBoost</span></code> (<em>Adaboost regularizado</em>), se considera la suma de todos los errores divididos por el número de muestras, y utiliza las siguientes funciones de pérdida <code class="docutils literal notranslate"><span class="pre">binary:logistic</span></code> (<span class="math notranslate nohighlight">\(-y(\log(\tilde{y}))+(1-y)\log(1-\tilde{y})\)</span>), <code class="docutils literal notranslate"><span class="pre">reg:logistic</span></code> (<span class="math notranslate nohighlight">\((y-\tilde{y})^{2}\)</span>) y para clasificación múltiple <a class="reference external" href="https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters">multi:softmax, multi:softprob</a>.</p></li>
</ul>
<ul class="simple">
<li><p>Para casos más generales, se recurre a <strong>esquemas de optimización numérica</strong>. <em>El mismo razonamiento se aplica a los árboles de regresión, donde ahora se utilizan funciones de pérdida para la regresión, como el error al cuadrado o el valor de error absoluto</em>. Estos esquemas también se conocen como árboles de regresión aditiva múltiple (MART).</p></li>
<li><p>Hay dos factores críticos en relación con los árboles boosting. Uno es el tamaño de los árboles, <span class="math notranslate nohighlight">\(J\)</span>, y el otro es la elección de <span class="math notranslate nohighlight">\(K\)</span>. En cuanto al <strong>tamaño de los árboles</strong>, normalmente se prueban diferentes tamaños, <span class="math notranslate nohighlight">\(4\leq J \leq 8\)</span>, y se selecciona el mejor. Con respecto al <strong>número de iteraciones</strong>, para valores grandes, el error de entrenamiento puede llegar a ser cercano a cero, pero el error de prueba puede aumentar debido al sobreajuste. Por lo tanto, hay que <strong>parar lo suficientemente pronto, normalmente controlando el rendimiento</strong>.</p></li>
</ul>
<ul class="simple">
<li><p>Otra forma de <strong>hacer frente al sobreajuste</strong> es emplear <strong>métodos de contracción (shrinkage)</strong>, que suelen ser <strong>equivalentes a la regularización</strong>. Por ejemplo, en la expansión por etapas de <span class="math notranslate nohighlight">\(F_{i}(x)\)</span> utilizada en el paso de optimización Eq <a class="reference internal" href="#equation-rec-optimization-eq">(33)</a>, se puede adoptar lo siguiente:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
F_{i}(\cdot)=F_{i-1}(\cdot)+\nu T(\cdot; \boldsymbol{\theta}_{i}).
\]</div>
<ul class="simple">
<li><p>El parámetro <span class="math notranslate nohighlight">\(\nu\)</span> toma valores pequeños y puede considerarse que <strong>controla la tasa de aprendizaje del procedimiento boosting</strong>. Se aconsejan valores inferiores a <span class="math notranslate nohighlight">\(\nu &lt; 0.1\)</span>. Sin embargo, <em>cuanto menor sea el valor de</em> <span class="math notranslate nohighlight">\(\nu\)</span>, <em>mayor deberá ser el valor de</em> <span class="math notranslate nohighlight">\(K\)</span> <em>para garantizar un buen rendimiento</em>.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="implementacion">
<h1>Implementación<a class="headerlink" href="#implementacion" title="Link to this heading">#</a></h1>
<section id="control-de-complejidad">
<h2>Control de complejidad<a class="headerlink" href="#control-de-complejidad" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Al construir un árbol como se describe en esta sección, hasta que todas las hojas sean puras, da lugar a modelos muy complejos y muy ajustados a los datos de entrenamiento. <code class="docutils literal notranslate"><span class="pre">La</span> <span class="pre">presencia</span> <span class="pre">de</span> <span class="pre">hojas</span> <span class="pre">puras</span> <span class="pre">significa</span> <span class="pre">que</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">es</span> <span class="pre">100%</span> <span class="pre">preciso</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>; cada punto de datos del conjunto de entrenamiento está en una hoja que tiene la clase mayoritaria correcta.</p></li>
<li><p>Hay dos estrategias comunes para evitar el overfitting: <code class="docutils literal notranslate"><span class="pre">detener</span> <span class="pre">la</span> <span class="pre">creación</span> <span class="pre">del</span> <span class="pre">árbol</span> <span class="pre">antes</span> <span class="pre">de</span> <span class="pre">tiempo</span> <span class="pre">(también</span> <span class="pre">llamada</span> <span class="pre">prepoda),</span> <span class="pre">o</span> <span class="pre">construir</span> <span class="pre">el</span> <span class="pre">árbol,</span> <span class="pre">pero</span> <span class="pre">luego</span> <span class="pre">eliminar</span> <span class="pre">o</span> <span class="pre">colapsar</span> <span class="pre">nodos</span> <span class="pre">que</span> <span class="pre">contienen</span> <span class="pre">poca</span> <span class="pre">información</span> <span class="pre">(también</span> <span class="pre">llamada</span> <span class="pre">poda</span> <span class="pre">posterior</span> <span class="pre">o</span> <span class="pre">simplemente</span> <span class="pre">poda)</span></code>. Los posibles criterios para la <code class="docutils literal notranslate"><span class="pre">prepoda</span></code> incluyen la <code class="docutils literal notranslate"><span class="pre">limitación</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">profundidad</span> <span class="pre">máxima</span> <span class="pre">del</span> <span class="pre">árbol,</span> <span class="pre">limitar</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">máximo</span> <span class="pre">de</span> <span class="pre">hojas,</span> <span class="pre">o</span> <span class="pre">exigir</span> <span class="pre">un</span> <span class="pre">número</span> <span class="pre">mínimo</span> <span class="pre">de</span> <span class="pre">puntos</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">nodo</span> <span class="pre">para</span> <span class="pre">seguir</span> <span class="pre">dividiéndolo</span></code>.</p></li>
<li><p>Los árboles de decisión en <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> se implementan en las clases <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> y <code class="docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code>. <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> sólo implementa la pre-poda, no la post-poda. Veamos el efecto de la poda a priori con más detalle en el conjunto de datos de cáncer de mama. Como siempre, importamos el conjunto de datos y lo dividimos en una parte de entrenamiento y otra de prueba. A continuación, construimos un  modelo utilizando la configuración por defecto de desarrollo completo del árbol (extendiendo el árbol hasta que todas las hojas sean puras). Fijamos el <code class="docutils literal notranslate"><span class="pre">random_state</span></code> en el árbol, que se utiliza para ruptura interna de de los lazos. Nótese que seleccionamos <code class="docutils literal notranslate"><span class="pre">stratify=cancer.target</span></code>, los datos se dividen de forma estratificada, utilizando <code class="docutils literal notranslate"><span class="pre">cancer.target</span></code> como las etiquetas de clase (ver <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html#stratification">stratified</a>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> 
                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 1.000
Accuracy on test set: 0.937
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Aquí la función <code class="docutils literal notranslate"><span class="pre">score</span></code> como en la mayoría de clasificadores, devuelve la precisión media en los datos de prueba y las etiquetas dadas. Como era de esperar, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">precisión</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">es</span> <span class="pre">del</span> <span class="pre">100%</span></code>, ya que las hojas son puras, el árbol creció lo suficiente como para poder memorizar perfectamente todas las etiquetas de los datos de entrenamiento. <code class="docutils literal notranslate"><span class="pre">La</span> <span class="pre">precisión</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">prueba</span> <span class="pre">es</span> <span class="pre">ligeramente</span> <span class="pre">peor</span> <span class="pre">que</span> <span class="pre">la</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">modelos</span> <span class="pre">lineales</span></code> que vimos anteriormente, que tenían una precisión de alrededor del 95%.</p></li>
<li><p>Si no restringimos la profundidad de un árbol de decisión, el árbol puede llegar a ser arbitrariamente profundo y complejo. <code class="docutils literal notranslate"><span class="pre">Los</span> <span class="pre">árboles</span> <span class="pre">no</span> <span class="pre">podados</span> <span class="pre">son,</span> <span class="pre">por</span> <span class="pre">tanto,</span> <span class="pre">son</span> <span class="pre">propensos</span> <span class="pre">a</span> <span class="pre">sobreajustarse</span> <span class="pre">y</span> <span class="pre">a</span> <span class="pre">no</span> <span class="pre">generalizar</span> <span class="pre">bien</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">nuevos</span> <span class="pre">datos</span></code>. Ahora apliquemos la pre-poda, la cual que dejará de desarrollar el árbol antes de ajustarse perfectamente a los datos de entrenamiento. Una opción es dejar de construir el árbol después de alcanzar una cierta profundidad. En este caso,<code class="docutils literal notranslate"><span class="pre">establecemos</span> <span class="pre">que</span> <span class="pre">la</span> <span class="pre">profundidad</span> <span class="pre">máxima</span> <span class="pre">sea</span> <span class="pre">de</span> <span class="pre">4</span> <span class="pre">(max_depth=4),</span> <span class="pre">lo</span> <span class="pre">que</span> <span class="pre">significa</span> <span class="pre">que</span> <span class="pre">sólo</span> <span class="pre">se</span> <span class="pre">pueden</span> <span class="pre">formular</span> <span class="pre">cuatro</span> <span class="pre">preguntas</span> <span class="pre">consecutivas</span></code>. La limitación de la profundidad del árbol disminuye el sobreajuste. Esto conduce a una menor precisión en el conjunto de entrenamiento, pero una mejora en el conjunto de prueba</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.988
Accuracy on test set: 0.951
</pre></div>
</div>
</div>
</div>
</section>
<section id="analisis-de-los-arboles-de-decision">
<h2>Análisis de los árboles de decisión<a class="headerlink" href="#analisis-de-los-arboles-de-decision" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Podemos visualizar un árbol de decisión utilizando la función <code class="docutils literal notranslate"><span class="pre">export_graphviz</span></code> del módulo <code class="docutils literal notranslate"><span class="pre">tree</span></code>. Esta función escribe un archivo en el formato <code class="docutils literal notranslate"><span class="pre">.dot</span></code> de archivo de texto para almacenar gráficos. Establecemos una opción para colorear los nodos, para reflejar la clase mayoritaria en cada nodo y pasamos los nombres de las clases y las características para que el árbol pueda ser etiquetado correctamente. El argumento <code class="docutils literal notranslate"><span class="pre">impurity</span></code> está relacionado con la probabilidad de que clasifiquemos incorrectamente un nuevo punto de datos de forma incorrecta, normalmente calculada mediante la métrica de entropia <code class="docutils literal notranslate"><span class="pre">giny</span></code> la cual se aborda teóricamente en la sección anterior.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span>
                <span class="n">out_file</span><span class="o">=</span><span class="s2">&quot;tree.dot&quot;</span><span class="p">,</span> 
                <span class="n">class_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;malignant&quot;</span><span class="p">,</span> <span class="s2">&quot;benign&quot;</span><span class="p">],</span>
                <span class="n">feature_names</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span> 
                <span class="n">impurity</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Podemos leer este archivo y visualizarlo, utilizando el modulo <code class="docutils literal notranslate"><span class="pre">graphviz</span></code> (o puede utilizar cualquier programa que pueda leer archivos <code class="docutils literal notranslate"><span class="pre">.dot</span></code>). Para que funcione <code class="docutils literal notranslate"><span class="pre">Graphviz</span></code> deberá además realizar la siguiente instalación, la cual es la única dependencia (ver <a class="reference external" href="https://forum.graphviz.org/t/new-simplified-installation-procedure-on-windows/224">Graphviz Instalación</a>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">graphviz</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;tree.dot&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">dot_graph</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">dot_graph</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">dot_graph</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/98d6b01af58197203ebce0fb15d7ac714068af7b9371ff4ee6e4aed94d0362f3.svg" src="_images/98d6b01af58197203ebce0fb15d7ac714068af7b9371ff4ee6e4aed94d0362f3.svg" /></div>
</div>
<ul class="simple">
<li><p>La visualización del árbol proporciona una <code class="docutils literal notranslate"><span class="pre">gran</span> <span class="pre">visión</span> <span class="pre">en</span> <span class="pre">profundidad</span> <span class="pre">de</span> <span class="pre">cómo</span> <span class="pre">el</span> <span class="pre">algoritmo</span> <span class="pre">realiza</span> <span class="pre">predicciones,</span> <span class="pre">y</span> <span class="pre">es</span> <span class="pre">un</span> <span class="pre">buen</span> <span class="pre">ejemplo</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">algoritmo</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">automático</span> <span class="pre">que</span> <span class="pre">puede</span> <span class="pre">fácilmente</span> <span class="pre">explicarse</span> <span class="pre">a</span> <span class="pre">no</span> <span class="pre">expertos</span></code>. Sin embargo, incluso con un árbol de profundidad cuatro, como se ve aquí, el árbol puede resultar un poco abrumador. Los árboles más profundos (una profundidad de 10 puede ser común) son aún más difíciles de entender.</p></li>
<li><p>Un método de inspección del árbol que puede ser útil es, averiguar qué camino toma realmente la mayoría de los datos. <code class="docutils literal notranslate"><span class="pre">Las</span> <span class="pre">n_samples</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">muestran</span> <span class="pre">en</span> <span class="pre">cada</span> <span class="pre">nodo</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">figura,</span> <span class="pre">entregan</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">muestras</span> <span class="pre">en</span> <span class="pre">ese</span> <span class="pre">nodo</span></code>, mientras que <code class="docutils literal notranslate"><span class="pre">value</span> <span class="pre">provee</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">muestras</span> <span class="pre">por</span> <span class="pre">clase</span></code>. Siguiendo las ramas hacia la derecha, vemos que el <code class="docutils literal notranslate"><span class="pre">worst</span> <span class="pre">radius</span></code> <span class="math notranslate nohighlight">\(\leq\)</span> <code class="docutils literal notranslate"><span class="pre">16.795</span></code> crea un nodo que contiene sólo <code class="docutils literal notranslate"><span class="pre">8</span> <span class="pre">muestras</span> <span class="pre">benignas</span> <span class="pre">pero</span> <span class="pre">134</span> <span class="pre">muestras</span> <span class="pre">malignas.</span> <span class="pre">El</span> <span class="pre">resto</span> <span class="pre">de</span> <span class="pre">este</span> <span class="pre">lado</span> <span class="pre">del</span> <span class="pre">árbol</span> <span class="pre">utiliza</span> <span class="pre">entonces</span> <span class="pre">algunas</span> <span class="pre">distinciones</span> <span class="pre">más</span> <span class="pre">finas</span> <span class="pre">para</span> <span class="pre">separar</span> <span class="pre">estas</span> <span class="pre">8</span> <span class="pre">muestras</span> <span class="pre">benignas</span> <span class="pre">restantes</span></code>. De las 142 muestras que fueron a la derecha en la división inicial, casi todas ellas (132) terminan en la hoja de la derecha. Tomando la izquierda en la raíz, para el <code class="docutils literal notranslate"><span class="pre">worst</span> <span class="pre">radio</span> <span class="pre">&gt;</span> <span class="pre">16.795</span></code> terminamos con <code class="docutils literal notranslate"><span class="pre">25</span> <span class="pre">muestras</span> <span class="pre">malignas</span> <span class="pre">y</span> <span class="pre">259</span> <span class="pre">muestras</span> <span class="pre">benignas</span></code>. Casi todas las muestras benignas acaban en la segunda hoja de la derecha, y la mayoría de las demás hojas contienen muy pocas muestras.</p></li>
</ul>
</section>
<section id="caracteristicas-importantes-en-los-arboles">
<h2>Características importantes en los árboles<a class="headerlink" href="#caracteristicas-importantes-en-los-arboles" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>En lugar de mirar todo el árbol, lo que puede ser agotador, hay algunas <code class="docutils literal notranslate"><span class="pre">propiedades</span> <span class="pre">útiles</span> <span class="pre">que</span> <span class="pre">podemos</span> <span class="pre">derivar</span> <span class="pre">para</span> <span class="pre">resumir</span> <span class="pre">el</span> <span class="pre">funcionamiento</span> <span class="pre">del</span> <span class="pre">árbol</span></code>. El resumen más utilizado es el de las <code class="docutils literal notranslate"><span class="pre">características</span> <span class="pre">importantes,</span> <span class="pre">que</span> <span class="pre">califica</span> <span class="pre">la</span> <span class="pre">importancia</span> <span class="pre">de</span> <span class="pre">cada</span> <span class="pre">característica</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">decisión</span> <span class="pre">que</span> <span class="pre">toma</span> <span class="pre">el</span> <span class="pre">árbol</span></code>. Es un número entre 0 y 1 para cada característica, donde 0 significa “no se utiliza en absoluto” y 1 significa “predice perfectamente el objetivo”. Las características siempre suman 1</p></li>
</ul>
<ul class="simple">
<li><p>Otra <code class="docutils literal notranslate"><span class="pre">excelente</span> <span class="pre">manera</span> <span class="pre">de</span> <span class="pre">visualizar</span> <span class="pre">predicciones</span> <span class="pre">a</span> <span class="pre">partir</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">random</span> <span class="pre">forest</span> <span class="pre">por</span> <span class="pre">ejemplo,</span> <span class="pre">es</span> <span class="pre">utilizando</span> <span class="pre">la</span> <span class="pre">librería</span></code> <a class="reference external" href="https://www.kaggle.com/code/prashant111/explain-your-model-predictions-with-lime/notebook">LIME</a> <code class="docutils literal notranslate"><span class="pre">de</span> <span class="pre">Python</span></code>. Con esta librería se pueden generar por cada instancias, figuras de cartaterísticas importantes y representar sus probabilidades de pertenecer a la clase predicha (<code class="docutils literal notranslate"><span class="pre">pruebela</span></code>).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Feature importances:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Feature importances:
[0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.01019737 0.04839825
 0.         0.         0.0024156  0.         0.         0.
 0.         0.         0.72682851 0.0458159  0.         0.
 0.0141577  0.         0.018188   0.1221132  0.01188548 0.        ]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Podemos visualizar las importancias de las características de forma similar a la forma en que visualizamos los coeficientes en el modelo lineal</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_feature_importances_cancer</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_features</span><span class="p">),</span> <span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature importance&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_feature_importances_cancer</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/21da4323c0f10c410aca14493cafecd90dd73b70824b2208d87783104503ea13.png" src="_images/21da4323c0f10c410aca14493cafecd90dd73b70824b2208d87783104503ea13.png" />
</div>
</div>
<ul class="simple">
<li><p>Aquí vemos que la característica utilizada en la división superior <code class="docutils literal notranslate"><span class="pre">(&quot;worst</span> <span class="pre">radio&quot;)</span></code> es, con mucho, la más importante. Esto confirma nuestra observación al analizar el árbol de que el primer nivel ya separa bastante bien las dos clases. Sin embargo, si una característica tiene un <code class="docutils literal notranslate"><span class="pre">feature_importance</span></code> bajo, no significa que esta característica sea poco informativa. Solo significa que la característica no fue elegida por el árbol, probablemente porque otra característica codifica la misma información.</p></li>
<li><p>Un valor <strong>negativo</strong> de importancia de característica significa que <strong>la característica aumenta la pérdida</strong>. Esto indica que tu modelo no está haciendo un buen uso de esta característica. Esto podría significar que tu modelo está mal ajustado (no tiene suficientes iteraciones y no ha utilizado la característica lo suficiente) o que <strong>la característica no es útil, por lo que podría intentar eliminarla para mejorar la calidad final</strong>.</p></li>
<li><p>A diferencia de los coeficientes de los modelos lineales, <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">importancias</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">son</span> <span class="pre">siempre</span> <span class="pre">positivas</span> <span class="pre">y</span> <span class="pre">no</span> <span class="pre">codifican</span> <span class="pre">la</span> <span class="pre">clase</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">que</span> <span class="pre">es</span> <span class="pre">indicativa</span> <span class="pre">una</span> <span class="pre">característica</span></code>. Las <code class="docutils literal notranslate"><span class="pre">feature_importance</span></code> nos dicen que el <code class="docutils literal notranslate"><span class="pre">&quot;worst</span> <span class="pre">radio&quot;</span></code> es importante, pero <strong>no si un</strong> <code class="docutils literal notranslate"><span class="pre">&quot;worst</span> <span class="pre">radio&quot;</span></code> <strong>alto es indicativo de que una muestra es benigna o maligna</strong>.</p></li>
<li><p>De hecho, puede que no haya una relación tan sencilla entre las características y la clase, como se puede ver en el siguiente ejemplo. La siguiente figura muestra un conjunto de datos bidimensional en el que <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">característica</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">eje</span></code> <span class="math notranslate nohighlight">\(y\)</span> <code class="docutils literal notranslate"><span class="pre">tiene</span> <span class="pre">una</span> <span class="pre">relación</span> <span class="pre">no</span> <span class="pre">monótona</span> <span class="pre">con</span> <span class="pre">la</span> <span class="pre">etiqueta</span> <span class="pre">de</span> <span class="pre">clase,</span> <span class="pre">y</span> <span class="pre">los</span> <span class="pre">límites</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">encontrados</span> <span class="pre">por</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">de</span> <span class="pre">decisión</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mglearn</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tree</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_tree_not_monotone</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Feature importances: [0. 1.]
</pre></div>
</div>
<img alt="_images/2176701a69e1bbb5fb4800d75a2ca0a7c5cc7498988f0ac1405013fc2db7399f.svg" src="_images/2176701a69e1bbb5fb4800d75a2ca0a7c5cc7498988f0ac1405013fc2db7399f.svg" /><img alt="_images/28e3da58b619d48fac14a532749ddb2dd81c38f97c450233b18c122566dd05fc.png" src="_images/28e3da58b619d48fac14a532749ddb2dd81c38f97c450233b18c122566dd05fc.png" />
</div>
</div>
<ul class="simple">
<li><p>El gráfico muestra un conjunto de datos con dos características y dos clases. Aquí, toda la información está contenida en <code class="docutils literal notranslate"><span class="pre">X[1]</span></code> , y <code class="docutils literal notranslate"><span class="pre">X[0]</span></code> no se utiliza en absoluto. Pero la relación entre X[1] y la clase de salida no es monótona, lo que significa que no podemos decir <code class="docutils literal notranslate"><span class="pre">&quot;un</span> <span class="pre">valor</span> <span class="pre">alto</span> <span class="pre">de</span> <span class="pre">X[0]</span> <span class="pre">significa</span> <span class="pre">la</span> <span class="pre">clase</span> <span class="pre">0,</span> <span class="pre">y</span> <span class="pre">un</span> <span class="pre">valor</span> <span class="pre">bajo</span> <span class="pre">significa</span> <span class="pre">la</span> <span class="pre">clase</span> <span class="pre">1&quot;</span> <span class="pre">(o</span> <span class="pre">viceversa)</span></code>. Aunque hemos centrado nuestra discusión aquí en los árboles de decisión para la clasificación, todo lo que se ha dicho es igualmente cierto para los árboles de decisión para la regresión, como se implementa en <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code>.</p></li>
<li><p>El uso y análisis de los árboles de regresión es muy similar al de los árboles de clasificación. Hay una propiedad particular del uso de modelos basados en árboles para regresión que queremos señalar, sin embargo, <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span> <span class="pre">(y</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">otros</span> <span class="pre">modelos</span> <span class="pre">de</span> <span class="pre">regresión</span> <span class="pre">basados</span> <span class="pre">en</span> <span class="pre">árboles)</span> <span class="pre">no</span> <span class="pre">son</span> <span class="pre">capaces</span> <span class="pre">de</span> <span class="pre">extrapolar,</span> <span class="pre">o</span> <span class="pre">hacer</span> <span class="pre">predicciones</span> <span class="pre">fuera</span> <span class="pre">del</span> <span class="pre">rango</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>. Veamos esto con más detalle, utilizando un conjunto de datos de los precios históricos de la memoria de los ordenadores (RAM). La siguiente figura muestra el conjunto de datos, con la fecha en el eje <span class="math notranslate nohighlight">\(x\)</span> y el precio de un megabyte de RAM en ese año en el eje <span class="math notranslate nohighlight">\(y\)</span>:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">ram_prices</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/lihkir/Data/main/ram_price.csv&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">ram_prices</span><span class="o">.</span><span class="n">date</span><span class="p">,</span> <span class="n">ram_prices</span><span class="o">.</span><span class="n">price</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Year&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Price in $/Mbyte&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a4f80ef9e5b650dfc414e9f3115942bd92ad8d537045fb2fac350ee75d7c0a7d.png" src="_images/a4f80ef9e5b650dfc414e9f3115942bd92ad8d537045fb2fac350ee75d7c0a7d.png" />
</div>
</div>
<ul class="simple">
<li><p>Nótese que en la escala logarítmica del eje <span class="math notranslate nohighlight">\(y\)</span>, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">relación</span> <span class="pre">parece</span> <span class="pre">ser</span> <span class="pre">bastante</span> <span class="pre">lineal</span> <span class="pre">y,</span> <span class="pre">por</span> <span class="pre">tanto,</span> <span class="pre">debería</span> <span class="pre">ser</span> <span class="pre">relativamente</span> <span class="pre">fácil</span> <span class="pre">de</span> <span class="pre">predecir</span></code>. Vamos a hacer una predicción para los años posteriores al 2000 utilizando los datos históricos hasta esa fecha como única característica. Compararemos dos modelos sencillos: un <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> y <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>.</p></li>
<li><p>Cambiamos la escala de los precios utilizando un logaritmo, para que la relación sea relativamente lineal. Esto no supone ninguna diferencia para el <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code>, pero supone una gran diferencia para el <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code>. Después de entrenar los modelos y hacer predicciones, aplicamos la función exponencial para deshacer la transformación del logaritmo. Realizamos predicciones sobre todo el conjunto de datos para su visualización, pero para una evaluación cuantitativa, sólo consideraríamos el conjunto de datos de prueba</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Utilizamos los datos históricos para prever los precios después del año 2000. <code class="docutils literal notranslate"><span class="pre">Realizamos</span> <span class="pre">predicción</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">precios</span> <span class="pre">en</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">fecha</span></code>. Utilizamos una transformación logarítmica para obtener una relación más sencilla de los datos con el objetivo</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_train</span> <span class="o">=</span> <span class="n">ram_prices</span><span class="p">[</span><span class="n">ram_prices</span><span class="o">.</span><span class="n">date</span> <span class="o">&lt;</span> <span class="mi">2000</span><span class="p">]</span>
<span class="n">data_test</span>  <span class="o">=</span> <span class="n">ram_prices</span><span class="p">[</span><span class="n">ram_prices</span><span class="o">.</span><span class="n">date</span> <span class="o">&gt;=</span> <span class="mi">2000</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data_train</span><span class="o">.</span><span class="n">date</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="c1"># Vector columna</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">data_train</span><span class="o">.</span><span class="n">price</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">linear_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">X_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ram_prices</span><span class="o">.</span><span class="n">date</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">pred_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_all</span><span class="p">)</span>
<span class="n">pred_lr</span> <span class="o">=</span> <span class="n">linear_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_all</span><span class="p">)</span>

<span class="n">price_tree</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">pred_tree</span><span class="p">)</span>
<span class="n">price_lr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">pred_lr</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Realizamos una figura para comparar las predicciones del árbol de decisión y del modelo de regresión lineal con la los datos reales de entrenamiento y de prueba</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">data_train</span><span class="o">.</span><span class="n">date</span><span class="p">,</span> <span class="n">data_train</span><span class="o">.</span><span class="n">price</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">data_test</span><span class="o">.</span><span class="n">date</span><span class="p">,</span> <span class="n">data_test</span><span class="o">.</span><span class="n">price</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Test data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">ram_prices</span><span class="o">.</span><span class="n">date</span><span class="p">,</span> <span class="n">price_tree</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Tree prediction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">ram_prices</span><span class="o">.</span><span class="n">date</span><span class="p">,</span> <span class="n">price_lr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Linear prediction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f00486635ed3885adb73d3b39a70d85a1b8551688a38e4760f7525eca54b143f.png" src="_images/f00486635ed3885adb73d3b39a70d85a1b8551688a38e4760f7525eca54b143f.png" />
</div>
</div>
<ul class="simple">
<li><p>La diferencia entre los modelos es bastante sorprendente. El modelo lineal se aproxima a los datos con una línea, como sabíamos que haría. Esta línea proporciona una previsión bastante buena para los datos de prueba (los años posteriores al 2000), mientras que pasa por alto algunas de las variaciones más finas en los datos de entrenamiento y de prueba. <code class="docutils literal notranslate"><span class="pre">El</span> <span class="pre">modelo</span> <span class="pre">de</span> <span class="pre">árbol,</span> <span class="pre">por</span> <span class="pre">su</span> <span class="pre">parte,</span> <span class="pre">hace</span> <span class="pre">predicciones</span> <span class="pre">perfectas</span> <span class="pre">sobre</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>, no restringimos la complejidad del árbol, por lo que aprendió de memoria todo el conjunto de datos. Sin embargo, <code class="docutils literal notranslate"><span class="pre">una</span> <span class="pre">vez</span> <span class="pre">que</span> <span class="pre">salimos</span> <span class="pre">del</span> <span class="pre">rango</span> <span class="pre">para</span> <span class="pre">el</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">arbol</span> <span class="pre">tiene</span> <span class="pre">datos,</span> <span class="pre">el</span> <span class="pre">modelo</span> <span class="pre">simplemente</span> <span class="pre">sigue</span> <span class="pre">prediciendo</span> <span class="pre">el</span> <span class="pre">último</span> <span class="pre">punto</span> <span class="pre">conocido</span></code>. El árbol no tiene la capacidad para generar <code class="docutils literal notranslate"><span class="pre">&quot;nuevas&quot;</span></code> respuestas, fuera de lo que se vio en los datos de de entrenamiento. Esta deficiencia se aplica a todos los modelos basados en árboles.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Puntos</span> <span class="pre">fuertes,</span> <span class="pre">puntos</span> <span class="pre">débiles</span> <span class="pre">y</span> <span class="pre">parámetros</span></code></strong></p>
<ul class="simple">
<li><p>Como ya se ha comentado, los parámetros que controlan la complejidad del modelo en los árboles de decisión son los parámetros de pre-selección que detienen la construcción del árbol antes de que esté completamente desarrollado. Por lo general, la elección de una de las estrategias de pre-poda y configuración de: <code class="docutils literal notranslate"><span class="pre">max_depth,</span> <span class="pre">max_leaf_nodes,</span> <span class="pre">min_samples_leaf</span></code> es suficiente para evitar el overfitting de la misma.</p></li>
<li><p>Los árboles de decisión tienen dos ventajas sobre muchos de los algoritmos que hemos analizado hasta ahora: <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">modelo</span> <span class="pre">resultante</span> <span class="pre">puede</span> <span class="pre">ser</span> <span class="pre">fácilmente</span> <span class="pre">visualizado</span> <span class="pre">y</span> <span class="pre">entendido</span> <span class="pre">por</span> <span class="pre">personas</span> <span class="pre">no</span> <span class="pre">expertas</span> <span class="pre">(al</span> <span class="pre">menos</span> <span class="pre">para</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">más</span> <span class="pre">pequeños)</span></code>, y los <code class="docutils literal notranslate"><span class="pre">algoritmos</span> <span class="pre">son</span> <span class="pre">completamente</span> <span class="pre">invariables</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">escala</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>. Como <code class="docutils literal notranslate"><span class="pre">cada</span> <span class="pre">característica</span> <span class="pre">se</span> <span class="pre">procesa</span> <span class="pre">por</span> <span class="pre">separado</span></code>, y las posibles divisiones de los datos no dependen de la escala, no es necesario un preprocesamiento como la normalización o la estandarización de las características para los algoritmos de árboles de decisión.</p></li>
<li><p>En particular, los árboles de decisión funcionan bien cuando se tienen características que están en escalas completamente diferentes, o una mezcla de características binarias y continuas. <code class="docutils literal notranslate"><span class="pre">El</span> <span class="pre">principal</span> <span class="pre">inconveniente</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">es</span> <span class="pre">que,</span> <span class="pre">incluso</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">uso</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">pre-poda,</span> <span class="pre">tienden</span> <span class="pre">a</span> <span class="pre">sobreajustarse</span> <span class="pre">y</span> <span class="pre">a</span> <span class="pre">proporcionar</span> <span class="pre">un</span> <span class="pre">pobre</span> <span class="pre">rendimiento</span> <span class="pre">de</span> <span class="pre">generalización</span></code>. Por lo tanto, en la mayoría de las aplicaciones, los <code class="docutils literal notranslate"><span class="pre">métodos</span> <span class="pre">combiandos</span></code> que analizamos a continuación suelen utilizarse en lugar de un único árbol de decisión.</p></li>
</ul>
</section>
<section id="ensamble-de-arboles-de-decision">
<h2>Ensamble de árboles de decisión<a class="headerlink" href="#ensamble-de-arboles-de-decision" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Los</span> <span class="pre">ensambles</span> <span class="pre">son</span> <span class="pre">métodos</span> <span class="pre">que</span> <span class="pre">combinan</span> <span class="pre">múltiples</span> <span class="pre">modelos</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">automático</span> <span class="pre">para</span> <span class="pre">crear</span> <span class="pre">modelos</span> <span class="pre">más</span> <span class="pre">potentes</span></code>. Hay muchos modelos en la literatura de aprendizaje automático que pertenecen a esta categoría, pero hay dos modelos de ensamble que han demostrado su eficacia en una amplia gama de conjuntos de datos de clasificación y regresión, ambos utilizan árboles de decisión como bloques de construcción: los <code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">forest</span></code> y los <code class="docutils literal notranslate"><span class="pre">gradient</span> <span class="pre">boosted</span> <span class="pre">decision</span> <span class="pre">trees.</span></code></p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Bosques</span> <span class="pre">aleatorios</span></code></strong></p>
<ul class="simple">
<li><p>Como acabamos de observar, uno de los principales inconvenientes de <code class="docutils literal notranslate"><span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">es</span> <span class="pre">que</span> <span class="pre">tienden</span> <span class="pre">a</span> <span class="pre">sobreajustar</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrenamiento.</span> <span class="pre">Los</span> <span class="pre">bosques</span> <span class="pre">aleatorios</span> <span class="pre">son</span> <span class="pre">una</span> <span class="pre">forma</span> <span class="pre">de</span> <span class="pre">abordar</span> <span class="pre">este</span> <span class="pre">problema</span></code>. Un bosque aleatorio es esencialmente una colección de árboles de decisión, donde cada árbol es ligeramente diferente de de los demás. La idea detrás de los bosques aleatorios es que cada árbol puede hacer un trabajo de predicción relativamente bien, pero es probable que se ajuste demasiado a una parte de los datos. <code class="docutils literal notranslate"><span class="pre">Si</span> <span class="pre">construimos</span> <span class="pre">muchos</span> <span class="pre">árboles,</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">cuales</span> <span class="pre">funcionan</span> <span class="pre">bien</span> <span class="pre">y</span> <span class="pre">se</span> <span class="pre">ajustan</span> <span class="pre">en</span> <span class="pre">exceso</span> <span class="pre">de</span> <span class="pre">diferentes</span> <span class="pre">maneras,</span> <span class="pre">podemos</span> <span class="pre">reducir</span> <span class="pre">la</span> <span class="pre">cantidad</span> <span class="pre">de</span> <span class="pre">sobreajuste</span> <span class="pre">promediando</span> <span class="pre">sus</span> <span class="pre">resultados</span></code>. Esta reducción de overfitting, al tiempo que se mantiene el poder de predicción de los árboles se puede verificar matemáticamente.</p></li>
<li><p>Para poner en práctica esta estrategia, tenemos que construir muchos árboles de decisión. Cada árbol debe hacer un trabajo aceptable de predicción del objetivo, y también debe ser diferente de los otros árboles. Los bosques aleatorios reciben su nombre de la inyección de aleatoriedad en la construcción de árboles para garantizar que cada árbol sea diferente. Hay dos formas de aleatorizar los árboles de un bosque aleatorio: <code class="docutils literal notranslate"><span class="pre">seleccionando</span> <span class="pre">los</span> <span class="pre">puntos</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">utilizados</span> <span class="pre">para</span> <span class="pre">construir</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">y</span> <span class="pre">seleccionando</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">en</span> <span class="pre">cada</span> <span class="pre">prueba</span> <span class="pre">de</span> <span class="pre">división</span></code>. Veamos este proceso con más detalle.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Construcción</span> <span class="pre">de</span> <span class="pre">bosques</span> <span class="pre">aleatorios</span></code></strong></p>
<ul class="simple">
<li><p>Para construir un modelo de bosque aleatorio (<code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">forest</span></code>), hay que decidir el número de árboles a construir (el parámetro <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> de <code class="docutils literal notranslate"><span class="pre">RandomForestRegressor</span></code> o <code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code>). Digamos que queremos construir 10 árboles. Estos árboles se construirán de forma completamente independiente unos de otros, y <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">algoritmo</span> <span class="pre">hará</span> <span class="pre">elecciones</span> <span class="pre">aleatorias</span> <span class="pre">diferentes</span> <span class="pre">para</span> <span class="pre">cada</span> <span class="pre">árbol</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">fin</span> <span class="pre">de</span> <span class="pre">asegurarse</span> <span class="pre">de</span> <span class="pre">que</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">son</span> <span class="pre">distintos</span></code>.</p></li>
<li><p>Para construir un árbol, primero tomamos lo que se llama una <code class="docutils literal notranslate"><span class="pre">muestra</span> <span class="pre">bootstrap</span></code> de nuestros datos. Es decir, a partir de nuestras <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> (puntos de datos), extraemos repetidamente un ejemplo al azar con reemplazo (lo que significa que la misma muestra puede ser elegida varias veces), <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> veces. Esto creará un conjunto de datos que es tan grande como el conjunto de datos original, pero en el que faltarán algunos puntos de dato (aproximadamente un tercio), y algunos se repetirán.</p></li>
<li><p>Para ilustrarlo, digamos que queremos crear una <code class="docutils literal notranslate"><span class="pre">muestra</span> <span class="pre">bootstrap</span></code> de la lista <code class="docutils literal notranslate"><span class="pre">['a',</span> <span class="pre">'b',</span> <span class="pre">'c',</span> <span class="pre">'d']</span></code>. Una posible muestra bootstrap sería <code class="docutils literal notranslate"><span class="pre">['b',</span> <span class="pre">'d',</span> <span class="pre">'d',</span> <span class="pre">'c']</span></code>. Otra muestra posible sería <code class="docutils literal notranslate"><span class="pre">['d',</span> <span class="pre">'a',</span> <span class="pre">'d',</span> <span class="pre">'a']</span></code>. A continuación, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">construye</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">basado</span> <span class="pre">en</span> <span class="pre">este</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">(muestra</span> <span class="pre">bootstrap)</span> <span class="pre">recién</span> <span class="pre">creado</span></code>. Sin embargo, el algoritmo que hemos descrito para el árbol de decisión se ha modificado ligeramente. En lugar de buscar la mejor prueba para cada nodo, el algoritmo <code class="docutils literal notranslate"><span class="pre">selecciona</span> <span class="pre">aleatoriamente</span> <span class="pre">un</span> <span class="pre">subconjunto</span> <span class="pre">de</span> <span class="pre">características</span> <span class="pre">y</span> <span class="pre">busca</span> <span class="pre">la</span> <span class="pre">mejor</span> <span class="pre">prueba</span> <span class="pre">posible</span> <span class="pre">que</span> <span class="pre">incluya</span> <span class="pre">una</span> <span class="pre">de</span> <span class="pre">estas</span> <span class="pre">características</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>El número de características que se seleccionan se controla con el parámetro <code class="docutils literal notranslate"><span class="pre">max_features</span></code>. La selección de un subconjunto de características se repite por separado en cada nodo, de modo que <code class="docutils literal notranslate"><span class="pre">cada</span> <span class="pre">nodo</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">puede</span> <span class="pre">tomar</span> <span class="pre">una</span> <span class="pre">decisión</span> <span class="pre">utilizando</span> <span class="pre">un</span> <span class="pre">subconjunto</span> <span class="pre">diferente</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">características</span></code>. <code class="docutils literal notranslate"><span class="pre">El</span> <span class="pre">muestreo</span> <span class="pre">bootstrap</span> <span class="pre">hace</span> <span class="pre">que</span> <span class="pre">cada</span> <span class="pre">árbol</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">del</span> <span class="pre">bosque</span> <span class="pre">aleatorio</span> <span class="pre">se</span> <span class="pre">construya</span> <span class="pre">en</span> <span class="pre">un</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">ligeramente</span> <span class="pre">diferente</span></code>. Debido a la selección de características en cada nodo, cada división de cada árbol opera con un subconjunto diferente de características. Juntos, estos dos mecanismos aseguran que todos los árboles del bosque aleatorio son diferentes.</p></li>
<li><p>En este proceso el parámetro <code class="docutils literal notranslate"><span class="pre">max_features</span></code> es crítico. Si establecemos <code class="docutils literal notranslate"><span class="pre">max_features</span></code> en <code class="docutils literal notranslate"><span class="pre">n_features</span></code>, <code class="docutils literal notranslate"><span class="pre">significa</span> <span class="pre">que</span> <span class="pre">cada</span> <span class="pre">división</span> <span class="pre">puede</span> <span class="pre">mirar</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos,</span> <span class="pre">y</span> <span class="pre">no</span> <span class="pre">se</span> <span class="pre">inyectará</span> <span class="pre">aleatoriedad</span> <span class="pre">ninguna</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">selección</span> <span class="pre">de</span> <span class="pre">características</span></code> (la aleatoriedad debida al bootstrap permanece, sin embargo). Si establecemos <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">en</span> <span class="pre">1,</span> <span class="pre">esto</span> <span class="pre">significa</span> <span class="pre">que</span> <span class="pre">las</span> <span class="pre">divisiones</span> <span class="pre">no</span> <span class="pre">tienen</span> <span class="pre">ninguna</span> <span class="pre">opción</span> <span class="pre">sobre</span> <span class="pre">qué</span> <span class="pre">característica</span> <span class="pre">probar</span></code>, y sólo pueden buscar sobre diferentes umbrales para la característica que fue seleccionada  al azar. Por lo tanto, un <code class="docutils literal notranslate"><span class="pre">max_feature</span></code> significa que los árboles del bosque aleatorio serán bastante similares y podrán con facilidad ajustarse a los datos, utilizando las características más distintivas. <code class="docutils literal notranslate"><span class="pre">Un</span> <span class="pre">max_features</span> <span class="pre">bajo</span> <span class="pre">significa</span> <span class="pre">que</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">del</span> <span class="pre">bosque</span> <span class="pre">aleatorio</span> <span class="pre">serán</span> <span class="pre">bastante</span> <span class="pre">diferentes,</span> <span class="pre">y</span> <span class="pre">que</span> <span class="pre">cada</span> <span class="pre">árbol</span> <span class="pre">puede</span> <span class="pre">necesitar</span> <span class="pre">ser</span> <span class="pre">muy</span> <span class="pre">profundo</span> <span class="pre">para</span> <span class="pre">ajustarse</span> <span class="pre">bien</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">datos</span></code>.</p></li>
<li><p>Para hacer una predicción utilizando el bosque aleatorio, el algoritmo realiza primero una predicción para cada árbol del bosque. <code class="docutils literal notranslate"><span class="pre">Para</span> <span class="pre">la</span> <span class="pre">regresión,</span> <span class="pre">podemos</span> <span class="pre">promediar</span> <span class="pre">estos</span> <span class="pre">resultados</span> <span class="pre">para</span> <span class="pre">obtener</span> <span class="pre">nuestra</span> <span class="pre">predicción</span> <span class="pre">final</span></code>. Para la clasificación, se utiliza una estrategia de <code class="docutils literal notranslate"><span class="pre">&quot;votación</span> <span class="pre">suave&quot;</span></code>. Esto significa que cada algoritmo hace una predicción <code class="docutils literal notranslate"><span class="pre">&quot;suave&quot;</span></code>, proporcionando una probabilidad para cada posible etiqueta de salida. <code class="docutils literal notranslate"><span class="pre">Las</span> <span class="pre">probabilidades</span> <span class="pre">predichas</span> <span class="pre">por</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">se</span> <span class="pre">promedian</span> <span class="pre">y</span> <span class="pre">se</span> <span class="pre">predice</span> <span class="pre">la</span> <span class="pre">clase</span> <span class="pre">con</span> <span class="pre">la</span> <span class="pre">mayor</span> <span class="pre">probabilidad</span></code>.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Análisis</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">bosques</span> <span class="pre">aleatorios</span></code></strong>. Apliquemos un bosque aleatorio compuesto por cinco árboles al conjunto de datos <code class="docutils literal notranslate"><span class="pre">two_moons</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomForestClassifier(n_estimators=5, random_state=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">RandomForestClassifier</label><div class="sk-toggleable__content"><pre>RandomForestClassifier(n_estimators=5, random_state=2)</pre></div></div></div></div></div></div></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Los</span> <span class="pre">árboles</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">construyen</span> <span class="pre">como</span> <span class="pre">parte</span> <span class="pre">del</span> <span class="pre">bosque</span> <span class="pre">aleatorio</span> <span class="pre">se</span> <span class="pre">almacenan</span> <span class="pre">en</span> <span class="pre">estimator_attribute</span></code>. Visualicemos los límites de decisión aprendidos por cada árbol, junto con su predicción agregada</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">tree</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">forest</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Tree </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_tree_partition</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">tree</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">forest</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.4</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Random Forest&quot;</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ff09e64f846f9ef5672fea1dc31a87d36469b896debd66291f3b20aa794c240a.png" src="_images/ff09e64f846f9ef5672fea1dc31a87d36469b896debd66291f3b20aa794c240a.png" />
</div>
</div>
<ul class="simple">
<li><p>Se puede ver claramente que los límites de decisión aprendidos por los cinco árboles son bastante diferentes. Cada uno de ellos comete algunos errores, ya que algunos de los puntos que se representan aquí no se incluyeron en los conjuntos de entrenamiento de los árboles, debido al muestreo bootstrap. <code class="docutils literal notranslate"><span class="pre">El</span> <span class="pre">bosque</span> <span class="pre">aleatorio</span> <span class="pre">se</span> <span class="pre">ajusta</span> <span class="pre">menos</span> <span class="pre">que</span> <span class="pre">cualquiera</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">por</span> <span class="pre">separado</span> <span class="pre">y</span> <span class="pre">proporciona</span> <span class="pre">un</span> <span class="pre">límite</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">mucho</span> <span class="pre">más</span> <span class="pre">intuitivo</span></code>. En cualquier aplicación real, utilizaríamos muchos más árboles (a menudo cientos o miles), lo que daría lugar a límites aún más suaves.</p></li>
</ul>
<ul class="simple">
<li><p>Como otro ejemplo, apliquemos un bosque aleatorio compuesto por 100 árboles en el conjunto de datos <code class="docutils literal notranslate"><span class="pre">Breast</span> <span class="pre">Cancer</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">forest</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">forest</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 1.000
Accuracy on test set: 0.972
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">El</span> <span class="pre">bosque</span> <span class="pre">aleatorio</span> <span class="pre">nos</span> <span class="pre">da</span> <span class="pre">una</span> <span class="pre">precisión</span> <span class="pre">del</span> <span class="pre">97%,</span> <span class="pre">mejor</span> <span class="pre">que</span> <span class="pre">los</span> <span class="pre">modelos</span> <span class="pre">lineales</span> <span class="pre">o</span> <span class="pre">un</span> <span class="pre">árbol</span> <span class="pre">de</span> <span class="pre">decisión</span> <span class="pre">único,</span> <span class="pre">sin</span> <span class="pre">necesidad</span> <span class="pre">de</span> <span class="pre">ajustar</span> <span class="pre">ningún</span> <span class="pre">parámetro</span></code>. Podríamos ajustar la configuración de <code class="docutils literal notranslate"><span class="pre">max_features</span></code>, o aplicar la pre-selección como hicimos con el árbol de decisión simple. Sin embargo, a menudo los parámetros por defecto del bosque aleatorio ya funcionan bastante bien. <code class="docutils literal notranslate"><span class="pre">Al</span> <span class="pre">igual</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">árbol</span> <span class="pre">de</span> <span class="pre">decisión,</span> <span class="pre">el</span> <span class="pre">bosque</span> <span class="pre">aleatorio</span> <span class="pre">proporciona</span> <span class="pre">importancias</span> <span class="pre">de</span> <span class="pre">características,</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">calculan</span> <span class="pre">agregando</span> <span class="pre">las</span> <span class="pre">importancias</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">del</span> <span class="pre">bosque</span></code>. Normalmente, las <code class="docutils literal notranslate"><span class="pre">importancias</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">proporcionadas</span> <span class="pre">por</span> <span class="pre">el</span> <span class="pre">bosque</span> <span class="pre">aleatorio</span> <span class="pre">son</span> <span class="pre">más</span> <span class="pre">fiables</span> <span class="pre">que</span> <span class="pre">las</span> <span class="pre">proporcionadas</span> <span class="pre">por</span> <span class="pre">un</span> <span class="pre">solo</span> <span class="pre">árbol</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_feature_importances_cancer</span><span class="p">(</span><span class="n">forest</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8f64f65deda339946f3ddef8a527b0c9b1f205caade65f6f3c56d803aa0d8aad.png" src="_images/8f64f65deda339946f3ddef8a527b0c9b1f205caade65f6f3c56d803aa0d8aad.png" />
</div>
</div>
<ul class="simple">
<li><p>Como puede ver, <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">bosque</span> <span class="pre">aleatorio</span> <span class="pre">da</span> <span class="pre">una</span> <span class="pre">importancia</span> <span class="pre">no</span> <span class="pre">nula</span> <span class="pre">a</span> <span class="pre">muchas</span> <span class="pre">más</span> <span class="pre">características</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">árbol</span> <span class="pre">simple</span></code>. Al igual que el árbol de decisión simple, el bosque aleatorio también da importancia a la característica <code class="docutils literal notranslate"><span class="pre">&quot;worst</span> <span class="pre">radius&quot;</span></code>, pero en realidad elige <code class="docutils literal notranslate"><span class="pre">&quot;worst</span> <span class="pre">perimeter&quot;</span></code> como la <code class="docutils literal notranslate"><span class="pre">característica</span> <span class="pre">más</span> <span class="pre">informativa</span></code>. <code class="docutils literal notranslate"><span class="pre">La</span> <span class="pre">aleatoriedad</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">construcción</span> <span class="pre">del</span> <span class="pre">bosque</span> <span class="pre">aleatorio</span> <span class="pre">obliga</span> <span class="pre">al</span> <span class="pre">algoritmo</span> <span class="pre">a</span> <span class="pre">considerar</span> <span class="pre">muchas</span> <span class="pre">explicaciones</span> <span class="pre">posibles</span></code>. El resultado es que el bosque aleatorio capta una imagen mucho más amplia de los datos que un árbol simple.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<p>La <code class="docutils literal notranslate"><span class="pre">importancia</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">característica</span></code> se calcula como la disminución de la impureza del nodo ponderada por la probabilidad de alcanzar ese nodo. <code class="docutils literal notranslate"><span class="pre">La</span> <span class="pre">probabilidad</span> <span class="pre">del</span> <span class="pre">nodo</span> <span class="pre">puede</span> <span class="pre">calcularse</span> <span class="pre">mediante</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">muestras</span> <span class="pre">que</span> <span class="pre">llegan</span> <span class="pre">al</span> <span class="pre">nodo,</span> <span class="pre">dividido</span> <span class="pre">por</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">total</span> <span class="pre">de</span> <span class="pre">muestras.</span> <span class="pre">Cuanto</span> <span class="pre">mayor</span> <span class="pre">sea</span> <span class="pre">el</span> <span class="pre">valor,</span> <span class="pre">más</span> <span class="pre">importante</span> <span class="pre">será</span> <span class="pre">la</span> <span class="pre">característica</span></code>.La <code class="docutils literal notranslate"><span class="pre">impureza</span> <span class="pre">de</span> <span class="pre">Gini</span></code> se calcula mediante la formula usada al inicio de esta sección</p>
<div class="math notranslate nohighlight">
\[
I(t)=\sum_{m=1}^{M}P(\omega_{m}|t)(1-P(\omega_{m}|t)).
\]</div>
</div>
<p><strong><code class="docutils literal notranslate"><span class="pre">Puntos</span> <span class="pre">fuertes,</span> <span class="pre">puntos</span> <span class="pre">débiles</span> <span class="pre">y</span> <span class="pre">parámetros</span></code></strong></p>
<ul class="simple">
<li><p>Los bosques aleatorios para la regresión y la clasificación se encuentran actualmente entre los métodos de aprendizaje automático más utilizados. <code class="docutils literal notranslate"><span class="pre">Son</span> <span class="pre">muy</span> <span class="pre">potentes,</span> <span class="pre">suelen</span> <span class="pre">funcionar</span> <span class="pre">bien</span> <span class="pre">sin</span> <span class="pre">necesidad</span> <span class="pre">de</span> <span class="pre">ajustar</span> <span class="pre">mucho</span> <span class="pre">los</span> <span class="pre">parámetros</span> <span class="pre">y</span> <span class="pre">no</span> <span class="pre">requieren</span> <span class="pre">el</span> <span class="pre">escalado</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">datos</span></code>. Esencialmente, los bosques aleatorios comparten todos los beneficios de los árboles de decisión, mientras que compensan algunas de sus deficiencias. Una razón para seguir utilizando los árboles de decisión es, <strong><code class="docutils literal notranslate"><span class="pre">si</span> <span class="pre">se</span> <span class="pre">necesita</span> <span class="pre">una</span> <span class="pre">representación</span> <span class="pre">compacta</span> <span class="pre">del</span> <span class="pre">proceso</span> <span class="pre">de</span> <span class="pre">toma</span> <span class="pre">de</span> <span class="pre">decisiones</span></code></strong>. Es básicamente imposible interpretar decenas o cientos de árboles en detalle, y los árboles de los bosques aleatorios tienden a ser más profundos que los árboles de decisión (debido al uso de subconjuntos de características). Por lo tanto, si se necesita resumir las predicciones de forma visual para los no expertos, un único árbol de decisión puede ser la mejor opción.</p></li>
<li><p>Aunque la construcción de bosques aleatorios en grandes conjuntos de datos puede llevar algo de tiempo, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">puede</span> <span class="pre">paralelizar</span> <span class="pre">a</span> <span class="pre">través</span> <span class="pre">de</span> <span class="pre">múltiples</span> <span class="pre">núcleos</span> <span class="pre">de</span> <span class="pre">CPU</span> <span class="pre">dentro</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">ordenador</span> <span class="pre">fácilmente</span></code>. Si utiliza un procesador multinúcleo (como casi todos los ordenadores modernos),  puede utilizar el parámetro <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code> para ajustar el número de núcleos a utilizar. El uso de más núcleos de la CPU dará lugar a un aumento lineal de la velocidad (utilizando dos núcleos, el entrenamiento del bosque aleatorio será el doble de rápido), pero especificar <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code> mayor que el número de núcleos no ayudará. <code class="docutils literal notranslate"><span class="pre">Puede</span> <span class="pre">establecer</span> <span class="pre">n_jobs=-1</span> <span class="pre">para</span> <span class="pre">utilizar</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">núcleos</span> <span class="pre">en</span> <span class="pre">su</span> <span class="pre">ordenador</span></code>.</p></li>
<li><p>Debe tener en cuenta que los bosques aleatorios, por su naturaleza, son aleatorios, y establecen diferentes estados aleatorios (o no establecen el <code class="docutils literal notranslate"><span class="pre">random_state</span></code> en absoluto) puede cambiar drásticamente el modelo que se construye. <code class="docutils literal notranslate"><span class="pre">Cuanto</span> <span class="pre">más</span> <span class="pre">árboles</span> <span class="pre">haya</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">bosque,</span> <span class="pre">más</span> <span class="pre">robusto</span> <span class="pre">será</span> <span class="pre">frente</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">elección</span> <span class="pre">del</span> <span class="pre">estado</span> <span class="pre">aleatorio</span></code>. Si quiere tener resultados reproducibles, es importante fijar el <code class="docutils literal notranslate"><span class="pre">random_state</span></code> a caulquier número entero. <code class="docutils literal notranslate"><span class="pre">Los</span> <span class="pre">bosques</span> <span class="pre">aleatorios</span> <span class="pre">no</span> <span class="pre">tienden</span> <span class="pre">a</span> <span class="pre">funcionar</span> <span class="pre">bien</span> <span class="pre">en</span> <span class="pre">datos</span> <span class="pre">muy</span> <span class="pre">dimensionales</span> <span class="pre">y</span> <span class="pre">escasos,</span> <span class="pre">como</span> <span class="pre">los</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">texto</span></code>. Para este tipo de datos, los modelos lineales pueden ser más apropiados. Los bosques aleatorios suelen funcionar bien incluso en conjuntos de datos muy grandes, y el entrenamiento se puede paralelizar fácilmente en muchos núcleos de la CPU de un ordenador potente.  Sin embargo, <code class="docutils literal notranslate"><span class="pre">los</span> <span class="pre">bosques</span> <span class="pre">aleatorios</span> <span class="pre">requieren</span> <span class="pre">más</span> <span class="pre">memoria</span> <span class="pre">y</span> <span class="pre">son</span> <span class="pre">más</span> <span class="pre">lentos</span> <span class="pre">de</span> <span class="pre">entrenar</span> <span class="pre">y</span> <span class="pre">predecir</span> <span class="pre">que</span> <span class="pre">los</span> <span class="pre">modelos</span> <span class="pre">lineales</span></code>. Si el tiempo y la memoria son importantes en una aplicación, puede tener sentido utilizar un modelo lineal en su lugar.</p></li>
<li><p>Los parámetros importantes a ajustar son <code class="docutils literal notranslate"><span class="pre">n_estimators</span> <span class="pre">,</span> <span class="pre">max_features</span></code>, y posiblemente opciones de pre-poda como <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>. En el caso de <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>, un número mayor es siempre mejor. <code class="docutils literal notranslate"><span class="pre">Promediar</span> <span class="pre">más</span> <span class="pre">árboles</span> <span class="pre">dará</span> <span class="pre">lugar</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">conjunto</span> <span class="pre">más</span> <span class="pre">robusto</span> <span class="pre">al</span> <span class="pre">reducir</span> <span class="pre">el</span> <span class="pre">sobreajuste</span></code>. Sin embargo, hay rendimientos decrecientes, y más árboles necesitan más memoria y más tiempo para entrenar. Una regla común es construir “tantos como tenga tiempo/memoria”.  Como se ha descrito anteriormente, <code class="docutils literal notranslate"><span class="pre">max_features</span></code> determina el grado de aleatoriedad de cada árbol, y un <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">pequeño</span> <span class="pre">reduce</span> <span class="pre">el</span> <span class="pre">sobreajuste</span></code>. En general, es una buena regla general utilizar los valores por defecto: <code class="docutils literal notranslate"><span class="pre">max_features=sqrt(n_features)</span></code> para la clasificación y <code class="docutils literal notranslate"><span class="pre">max_features=log2(n_features)</span></code> para la regresión. Añadir <code class="docutils literal notranslate"><span class="pre">max_features</span></code> o <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> puede mejorar a veces el rendimiento. También puede reducir drásticamente los requisitos de espacio y tiempo para el entrenamiento y la predicción.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Árboles</span> <span class="pre">de</span> <span class="pre">regresión</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">reforzado</span> <span class="pre">(máquinas</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">reforzado)</span></code></strong></p>
<ul class="simple">
<li><p>El árbol de regresión de gradiente reforzado es otro método de conjunto que combina múltiples árboles de decisión. A pesar de la <code class="docutils literal notranslate"><span class="pre">&quot;regresión&quot;</span></code> en el nombre, estos modelos pueden utilizarse para la regresión y la clasificación. A diferencia del enfoque de bosque aleatorio, <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">refuerzo</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">funciona</span> <span class="pre">construyendo</span> <span class="pre">árboles</span> <span class="pre">en</span> <span class="pre">forma</span> <span class="pre">de</span> <span class="pre">serie,</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">que</span> <span class="pre">cada</span> <span class="pre">árbol</span> <span class="pre">trata</span> <span class="pre">de</span> <span class="pre">corregir</span> <span class="pre">los</span> <span class="pre">errores</span> <span class="pre">del</span> <span class="pre">anterior</span></code>. Por defecto, no hay aleatoriedad en los árboles de regresión de gradiente; en su lugar, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">utiliza</span> <span class="pre">una</span> <span class="pre">fuerte</span> <span class="pre">pre-poda</span></code>. Los árboles de impulso por gradiente suelen utilizar árboles muy poco profundos, de una a cinco profundidades, lo que hace que el modelo sea más pequeño en términos de memoria y que las predicciones sean más rápidas.</p></li>
<li><p>La idea principal del <code class="docutils literal notranslate"><span class="pre">gradient</span> <span class="pre">boosting</span></code> es combinar muchos modelos simples (en este contexto conocidos como aprendices débiles), como árboles poco profundos. Cada árbol sólo puede proporcionar buenas predicciones sobre una parte de los datos, por lo que se añaden más y más árboles para mejorar el rendimiento. <code class="docutils literal notranslate"><span class="pre">Los</span> <span class="pre">árboles</span> <span class="pre">con</span> <span class="pre">refuerzo</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">suelen</span> <span class="pre">ser</span> <span class="pre">los</span> <span class="pre">ganadores</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">concursos</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">automático</span> <span class="pre">y</span> <span class="pre">se</span> <span class="pre">utilizan</span> <span class="pre">mucho</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">industria</span></code>. Suelen ser un poco más sensibles a los ajustes de los parámetros que los bosques aleatorios, pero pueden proporcionar una mayor precisión si los parámetros se ajustan correctamente.</p></li>
<li><p>Aparte de la pre-poda y el número de árboles en el conjunto, otro parámetro importante del gradient boosting es la <code class="docutils literal notranslate"><span class="pre">tasa</span> <span class="pre">de</span> <span class="pre">aprendizaje,</span> <span class="pre">que</span> <span class="pre">controla</span> <span class="pre">la</span> <span class="pre">intensidad</span> <span class="pre">con</span> <span class="pre">la</span> <span class="pre">que</span> <span class="pre">cada</span> <span class="pre">árbol</span> <span class="pre">trata</span> <span class="pre">de</span> <span class="pre">corregir</span> <span class="pre">los</span> <span class="pre">errores</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">anteriores</span></code>. Una tasa de aprendizaje alta significa que cada árbol puede hacer correcciones más fuertes, lo que permite modelos más complejos. Si se añaden más árboles al conjunto, lo que puede conseguirse aumentando <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>, también aumenta la complejidad del modelo, ya que éste tiene más oportunidades de corregir errores en el conjunto de entrenamiento.</p></li>
</ul>
<ul class="simple">
<li><p>Este es un ejemplo del uso del clasificador <code class="docutils literal notranslate"><span class="pre">GradientBoosting</span></code> en el conjunto de datos del <code class="docutils literal notranslate"><span class="pre">Breast</span> <span class="pre">Cancer</span></code>. Por defecto, se utilizan 100 árboles de profundidad máxima 3 y una tasa de aprendizaje de 0.1</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gbrt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gbrt</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gbrt</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 1.000
Accuracy on test set: 0.965
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Como la <code class="docutils literal notranslate"><span class="pre">precisión</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">es</span> <span class="pre">del</span> <span class="pre">100%,</span> <span class="pre">es</span> <span class="pre">probable</span> <span class="pre">que</span> <span class="pre">estemos</span> <span class="pre">sobreajustando.</span> <span class="pre">Para</span> <span class="pre">reducir</span> <span class="pre">el</span> <span class="pre">sobreajuste,</span> <span class="pre">podemos</span> <span class="pre">aplicar</span> <span class="pre">una</span> <span class="pre">pre-poda</span> <span class="pre">más</span> <span class="pre">fuerte</span> <span class="pre">limitando</span> <span class="pre">la</span> <span class="pre">profundidad</span> <span class="pre">máxima</span> <span class="pre">o</span> <span class="pre">reducir</span> <span class="pre">la</span> <span class="pre">tasa</span> <span class="pre">de</span> <span class="pre">aprendizaje</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gbrt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gbrt</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gbrt</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.991
Accuracy on test set: 0.972
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">gbrt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gbrt</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gbrt</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.988
Accuracy on test set: 0.958
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Ambos métodos para disminuir la complejidad del modelo <code class="docutils literal notranslate"><span class="pre">redujeron</span> <span class="pre">la</span> <span class="pre">precisión</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento,</span> <span class="pre">como</span> <span class="pre">era</span> <span class="pre">de</span> <span class="pre">esperar</span></code>. En este caso, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">reducción</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">profundidad</span> <span class="pre">máxima</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">proporcionó</span> <span class="pre">una</span> <span class="pre">mejora</span> <span class="pre">significativa</span> <span class="pre">del</span> <span class="pre">modelo,</span> <span class="pre">mientras</span> <span class="pre">que</span> <span class="pre">la</span> <span class="pre">reducción</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">tasa</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">sólo</span> <span class="pre">aumentó</span> <span class="pre">la</span> <span class="pre">precisión</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">ligeramente</span></code>. En cuanto a los otros modelos basados en árboles de decisión, podemos volver a visualizar las características para obtener más información sobre nuestro modelo. Como utilizamos 100 árboles, es poco práctico inspeccionarlos todos, aunque todos tengan una profundidad de 1.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">gbrt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plot_feature_importances_cancer</span><span class="p">(</span><span class="n">gbrt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/77bea8b4765afeda6b271618487a48389f96e2f79087c526b4e646891eeaec45.png" src="_images/77bea8b4765afeda6b271618487a48389f96e2f79087c526b4e646891eeaec45.png" />
</div>
</div>
<ul class="simple">
<li><p>Podemos ver que las importancias de las características de los árboles <code class="docutils literal notranslate"><span class="pre">gradient-boost</span></code> son algo similares a las de los bosques aleatorios, aunque el refuerzo del gradiente ignora por completo algunas de las características. Como tanto el refuerzo de gradiente como los bosques aleatorios funcionan bien en tipos de datos similares, <code class="docutils literal notranslate"><span class="pre">un</span> <span class="pre">enfoque</span> <span class="pre">común</span> <span class="pre">es</span> <span class="pre">probar</span> <span class="pre">primero</span> <span class="pre">los</span> <span class="pre">bosques</span> <span class="pre">aleatorios,</span> <span class="pre">que</span> <span class="pre">funcionan</span> <span class="pre">con</span> <span class="pre">bastante</span> <span class="pre">solidez.</span> <span class="pre">Si</span> <span class="pre">los</span> <span class="pre">bosques</span> <span class="pre">aleatorios</span> <span class="pre">funcionan</span> <span class="pre">bien,</span> <span class="pre">pero</span> <span class="pre">el</span> <span class="pre">tiempo</span> <span class="pre">de</span> <span class="pre">predicción</span> <span class="pre">es</span> <span class="pre">un</span> <span class="pre">problema,</span> <span class="pre">o</span> <span class="pre">si</span> <span class="pre">es</span> <span class="pre">importante</span> <span class="pre">exprimir</span> <span class="pre">el</span> <span class="pre">último</span> <span class="pre">porcentaje</span> <span class="pre">de</span> <span class="pre">precisión</span> <span class="pre">del</span> <span class="pre">modelo</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">automático,</span> <span class="pre">pasar</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">refuerzo</span> <span class="pre">por</span> <span class="pre">gradiente</span> <span class="pre">suele</span> <span class="pre">ser</span> <span class="pre">útil</span></code>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">Si</span> <span class="pre">quiere</span> <span class="pre">aplicar</span> <span class="pre">el</span> <span class="pre">refuerzo</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">problema</span> <span class="pre">a</span> <span class="pre">gran</span> <span class="pre">escala,</span> <span class="pre">puede</span> <span class="pre">que</span> <span class="pre">merezca</span> <span class="pre">la</span> <span class="pre">pena</span> <span class="pre">investigar</span> <span class="pre">el</span> <span class="pre">paquete</span> <span class="pre">xgboost</span> <span class="pre">y</span> <span class="pre">su</span> <span class="pre">interfaz</span> <span class="pre">de</span> <span class="pre">Python,</span> <span class="pre">que</span> <span class="pre">hasta</span> <span class="pre">el</span> <span class="pre">momento</span> <span class="pre">es</span> <span class="pre">más</span> <span class="pre">rápido</span> <span class="pre">(y</span> <span class="pre">a</span> <span class="pre">veces</span> <span class="pre">más</span> <span class="pre">fácil</span> <span class="pre">de</span> <span class="pre">usar)</span> <span class="pre">que</span> <span class="pre">la</span> <span class="pre">implementación</span> <span class="pre">de</span> <span class="pre">scikit-learn</span> <span class="pre">en</span> <span class="pre">muchos</span> <span class="pre">conjuntos</span> <span class="pre">de</span> <span class="pre">datos</span></code></strong>.</p></li>
</ul>
<p><strong><code class="docutils literal notranslate"><span class="pre">Puntos</span> <span class="pre">fuertes,</span> <span class="pre">puntos</span> <span class="pre">débiles</span> <span class="pre">y</span> <span class="pre">parámetros</span></code></strong></p>
<ul class="simple">
<li><p>Los árboles de decisión con refuerzo de gradiente se encuentran entre los modelos más potentes y utilizados para el aprendizaje supervisado. <code class="docutils literal notranslate"><span class="pre">Su</span> <span class="pre">principal</span> <span class="pre">inconveniente</span> <span class="pre">es</span> <span class="pre">que</span> <span class="pre">requieren</span> <span class="pre">un</span> <span class="pre">ajuste</span> <span class="pre">cuidadoso</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">parámetros</span> <span class="pre">y</span> <span class="pre">pueden</span> <span class="pre">tardar</span> <span class="pre">mucho</span> <span class="pre">tiempo</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>. Al igual que otros modelos basados en árboles, el algoritmo funciona bien sin escalar y con una mezcla de características binarias y continuas. Al igual que otros modelos basados en árboles tampoco suele funcionar bien con datos dispersos de alta dimensión.</p></li>
<li><p>Los principales parámetros de los modelos de árbol de gradiente reforzado son el número de árboles, <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>, y el <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> que <code class="docutils literal notranslate"><span class="pre">controla</span> <span class="pre">el</span> <span class="pre">grado</span> <span class="pre">en</span> <span class="pre">que</span> <span class="pre">cada</span> <span class="pre">árbol</span> <span class="pre">puede</span> <span class="pre">corregir</span> <span class="pre">los</span> <span class="pre">errores</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">árboles</span> <span class="pre">anteriores</span></code>. Estos dos parámetros están muy interconectados, ya que una tasa de aprendizaje más baja significa que se necesitan más árboles para construir un modelo de complejidad similar. A diferencia de los bosques aleatorios, en los que un valor de <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> es siempre mejor, el aumento de <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> en el <code class="docutils literal notranslate"><span class="pre">gradient</span> <span class="pre">boosting</span></code> conduce a un modelo más complejo, lo que puede llevar a un sobreajuste. Una práctica común es ajustar <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> dependiendo del presupuesto de tiempo y memoria, y luego buscar sobre diferentes tasas de aprendizaje.</p></li>
<li><p>Otro parámetro importante es <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> (o alternativamente <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>), para reducir la complejidad de cada árbol. <code class="docutils literal notranslate"><span class="pre">Por</span> <span class="pre">lo</span> <span class="pre">general,</span> <span class="pre">la</span> <span class="pre">profundidad</span> <span class="pre">máxima</span> <span class="pre">es</span> <span class="pre">muy</span> <span class="pre">baja</span> <span class="pre">para</span> <span class="pre">los</span> <span class="pre">modelos</span> <span class="pre">de</span> <span class="pre">gradiente,</span> <span class="pre">a</span> <span class="pre">menudo</span> <span class="pre">no</span> <span class="pre">más</span> <span class="pre">allá</span> <span class="pre">de</span> <span class="pre">cinco</span> <span class="pre">divisiones</span></code>.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "ml_tf"
        },
        kernelOptions: {
            name: "ml_tf",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ml_tf'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="bayes_model.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Clasificadores Naive Bayes</p>
      </div>
    </a>
    <a class="right-next"
       href="svm_model.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Máquinas de vectores de soporte</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Árboles de decisión</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis">Análisis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combinacion-de-clasificadores">Combinación de clasificadores</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#enfoque-boosting">Enfoque Boosting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#la-funcion-log-loss">La función Log-Loss</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#arboles-boosting">Arboles Boosting</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#implementacion">Implementación</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#control-de-complejidad">Control de complejidad</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-de-los-arboles-de-decision">Análisis de los árboles de decisión</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#caracteristicas-importantes-en-los-arboles">Características importantes en los árboles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensamble-de-arboles-de-decision">Ensamble de árboles de decisión</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lihki Rubio, Ph.D.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>Lihki Rubio, Ph.D. All rights reserved.</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>