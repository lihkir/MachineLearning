
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>8. Redes Neuronales y Deep Learning &#8212; Machine Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=530fe47d" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/custom-checkpoint.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ann_model';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/custom.js?v=14184634"></script>
    <script src="_static/.ipynb_checkpoints/custom-checkpoint.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="9. Deep Learning para Visión por Computadora" href="dl_computer_vision.html" />
    <link rel="prev" title="7. Máquinas de vectores de soporte" href="svm_model.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/fotolihki.jpg" class="logo__image only-light" alt="Machine Learning - Home"/>
    <script>document.write(`<img src="_static/fotolihki.jpg" class="logo__image only-dark" alt="Machine Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Profesor: Dr. Lihki Rubio
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="supervised_intro.html">1. Aprendizaje supervisado</a></li>
<li class="toctree-l1"><a class="reference internal" href="knn_model.html">2. <span class="math notranslate nohighlight">\(k\)</span>-vecinos más cercanos</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_model.html">3. Regresión Ridge y Lasso</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes_model.html">4. Clasificador Bayesiano</a></li>
<li class="toctree-l1"><a class="reference internal" href="decisiontree_model.html">5. Random Forest y XGBoost</a></li>

<li class="toctree-l1"><a class="reference internal" href="svm_model.html">7. Máquinas de vectores de soporte</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. Redes Neuronales y Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl_computer_vision.html">9. Deep Learning para Visión por Computadora</a></li>




<li class="toctree-l1"><a class="reference internal" href="practical_pca.html">14. Análisis de Componentes Principales</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_evaluation.html">15. Evaluación de modelos</a></li>
<li class="toctree-l1"><a class="reference internal" href="chains_pipelines.html">16. Cadenas de Algoritmos y Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">17. Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">18. Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/ann_model.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Redes Neuronales y Deep Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradiente-descendente">8.1. Gradiente descendente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales">8.2. Redes neuronales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-perceptron">8.3. El perceptrón</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-multicapa-feed-forward">8.4. Redes Neuronales Multicapa Feed-Forward</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-totalmente-conectadas">8.5. Redes Totalmente Conectadas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-algoritmo-de-backpropagation">8.6. El Algoritmo De Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-esquema-de-backpropagation-para-gradiente-descendente">8.7. El Esquema De Backpropagation Para Gradiente Descendente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-gradientes">8.8. Cálculo de gradientes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-delta-nj-r">8.9. Cálculo de <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-de-redes-neuronales">8.10. Tuning de Redes Neuronales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-de-malware-por-api-calls">8.11. Análisis de Malware por API calls</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-convolucionales">8.12. Redes Neuronales Convolucionales</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-necesidad-de-convoluciones">8.12.1. La necesidad de convoluciones</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-etapa-de-convolucion">8.12.2. La etapa de convolución</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#el-paso-de-la-no-linealidad">8.12.3. El paso de la no linealidad</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-etapa-de-agrupacion">8.12.4. La etapa de agrupación</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolucion-sobre-volumenes">8.12.5. Convolución sobre volúmenes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#red-en-red-y-convolucion-1-1">8.12.6. Red en red y convolución 1 × 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arquitectura-cnn-completa">8.12.7. Arquitectura CNN completa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arquitecturas-de-redes-neuronales-convolucionales">8.12.8. Arquitecturas de Redes Neuronales Convolucionales</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lenet-5">8.12.9. LeNet-5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet">8.12.10. AlexNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vgg-16">8.12.11. VGG-16</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#googlenet-y-la-red-inception">8.12.12. GoogleNet y la red Inception</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-residuales-resnets">8.12.13. Redes residuales (ResNets)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reconocimiento-facial-de-emociones">8.12.14. Reconocimiento facial de emociones</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-recurrentes">8.13. Redes Neuronales Recurrentes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-en-tiempo">8.13.1. Backpropagation en tiempo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#desvanecimiento-y-explosion-de-gradientes">8.13.2. Desvanecimiento y explosión de gradientes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#red-de-memoria-a-largo-plazo-lstm">8.14. Red de memoria a largo plazo (LSTM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#atencion-y-memoria">8.15. Atención y Memoria</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entrenamiento-adversario">8.16. Entrenamiento adversario</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-generativos-profundos">8.17. Modelos Generativos Profundos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maquina-de-boltzmann-restringida">8.17.1. Máquina de Boltzmann Restringida</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoders">8.18. Autoencoders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-procesamiento-del-lenguaje-natural">8.19. Aplicación: Procesamiento del Lenguaje Natural</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="redes-neuronales-y-deep-learning">
<h1><span class="section-number">8. </span>Redes Neuronales y Deep Learning<a class="headerlink" href="#redes-neuronales-y-deep-learning" title="Link to this heading">#</a></h1>
<section id="gradiente-descendente">
<h2><span class="section-number">8.1. </span>Gradiente descendente<a class="headerlink" href="#gradiente-descendente" title="Link to this heading">#</a></h2>
<ul>
<li><p>El <em><strong>método de gradiente descendente</strong></em> es uno de los mas ampliamente usados para la <em><strong>minimización iterativa de una función de costo diferenciable</strong></em>, <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta}),~\boldsymbol{\theta}\in\mathbb{R}^{l}\)</span>. Como cualquier otra técnica iterativa, el método <em><strong>parte de una estimación inicial</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span>, <em><strong>y genera una sucesión</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)},~i=1,2,\dots,\)</span> tal que:</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)},~ i &gt;0,~\mu_{i}&gt;0.
    \]</div>
</li>
<li><p>La diferencia entre cada método radica en la forma que <span class="math notranslate nohighlight">\(\mu_{i}\)</span> y <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> son seleccionados. <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> es conocido como la <em><strong>dirección de actualización o de búsqueda</strong></em>. La sucesión <span class="math notranslate nohighlight">\(\mu_{i}\)</span> es conocida como el <em><strong>tamaño o longitud de paso</strong></em> en la <span class="math notranslate nohighlight">\(i\)</span>-ésima iteración, estos valores pueden ser constantes o cambiar. En el método de gradiente descendente, <em><strong>la selección de</strong></em> <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> <em><strong>es realizada para garantizar que</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta}^{(i)})&lt;J(\boldsymbol{\theta}^{(i-1)})\)</span>, excepto en el minimizador <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\star}\)</span>.</p></li>
</ul>
<figure class="align-center" id="curva-nivel">
<a class="reference internal image-reference" href="_images/curva_nivel.png"><img alt="_images/curva_nivel.png" src="_images/curva_nivel.png" style="width: 504.8px; height: 386.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">Función de coste en el espacio de parámetros bidimensional.</span><a class="headerlink" href="#curva-nivel" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Suponga que en la iteración <span class="math notranslate nohighlight">\(i-1\)</span> el valor <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span> <em><strong>ha sido obtenido</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta}^{(i)})=J(\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)})\approx J(\boldsymbol{\theta}^{(i-1)})+\mu_{i}\cdot\nabla^{T}J(\boldsymbol{\theta}^{(i-1)})\Delta\boldsymbol{\theta}^{(i-1)}.
\]</div>
<ul class="simple">
<li><p>Nótese que <em><strong>seleccionando la dirección tal que</strong></em> <span class="math notranslate nohighlight">\(\nabla^{T}J(\boldsymbol{\theta}^{(i-1)})\Delta\boldsymbol{\theta}^{(i)}&lt;0\)</span>, <em><strong>garantizará que</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)})&lt;J(\boldsymbol{\theta}^{(i-1)})\)</span>. Tal selección de <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> y <span class="math notranslate nohighlight">\(\nabla J(\boldsymbol{\theta}^{(i-1)})\)</span> debe formar un <em><strong>ángulo obtuso</strong></em>. Las curvas de nivel asociadas a <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span> pueden tomar cualquier forma, la cual va a <em><strong>depender de como está definido</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span> se supone diferenciable, por lo tanto, las <em><strong>curvas de nivel o contornos deben ser suaves y aceptar un plano tangente en cualquier punto</strong></em>. Además, de los cursos de cálculo sabemos que el <em><strong>vector gradiente</strong></em> <span class="math notranslate nohighlight">\(\nabla J(\boldsymbol{\theta})\)</span> <em><strong>es perpendicular al plano tangente</strong></em> (recta tangente) <em><strong>a la correspondiente curva de nivel en el punto</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. Nótese que <em><strong>seleccionando la dirección de búsqueda</strong></em> <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> <em><strong>que forma un angulo obtuso con el gradiente, se coloca a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)}\)</span> <em><strong>en un punto sobre el contorno el cual corresponde a un valor menor que</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>Dos problemas surgen ahora:</p>
<ol class="arabic simple">
<li><p><em><strong>Escoger la mejor dirección de búsqueda</strong></em></p></li>
<li><p>Calcular <em><strong>que tan lejos es aceptable un movimiento a traves de esta dirección</strong></em>.</p></li>
</ol>
</li>
</ul>
<figure class="align-center" id="maximun-dec-cost-function">
<a class="reference internal image-reference" href="_images/maximun_dec_cost_function.png"><img alt="_images/maximun_dec_cost_function.png" src="_images/maximun_dec_cost_function.png" style="width: 495.20000000000005px; height: 365.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text">El vector gradiente en un punto <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> es perpendicular al plano tangente (línea punteada) en la curva de nivel que cruza <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. La dirección de descenso forma un ángulo obtuso, <span class="math notranslate nohighlight">\(\phi\)</span>, con el vector gradiente.</span><a class="headerlink" href="#maximun-dec-cost-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Nótese que <em><strong>si</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}\|\Delta\boldsymbol{\theta}^{(i)}\|\)</span> <em><strong>es demasiado grande, entonces el nuevo punto puede ser colocado en un contorno correspondiente a un valor mayor al del actual</strong></em> contorno.</p></li>
</ul>
<figure class="align-center" id="curva-nivel-cost-function">
<a class="reference internal image-reference" href="_images/curva_nivel_cost_function.png"><img alt="_images/curva_nivel_cost_function.png" src="_images/curva_nivel_cost_function.png" style="width: 484.2px; height: 359.1px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">Las correspondientes curvas de nivel para la función de coste, en el plano bidimensional. Nótese que a medida que nos alejamos del valor óptimo, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\star}\)</span>, los valores de <span class="math notranslate nohighlight">\(c\)</span> aumentan.</span><a class="headerlink" href="#curva-nivel-cost-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Para abordar (1), <em><strong>supongamos que</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}=1\)</span> y <em><strong>buscamos todos los vectores</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> <em><strong>con norma Euclidiana unitaria, con inicio (cola) en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span>. Entonces, para todas las posibles direcciones, la que entrega el valor más negativo del producto interno, <span class="math notranslate nohighlight">\(\nabla^{T}J(\boldsymbol{\theta}^{(i-1)})z\)</span>, es aquella de gradiente negativo</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
z=-\frac{\nabla J(\boldsymbol{\theta}^{(i-1)})}{\|\nabla J(\boldsymbol{\theta}^{(i-1)}\|}
\]</div>
<ul class="simple">
<li><p>Centrando <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span> en la bola con norma Euclideana uno. <em><strong>De todos los vectores con norma unitaria y origen en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span>, <em><strong>seleccionamos aquel que apunta en la dirección negativa del gradiente</strong></em>. Por lo tanto, para todos los vectores con norma Euclidiana 1, la <em><strong>dirección de descenso mas pronunciada coincide con la dirección del gradiente descendente, negativo</strong></em>, y la correspondiente actualización recursiva se convierte en</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}-\mu_{i}\nabla J(\boldsymbol{\theta}^{(i-1)}),\quad\text{Gradiente descendente}.
\]</div>
<figure class="align-center" id="fig-desc-gradient">
<a class="reference internal image-reference" href="_images/desc_gradient.png"><img alt="_images/desc_gradient.png" src="_images/desc_gradient.png" style="width: 480.6px; height: 352.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.4 </span><span class="caption-text">Representación del gradiente negativo el cual conduce a la máxima disminución de la función de coste.</span><a class="headerlink" href="#fig-desc-gradient" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La selección de <span class="math notranslate nohighlight">\(\mu_{i}\)</span> debe ser realizada de tal forma que <em><strong>garantice convergencia de la secuencia de minimización</strong></em>. Nótese que <em><strong>el algoritmo puede oscilar en torno al mínimo sin converger, si no seleccionamos la dirección correcta</strong></em>. La selección de <span class="math notranslate nohighlight">\(\mu_{i}\)</span> <em><strong>dependerá de la convergencia a cero del error entre</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)}\)</span> <em><strong>y el mínimo real en forma de serie geométrica</strong></em>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Ejercicio</p>
<ul class="simple">
<li><p>Por ejemplo, para el caso de la función de coste del <em><strong>error cuadrático medio</strong></em>, la longitud de paso está dada por: <span class="math notranslate nohighlight">\(0&lt;\mu&lt;2/\lambda_{\max}\)</span>, donde <span class="math notranslate nohighlight">\(\lambda_{\max}\)</span> el máximo eigenvalor de la matriz de covarianza <span class="math notranslate nohighlight">\(\Sigma_{x}=\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^{T}]\)</span>, donde <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})=E[(y-\boldsymbol{\theta}^{T}\boldsymbol{x})^{2}]\)</span> (ver Sección 5.3 <span id="id1">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>).</p></li>
</ul>
</div>
</section>
<section id="redes-neuronales">
<h2><span class="section-number">8.2. </span>Redes neuronales<a class="headerlink" href="#redes-neuronales" title="Link to this heading">#</a></h2>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Las redes neuronales son <em><strong>sistemas de aprendizaje compuestos por neuronas conectadas en capas que ajustan sus conexiones para aprender</strong></em>. Tras un período de <em><strong>25 años desde su inicio, las redes neuronales se convirtieron en la norma en el aprendizaje automático</strong></em>. En un principio, dominaron durante una década, pero <em><strong>luego fueron superadas por máquinas de vectores de soporte</strong></em>.</p></li>
<li><p>Sin embargo, <em><strong>desde 2010, las redes neuronales profundas se han vuelto populares gracias a mejoras en la tecnología y la disponibilidad de grandes conjuntos de datos</strong></em>, impulsando el campo del aprendizaje automático.</p></li>
</ul>
</div>
</section>
<section id="el-perceptron">
<h2><span class="section-number">8.3. </span>El perceptrón<a class="headerlink" href="#el-perceptron" title="Link to this heading">#</a></h2>
<ul>
<li><p>Nuestro punto de partida será considerar el problema simple de una <em><strong>tarea de clasificación conformada por dos clases linealmente separables</strong></em>. En otras palabras, dado un conjunto de muestras de entrenamiento, <span class="math notranslate nohighlight">\((y_{n}, \boldsymbol{x}_{n})\)</span>, <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span>, con <span class="math notranslate nohighlight">\(y_{n}\in\{-1,+1\},~\boldsymbol{x}_{n}\in\mathbb{R}^{l}\)</span>, suponemos que existe un hiperplano</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\theta}_{\star}^{T}\boldsymbol{x}=0,
    \]</div>
<p>tal que,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{cases}
    \boldsymbol{\theta}_{\star}^{T}\boldsymbol{x}&amp;&gt;0,\quad\text{si}\quad\boldsymbol{x}\in\omega_{1}\\
    \boldsymbol{\theta}_{\star}^{T}\boldsymbol{x}&amp;&lt;0,\quad\text{si}\quad\boldsymbol{x}\in\omega_{2}
    \end{cases}
    \end{split}\]</div>
<p>En otras palabras, <em><strong>dicho hiperplano clasifica correctamente todos los puntos del conjunto de entrenamiento</strong></em>. Para simplificar, el <em><strong>término de sesgo del hiperplano ha sido absorbido en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\star}\)</span> después de extender la dimensionalidad del espacio de entrada en uno. El objetivo ahora es <em><strong>desarrollar un algoritmo que calcule iterativamente un hiperplano que clasifique correctamente todos los patrones de ambas clases</strong></em>. Para ello, se adopta una función de costo.</p>
</li>
</ul>
<ul class="simple">
<li><p>Sea <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> la <em><strong>estimación del vector de parámetros desconocidos, disponible en la actual iteración</strong></em>. Entonces hay dos posibilidades. La primera es que <em><strong>todos los puntos estén clasificados correctamente</strong></em>; esto significa que se ha obtenido una solución. La otra alternativa es que <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> <em><strong>clasifique correctamente algunos de los puntos</strong></em> y el resto estén mal clasificados.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Costo perceptrón</p>
<p>Sea <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> el conjunto de todas las muestras mal clasificadas. La <em><strong><code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">costo,</span> <span class="pre">perceptrón</span></code></strong></em> se define como</p>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta})=-\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}:\quad\textsf{Costo perceptrón},
\]</div>
<p>donde</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y_{n}=
\begin{cases}
+1,&amp;\quad\text{si}~\boldsymbol{x}\in\omega_{1}\\
-1,&amp;\quad\text{si}~\boldsymbol{x}\in\omega_{2}.
\end{cases}
\end{split}\]</div>
</div>
<ul class="simple">
<li><p>Nótese que la función <em><strong>la función de costo es no negativa</strong></em>. En efecto, dado que la suma es sobre los puntos mal clasificados, si <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\in\omega_{1}~(\omega_{2}),~\)</span> entonces <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}\leq (\geq)~0\)</span>, entregando así un producto <span class="math notranslate nohighlight">\(-y_{n}\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}\geq0\)</span>.</p></li>
<li><p><em><strong>La función de costo es cero, si no existen puntos mal clasificados</strong></em>, esto es, <span class="math notranslate nohighlight">\(\mathcal{Y}=\emptyset\)</span>. La función de costo perceptrón <em><strong>no es diferenciable en todos los puntos, es lineal por tramos</strong></em>. Si reescribimos <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span> en una forma ligeramente diferente:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta})=\left(-\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}^{T}\right)\boldsymbol{\theta}.
\]</div>
<ul class="simple">
<li><p><em><strong>Nótese que esta es una función lineal con respeto a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, siempre que el conjunto de puntos mal clasificados permanezca igual. Además, <em><strong>nótese que ligeros cambios del valor</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> <em><strong>corresponden a cambios de posición del respectivo hiperplano</strong></em>. Como consecuencia, existirá un punto donde el número de muestras mal clasificadas en <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, repentinamente cambia; este es el tiempo donde <em><strong>una muestra en el conjunto de entrenamiento cambia su posición relativa con respecto a el hiperplano en movimiento</strong></em>, y en consecuencia, el conjunto <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> es modificado. Después de este cambio, el conjunto, <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span>, corresponderá a una nueva  función lineal.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">El algoritmo perceptrón</p>
<p>A partir del <em><strong>método de subgradientes</strong></em> se puede verificar fácilmente que, iniciando desde un punto arbitrario, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span>, el siguiente método iterativo,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\mu_{i}\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}:\quad\text{Regla perceptrón}, 
\]</div>
<p>converge después de un <em>número finito de pasos</em>. La sucesión de parámetros <span class="math notranslate nohighlight">\(\mu_{i}\)</span> es seleccionada adecuadamente para garantizar convergencia.</p>
</div>
<ul class="simple">
<li><p>Nótese que usando el <em>método de subgradiente (ver apéndice)</em> o <em><strong>gradiente descendente</strong></em> se tiene que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{\theta}^{(i)}&amp;=\boldsymbol{\theta}^{(i-1)}-\mu_{i}J'(\boldsymbol{\theta}^{(i-1)})\\
&amp;=\boldsymbol{\theta}^{(i-1)}-\mu_{i}\left(-\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}^{T}\right)\\
&amp;=\boldsymbol{\theta}^{(i-1)}+\mu_{i}\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Otra versión del algoritmo considera una <em><strong>muestra por iteración en un esquema cíclico, hasta que el algoritmo converge</strong></em>. Denotemos por <span class="math notranslate nohighlight">\(y_{(i)}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{x}_{i},~i\in\{1,2,\dots,N\}\)</span> los <em><strong>pares de entrenamiento presentados al algoritmo en la iteración</strong></em> <span class="math notranslate nohighlight">\(i\)</span><em><strong>-ésima</strong></em>. Entonces, la iteración de actualización se convierte en:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-perceptron-algo2">
<span class="eqno">(8.1)<a class="headerlink" href="#equation-eq-perceptron-algo2" title="Link to this equation">#</a></span>\[\begin{split}
\boldsymbol{\theta}^{(i)}=
\begin{cases}
\boldsymbol{\theta}^{(i-1)}+\mu_{i}y_{(i)}\boldsymbol{x}_{(i)},&amp;\quad\text{si}\,\boldsymbol{x}_{(i)}\,\text{es mal clasificado por}\,\boldsymbol{\theta}^{(i-1)},\\
\boldsymbol{\theta}^{(i-1)},&amp;\quad\text{otro caso}.
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>Esto es, partiendo de una estimación inicial de forma random, <em><strong>inicializando</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span> <em><strong>con algunos valores pequeños, testeamos cada una de las muestras</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n},~n=1,2,\dots,N\)</span>. <em><strong>Cada vez que una muestra es mal clasificada, se toma acción por medio de la regla perceptrón para una corrección</strong></em>. <em><strong>En otro caso, ninguna acción es requerida</strong></em>.</p></li>
<li><p>Una vez que todas las muestras han sido consideradas, decimos que una <em><strong><code class="docutils literal notranslate"><span class="pre">época</span> <span class="pre">(epoch)</span></code></strong></em> ha sido completada. <em><strong>Si no se obtiene convergencia, todas las muestras son reconsideradas en una segunda época, y así sucesivamente</strong></em>. La versión de este algoritmo es conocida como esquema <em><strong><code class="docutils literal notranslate"><span class="pre">pattern-by-pattern</span></code></strong></em>. Algunas veces también es referido como el <em><strong>algoritmo online</strong></em>. Nótese que el número total de datos muestrales es fijo, y que el algoritmo las considera en forma cíclica, época por época (<em><strong>epoch-by-epoch</strong></em>).</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Después de un número finito de épocas, se garantiza que el algoritmo es convergente (<em>convexidad de <span class="math notranslate nohighlight">\(J\)</span> y clases linealmente separables</em>). <em><strong>Nótese que para obtener dicha convergencia, la sucesión</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}\)</span> <em><strong>debe ser seleccionada apropiadamente</strong></em>. Sin embargo, para el caso del <em><strong>algoritmo perceptrón, la convergencia es garantizada</strong></em>, aun cuando <span class="math notranslate nohighlight">\(\mu_{i}\)</span> es una constante positiva, <span class="math notranslate nohighlight">\(\mu_{i}=\mu&gt;0\)</span>, <em><strong>usualmente tomado igual a uno</strong></em>.</p></li>
<li><p>La formulación en <a class="reference internal" href="#equation-eq-perceptron-algo2">(8.1)</a> es conocida también como la filosofía de aprendizaje <em><strong><code class="docutils literal notranslate"><span class="pre">reward-punishment</span></code></strong></em>. Si la actual estimación es exitosa en la predicción de la clase del respectivo patrón, ninguna acción es tomada (<em><strong>reward</strong></em>), en otro caso, el algoritmo es obligado a realizar una actualización (<em><strong>punishment</strong></em>).</p></li>
</ul>
</div>
<figure class="align-center" id="perceptron-rule">
<img alt="_images/perceptron_rule.png" src="_images/perceptron_rule.png" />
<figcaption>
<p><span class="caption-number">Fig. 8.5 </span><span class="caption-text">El punto <span class="math notranslate nohighlight">\(x\)</span> está mal clasificado por la línea roja. La regla perceptrón gira el hiperplano hacia el punto <span class="math notranslate nohighlight">\(x\)</span>, para intentar incluirlo en el lado correcto del nuevo hiperplano y clasificarlo correctamente.</span><a class="headerlink" href="#perceptron-rule" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>La <a class="reference internal" href="#perceptron-rule"><span class="std std-numref">Fig. 8.5</span></a> ofrece una interpretación geométrica de la <em><strong>regla del perceptrón</strong></em>. Supongamos que la muestra <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> está mal clasificada por el hiperplano, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span>. Como sabemos, por geometría analítica, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span> <em><strong>corresponde a un vector que es perpendicular al hiperplano que está definido por este vector</strong></em>. Como <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> se encuentra en el lado <span class="math notranslate nohighlight">\((-)\)</span> del hiperplano y está mal clasificado, pertenece a la clase <span class="math notranslate nohighlight">\(\omega_{1}\)</span>; asumiendo <span class="math notranslate nohighlight">\(\mu = 1\)</span>, la corrección aplicada por el algoritmo es</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\boldsymbol{x},
    \end{split}\]</div>
<p>y su efecto es <em><strong>girar el hiperplano en dirección a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> para colocarlo en el lado <span class="math notranslate nohighlight">\((+)\)</span> del nuevo hiperplano, que está definido por la estimación actualizada <span class="math notranslate nohighlight">\(\boldsymbol{\theta^{(i)}}\)</span>. El <em><strong>algoritmo perceptrón</strong></em> en su modo de funcionamiento patrón por patrón (<em><strong>pattern-by-pattern</strong></em>) se resume en el siguiente algoritmo:</p>
</li>
</ul>
<div class="proof algorithm admonition" id="my_algorithm_pattern_by_pattern">
<p class="admonition-title"><span class="caption-number">Algorithm 8.1 </span> (Algoritmo perceptrón <em>pattern-by-pattern</em>)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inicialización</strong></p>
<ol class="arabic simple">
<li><p>Inicializar <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span>; usualmente, de forma <em><strong>random, pequeño</strong></em></p></li>
<li><p>Seleccionar <span class="math notranslate nohighlight">\(\mu\)</span>; usualmente <em><strong>establecido como uno</strong></em></p></li>
<li><p><span class="math notranslate nohighlight">\(i=1\)</span></p></li>
</ol>
<p><strong>Repeat</strong> Cada iteración corresponde a un <em><strong>epoch</strong></em></p>
<ol class="arabic">
<li><p><em><strong>counter = 0</strong></em>; Contador del número de actualizaciones por <em><strong>epoch</strong></em></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span> <strong>Do</strong> Para cada <em><strong>epoch</strong></em>, todas las muestras son presentadas una vez</p>
<p><strong>If</strong>(<span class="math notranslate nohighlight">\(y_{n}\boldsymbol{x}_{n}^{T}\leq0\)</span>) <strong>Then</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\mu y_{n}\boldsymbol{x}_{n}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i=i+1\)</span></p></li>
<li><p>counter = counter + 1</p></li>
</ol>
<p><strong>End For</strong></p>
</li>
<li><p><strong>Until</strong> counter = 0</p></li>
</ol>
</section>
</div><ul class="simple">
<li><p>Una vez que el <em><strong>algoritmo perceptrón se ha ejecutado y converge</strong></em>, tenemos los <em><strong>pesos</strong></em>, <span class="math notranslate nohighlight">\(\theta_{i},~i = 1,2,\dots,l\)</span>, <em><strong>de las sinapsis de la neurona/perceptrón</strong></em> asociada, así como el término de sesgo <span class="math notranslate nohighlight">\(\theta_{0}\)</span>. Ahora se pueden <em><strong>utilizar para clasificar patrones desconocidos</strong></em>. Las características <span class="math notranslate nohighlight">\(x_{i}, i = 1, 2,\dots,l\)</span>, se aplican a los nodos de entrada. A su vez, <em><strong>cada característica se multiplica por la sinapsis respectiva (peso), y luego se añade el término de sesgo en su combinación lineal</strong></em>.</p></li>
<li><p>El resultado de esta operación <em><strong>pasa por una función no lineal</strong></em>, <span class="math notranslate nohighlight">\(f\)</span>, conocida como <em><strong>función de activación</strong></em> (ver <a class="reference external" href="https://en.wikipedia.org/wiki/Activation_function">Activation function</a>). Dependiendo de la forma de la no linealidad, se producen diferentes tipos de neuronas. La mas clásica conocida como <em><strong>neurona McCulloch-Pitts</strong></em>, la función de <em><strong>activación es la de Heaviside</strong></em>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
f(z)=
\begin{cases}
1,&amp;\quad\text{si}~z&gt;0,\\
0,&amp;\quad\text{si}~z\leq0.
\end{cases}
\end{split}\]</div>
<figure class="align-center" id="mcculloch-pitts">
<a class="reference internal image-reference" href="_images/mcculloch_pitts.png"><img alt="_images/mcculloch_pitts.png" src="_images/mcculloch_pitts.png" style="width: 532.0px; height: 167.20000000000002px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.6 </span><span class="caption-text">Arquitectura básica de neuronas/perceptrones.</span><a class="headerlink" href="#mcculloch-pitts" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En la arquitectura básica de neuronas/perceptrones, las <em><strong>características de entrada se aplican a los nodos de entrada y se ponderan por los respectivos pesos que definen las sinapsis</strong></em>. A continuación <em><strong>se añade el término de sesgo en su combinación lineal y el resultado es empujado a través de la no linealidad</strong></em>. En la neurona <em><strong>McCulloch-Pitts</strong></em>, la salida es 1 para los patrones de la clase <span class="math notranslate nohighlight">\(\omega_{1}\)</span> o 0 para la clase <span class="math notranslate nohighlight">\(\omega_{2}\)</span>. La suma y la operación no lineal se unen para simplificar el gráfico.</p></li>
</ul>
<figure class="align-center" id="hidden-layer-activationf-function-fig">
<a class="reference internal image-reference" href="_images/hidden_layer_activationf_function.png"><img alt="_images/hidden_layer_activationf_function.png" src="_images/hidden_layer_activationf_function.png" style="width: 508.9px; height: 256.2px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.7 </span><span class="caption-text">Selección de función de activación para <em><strong>hidden layers</strong></em>. (Fuente <span id="id2">[<a class="reference internal" href="biblio.html#id56" title="J. Brownlee and Machine Learning Mastery. Deep Learning with Python: Develop Deep Learning Models on Theano and TensorFlow Using Keras. Machine Learning Mastery, 2017. URL: https://books.google.com.co/books?id=eJw2nQAACAAJ.">Brownlee and Mastery, 2017</a>]</span>).</span><a class="headerlink" href="#hidden-layer-activationf-function-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Para las capas ocultas (<em><strong><code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">layer</span></code></strong></em>), la función de <em><strong>activación tangente hiperbólica suele funcionar mejor que la sigmoidea logística</strong></em>. Tanto la función <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> como <code class="docutils literal notranslate"><span class="pre">tanh</span></code> pueden hacer que el modelo sea más <em><strong><code class="docutils literal notranslate"><span class="pre">susceptible</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">problemas</span> <span class="pre">durante</span> <span class="pre">el</span> <span class="pre">entrenamiento,</span> <span class="pre">a</span> <span class="pre">través</span> <span class="pre">del</span> <span class="pre">llamado</span> <span class="pre">problema</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">desvanecientes</span></code></strong></em>. Los modelos modernos de redes neuronales con arquitecturas comunes, como <code class="docutils literal notranslate"><span class="pre">MLP</span></code> y <code class="docutils literal notranslate"><span class="pre">CNN</span></code>, <em><strong>harán uso de la función de activación</strong></em> <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>, o extensiones.</p></li>
<li><p>Las <em><strong>redes recurrentes</strong></em> suelen utilizar funciones de activación <code class="docutils literal notranslate"><span class="pre">tanh</span></code> o <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>, o incluso ambas. Por ejemplo, la <em><strong>LSTM suele utilizar la activación <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> para las conexiones recurrentes y la activación <code class="docutils literal notranslate"><span class="pre">tanh</span></code> para la salida</strong></em>.</p></li>
</ul>
<figure class="align-center" id="output-layer-activation-function">
<a class="reference internal image-reference" href="_images/output_layer_activation_function.png"><img alt="_images/output_layer_activation_function.png" src="_images/output_layer_activation_function.png" style="width: 485.79999999999995px; height: 301.7px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.8 </span><span class="caption-text">Selección de función de activación para <em><strong>hidden layers</strong></em>. (Fuente <span id="id3">[<a class="reference internal" href="biblio.html#id56" title="J. Brownlee and Machine Learning Mastery. Deep Learning with Python: Develop Deep Learning Models on Theano and TensorFlow Using Keras. Machine Learning Mastery, 2017. URL: https://books.google.com.co/books?id=eJw2nQAACAAJ.">Brownlee and Mastery, 2017</a>]</span>).</span><a class="headerlink" href="#output-layer-activation-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Si su problema es de <em><strong>regresión, debería utilizar una función de activación lineal en el <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">layer</span></code></strong></em>. Si su problema es de <em><strong>clasificación</strong></em>, el modelo predice la probabilidad de pertenencia a una clase, que se puede convertir en una <em><strong>etiqueta de clase mediante redondeo (para <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a>)</strong></em> o <em><strong><code class="docutils literal notranslate"><span class="pre">argmax</span></code> (para <a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a>)</strong></em>. Usualmente, <code class="docutils literal notranslate"><span class="pre">softmax</span></code> es usada en clasificación múltiple cuando las clases son <em><strong>mutuamente excluyentes</strong></em> en caso contrario puede usar la función de activación <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> para cada output.</p></li>
</ul>
</section>
<section id="redes-neuronales-multicapa-feed-forward">
<h2><span class="section-number">8.4. </span>Redes Neuronales Multicapa Feed-Forward<a class="headerlink" href="#redes-neuronales-multicapa-feed-forward" title="Link to this heading">#</a></h2>
<ul>
<li><p><em><strong>Una sola neurona está asociada a un hiperplano</strong></em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    H: \theta_{1}x_{1}+\theta_{2}x_{2}+\cdots+\theta_{l}x_{l}+\theta_{0}=0,
    \end{split}\]</div>
<p>en el espacio de entrada, y la <em><strong>clasificación se basa en una función no lineal</strong></em> que produce un resultado de uno o cero según en qué lado del hiperplano <span class="math notranslate nohighlight">\(H\)</span> se encuentre un punto. A continuación, se mostrará cómo <em><strong>combinar varias neuronas en capas para crear clasificadores no lineales</strong></em>. Seguiremos un enfoque constructivo simple que será útil al abordar aspectos de las redes neuronales, especialmente en el contexto de arquitecturas profundas de aprendizaje profundo (<em><strong>deep learning</strong></em>).</p>
</li>
</ul>
<ul class="simple">
<li><p>Como punto de partida, consideramos el caso en el que las <em><strong>clases del espacio de características están formadas por uniones de regiones poliédricas</strong></em></p></li>
</ul>
<figure class="align-center" id="regiones-poliedricas">
<a class="reference internal image-reference" href="_images/regiones_poliedricas.png"><img alt="_images/regiones_poliedricas.png" src="_images/regiones_poliedricas.png" style="width: 452.9px; height: 290.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.9 </span><span class="caption-text">Clases formadas por uniones de regiones poliédricas. Las regiones se etiquetan según el lado en el que se encuentran, con respecto a las tres líneas, <span class="math notranslate nohighlight">\(H_{1}, H_{2}\)</span> y <span class="math notranslate nohighlight">\(H_{3}\)</span>.</span><a class="headerlink" href="#regiones-poliedricas" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Las regiones poliédricas se forman como <em><strong>intersecciones de semiespacios, cada uno de ellos asociado a un hiperplano</strong></em>. En la <a class="reference internal" href="#regiones-poliedricas"><span class="std std-numref">Fig. 8.9</span></a>, hay tres hiperplanos (líneas rectas en <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>), indicados como <span class="math notranslate nohighlight">\(H_{1}, H_{2}, H_{3}\)</span>, que dan lugar a siete regiones poliédricas. Para cada hiperplano se indican los lados <span class="math notranslate nohighlight">\((+)\)</span> y <span class="math notranslate nohighlight">\((-)\)</span> (semiespacios).</p></li>
<li><p>Cada una de las regiones se <em><strong>etiqueta con un triplete de números binarios</strong></em>, según el lado que se encuentra con respecto a <span class="math notranslate nohighlight">\(H_{1}, H_{2}, H_{3}\)</span>. Por ejemplo, la región etiquetada como <span class="math notranslate nohighlight">\((101)\)</span> se encuentra en el lado <span class="math notranslate nohighlight">\((+)\)</span> de <span class="math notranslate nohighlight">\(H_{1}\)</span>, el lado <span class="math notranslate nohighlight">\((-)\)</span> de <span class="math notranslate nohighlight">\(H_{2}\)</span> y el lado <span class="math notranslate nohighlight">\((+)\)</span> de <span class="math notranslate nohighlight">\(H_{3}\)</span>.</p></li>
</ul>
<figure class="align-center" id="poliedros-neurons">
<a class="reference internal image-reference" href="_images/poliedros_neurons.png"><img alt="_images/poliedros_neurons.png" src="_images/poliedros_neurons.png" style="width: 613.9px; height: 301.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.10 </span><span class="caption-text">(A) Las neuronas de la primera capa oculta son activadas por los valores de las características aplicadas en los nodos de entrada y forman las regiones poliédricas. (B) Las neuronas de la segunda capa tienen como entradas las salidas de la primera capa, y así
forman las clases.</span><a class="headerlink" href="#poliedros-neurons" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La <a class="reference internal" href="#poliedros-neurons"><span class="std std-numref">Fig. 8.10</span></a> muestra tres neuronas, correspondientes a los tres hiperplanos, <span class="math notranslate nohighlight">\(H_{1}, H_{2}\)</span> y <span class="math notranslate nohighlight">\(H_{3}\)</span>, de la <a class="reference internal" href="#regiones-poliedricas"><span class="std std-numref">Fig. 8.9</span></a>, respectivamente. Las salidas asociadas, denotadas como <span class="math notranslate nohighlight">\(y_{1}, y_{2}\)</span>, y <span class="math notranslate nohighlight">\(y_{3}\)</span>, forman la <em><strong>etiqueta de la región a la que el patrón de entrada correspondiente pertenece</strong></em>. De hecho, si los pesos de las sinapsis se han fijado adecuadamente, entonces, <em><strong>si un patrón se origina en la región, digamos</strong></em>, <span class="math notranslate nohighlight">\((010)\)</span>, <em><strong>la primera neurona de la izquierda asigna un cero</strong></em> <span class="math notranslate nohighlight">\((y_{1} = 0)\)</span>, <em><strong>la del medio un uno</strong></em> <span class="math notranslate nohighlight">\((y_{2} = 1)\)</span>, <em><strong>y la de la derecha un cero</strong></em> <span class="math notranslate nohighlight">\((y_{3} = 0)\)</span>.</p></li>
<li><p>En otras palabras, <em><strong>combinando las tres neuronas, hemos logrado un mapeo del espacio de características de entrada en el espacio tridimensional</strong></em>. Más concretamente, el <em><strong>mapeo se realiza en los vértices del cubo unitario</strong></em> en <span class="math notranslate nohighlight">\(\mathbb{R}^{3}\)</span>, como se muestra en la <a class="reference internal" href="#mapping-input-feature"><span class="std std-numref">Fig. 8.11</span></a></p></li>
</ul>
<figure class="align-center" id="mapping-input-feature">
<a class="reference internal image-reference" href="_images/mapping_input_feature.png"><img alt="_images/mapping_input_feature.png" src="_images/mapping_input_feature.png" style="width: 449.4px; height: 411.59999999999997px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.11 </span><span class="caption-text">Las neuronas de la primera capa oculta realizan un mapeo del espacio de características de entrada a los vértices de un hipercubo unitario. El vértice 110, denotado como un círculo sin sombrear, no corresponde a ninguna región.</span><a class="headerlink" href="#mapping-input-feature" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Ahora utilizaremos esta nueva <em><strong>representación, proporcionada por las salidas de las neuronas de la primera capa oculta</strong></em>, como entrada que alimenta las neuronas de una segunda capa oculta, la cual se construye de la siguiente forma. Elegimos todas las <em><strong>regiones que pertenecen a una clase</strong></em>. Para nuestro ejemplo de la <a class="reference internal" href="#mapping-input-feature"><span class="std std-numref">Fig. 8.11</span></a>, seleccionamos las dos regiones que corresponden a la clase <span class="math notranslate nohighlight">\(\omega_{1}\)</span>, es decir, <span class="math notranslate nohighlight">\((000)\)</span> y <span class="math notranslate nohighlight">\((111)\)</span>. Recordemos que todos los <em><strong>puntos de estas regiones se mapean a los respectivos vértices del cubo unitario</strong></em> en <span class="math notranslate nohighlight">\(\mathbb{R}^{3}\)</span>. Sin embargo, en este nuevo espacio transformado, <em><strong>cada uno de los vértices es linealmente separable del resto</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>Observe que <em><strong>la salida</strong></em> <span class="math notranslate nohighlight">\(z_{1}\)</span> <em><strong>de la neurona izquierda arroja un 1 sólo si el patrón de entrada se origina en la región</strong></em> <span class="math notranslate nohighlight">\(000\)</span> y <em><strong>0 para todos los demás patrones</strong></em>. Para la neurona de la derecha, la salida <span class="math notranslate nohighlight">\(z_{2}\)</span> será 1 para todos los patrones procedentes de la región <span class="math notranslate nohighlight">\((111)\)</span> y cero para el resto (ver <a class="reference internal" href="#mapping-input-feature"><span class="std std-numref">Fig. 8.11</span></a>). Nótese que <em><strong>esta segunda capa de neuronas ha realizado un segundo mapeo</strong></em>, esta vez a los vértices del rectángulo unitario en <span class="math notranslate nohighlight">\(\mathbb{R}^{2}\)</span>.</p></li>
</ul>
<figure class="align-center" id="map-layer-intor2">
<a class="reference internal image-reference" href="_images/map_layer_intor2.png"><img alt="_images/map_layer_intor2.png" src="_images/map_layer_intor2.png" style="width: 507.20000000000005px; height: 364.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.12 </span><span class="caption-text">Los patrones de la clase <span class="math notranslate nohighlight">\(\omega_{1}\)</span> <em><strong>se asignan a (01) o a (10)</strong></em> y los patrones de la clase <span class="math notranslate nohighlight">\(\omega_{2}\)</span> <em><strong>se asignan a (00)</strong></em>. Clases ahora linealmente separables a través de una línea recta realizada por una neurona.</span><a class="headerlink" href="#map-layer-intor2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Este mapeo proporciona una <em><strong>nueva representación de los patrones de entrada</strong></em>, y esta representación <em><strong>codifica la información relacionada con las clases de las regiones</strong></em>. La <a class="reference internal" href="#map-layer-intor2"><span class="std std-numref">Fig. 8.12</span></a> muestra el mapeo a los vértices del rectángulo unitario en el espacio <span class="math notranslate nohighlight">\((z_{1}, z_{2})\)</span>. Nótese que todos los puntos procedentes de la clase <span class="math notranslate nohighlight">\(\omega_{2}\)</span> están mapeados a <span class="math notranslate nohighlight">\((00)\)</span> y los puntos de la clase <span class="math notranslate nohighlight">\(\omega_{1}\)</span> están asignados a <span class="math notranslate nohighlight">\((10)\)</span> o a <span class="math notranslate nohighlight">\((01)\)</span>. Esto es, <em><strong>mediante mapeos sucesivos, hemos transformado nuestra tarea, originalmente, linealmente no separable a una que es linealmente separable</strong></em>.</p></li>
<li><p>En efecto, el punto <span class="math notranslate nohighlight">\((00)\)</span> puede separarse linealmente de <span class="math notranslate nohighlight">\((01)\)</span> y <span class="math notranslate nohighlight">\((10)\)</span>, y esto puede realizarse mediante una neurona adicional que opera en el espacio <span class="math notranslate nohighlight">\((z_{1}, z_{2})\)</span>; la cual se conoce como la neurona de salida, porque proporciona la decisión final de clasificación.</p></li>
</ul>
<figure class="align-center" id="final-neural-network-fig">
<a class="reference internal image-reference" href="_images/final_neural_network.png"><img alt="_images/final_neural_network.png" src="_images/final_neural_network.png" style="width: 317.79999999999995px; height: 376.59999999999997px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.13 </span><span class="caption-text">Red neuronal feed-forward de tres capas. Comprende la capa de entrada (no de procesamiento), dos capas ocultas y una capa de salida de neuronas</span><a class="headerlink" href="#final-neural-network-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La red final resultante se muestra en la <a class="reference internal" href="#final-neural-network-fig"><span class="std std-numref">Fig. 8.13</span></a>. Llamamos a esta red <em><strong>feed-forward</strong></em>, porque la <em><strong>información fluye hacia adelante desde la capa de entrada a la de salida</strong></em>. Se compone de la <em><strong>capa de entrada, que es una capa no procesadora</strong></em>, <em><strong>dos capas ocultas</strong></em>, y <em><strong>una capa de salida</strong></em>. Llamamos a esta red neuronal, <em><strong>red de tres capas</strong></em>, sin contar la capa de entrada de nodos no procesadores. Esta red neuronal de tres capas puede resolver cualquier tarea de clasificación, en la que las clases están formadas por uniones de regiones poliédricas.</p></li>
</ul>
<ul class="simple">
<li><p>Considerando esta estructura de la <em><strong>red multicapa</strong></em>. Nuestro interés ahora se centrará en <em><strong>buscar formas de estimar los pesos desconocidos de las sinapsis y los sesgos de las neuronas</strong></em>. Sin embargo, desde un punto de vista conceptual, debemos <em><strong>recordar que cada capa realiza un mapeo en un nuevo espacio, y cada mapeo proporciona una representación diferente</strong></em> de los datos de entrada, hasta la última capa, donde la tarea se ha transformado en una que es fácil de resolver.</p></li>
</ul>
</section>
<section id="redes-totalmente-conectadas">
<span id="fully-connected-ann"></span><h2><span class="section-number">8.5. </span>Redes Totalmente Conectadas<a class="headerlink" href="#redes-totalmente-conectadas" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Para resumir de manera más formal el tipo de <em><strong>operaciones que tienen lugar en una red totalmente conectada</strong></em>, centrémonos en, por ejemplo, la <em><strong>capa</strong></em> <span class="math notranslate nohighlight">\(r\)</span> <em><strong>de una red neuronal multicapa y supongamos que está formada por</strong></em> <span class="math notranslate nohighlight">\(k_{r}\)</span> <em><strong>neuronas</strong></em>. El vector de <em><strong>entrada a esta capa está formado por las salidas de los nodos de la capa anterior, que se denomina</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{y}^{r-1}\)</span>.</p></li>
<li><p>Sea <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}\)</span> el vector de los <em><strong>pesos sinápticos, incluido el término de sesgo, asociado a la neurona</strong></em> <span class="math notranslate nohighlight">\(j\)</span> <em><strong>de la capa</strong></em> <span class="math notranslate nohighlight">\(r\)</span>, donde <span class="math notranslate nohighlight">\(j = 1,2,\dots, k_{r}\)</span>. La <em><strong>dimensión respectiva</strong></em> de este vector es <span class="math notranslate nohighlight">\(k_{r-1} + 1\)</span>, donde <span class="math notranslate nohighlight">\(k_{r-1}\)</span> es el <em><strong>número de neuronas de la capa anterior</strong></em>, <span class="math notranslate nohighlight">\(r-1\)</span>, <em><strong>y el aumento en 1 representa el término de sesgo</strong></em>. Entonces las operaciones realizadas, antes de la no linealidad, son los productos internos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
z_{j}^{r}=\boldsymbol{\theta}_{j}^{rT}\boldsymbol{y}^{r-1},\quad j=1,2,\dots,k_{r}.
\]</div>
<ul class="simple">
<li><p>Colocando todos los valores de salida en un vector <span class="math notranslate nohighlight">\(\boldsymbol{z}^{r}=[z_{1}^{r}, z_{2}^{r},\dots,z_{k_{r}}^{r}]^{T}\)</span>, y <em><strong>agrupando todos los vectores sinápticos como filas</strong></em>, una debajo de la otra, en una matriz, podemos escribir colectivamente</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{z}^{r}=\Theta\boldsymbol{y}^{r-1},\quad\text{donde}\quad\Theta:=[\boldsymbol{\theta}_{1}^{r}, \boldsymbol{\theta}_{2}^{r},\dots, \boldsymbol{\theta}_{k_{r}}^{r}].
\]</div>
<ul class="simple">
<li><p>El vector de las salidas de la <span class="math notranslate nohighlight">\(r\)</span> th capa oculta, después de <em><strong>empujar cada</strong></em> <span class="math notranslate nohighlight">\(z_{i}^{r}\)</span> <em><strong>a través de la no linealidad</strong></em> <span class="math notranslate nohighlight">\(f\)</span>, está finalmente dado por</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{y}^{r}=
\begin{bmatrix}
1\\
f(\boldsymbol{z}^{r})
\end{bmatrix}
\end{split}\]</div>
<ul class="simple">
<li><p>La siguiente figura describe como es creada la <span class="math notranslate nohighlight">\(j\)</span>-ésima capa de la red neuronal full conectada</p></li>
</ul>
<figure class="align-center" id="id54">
<a class="reference internal image-reference" href="_images/fully_connected_net.png"><img alt="_images/fully_connected_net.png" src="_images/fully_connected_net.png" style="width: 600.0px; height: 317.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.14 </span><span class="caption-text"><span class="math notranslate nohighlight">\(j\)</span>-ésimo elemento de la capa <span class="math notranslate nohighlight">\(r\)</span> de la red totalmente conectada.</span><a class="headerlink" href="#id54" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="important admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La notación anterior significa que <span class="math notranslate nohighlight">\(f\)</span> <em><strong>actúa sobre cada uno de los respectivos componentes del vector</strong></em>, individualmente, y la <em><strong>extensión del vector en uno es para dar cuenta de los términos de sesgo</strong></em> en la práctica estándar. Para redes grandes, con muchas capas y muchos nodos por capa, este tipo de conectividad resulta ser muy costoso en términos del número de parámetros (pesos), que es del orden de <span class="math notranslate nohighlight">\(k_{r}k_{r-1}\)</span>.</p></li>
<li><p>Por ejemplo, si <span class="math notranslate nohighlight">\(k_{r-1} = 1000\)</span> y <span class="math notranslate nohighlight">\(k_{r} = 1000\)</span>, <em><strong>esto equivale a un orden de 1 millón de parámetros</strong></em>. Tenga en cuenta que este número es la contribución de los parámetros de una sola de las capas. Sin embargo, <em><strong>un gran número de parámetros hace que una red sea vulnerable al sobreajuste</strong></em>, cuando se trata de entrenamiento</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Se pueden emplear las llamadas <em><strong>técnicas de reparto de pesos</strong></em>, en las que un conjunto de parámetros es compartido entre un número de conexiones, a través de restricciones adecuadamente incorporadas. Las <em><strong>redes neuronales recurrentes y convolucionales</strong></em> pertenecen a esta familia de redes de peso compartido. En una red convolucional, las <em><strong>convoluciones sustituyen a las operaciones de producto interno</strong></em>, lo que permite un reparto de pesos importante que conduce a una reducción sustancial del número de parámetros.</p></li>
</ul>
</section>
<section id="el-algoritmo-de-backpropagation">
<h2><span class="section-number">8.6. </span>El Algoritmo De Backpropagation<a class="headerlink" href="#el-algoritmo-de-backpropagation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Una red neuronal considera una <em><strong>función paramétrica no lineal</strong></em>, <span class="math notranslate nohighlight">\(\hat{y} = f_{\boldsymbol{\theta}}(\boldsymbol{x})\)</span>, donde <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> representa todos los pesos/sesgo presentes en la red. Por lo tanto, el entrenamiento de una red neuronal no parece ser diferente del entrenamiento de cualquier otro modelo de predicción paramétrica.</p></li>
<li><p>Todo lo que se necesita es <em><strong>(a)</strong></em> un <em><strong>conjunto de muestras de entrenamiento</strong></em>, <em><strong>(b)</strong></em> una <em><strong>función de pérdida</strong></em> <span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>, y <em><strong>(c)</strong></em> un <em><strong>esquema iterativo</strong></em>, por ejemplo, el <em><strong>gradiente descendente</strong></em>, para realizar la optimización de la función de coste asociada (<em><strong>pérdida empírica</strong></em>).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta})=\sum_{n=1}^{N}\mathcal{L}(y_{n}, f_{\boldsymbol{\theta}}(\boldsymbol{x}_{n})).
\]</div>
<ul class="simple">
<li><p>La dificultad del entrenamiento de las redes neuronales radica en su <em><strong>estructura multicapa que complica el cálculo de los gradientes, que intervienen en la optimización</strong></em>. Además, la neurona <em><strong>McCulloch-Pitts</strong></em> se basa en la función de activación discontinua <em><strong>Heaviside no diferenciable</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Neurona</span> <span class="pre">sigmoidea</span> <span class="pre">logística</span></code></strong></em>: Una posibilidad es adoptar la función <em><strong>sigmoidea logística</strong></em>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f(z)=\sigma(z):=\frac{1}{1+\exp(-az)}.
\]</div>
<ul class="simple">
<li><p>Nótese que <em><strong>cuanto mayor sea el valor del parámetro</strong></em> <span class="math notranslate nohighlight">\(a\)</span>, <em><strong>la gráfica correspondiente se acerca más a la de la función de Heaviside</strong></em> (ver <a class="reference internal" href="#sigmoid-act-function"><span class="std std-numref">Fig. 8.15</span></a>).</p></li>
</ul>
<figure class="align-center" id="sigmoid-act-function">
<a class="reference internal image-reference" href="_images/sigmoid_act_function.png"><img alt="_images/sigmoid_act_function.png" src="_images/sigmoid_act_function.png" style="width: 507.20000000000005px; height: 471.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.15 </span><span class="caption-text">Función <code class="docutils literal notranslate"><span class="pre">sigmoidea</span> <span class="pre">logística</span></code> para diferentes valores del parámetro <span class="math notranslate nohighlight">\(a\)</span>.</span><a class="headerlink" href="#sigmoid-act-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Otra posibilidad sería <em><strong>utilizar la función</strong></em>,</p>
<div class="math notranslate nohighlight">
\[
    f(z)=a\tanh\left(\frac{cz}{2}\right),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(c\)</span> y <span class="math notranslate nohighlight">\(a\)</span> son parámetros de control. El gráfico de esta función se muestra en la <a class="reference internal" href="#tanh-act-function"><span class="std std-numref">Fig. 8.16</span></a>. Nótese que a diferencia de la sigmoidea logística, esta es una <em><strong>función no simétrica</strong></em>, es decir, <span class="math notranslate nohighlight">\(f(-z)=-f(z)\)</span>. Ambas son también conocidas como <em><strong>funciones de reducción, porque limitan la salida a un rango finito de valores</strong></em>.</p>
</li>
</ul>
<figure class="align-center" id="tanh-act-function">
<a class="reference internal image-reference" href="_images/tanh_act_function.png"><img alt="_images/tanh_act_function.png" src="_images/tanh_act_function.png" style="width: 693.6px; height: 435.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.16 </span><span class="caption-text">Función de reducción de la tangente hiperbólica para <span class="math notranslate nohighlight">\(a = 1.7\)</span> y <span class="math notranslate nohighlight">\(c = 4/3\)</span>.</span><a class="headerlink" href="#tanh-act-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Recordemos que la regla de <em><strong>actualización del algoritmo gradiente descendente</strong></em>, en su versión unidimensional se convierte en</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \theta(new)=\theta(old)-\mu\left.\frac{d J}{d\theta}\right|_{\theta(old)},
    \end{split}\]</div>
<p>y las iteraciones parten de un punto inicial arbitrario, <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span>.</p>
</li>
<li><p>Si en la iteración actual el algoritmo está digamos, en el punto <span class="math notranslate nohighlight">\(\theta(old) = \theta_{1}\)</span>, entonces se moverá hacia el mínimo local, <span class="math notranslate nohighlight">\(\theta_{l}\)</span>. Esto se debe a que la <em><strong>derivada del coste en</strong></em> <span class="math notranslate nohighlight">\(\theta_{1}\)</span> <em><strong>es igual a la tangente</strong></em> <span class="math notranslate nohighlight">\(\phi_{1}\)</span> (ver <a class="reference internal" href="#convex-function-saddle-point"><span class="std std-numref">Fig. 8.17</span></a>), que es negativa (el ángulo es obtuso) y la actualización, <span class="math notranslate nohighlight">\(\theta(new)\)</span>, se moverá a la derecha, hacia el mínimo local, <span class="math notranslate nohighlight">\(\theta_{l}\)</span>.</p></li>
</ul>
<figure class="align-center" id="convex-function-saddle-point">
<a class="reference internal image-reference" href="_images/convex_function_saddle_point.png"><img alt="_images/convex_function_saddle_point.png" src="_images/convex_function_saddle_point.png" style="width: 635.2px; height: 354.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.17 </span><span class="caption-text">Función no convexa global, con mínimos locales y puntos de silla.</span><a class="headerlink" href="#convex-function-saddle-point" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="important admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La <em><strong>elección del tamaño del paso</strong></em>, <span class="math notranslate nohighlight">\(\mu\)</span>, <em><strong>es crítica para la convergencia del algoritmo</strong></em>. En problemas reales en espacios multidimensionales, el número de mínimos locales puede ser grande, por lo que el algoritmo puede converger a uno local. Sin embargo, esto no es necesariamente una mala noticia. Si este <em><strong>mínimo local es lo suficientemente profundo</strong></em>, es decir, si el valor de la función de coste en este punto, por ejemplo, <span class="math notranslate nohighlight">\(J(\theta_{l})\)</span>, <em><strong>no es mucho mayor que el alcanzado en el mínimo global</strong></em>, es decir, <span class="math notranslate nohighlight">\(J(\theta_{g})\)</span>, la <em><strong>convergencia a dicho mínimo local puede corresponder a una buena solución</strong></em>.</p></li>
<li><p>Una opción para evitar caer en mínimos locales es utilizar un método de optimización diferente, por ejemplo, <em><strong>Adam, RMSprop o AdaGrad</strong></em> que pueden adaptarse mejor a la topografía del error y ayudar a escapar de los mínimos locales. Explorar <em><strong>otras funciones de coste</strong></em> también es otra opción, o <em><strong>utilizar diferentes métodos de regularización</strong></em> (<span class="math notranslate nohighlight">\(L1, L2\)</span>), dado que, <em><strong>si el modelo es muy complejo, puede ser más propenso a caer en mínimos locales</strong></em>.</p></li>
</ul>
</div>
</section>
<section id="el-esquema-de-backpropagation-para-gradiente-descendente">
<h2><span class="section-number">8.7. </span>El Esquema De Backpropagation Para Gradiente Descendente<a class="headerlink" href="#el-esquema-de-backpropagation-para-gradiente-descendente" title="Link to this heading">#</a></h2>
<p>Habiendo adoptado una función de activación diferenciable, estamos listos para proceder a desarrollar el <em><strong><code class="docutils literal notranslate"><span class="pre">esquema</span> <span class="pre">iterativo</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">descendente</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">minimización</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span></code></strong></em>. Formularemos la tarea en un marco general.</p>
<ul class="simple">
<li><p>Sea <span class="math notranslate nohighlight">\((\boldsymbol{y}_{n}, \boldsymbol{x}_{n}), n = 1, 2,\dots, N\)</span>, el conjunto de muestras de entrenamiento. <em><strong>Nótese que hemos asumido múltiples variables output</strong></em>, como vectores. Suponemos que la red consta de <span class="math notranslate nohighlight">\(L\)</span> capas, <span class="math notranslate nohighlight">\(L-1\)</span> capas ocultas y una capa de salida. Cada capa consta de <span class="math notranslate nohighlight">\(k_{r}, r = 1, 2,\dots, L\)</span>, neuronas. Así, los vectores de salida (objetivo/deseado) son</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y}_{n}=[y_{n1}, y_{n2},\dots, y_{nk_{L}}]^{T}\in\mathbb{R}^{K_{L}},\quad n=1,2,\dots,N.
\]</div>
<ul class="simple">
<li><p>Para ciertas derivaciones matemáticas, también denotamos el número de nodos de entrada como <span class="math notranslate nohighlight">\(k_{0}\)</span>; es decir, <span class="math notranslate nohighlight">\(k_{0} = l\)</span>, donde <span class="math notranslate nohighlight">\(l\)</span> es la <em><strong>dimensionalidad del espacio de características de entrada</strong></em>.</p></li>
<li><p>Sea <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}\)</span> el <em><strong>vector de los pesos sinápticos asociados a la</strong></em> <span class="math notranslate nohighlight">\(j\)</span><em><strong>-th neurona de la</strong></em> <span class="math notranslate nohighlight">\(r\)</span><em><strong>-th capa</strong></em>, con <span class="math notranslate nohighlight">\(j = 1, 2,\dots, k_{r}\)</span> y <span class="math notranslate nohighlight">\(r = 1, 2,\dots,L\)</span>, donde el término de sesgo se incluye en <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}\)</span>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-parameters-vector-def">
<span class="eqno">(8.2)<a class="headerlink" href="#equation-parameters-vector-def" title="Link to this equation">#</a></span>\[
\boldsymbol{\theta}_{j}^{r}:=[\theta_{j0}^{r}, \theta_{j1}^{r},\dots, \theta_{jk_{r-1}}^{r}]^{T}.
\]</div>
<ul class="simple">
<li><p>Los <em><strong>pesos sinápticos en la capa <span class="math notranslate nohighlight">\(r\)</span> enlazan la neurona respectiva con todas las neuronas de la capa <span class="math notranslate nohighlight">\(k_{r-1}\)</span></strong></em> (véase la <a class="reference internal" href="#synaptic-weights-link"><span class="std std-numref">Fig. 8.18</span></a>). El paso iterativo básico para el esquema de gradiente descendente se escribe como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-update-equations-gd">
<span class="eqno">(8.3)<a class="headerlink" href="#equation-update-equations-gd" title="Link to this equation">#</a></span>\[
\boldsymbol{\theta}_{j}^{r}(\text{new})=\boldsymbol{\theta}_{j}^{r}(old)+\Delta\boldsymbol{\theta}_{j}^{r},\quad
\Delta\boldsymbol{\theta}_{j}^{r}:=-\mu\left.\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{r}}\right|_{\boldsymbol{\theta}_{j}^{r}(old)}.
\]</div>
<ul class="simple">
<li><p>El parámetro <span class="math notranslate nohighlight">\(\mu\)</span> es el <em><strong>tamaño de paso</strong></em> definido por el usuario (también puede depender de la iteración) y <span class="math notranslate nohighlight">\(J\)</span> denota la <em><strong>función de coste</strong></em>.</p></li>
</ul>
<figure class="align-center" id="synaptic-weights-link">
<a class="reference internal image-reference" href="_images/synaptic_weights_link.png"><img alt="_images/synaptic_weights_link.png" src="_images/synaptic_weights_link.png" style="width: 529.6px; height: 355.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.18 </span><span class="caption-text">Enlaces y las variables asociadas de la <span class="math notranslate nohighlight">\(j\)</span> th neurona en la <span class="math notranslate nohighlight">\(r\)</span> th capa. <span class="math notranslate nohighlight">\(y_{k}^{r-1}\)</span> es la salida de la <span class="math notranslate nohighlight">\(k\)</span> th neurona de la <span class="math notranslate nohighlight">\((r - 1)\)</span> th capa y <span class="math notranslate nohighlight">\(\theta_{jk}^{r}\)</span> es el peso respectivo que conecta estas dos neuronas.</span><a class="headerlink" href="#synaptic-weights-link" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Las ecuaciones de actualización <a class="reference internal" href="#equation-update-equations-gd">(8.3)</a> comprenden el par del esquema de gradiente descendente  para la optimización. Como se ha dicho anteriormente, la dificultad de las redes neuronales <em><strong>feed-forward</strong></em> surge de su estructura multicapa. Para calcular los gradientes en la Ecuación <a class="reference internal" href="#equation-update-equations-gd">(8.3)</a>, para todas las neuronas, en todas las capas, se deben seguir dos pasos en su cálculo</p>
<ul>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Forward</span> <span class="pre">computations</span></code></strong></em>: Para un vector de entrada dado <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}, n = 1, 2,\dots, N\)</span>, se utilizan las estimaciones actuales de los parámetros (pesos sinápticos) (<span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}(old)\)</span>) y <em><strong>calcula todas las salidas de todas las neuronas en todas las capas</strong></em>, denotadas como <span class="math notranslate nohighlight">\(y_{nj}^{r}\)</span>; en la <a class="reference internal" href="#synaptic-weights-link"><span class="std std-numref">Fig. 8.18</span></a>, se ha suprimido el índice <span class="math notranslate nohighlight">\(n\)</span> para no afectar la notación.</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Backward</span> <span class="pre">computations</span></code></strong></em>: Utilizando las salidas neuronales calculadas anteriormente junto con los valores objetivos conocidos, <span class="math notranslate nohighlight">\(y_{nk}\)</span>, de la capa de salida, <em><strong>se calculan los gradientes de la función de coste</strong></em>. Esto implica <span class="math notranslate nohighlight">\(L\)</span> pasos, es decir, tantos como el número de capas. La secuencia de los pasos algorítmicos se indica a continuación:</p>
<ul>
<li><p>Calcular el gradiente de la función de coste con respecto a los parámetros de las neuronas de la última capa, es decir, <span class="math notranslate nohighlight">\(\partial J/\partial\boldsymbol{\theta}_{j}^{L}, j = 1, 2,\dots, k_{L}\)</span>.</p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r = L-1\)</span> to <span class="math notranslate nohighlight">\(1\)</span>, <strong>Do</strong></p>
<p>Calcular los gradientes con respecto a los parámetros asociados a las neuronas de la <span class="math notranslate nohighlight">\(r\)</span> th capa, es decir, <span class="math notranslate nohighlight">\(\partial J/\partial\boldsymbol{\theta}_{k}^{r}, k= 1, 2,\dots, k_{r}\)</span> basado en todos los gradientes <span class="math notranslate nohighlight">\(\partial J/\partial\boldsymbol{\theta}_{j}^{r+1}, j= 1, 2,\dots, k_{r+1}\)</span>, con respecto a los parámetros de la capa <span class="math notranslate nohighlight">\(r + 1\)</span> que se han calculado en el paso anterior.</p>
</li>
<li><p><strong>End For</strong></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Ejemplo</p>
<ul class="simple">
<li><p>Analicemos un ejemplo concreto de <em><strong>forward computations</strong></em>, seguido de un enfoque <em><strong>backward computations</strong></em>. El ejemplo de <em><strong>entrenamiento será</strong></em> (<span class="math notranslate nohighlight">\(x=2.1\)</span>, <span class="math notranslate nohighlight">\(y=4\)</span>), el <em><strong>peso</strong></em> inicial será <span class="math notranslate nohighlight">\(w:=\theta_{1}=1\)</span>, el <em><strong>sesgo</strong></em> será <span class="math notranslate nohighlight">\(b:=\theta_{0}=0\)</span> y <span class="math notranslate nohighlight">\(\textsf{Loss}=L\)</span>. <span class="math notranslate nohighlight">\(\hat{y}\)</span> representa la <em><strong>salida de la neurona y el valor predicho</strong></em>, mientras que la pérdida se refiere a la <em><strong>pérdida por error cuadrático</strong></em></p></li>
</ul>
</div>
<figure class="align-center" id="backpropagation-example-fig">
<a class="reference internal image-reference" href="_images/backpropagation_example.png"><img alt="_images/backpropagation_example.png" src="_images/backpropagation_example.png" style="width: 600.0px; height: 82.60000000000001px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.19 </span><span class="caption-text">Ejemplo de red neuronal para <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code></span><a class="headerlink" href="#backpropagation-example-fig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Forward</span> <span class="pre">computaion</span></code></strong></em>:</p></li>
</ul>
<figure class="align-center" id="forwardcomputation-example-fig">
<a class="reference internal image-reference" href="_images/forwardcomputation_example.png"><img alt="_images/forwardcomputation_example.png" src="_images/forwardcomputation_example.png" style="width: 600.0px; height: 76.4px;" /></a>
</figure>
<ul class="simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Backward</span> <span class="pre">computation</span></code></strong></em>: Ahora podemos retroceder (<code class="docutils literal notranslate"><span class="pre">backpropagation</span></code>) y calcular las <em><strong>derivadas parciales aplicando la regla de la cadena</strong></em> para obtener</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial\textsf{Loss}}{\partial w} &amp;= \frac{\partial\hat{y}}{\partial w}\frac{\partial\textsf{Loss}}{\partial\hat{y}}=x(2(\hat{y}-y)),~\text{entonces, para}~w=1, b=0\\
\left.\frac{\partial\textsf{Loss}}{\partial w}\right|_{x=2.1,y=4} &amp;= \left.x(2(\hat{y}-y))\right|_{x=2.1,y=4}=(2.1)(2(2.1-4))=-7.98
\end{align*}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial\textsf{Loss}}{\partial b} &amp;= \frac{\partial\hat{y}}{\partial b}\frac{\partial\textsf{Loss}}{\partial\hat{y}}=1\cdot(2(\hat{y}-y)),~\text{entonces, para}~w=1, b=0\\
\left.\frac{\partial\textsf{Loss}}{\partial b}\right|_{x=2.1,y=4} &amp;= \left.1\cdot(2(\hat{y}-y))\right|_{x=2.1,y=4}=(1)(2(2.1-4))=-3.8
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Luego, actualizamos los parámetros en la dirección opuesta al gradiente para disminuir la pérdida, con una tasa de aprendizaje de <span class="math notranslate nohighlight">\(\textsf{lr}:=\mu=0.01\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
w &amp;= w-\textsf{lr}\cdot\frac{\partial\textsf{Loss}}{\partial w}=(1)-(0.01)(-7.98)=1.0798\\
b &amp;= b-\textsf{lr}\cdot\frac{\partial\textsf{Loss}}{\partial b}=(0)-(0.01)(-3.8)=0.038
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Para evaluar el <em><strong>rendimiento de nuestros parámetros ajustados</strong></em>, realizaremos una pasada adicional</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\textsf{Loss}=((1.0798)(2.1)+0.038-4)^{2}=2.87
\]</div>
<ul class="simple">
<li><p>La <em><strong>pérdida total disminuyó de 3.61 a 2.87</strong></em>, lo que representa una <em><strong>reducción de 0.74</strong></em>. ¡La pérdida ha bajado!</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>El esquema de cálculo hacia atrás <em><strong>backpropagation</strong></em> es una aplicación directa de la <em><strong>regla de la cadena para las derivadas</strong></em>, y comienza con el paso inicial de <em><strong>calcular las derivadas asociadas a la última capa (de salida)</strong></em>, que resulta ser sencillo.</p></li>
<li><p>A continuación, el algoritmo “fluye” hacia atrás en la jerarquía de capas. Esto se debe a la naturaleza de la red multicapa, donde las <em><strong>salidas, capa tras capa, se forman como funciones de funciones</strong></em>.</p></li>
<li><p>El gradiente en la <em><strong>última capa es simplemente la derivada de la función de pérdida con respecto a la salida de esa capa</strong></em>. Como <em><strong>no hay capas posteriores que influyan en su gradiente</strong></em>, el cálculo es más directo y <em><strong>no requiere aplicar la regla de la cadena</strong></em> a través de varias capas.</p></li>
</ul>
</div>
<ul>
<li><p>En efecto, centrémonos en la salida <span class="math notranslate nohighlight">\(y_{k}^{r}\)</span> de la neurona <span class="math notranslate nohighlight">\(k\)</span> en la capa <span class="math notranslate nohighlight">\(r\)</span>. Entonces tenemos</p>
<div class="math notranslate nohighlight">
\[
    y_{k}^{r}=f(\boldsymbol{\theta}_{k}^{r^T}\boldsymbol{y}^{r-1}),\quad k=1,2,\dots, k_{r},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\boldsymbol{y}^{r-1}\)</span> es el vector (ampliado) que comprende todas las salidas de la capa anterior, <span class="math notranslate nohighlight">\(r-1\)</span>, y <span class="math notranslate nohighlight">\(f\)</span> denota la no-linealidad.</p>
</li>
</ul>
<ul>
<li><p>De acuerdo con lo anterior, la salida de la <span class="math notranslate nohighlight">\(j\)</span>-th neurona en la siguiente capa viene dada por</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    y_{j}^{r+1}=f(\boldsymbol{\theta}_{j}^{r+1^T}\boldsymbol{y}^{r})=
    f\left(\boldsymbol{\theta}_{j}^{r+1^{T}}
    \begin{bmatrix}
    1\\
    f(\Theta^{r}\boldsymbol{y}^{r-1})
    \end{bmatrix}
    \right)=
    f\left(\boldsymbol{\theta}_{j}^{r+1^{T}}
    \begin{bmatrix}
    1\\
    f(\boldsymbol{y}^{r})
    \end{bmatrix}
    \right),
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\Theta^{r}:=[\boldsymbol{\theta}_{1}^{r}, \boldsymbol{\theta}_{2}^{r},\dots,\boldsymbol{\theta}_{k_{r}}]^{T}\)</span> denota la matriz cuyas columnas corresponden al vector de pesos en el layer <span class="math notranslate nohighlight">\(r\)</span>.</p>
</li>
</ul>
<ul class="simple">
<li><p>Nótese que obtuvimos <em><strong>evaluación de “una función interna bajo una función externa”</strong></em>. Claramente, esto continúa a medida que avanzamos en la jerarquía. Esta <em><strong>estructura de evaluación de funciones internas por funciones externas</strong></em>, es el subproducto de la <em><strong>naturaleza multicapa de las redes neuronales, la cual es una operación altamente no lineal</strong></em>, que da lugar a la dificultad de calcular los gradientes, a diferencia de otros modelos, como por ejemplo <em><strong>SVM</strong></em>.</p></li>
<li><p>Sin embargo, se puede observar fácilmente que <em><strong>el cálculo de los gradientes con respecto a los parámetros que definen la <code class="docutils literal notranslate"><span class="pre">capa</span> <span class="pre">de</span> <span class="pre">salida</span></code> no plantea ninguna dificultad</strong></em>. En efecto, la salida de la <span class="math notranslate nohighlight">\(j\)</span>-th neurona de la última capa (que es en realidad la respectiva estimación de salida actual) se escribe como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y}_{j}:=y_{j}^{L}=f(\boldsymbol{\theta}_{j}^{L^{T}}\boldsymbol{y}^{L-1}).
\]</div>
<ul class="simple">
<li><p>Dado que <span class="math notranslate nohighlight">\(\boldsymbol{y}^{L-1}\)</span> es <em><strong>conocido, después de los cálculos durante el paso adelante (<code class="docutils literal notranslate"><span class="pre">forward</span> <span class="pre">computaion</span></code>)</strong></em>, tomando la derivada con respecto a <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{L}\)</span> es sencillo; <em><strong>no hay ninguna operación de función sobre función</strong></em>. Por esto es que <em><strong>empezamos por la capa superior y luego nos movemos hacia atrás</strong></em>. Debido a su <em><strong>importancia histórica</strong></em>, se dará la derivación completa del algoritmo <em><strong>backpropagation</strong></em>.</p></li>
</ul>
<ul>
<li><p>Para la derivación detallada del algoritmo backpropagation, <em><strong>se adopta como ejemplo la función de pérdida del error cuadrático</strong></em>, es decir</p>
<div class="math notranslate nohighlight" id="equation-gradient-desc-scheme">
<span class="eqno">(8.4)<a class="headerlink" href="#equation-gradient-desc-scheme" title="Link to this equation">#</a></span>\[
    J(\boldsymbol{\theta})=\sum_{n=1}^{N}J_{n}(\boldsymbol{\theta})\quad\text{y}\quad J_{n}(\boldsymbol{\theta})=\frac{1}{2}\sum_{k=1}^{k_{L}}(\hat{y}_{nk}-y_{nk})^{2},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\hat{y}_{nk},~k=1,2,\dots,k_{L}\)</span>, son las <em><strong>estimaciones proporcionadas en los correspondientes nodos de salida de la red</strong></em>. Las consideraremos como los elementos de un vector correspondiente, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>.</p>
</li>
</ul>
</section>
<section id="calculo-de-gradientes">
<h2><span class="section-number">8.8. </span>Cálculo de gradientes<a class="headerlink" href="#calculo-de-gradientes" title="Link to this heading">#</a></h2>
<ul>
<li><p>Sea <span class="math notranslate nohighlight">\(z_{nj}^{r}\)</span> la <em><strong>salida del combinador lineal</strong></em> de la <span class="math notranslate nohighlight">\(j\)</span>-th neurona en la capa <span class="math notranslate nohighlight">\(r\)</span> en el <em><strong>instante de tiempo</strong></em> <span class="math notranslate nohighlight">\(n\)</span>, cuando se aplica el patrón <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> en los nodos de entrada (véase la <a class="reference internal" href="#synaptic-weights-link"><span class="std std-numref">Fig. 8.18</span></a>). Entonces, para <span class="math notranslate nohighlight">\(n, j\)</span> fijos, podemos escribir</p>
<div class="math notranslate nohighlight" id="equation-eq-znj">
<span class="eqno">(8.5)<a class="headerlink" href="#equation-eq-znj" title="Link to this equation">#</a></span>\[
    z_{nj}^{r}=\sum_{m=1}^{k_{r-1}}\theta_{jm}^{r}y_{nm}^{r-1}+\theta_{j0}^{r}=\sum_{m=0}^{k_{r-1}}\theta_{jm}^{r}y_{nm}^{r-1}=\boldsymbol{\theta}_{j}^{r^{T}}\boldsymbol{y}_{n}^{r-1},
    \]</div>
<p>donde por definición</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{y}_{n}^{r-1}:=[1, y_{n1}^{r-1},\dots, y_{nk_{r-1}}^{r-1}]^{T},
    \]</div>
<p>y <span class="math notranslate nohighlight">\(y_{n0}^{r}\equiv 1,~\forall~r, n\)</span> y <span class="math notranslate nohighlight">\(\theta_{j}^{r}\)</span> ha sido definido en la Ecuación <a class="reference internal" href="#equation-parameters-vector-def">(8.2)</a>.</p>
</li>
</ul>
<ul class="simple">
<li><p>Para las neuronas de la capa de salida <span class="math notranslate nohighlight">\(r=L,~y_{nm}^{L}=\hat{y}_{nm},~m=1,2,\dots, k_{L}\)</span>, y para <span class="math notranslate nohighlight">\(r=1\)</span>, tenemos <span class="math notranslate nohighlight">\(y_{nm}^{1}=x_{nm},~m=1,2,\dots, k_{1}\)</span>; esto es, <span class="math notranslate nohighlight">\(y_{nm}^{1}\)</span> se fija igual a los <em><strong>valores de las características de entrada</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>Por lo tanto, teniendo en cuenta que <span class="math notranslate nohighlight">\(z_{j}^{r}=\boldsymbol{\theta}_{j}^{rT}\boldsymbol{y}^{r-1},~j=1,2,\dots,k_{r}\)</span> y <span class="math notranslate nohighlight">\(\hat{y}_{j}:=y_{j}^{r}=f(\boldsymbol{\theta}_{j}^{r^{T}}\boldsymbol{y}^{r-1})=f(z_{j}^{r})\)</span>, con base en la Ecuación <a class="reference internal" href="#equation-gradient-desc-scheme">(8.4)</a> podemos escribir las derivadas parciales</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial J_{n}}{\partial\boldsymbol{\theta}_{j}^{r}}=\frac{\partial J_{n}}{\partial z_{nj}^{r}}\frac{\partial z_{nj}^{r}}{\partial\boldsymbol{\theta}_{j}^{r}}=\frac{\partial J_{n}}{\partial z_{nj}^{r}}\boldsymbol{y}_{n}^{r-1}.
\]</div>
<ul class="simple">
<li><p>Definamos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\delta_{nj}^{r}:=\frac{\partial J_{n}}{\partial z_{nj}^{r}}.
\]</div>
<ul class="simple">
<li><p>Entonces la Ecuación <a class="reference internal" href="#equation-update-equations-gd">(8.3)</a> puede escribirse como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-delta-theta-jr">
<span class="eqno">(8.6)<a class="headerlink" href="#equation-eq-delta-theta-jr" title="Link to this equation">#</a></span>\[
\Delta\boldsymbol{\theta}_{j}^{r}=\left.-\mu\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{r}}\right|_{\boldsymbol{\theta}_{j}^{r}(\text{old})}=-\mu\frac{\partial}{\partial\boldsymbol{\theta}_{j}^{r}}\left.\sum_{n=1}^{N}J_{n}\right|_{\boldsymbol{\theta}_{j}^{r}(\text{old})}=-\mu\sum_{n=1}^{N}\delta_{nj}^{r}\boldsymbol{y}_{n}^{r-1},\quad r=1,2,\dots,L.
\]</div>
</section>
<section id="calculo-de-delta-nj-r">
<h2><span class="section-number">8.9. </span>Cálculo de <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span><a class="headerlink" href="#calculo-de-delta-nj-r" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Este es el <em><strong>cálculo principal del algoritmo backpropagation</strong></em>. Para el cálculo de los gradientes, <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span>, se <em><strong>comienza en la última capa</strong></em>, <span class="math notranslate nohighlight">\(r = L\)</span>, y se <em><strong>procede hacia atrás</strong></em>, hacia <span class="math notranslate nohighlight">\(r = 1\)</span>; esta “filosofía” justifica el nombre dado al algoritmo.</p></li>
</ul>
<ol class="arabic">
<li><p><span class="math notranslate nohighlight">\(r=L\)</span>: Tenemos que</p>
<div class="math notranslate nohighlight">
\[
    \delta_{nj}^{L}:=\frac{\partial J_{n}}{\partial z_{nj}^{L}}.
    \]</div>
<p>Para la función de pérdida del <em><strong>error al cuadrado</strong></em>,</p>
<div class="math notranslate nohighlight">
\[
    J_{n}=\frac{1}{2}\sum_{k=1}^{k_{L}}\left(\hat{y}_{nk}-y_{nk}\right)^{2}=\frac{1}{2}\sum_{k=1}^{k_{L}}\left(f(z_{nk}^{L})-y_{nk}\right)^{2}.
    \]</div>
<p>Por lo tanto,</p>
<div class="math notranslate nohighlight" id="equation-eq-delta-njl">
<span class="eqno">(8.7)<a class="headerlink" href="#equation-eq-delta-njl" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \delta_{nj}^{L}=\frac{\partial}{\partial z_{nj}^{L}}\left(\frac{1}{2}\sum_{k=1}^{k_{L}}\left(f(z_{nk}^{L})-y_{nk}\right)^{2}\right)&amp;=(f(z_{nj}^{L})-y_{nj})f'(z_{nj}^{L})\\
    &amp;=(\hat{y}_{nj}-y_{nj})f'(z_{nj}^{L})=e_{nj}f'(z_{nj}^{L}),
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(j=1,2,\dots, k_{L}\)</span>, <span class="math notranslate nohighlight">\(f'\)</span> denota la derivada de <span class="math notranslate nohighlight">\(f\)</span> y <span class="math notranslate nohighlight">\(e_{nj}\)</span> es el error asociado con el <span class="math notranslate nohighlight">\(j\)</span>-th output en el tiempo <span class="math notranslate nohighlight">\(n\)</span>. <em><strong>Nótese que para el último layer, el cálculo del gradiente</strong></em>, <span class="math notranslate nohighlight">\(\delta_{nj}^{L}\)</span> <em><strong>es sencillo</strong></em>.</p>
</li>
</ol>
<ol class="arabic" start="2">
<li><p><span class="math notranslate nohighlight">\(r&lt;L\)</span>: Debido a la <em><strong>dependencia sucesiva entre las capas</strong></em>, el valor de <span class="math notranslate nohighlight">\(z_{nj}^{r-1}\)</span> <em><strong>influye en todos los valores</strong></em> <span class="math notranslate nohighlight">\(z_{nk}^{r},~k = 1, 2,\dots, k_{r}\)</span>, <em><strong>de la capa siguiente</strong></em>. Empleando la <em><strong>regla de la cadena</strong></em> para la diferenciación, obtenemos, para <span class="math notranslate nohighlight">\(r=L, L-1,\dots, 2\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-partial-der-znk1">
<span class="eqno">(8.8)<a class="headerlink" href="#equation-partial-der-znk1" title="Link to this equation">#</a></span>\[
    \delta_{nj}^{r-1}=\frac{\partial J_{n}}{\partial z_{nj}^{r-1}}=\sum_{k=1}^{k_{r}}\frac{\partial J_{n}}{\partial z_{nk}^{r}}\frac{\partial z_{nk}^{r}}{\partial z_{nj}^{r-1}},
    \]</div>
<p>o</p>
<div class="math notranslate nohighlight" id="equation-partial-der-znk2">
<span class="eqno">(8.9)<a class="headerlink" href="#equation-partial-der-znk2" title="Link to this equation">#</a></span>\[
    \delta_{nj}^{r-1}=\sum_{k=1}^{k_{r}}\delta_{nk}^{r}\frac{\partial z_{nk}^{r}}{\partial z_{nj}^{r-1}}.
    \]</div>
<p>Además, usando la Ecuación <a class="reference internal" href="#equation-eq-znj">(8.5)</a> se tiene,</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial z_{nk}^{r}}{\partial z_{nj}^{r-1}}=\frac{\partial}{\partial z_{nj}^{r-1}}\left(\sum_{m=0}^{k_{r-1}}\theta_{km}^{r}y_{nm}^{r-1}\right),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\textcolor{blue}{y_{nm}^{r-1}=f(z_{nm}^{r-1})}=f(\boldsymbol{\theta}_{m}^{r-1^{T}}\boldsymbol{y}_{n}^{r-2})\)</span>. Entonces,</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial z_{nk}^{r}}{\partial z_{nj}^{r-1}}=\theta_{kj}^{r}f'(z_{nj}^{r-1}),
    \]</div>
<p>y combinando las Ecuaciones <a class="reference internal" href="#equation-partial-der-znk1">(8.8)</a> y <a class="reference internal" href="#equation-partial-der-znk2">(8.9)</a>, obtenemos la regla recursiva</p>
<div class="math notranslate nohighlight">
\[
    \delta_{nj}^{r-1}=\left(\sum_{k=1}^{k_{r}}\delta_{nk}^{r}\theta_{kj}^{r}\right)f'(z_{nj}^{r-1}).
    \]</div>
</li>
</ol>
<ul>
<li><p>Manteniendo la misma notación en la Ecuación <a class="reference internal" href="#equation-eq-delta-njl">(8.7)</a>, definimos</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    e_{nj}^{r-1}:=\sum_{k=1}^{k_{r}}\delta_{nk}^{r}\theta_{kj}^{r},
    \end{split}\]</div>
<p>y finalmente obtenemos,</p>
<div class="math notranslate nohighlight" id="equation-eq-delta-njr-1">
<span class="eqno">(8.10)<a class="headerlink" href="#equation-eq-delta-njr-1" title="Link to this equation">#</a></span>\[
    \delta_{nj}^{r-1}=e_{nj}^{r-1}f'(z_{nj}^{r-1}).
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>El único cálculo que queda es la derivada de <span class="math notranslate nohighlight">\(f\)</span>. Para el caso de la <em><strong>función sigmoidea logística</strong></em> se demuestra fácilmente que es igual a (<em><strong><code class="docutils literal notranslate"><span class="pre">verifíquelo</span></code></strong></em>)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f'(z)=af(z)(1-f(z)).
\]</div>
<ul class="simple">
<li><p>La derivación se ha completado y el esquema <em><strong>backpropagation neural network</strong></em> se resume en el siguiente algoritmo</p></li>
</ul>
<div class="proof algorithm admonition" id="my_algorithm_backpropagation">
<p class="admonition-title"><span class="caption-number">Algorithm 8.2 </span> (Algoritmo Backpropagation Gradiente Descendente)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inicialización</strong></p>
<ol class="arabic simple">
<li><p><em><strong>Inicializar todos los pesos y sesgos sinápticos al azar con valores pequeños</strong></em>, pero no muy pequeños.</p></li>
<li><p><em><strong>Seleccione el tamaño del paso</strong></em> <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>Fije <span class="math notranslate nohighlight">\(y_{nj}^{1}=x_{nj},\quad j=1,2,\dots,k_{1}:=l,\quad n=1,2,\dots,N\)</span></p></li>
</ol>
<p><strong>Repeat</strong> Cada repetición completa un <em><strong>epoch</strong></em></p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span> <strong>Do</strong></p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r=1,2,\dots,L\)</span> <strong>Do</strong> Cálculo <em><strong>Forward</strong></em></p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j=1,2,\dots,k_{r}\)</span> <strong>Do</strong></p>
<p>Calcule <span class="math notranslate nohighlight">\(z_{nj}^{r}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-znj">(8.5)</a>
Calcule <span class="math notranslate nohighlight">\(y_{nj}^{r}=f(z_{nj}^{r})\)</span></p>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j = 1, 2,\dots, k_{L}\)</span>, <strong>Do</strong>; Cálculo <em><strong>Backward</strong></em> (<em><strong>output layer</strong></em>)</p>
<p>Calcule <span class="math notranslate nohighlight">\(\delta_{nj}^{L}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-delta-njr-1">(8.10)</a></p>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r=L, L-1,\dots, 2\)</span>, <strong>Do</strong>; Cálculo <em><strong>Backward</strong></em> (<em><strong>hidden layers</strong></em>)</p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j=1,2,\dots, k_{r}\)</span>, <strong>Do</strong></p>
<p>Calcule <span class="math notranslate nohighlight">\(\delta_{nj}^{r-1}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-delta-njr-1">(8.10)</a></p>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r=1,2,\dots,L\)</span>, <strong>Do</strong>: Actualice los pesos</p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j=1,2,\dots,k_{r}\)</span>, <strong>Do</strong></p>
<p>Calcule <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}_{j}^{r}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-delta-theta-jr">(8.6)</a></p>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}=\boldsymbol{\theta}_{j}^{r}+\Delta\boldsymbol{\theta}_{j}^{r}\)</span></p>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>Until</strong> Un criterio de parada se cumpla.</p></li>
</ol>
</section>
</div><ul class="simple">
<li><p>El algoritmo de <em><strong>backpropagation</strong></em> puede reivindicar una serie de padres. La popularización del algoritmo se asocia con el artículo clásico <span id="id4">[<a class="reference internal" href="biblio.html#id21" title="David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533–536, 1986.">Rumelhart <em>et al.</em>, 1986</a>]</span>, donde se proporciona la derivación del algoritmo. La idea de <em><strong>backpropagation</strong></em> también aparece en <span id="id5">[<a class="reference internal" href="biblio.html#id22" title="Arthur E Bryson Jr, Walter F Denham, and Stewart E Dreyfus. Optimal programming problems with inequality constraints. AIAA journal, 1(11):2544–2550, 1963.">Bryson Jr <em>et al.</em>, 1963</a>]</span> en el contexto del control óptimo.</p></li>
<li><p>Existen diferentes variaciones del algoritmo <em><strong>backpropagation</strong></em>, tales como: <em><strong>Gradiende descendente con término de momento, Algoritmo de momentos de Nesterov’s, Algoritmo AdaGrad, RMSProp con momento de Nesterov, Algortimo de estimación de momentos adaptativo</strong></em> los cuales pueden ser utlizados para resolver la tarea de optimización (ver <span id="id6">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>).</p></li>
</ul>
</section>
<section id="tuning-de-redes-neuronales">
<h2><span class="section-number">8.10. </span>Tuning de Redes Neuronales<a class="headerlink" href="#tuning-de-redes-neuronales" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Profundizaremos en la <em><strong>mecánica del Perceptrón Multicapa (MLP)</strong></em> empleando el <code class="docutils literal notranslate"><span class="pre">MLPClassifier()</span></code> <em><strong>en el conjunto de datos</strong></em> <code class="docutils literal notranslate"><span class="pre">two_moons</span></code> que se utilizó anteriormente en esta sección</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">mglearn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">adam</span></code> <em><strong>Optimizer</strong></em>: <code class="docutils literal notranslate"><span class="pre">adam</span></code> es un acrónimo de <em><strong>Adaptive Moment Estimation</strong></em> y es uno de los algoritmos de optimización más populares y efectivos utilizados en el <em><strong>entrenamiento de redes neuronales</strong></em> (ver <span id="id7">[<a class="reference internal" href="biblio.html#id62" title="Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.">Kingma and Ba, 2014</a>]</span>). Es una variante avanzada del gradiente descendente que combina las ideas de los <em><strong>métodos de momentum</strong></em> y <em><strong>RMSProp</strong></em> para adaptarse mejor a diferentes problemas de optimización.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Características de Adam</p>
<ul class="simple">
<li><p><em><strong>Adaptative Learning Rate</strong></em>: Calcula tasas de aprendizaje adaptativas para cada parámetro utilizando estimaciones de primer y segundo orden de los momentos (media y varianza) de los gradientes.</p></li>
<li><p><em><strong>Momentum</strong></em>:Utiliza el concepto de <em><strong>momentum para acumular una media móvil exponencial de los gradientes pasados</strong></em> (primer momento) y una <em><strong>media móvil exponencial de los cuadrados de los gradientes pasados</strong></em> (segundo momento).</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.3</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1868535f75ae2a969bab12c376908528656a191882bc1d8640d31beb96033220.png" src="_images/1868535f75ae2a969bab12c376908528656a191882bc1d8640d31beb96033220.png" />
</div>
</div>
<ul class="simple">
<li><p>Evidentemente, la <em><strong>red neuronal</strong></em> ha adquirido un <em><strong>límite de decisión decisivamente no lineal</strong></em> pero relativamente gradual</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Se empleó el <em><strong>algoritmo</strong></em> <code class="docutils literal notranslate"><span class="pre">'lbfgs'</span></code> (ver <a class="reference external" href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">Broyden–Fletcher–Goldfarb–Shanno algorithm (L-BFGS)</a>, <em><strong>optimizador de la familia de los métodos cuasi-Newton.)</strong></em>. En particular, el <em><strong>MLP emplea 100 nodos ocultos por defecto</strong></em>, un número considerable para este conjunto de datos compacto. Sin embargo, <em><strong>incluso con un número reducido de nodos, se puede lograr un resultado satisfactorio</strong></em></p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Optimizador L-BFGS</p>
<ul class="simple">
<li><p>L-BFGS es una variante del algoritmo BFGS, que es un <em><strong>método de optimización de segundo orden</strong></em>. A diferencia de los métodos de primer orden, que utilizan únicamente el gradiente de la función de pérdida, los métodos de segundo orden también <em><strong>utilizan la información de la segunda derivada (la matriz Hessiana) para mejorar la dirección de los pasos de actualización</strong></em>. L-BFGS <em><strong>ajusta internamente las tasas de aprendizaje basadas en la información de segundo orden</strong></em>.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>MLPClassifier(hidden_layer_sizes=[10], max_iter=400, random_state=0,
              solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;MLPClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html">?<span>Documentation for MLPClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>MLPClassifier(hidden_layer_sizes=[10], max_iter=400, random_state=0,
              solver=&#x27;lbfgs&#x27;)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.3</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dd2bd6982dafeb7432f378d4aad6425efd5f862dd5fdebf08a6ac351bbb4bd8d.png" src="_images/dd2bd6982dafeb7432f378d4aad6425efd5f862dd5fdebf08a6ac351bbb4bd8d.png" />
</div>
</div>
<ul class="simple">
<li><p>Al <em><strong>reducir las unidades ocultas a 10 hace que el límite de decisión tenga un aspecto algo irregular</strong></em>. La <em><strong>no linealidad por defecto empleada es la</strong></em> <code class="docutils literal notranslate"><span class="pre">&quot;relu&quot;</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">line</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;relu(x), tanh(x)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3c26314ef087f3a6c368e94182cff437ca31af95e28ca767daca3e718a44cae0.png" src="_images/3c26314ef087f3a6c368e94182cff437ca31af95e28ca767daca3e718a44cae0.png" />
</div>
</div>
<ul class="simple">
<li><p>En el caso de <em><strong>una sola capa oculta</strong></em>, esto implica que la <em><strong>función de decisión comprende 10 segmentos de línea recta</strong></em>. Para conseguir un <em><strong>límite de decisión más gradual</strong></em>, las alternativas incluyen <em><strong>aumentar las unidades ocultas, introducir una segunda capa oculta o emplear la no linealidad</strong></em> <code class="docutils literal notranslate"><span class="pre">&quot;tanh&quot;</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>Utilizando <em><strong>dos capas ocultas, con 10 unidades cada una</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-2 {
  color: var(--sklearn-color-text);
}

#sk-container-id-2 pre {
  padding: 0;
}

#sk-container-id-2 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-2 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-2 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-2 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-2 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-2 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-2 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-2 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-2 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-2 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-2 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-2 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-2 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-2 div.sk-label label.sk-toggleable__label,
#sk-container-id-2 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-2 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-2 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-2 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-2 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-2 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-2 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-2 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-2 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>MLPClassifier(hidden_layer_sizes=[10, 10], random_state=0, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;MLPClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html">?<span>Documentation for MLPClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>MLPClassifier(hidden_layer_sizes=[10, 10], random_state=0, solver=&#x27;lbfgs&#x27;)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.3</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6985cde3fd8482a21615d9e6cbe5e787ddd77fd3aa8228d20220e9df01b5c3da.png" src="_images/6985cde3fd8482a21615d9e6cbe5e787ddd77fd3aa8228d20220e9df01b5c3da.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-3 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-3 {
  color: var(--sklearn-color-text);
}

#sk-container-id-3 pre {
  padding: 0;
}

#sk-container-id-3 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-3 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-3 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-3 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-3 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-3 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-3 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-3 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-3 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-3 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-3 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-3 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-3 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-3 div.sk-label label.sk-toggleable__label,
#sk-container-id-3 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-3 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-3 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-3 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-3 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-3 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-3 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-3 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-3 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=[10, 10], max_iter=400,
              random_state=0, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked><label for="sk-estimator-id-3" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;MLPClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html">?<span>Documentation for MLPClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=[10, 10], max_iter=400,
              random_state=0, solver=&#x27;lbfgs&#x27;)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.3</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/82e32a023aaa1255f7ea8db5ac791b0fa38444a258dbcdff445ddb77ef6cdf89.png" src="_images/82e32a023aaa1255f7ea8db5ac791b0fa38444a258dbcdff445ddb77ef6cdf89.png" />
</div>
</div>
<ul class="simple">
<li><p>Además, podemos <em><strong>influir en la complejidad de una red neuronal incorporando una penalización</strong></em> <span class="math notranslate nohighlight">\(L2\)</span>, similar al enfoque de la <em><strong>regresión de ridge y los clasificadores lineales</strong></em>. El parámetro responsable de esto en el <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> <em><strong>es</strong></em> <code class="docutils literal notranslate"><span class="pre">'alpha'</span></code>, análogo a los modelos de regresión lineal. Por <em><strong>defecto, asume un valor mínimo (regularización limitada)</strong></em>. El siguiente experimento incluye <em><strong>dos capas ocultas</strong></em>, cada una de ellas compuesta por 10 o 100 unidades y <em><strong>diferentes valores de</strong></em> <code class="docutils literal notranslate"><span class="pre">alpha</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">axx</span><span class="p">,</span> <span class="n">n_hidden_nodes</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axx</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]):</span>
        <span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">[</span><span class="n">n_hidden_nodes</span><span class="p">,</span> <span class="n">n_hidden_nodes</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
        <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;n_hidden=[</span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">]</span><span class="se">\n</span><span class="s2">alpha=</span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_hidden_nodes</span><span class="p">,</span> <span class="n">n_hidden_nodes</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/dab81b971453635ecb4abe5ac76e581048cc187b2799014519820f7f39e49d3f.png" src="_images/dab81b971453635ecb4abe5ac76e581048cc187b2799014519820f7f39e49d3f.png" />
</div>
</div>
<div class="important admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Como habrá podido comprobar, <em><strong>existen multitud de métodos para gestionar la complejidad de una red neuronal</strong></em>, como el <em><strong>número de capas ocultas, las unidades en cada capa oculta y la regularización</strong></em> (<code class="docutils literal notranslate"><span class="pre">alpha</span></code>), entre otros. Un rasgo fundamental de las redes neuronales reside en su <em><strong><code class="docutils literal notranslate"><span class="pre">configuración</span> <span class="pre">aleatoria</span> <span class="pre">inicial</span> <span class="pre">de</span> <span class="pre">pesos</span> <span class="pre">antes</span> <span class="pre">de</span> <span class="pre">comenzar</span> <span class="pre">el</span> <span class="pre">aprendizaje</span></code></strong></em>.</p></li>
<li><p>Esta aleatoriedad influye en el modelo aprendido resultante. Así, <em><strong>el empleo de parámetros idénticos puede dar lugar a modelos distintos debido a semillas aleatorias diferentes</strong></em>. Mientras que este efecto es más pronunciado para redes pequeñas, es <em><strong>menos significativo para redes de tamaño adecuado donde la complejidad se elige apropiadamente</strong></em>.</p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()):</span>
    <span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
    <span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">mlp</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b4a1d6f24acb30184784b60f1745e36e70aeb1606cb263500d408e617ae1aa74.png" src="_images/b4a1d6f24acb30184784b60f1745e36e70aeb1606cb263500d408e617ae1aa74.png" />
</div>
</div>
<ul class="simple">
<li><p>Para una <em><strong>comprensión más completa de las redes neuronales en escenarios prácticos</strong></em>, implementaremos el <em><strong>MLPClassifier en el conjunto de datos Breast Cancer</strong></em>. Nuestro paso <em><strong>inicial consiste en utilizar los parámetros por defecto</strong></em>. Queda como tarea para el estudiante, utilizar <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> y <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cancer data per-feature maxima:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cancer data per-feature maxima:
[2.811e+01 3.928e+01 1.885e+02 2.501e+03 1.634e-01 3.454e-01 4.268e-01
 2.012e-01 3.040e-01 9.744e-02 2.873e+00 4.885e+00 2.198e+01 5.422e+02
 3.113e-02 1.354e-01 3.960e-01 5.279e-02 7.895e-02 2.984e-02 3.604e+01
 4.954e+01 2.512e+02 4.254e+03 2.226e-01 1.058e+00 1.252e+00 2.910e-01
 6.638e-01 2.075e-01]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-4 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-4 {
  color: var(--sklearn-color-text);
}

#sk-container-id-4 pre {
  padding: 0;
}

#sk-container-id-4 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-4 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-4 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-4 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-4 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-4 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-4 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-4 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-4 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-4 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-4 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-4 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-4 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-4 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-4 div.sk-label label.sk-toggleable__label,
#sk-container-id-4 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-4 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-4 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-4 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-4 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-4 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-4 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-4 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-4 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-4 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-4 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>MLPClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" checked><label for="sk-estimator-id-4" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;MLPClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html">?<span>Documentation for MLPClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>MLPClassifier(random_state=42)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.94
Accuracy on test set: 0.92
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>El <em><strong>MLP muestra una precisión notable, aunque no a la par de otros modelos</strong></em>. Al igual que en el caso anterior del <code class="docutils literal notranslate"><span class="pre">SVC</span></code>, esta <em><strong>discrepancia se debe probablemente al escalado de los datos</strong></em>. Las <em><strong>redes neuronales exigen una varianza uniforme entre las características de entrada</strong></em>, idealmente con una <em><strong>media de 0 y una varianza de 1</strong></em>.</p></li>
<li><p>Para satisfacer estos criterios, <em><strong>los datos deben reescalarse</strong></em>. Mientras que aquí estamos implementando este proceso manualmente, en el capítulo <em><strong>Evaluación de modelos y Pipelines</strong></em> se usa <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> para el <em><strong>manejo automatizado de este procedimiento</strong></em></p></li>
</ul>
<ul class="simple">
<li><p>Calculamos el <em><strong>valor medio por característica</strong></em> en el conjunto de entrenamiento</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean_on_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Calculamos la <em><strong>desviación estándar de cada característica</strong></em> en el conjunto de entrenamiento</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">std_on_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Procedemos con el proceso de <em><strong>estandarización a media 0 y desviación 1</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span> <span class="o">-</span> <span class="n">mean_on_train</span><span class="p">)</span> <span class="o">/</span> <span class="n">std_on_train</span>
<span class="n">X_test_scaled</span>  <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span> <span class="o">-</span> <span class="n">mean_on_train</span><span class="p">)</span> <span class="o">/</span> <span class="n">std_on_train</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-5 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-5 {
  color: var(--sklearn-color-text);
}

#sk-container-id-5 pre {
  padding: 0;
}

#sk-container-id-5 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-5 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-5 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-5 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-5 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-5 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-5 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-5 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-5 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-5 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-5 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-5 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-5 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-5 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-5 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-5 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-5 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-5 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-5 div.sk-label label.sk-toggleable__label,
#sk-container-id-5 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-5 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-5 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-5 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-5 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-5 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-5 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-5 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-5 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-5 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-5 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-5" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>MLPClassifier(max_iter=400, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" checked><label for="sk-estimator-id-5" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;MLPClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html">?<span>Documentation for MLPClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>MLPClassifier(max_iter=400, random_state=0)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 1.000
Accuracy on test set: 0.972
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Dado que se aprecia una <em><strong>disparidad entre el rendimiento de entrenamiento y el de prueba</strong></em>, se puede intentar <em><strong>mejorar el rendimiento de generalización reduciendo la complejidad del modelo</strong></em>. En este escenario, optamos por <em><strong>intensificar el parámetro</strong></em> <code class="docutils literal notranslate"><span class="pre">&quot;alpha&quot;</span></code> <em><strong>(de 0.0001 a 1)</strong></em> para <em><strong>imponer una regularización más potente de los pesos</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-6 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-6 {
  color: var(--sklearn-color-text);
}

#sk-container-id-6 pre {
  padding: 0;
}

#sk-container-id-6 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-6 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-6 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-6 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-6 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-6 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-6 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-6 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-6 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-6 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-6 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-6 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-6 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-6 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-6 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-6 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-6 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-6 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-6 div.sk-label label.sk-toggleable__label,
#sk-container-id-6 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-6 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-6 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-6 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-6 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-6 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-6 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-6 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-6 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-6 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-6 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-6" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>MLPClassifier(alpha=1, max_iter=1000, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" checked><label for="sk-estimator-id-6" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;MLPClassifier<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html">?<span>Documentation for MLPClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>MLPClassifier(alpha=1, max_iter=1000, random_state=0)</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on training set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy on test set: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy on training set: 0.988
Accuracy on test set: 0.972
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><em><strong>Descubrir lo que ha aprendido una red neuronal suele ser todo un reto</strong></em>. Una técnica para comprender mejor los conocimientos adquiridos consiste en <em><strong>analizar los pesos del modelo</strong></em>. Puede ver un ejemplo en la galería de <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. Sin embargo, para el conjunto de <em><strong>datos de Cáncer de Mama, la comprensión puede ser algo desafiante</strong></em>.</p></li>
<li><p>La siguiente rutina muestra los <em><strong>pesos aprendidos, conectando la entrada a la primera capa oculta</strong></em>. Las <em><strong>filas corresponden a las 30 características de entrada</strong></em>, mientras que las <em><strong>columnas corresponden a las 100 unidades ocultas</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">coefs_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span> <span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Columns in weight matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Input feature&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/277e080f05fc51092dd4841cb293c2c1612df7b878b52a92822dbd3fbee2fdce.png" src="_images/277e080f05fc51092dd4841cb293c2c1612df7b878b52a92822dbd3fbee2fdce.png" />
</div>
</div>
<ul class="simple">
<li><p>Una posible inferencia que podemos hacer es que las <em><strong><code class="docutils literal notranslate"><span class="pre">características</span> <span class="pre">que</span> <span class="pre">tienen</span> <span class="pre">pesos</span> <span class="pre">muy</span> <span class="pre">pequeños</span> <span class="pre">para</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">unidades</span> <span class="pre">ocultas</span> <span class="pre">son</span> <span class="pre">&quot;menos</span> <span class="pre">importantes&quot;</span> <span class="pre">para</span> <span class="pre">el</span> <span class="pre">modelo</span></code></strong></em>. Podemos ver que <em><strong>“mean smoothness”</strong></em> y <em><strong>“mean compactness”</strong></em>, además de las características encontradas entre <em><strong>“smoothness error”</strong></em> y <em><strong>“fractal dimension error”</strong></em>, tienen <em><strong>pesos relativamente bajos comparados con otras características</strong></em>. Esto podría significar que se trata de <em><strong>rasgos menos relevantes o, posiblemente, que no las representamos de forma que la red neuronal pudiera utilizarlas.</strong></em></p></li>
<li><p>También podemos <em><strong>visualizar los pesos que conectan la capa oculta con la capa de salida, pero son aún más difíciles de interpretar</strong></em>. Aunque <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> y <code class="docutils literal notranslate"><span class="pre">MLPRegressor</span></code> <em><strong>proporcionan interfaces fáciles de usar</strong></em> para las arquitecturas de redes neuronales más comunes, <em><strong>solo capturan un pequeño subconjunto de lo que es posible con las redes neuronales</strong></em>. Si está interesado en trabajar con <em><strong>modelos más flexibles o modelos más grandes</strong></em>, se recomienda mirar más allá de <em><strong>scikit-learn en las fantásticas bibliotecas de aprendizaje profundo</strong></em>.</p></li>
<li><p>Para los usuarios de <code class="docutils literal notranslate"><span class="pre">Python</span></code>, las más conocidas son <code class="docutils literal notranslate"><span class="pre">keras,</span> <span class="pre">lasagna</span></code> y `tensor-flow. Estas bibliotecas proporcionan una interfaz mucho más <em><strong>flexible para construir redes neuronales</strong></em> y seguir el rápido progreso en la investigación del aprendizaje profundo, también permiten el uso de <em><strong>unidades de procesamiento gráfico (GPU) de alto rendimiento que scikit-learn no soporta</strong></em>.</p></li>
</ul>
</section>
<section id="analisis-de-malware-por-api-calls">
<h2><span class="section-number">8.11. </span>Análisis de Malware por API calls<a class="headerlink" href="#analisis-de-malware-por-api-calls" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>El siguiente conjunto de datos forma parte de una investigación sobre la <em><strong>detección y clasificación de Malware mediante Deep Learning</strong></em> (ver <a class="reference external" href="https://www.techrxiv.org/articles/preprint/Behavioral_Malware_Detection_Using_Deep_Graph_Convolutional_Neural_Networks/10043099">Oliveira, Angelo; Sassi, Renato José (2019)</a>). Contiene <em><strong>42.797 secuencias de llamadas a la API de malware y 1.079 secuencias de llamadas a la API de goodware</strong></em>. Cada secuencia de llamadas a la API se compone de las <em><strong>100 primeras llamadas a la API consecutivas no repetidas asociadas al proceso principal</strong></em>, extraídas de los elementos <em><strong>“calls”</strong></em> de los informes de <a class="reference external" href="https://cuckoosandbox.org/">Cuckoo Sandbox</a>.</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Características</span></code></strong></em></p>
<ol class="arabic simple">
<li><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Nombre</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">columna</span></code>: hash</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Descripción</span></code>: El hash MD5 del ejemplo</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Tipo</span></code>: Cadena de 32 bytes</p></li>
</ul>
</li>
<li><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Nombre</span> <span class="pre">de</span> <span class="pre">columna</span></code>: t_0, t_1,…, t99</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Descripción</span></code>: Llamada a la API</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Tipo</span></code>: Entero (0-306)</p></li>
</ul>
</li>
<li><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Nombre</span> <span class="pre">de</span> <span class="pre">columna</span></code>: malware</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Descripción</span></code>: Clase</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Tipo</span></code>: Entero: 0 (Goodware) o 1 (Malware)</p></li>
</ul>
</li>
</ol>
</li>
</ul>
<figure class="align-center" id="trojan-horse-malware">
<a class="reference internal image-reference" href="_images/trojan_horse_malware.png"><img alt="_images/trojan_horse_malware.png" src="_images/trojan_horse_malware.png" style="width: 495.0px; height: 483.59999999999997px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.20 </span><span class="caption-text">Gráfico de comportamiento del malware troyano con hash MD5 ac65ce897a1f0dc273e8dc54fe3768ec.</span><a class="headerlink" href="#trojan-horse-malware" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pylab</span> <span class="k">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span> 
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span><span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_recall_curve</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">process_time</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/lihkir/Data/main/api_call_sequence_per_malware.csv&quot;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(43876, 102)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>La siguiente es una construida en el presente curso, la cual puede ser útil para quienes solo desean presentar unas cuantas columnas al inicio y al final de cada <code class="docutils literal notranslate"><span class="pre">pandas</span></code>. Solo requiere del nombre del <code class="docutils literal notranslate"><span class="pre">pandas</span></code> y el número de columnas que deseamos visualziar al inicio y al final. Es bastante util, para utilizar en los <code class="docutils literal notranslate"><span class="pre">Jupyter</span> <span class="pre">Books</span></code>, donde el espacio horizontal es reducido.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">idx_lr</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_new_cols</span><span class="p">):</span>
    <span class="n">n_data_cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">idx_l</span>  <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_new_cols</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">idx_r</span>  <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_data_cols</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_data_cols</span> <span class="o">-</span> <span class="n">n_new_cols</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">idx_l</span> <span class="o">+</span> <span class="n">idx_r</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Para imprimir por ejemplo las tres primeras columnas al inicio y al final del pandas <em><strong>data</strong></em> utilizamos la orden <code class="docutils literal notranslate"><span class="pre">data.iloc[:,</span> <span class="pre">idx_lr(data,</span> <span class="pre">4)]</span></code>. Nótese que no contamos con datos faltantes que requieran de un procedimiento de imputación (ver <a class="reference external" href="https://lihkir.github.io/ShinyDash/an%C3%A1lisis-con-datos-faltantes.html">Análisis con datos faltantes</a>). Además, nótese que todas las columnas con la excepción de <code class="docutils literal notranslate"><span class="pre">hash</span></code> corresponde a datos numericos discretos.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">idx_lr</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">4</span><span class="p">)]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>hash</th>
      <th>t_0</th>
      <th>t_1</th>
      <th>t_2</th>
      <th>t_97</th>
      <th>t_98</th>
      <th>t_99</th>
      <th>malware</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>071e8c3f8922e186e57548cd4c703a5d</td>
      <td>112</td>
      <td>274</td>
      <td>158</td>
      <td>208</td>
      <td>56</td>
      <td>71</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>33f8e6d08a6aae939f25a8e0d63dd523</td>
      <td>82</td>
      <td>208</td>
      <td>187</td>
      <td>171</td>
      <td>215</td>
      <td>35</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>b68abd064e975e1c6d5f25e748663076</td>
      <td>16</td>
      <td>110</td>
      <td>240</td>
      <td>65</td>
      <td>113</td>
      <td>112</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>72049be7bd30ea61297ea624ae198067</td>
      <td>82</td>
      <td>208</td>
      <td>187</td>
      <td>302</td>
      <td>228</td>
      <td>302</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>c9b3700a77facf29172f32df6bc77f48</td>
      <td>82</td>
      <td>240</td>
      <td>117</td>
      <td>260</td>
      <td>141</td>
      <td>260</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# NaN values:&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># NaN values: 0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">dtypes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hash       object
t_0         int64
t_1         int64
t_2         int64
t_3         int64
            ...  
t_96        int64
t_97        int64
t_98        int64
t_99        int64
malware     int64
Length: 102, dtype: object
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Pasamos ahora a eliminar aquellas variables (columnas) irrelevantes en el análisis predictivo, a saber la columna <code class="docutils literal notranslate"><span class="pre">hash</span></code> del pandas <code class="docutils literal notranslate"><span class="pre">data</span></code> y creamos uno nuevo al cual nombraremos <code class="docutils literal notranslate"><span class="pre">data_new</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_new</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;hash&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">data_new</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(43876, 101)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Verifiquemos si nuestros datos están desbalanceados respecto a las clases de interés <em><strong>0 (Goodware) o 1 (Malware)</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc_file_defaults</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cnt_pro</span> <span class="o">=</span> <span class="n">data_new</span><span class="p">[</span><span class="s1">&#39;malware&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">cnt_pro</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">cnt_pro</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of data&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Malware Type&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a397b2a059e1eaf76e02b3f57721844d8e6cd469ff82f714bd2fff1d1ed85b27.png" src="_images/a397b2a059e1eaf76e02b3f57721844d8e6cd469ff82f714bd2fff1d1ed85b27.png" />
</div>
</div>
<ul class="simple">
<li><p>Claramente, existe un desbalance entre los tipos de Malware. Por otro lado, verifiquemos para algunos Hash, como se distribuyen la frecuencia de cada una de sus API calls. Las columnas representan las <em><strong>frecuencias de cada API call</strong></em> representado por los valores <code class="docutils literal notranslate"><span class="pre">t_0,</span> <span class="pre">t_1,...,</span> <span class="pre">t99</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Malware API Calls by Hash&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">0.92</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">axs</span><span class="o">.</span><span class="n">ravel</span><span class="p">()):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">n_hash</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_new</span><span class="p">))</span>
    <span class="n">data_new</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">n_hash</span><span class="p">,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Hash: &#39;</span><span class="o">+</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;hash&#39;</span><span class="p">][</span><span class="n">n_hash</span><span class="p">],</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;API Call&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/18cde9be2d27606a6bb9ca0683ecc9d63c42ac685bdd4e44e774e7408ab1aee6.png" src="_images/18cde9be2d27606a6bb9ca0683ecc9d63c42ac685bdd4e44e774e7408ab1aee6.png" />
</div>
</div>
<ul class="simple">
<li><p>Procedemos a hacer uso de <code class="docutils literal notranslate"><span class="pre">make_pipeline</span></code> y <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> para obtener parámetros del mejor modelo. En este caso utilizaremos el clasificador <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code>, el cual se estudió en clase de forma analítica.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}-\mu_{i}\nabla J(\boldsymbol{\theta}^{(i-1)}),\quad\text{Gradiente descendente}.
\]</div>
<ul class="simple">
<li><p>El algoritmo clasificador <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> denota el valor de la <em><strong>longitud de paso</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}\)</span> como <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, el cual asume los valores <em><strong>constant, invscaling, adaptive</strong></em>. En este ejemplo se ha probado con los tres parámetros, obteniendo siempre el mejor score cuando <code class="docutils literal notranslate"><span class="pre">learning_rate</span> <span class="pre">=</span> <span class="pre">constant</span></code>. Por otro lado, <code class="docutils literal notranslate"><span class="pre">hidden_layer_sizes</span></code> representa el número de neuronas en el <span class="math notranslate nohighlight">\(i\)</span>th layer. En este problema de clasificación optamos por seleccionar desde una a tres capas escondidas (<em><strong>hidden layers</strong></em>), pero siempre el mejor score entregado por <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>, fue obtenido considerando una sola capa. Por lo tanto, en este problema hacemos solo un grid search sobre el número de neuronas sobre una sola capa.</p></li>
<li><p>Dado que nuestro problema es de clasificación binaria, también consideramos en el <code class="docutils literal notranslate"><span class="pre">param_grid</span></code>, solo <code class="docutils literal notranslate"><span class="pre">activation</span> <span class="pre">=</span> <span class="pre">logistic</span></code>. Las opciones del clasificador son <code class="docutils literal notranslate"><span class="pre">identity,</span> <span class="pre">logistic,</span> <span class="pre">tanh,</span> <span class="pre">relu</span></code>, pero <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> entregó el mejor score usando <code class="docutils literal notranslate"><span class="pre">logistic</span></code> como era de esperarse. El parámetro <code class="docutils literal notranslate"><span class="pre">alpha</span></code> es la fuerza del término de regularización <span class="math notranslate nohighlight">\(L^2\)</span>, similar al utilizado en la regresión <code class="docutils literal notranslate"><span class="pre">ridge</span></code> cuando deseamos reducir la complejidad de un modelo. Puede revisar la documentación del clasificador para ver todos los parámetros asociados, y aquellos que son por defecto (ver <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">MLPClassifier</a>).</p></li>
</ul>
<figure class="align-center" id="final-neural-network">
<a class="reference internal image-reference" href="_images/final_neural_network.png"><img alt="_images/final_neural_network.png" src="_images/final_neural_network.png" style="width: 317.79999999999995px; height: 376.59999999999997px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.21 </span><span class="caption-text">Red neuronal feed-forward de tres capas.</span><a class="headerlink" href="#final-neural-network" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><em><strong>La capa de entrada</strong></em></p>
<ul class="simple">
<li><p>En general las ANNs tienen exactamente una capa de entrada. Con respecto al número de neuronas que componen esta capa, este parámetro se determina de forma completa y única, una vez que se conoce la forma de los datos de entrenamiento. <em><strong>En concreto, el número de neuronas que componen esa capa es igual al número de características (columnas) de sus datos</strong></em>. Algunas configuraciones de ANNs añaden un nodo adicional para un término de sesgo.</p></li>
</ul>
<p><em><strong>La capa de salida</strong></em></p>
<ul class="simple">
<li><p>Al igual que la capa de entrada, <em><strong>cada ANN tiene exactamente una capa de salida. Determinar su tamaño (número de neuronas) es sencillo; está completamente determinado por la configuración del modelo elegido.</strong></em> Si la ANN es un <em><strong>regresor</strong></em>, la capa de salida tiene un solo nodo (<em><strong>Time Series Forecasting</strong></em>). Si la ANN es un <em><strong>clasificador</strong></em>, entonces también tiene un solo nodo, a menos que se utilice <code class="docutils literal notranslate"><span class="pre">softmax</span></code>, en cuyo caso la capa de salida tiene un nodo por cada etiqueta de clase en su modelo.</p></li>
</ul>
<p><em><strong>Las capas ocultas</strong></em></p>
<ul class="simple">
<li><p><em><strong>¿Cuántas capas ocultas?</strong></em>. Si los datos son <em><strong>linealmente separables (lo que a menudo se sabe cuando se empieza a codificar una ANN, SVM puede servir de test)</strong></em>, entonces no se necesita ninguna capa oculta. Por supuesto, tampoco se necesita una ANN para resolver los datos, pero está seguirá haciendo su trabajo.</p></li>
<li><p>Sobre la configuración de las capas ocultas en las ANNs, existe un consenso dentro de este tema, y es la diferencia de rendimiento al añadir capas ocultas adicionales: <em><strong>las situaciones en las que el rendimiento mejora con una segunda (o tercera, etc.) capa oculta son muy pocas</strong></em>. Una capa oculta es suficiente para la gran mayoría de los problemas.</p></li>
<li><p>Entonces, <em><strong>¿qué pasa con el tamaño de la(s) capa(s) oculta(s), cuántas neuronas?</strong></em>. Existen algunas reglas empíricas; de ellas, la más utilizada es <em><strong>‘The optimal size of the hidden layer is usually between the size of the input and size of the output layers’</strong></em>. <em><strong>Jeff Heaton, the author of Introduction to Neural Networks in Java</strong></em>.</p></li>
</ul>
<ul>
<li><p>Hay una regla empírica adicional que ayuda en los problemas de aprendizaje supervisado. Normalmente <em><strong>se puede evitar el sobreajuste si se mantiene el número de neuronas por debajo de</strong></em>:</p>
<div class="math notranslate nohighlight">
\[
    N_{h}=\frac{N_{s}}{(\alpha\cdot(N_{i}+N_{o}))}
    \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N_{i}=\)</span> número de neuronas de entrada</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{o}=\)</span> número de neuronas de salida</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{s}=\)</span> número de muestras en el conjunto de datos de entrenamiento</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha=\)</span> un factor de escala arbitrario, normalmente 2-10</p></li>
</ul>
</li>
<li><p>Un valor de <span class="math notranslate nohighlight">\(\alpha=2\)</span> suele funcionar <em><strong>sin sobreajustar</strong></em>. Se puede pensar en <span class="math notranslate nohighlight">\(\alpha\)</span> como el <em><strong>factor de ramificación efectivo o el número de pesos distintos de cero para cada neurona</strong></em>. Las capas de salida harán que el factor de ramificación “efectivo” sea muy inferior al factor de ramificación medio real de la red. Para <em><strong>profundizar mas en el diseño de redes neuronales, ver el siguiente texto de</strong></em> <a class="reference external" href="https://hagan.okstate.edu/nnd.html">Martin Hagan</a>.</p></li>
</ul>
<ul class="simple">
<li><p>En resumen, para la mayoría de los problemas, probablemente se podría obtener un rendimiento decente (incluso sin un segundo paso de optimización) estableciendo la configuración de la capa oculta utilizando sólo dos reglas:</p>
<ul>
<li><p><em><strong>el número de capas ocultas es igual a uno</strong></em></p></li>
<li><p><em><strong>el número de neuronas de esa capa es la media de las neuronas de las capas de entrada y salida.</strong></em> ¡Nótese que en el ejemplo de esta sección el número de columnas para <span class="math notranslate nohighlight">\(X\)</span> es 100!.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Pasamos ahora a implementar un modelo de clasificación para el conjunto de datos relacionados con: <em><strong>Análisis de Malware por API calls</strong></em>. Utilizaremos la clase <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> y como preprocesamiento, estandarizaremos nuestros datos usando la clase <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>. Para encontrar los mejores parámetros y evita problemas de <em><strong>data leakage</strong></em>, utilizaremos <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> y <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> tal como se explicó en secciones anteriores.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;mlpclassifier__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">],</span>
              <span class="s1">&#39;mlpclassifier__hidden_layer_sizes&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">10</span><span class="p">,),</span> <span class="p">(</span><span class="mi">100</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,)],</span>
              <span class="s1">&#39;mlpclassifier__activation&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;logistic&#39;</span><span class="p">],</span>
              <span class="s1">&#39;mlpclassifier__learning_rate&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;constant&#39;</span><span class="p">]}</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Construimos nuestra matriz de caracteristicas, o variable explicativa <em><strong>X</strong></em> y el vector de clases o la variable respuesta <em><strong>y</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">data_new</span><span class="p">[</span><span class="s1">&#39;malware&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data_new</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;malware&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Realizamos la división de nuestros datos a priori, con el objetivo de evitar <em><strong>data leakage</strong></em>. Consideramos el mismo <code class="docutils literal notranslate"><span class="pre">random_state</span></code> utilizado en el clasificador, con el objetivo de que los resutlados sean reproducibles.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>El siguiente ciclo <em><strong>if</strong></em> ha sido implementado en este curso para evitar reentrenar nuestro modelo clasificador. Nótese que si el algoritmo no detecta que existe un archivo de extensión <code class="docutils literal notranslate"><span class="pre">.pkl</span></code>, reentrena el modelo, de lo contrario, utiliza el modelo entrenado a priori, el cual ha sido guardado en el archivo <code class="docutils literal notranslate"><span class="pre">.pkl</span></code>. Los archivo <code class="docutils literal notranslate"><span class="pre">Pickle</span></code> de extensión <code class="docutils literal notranslate"><span class="pre">.pkl</span></code> en simples palabras se encargan de <em><strong>serializar un objeto</strong></em>, esto es transformar el mismo en una cadena de bytes única que puede ser guardada en un archivo (<em><strong>de extensión .pkl</strong></em>), archivo que podemos desempaquetar después y trabajar con su contenido. Para poder utlizarlo necesitamos importarlo con la orden <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">pickle</span></code>, e instalar la librería (<em><strong>Pickel viene por defecto desde Python 3.9</strong></em>) si así se requiere utilizando:</p></li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>pickle
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">dump</span><span class="p">,</span> <span class="n">load</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;grid_mlp.joblib&quot;</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;grid_mlp.joblib&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">time_start</span> <span class="o">=</span> <span class="n">process_time</span><span class="p">()</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>
    <span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">time_stop</span> <span class="o">=</span> <span class="n">process_time</span><span class="p">()</span>
    <span class="n">str_cpu_time</span> <span class="o">=</span> <span class="s2">&quot;GridSearchCV CPU time: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">((</span><span class="n">time_stop</span><span class="o">-</span><span class="n">time_start</span><span class="p">)</span><span class="o">*</span><span class="mf">0.6</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; minutes&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">str_cpu_time</span><span class="p">)</span> 
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;cpu_time.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">str_cpu_time</span><span class="p">)</span>
    <span class="n">dump</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="s1">&#39;grid_mlp.joblib&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best CV score = </span><span class="si">%0.3f</span><span class="s2">:&quot;</span> <span class="o">%</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best CV score = 0.958:
Best parameters:
{&#39;mlpclassifier__activation&#39;: &#39;logistic&#39;, &#39;mlpclassifier__alpha&#39;: 0.01, &#39;mlpclassifier__hidden_layer_sizes&#39;: (100,), &#39;mlpclassifier__learning_rate&#39;: &#39;constant&#39;}
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Utilizamos el <code class="docutils literal notranslate"><span class="pre">f1_score</span></code>, el cual, de acuerdo a lo estudiado en secciones previas, calcula la media armonica entre <code class="docutils literal notranslate"><span class="pre">precision</span></code> y <code class="docutils literal notranslate"><span class="pre">recall</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F1 Score of the classifier is: </span><span class="si">{</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>F1 Score of the classifier is: 0.9937633808061063
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Además podemos usar <code class="docutils literal notranslate"><span class="pre">classification_report</span></code> y la matriz de confusión, para identificar aquellas clases que fueron incorrectamente clasificadas por nuestro modelo</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

           0       0.94      0.56      0.70       283
           1       0.99      1.00      0.99     10686

    accuracy                           0.99     10969
   macro avg       0.96      0.78      0.85     10969
weighted avg       0.99      0.99      0.99     10969
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mlp_cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mlp_cm</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">linecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;BuPu&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MLP Confusion Matrix&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Y predict&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y test&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/98e9c819a3f79440e7ffc629470627a7e5aa39f58b08fb1deb64613295fa2688.png" src="_images/98e9c819a3f79440e7ffc629470627a7e5aa39f58b08fb1deb64613295fa2688.png" />
</div>
</div>
<ul class="simple">
<li><p>Nótese que la <code class="docutils literal notranslate"><span class="pre">clase</span> <span class="pre">0</span></code>, presenta el mayor número de clasificaciones erroneas <code class="docutils literal notranslate"><span class="pre">(FP)</span></code>, obtenidas por nuestra ANN. Justamente esto se debe al desbalance notorio en nuestros datos, el cual se inclina hacia la clases 1, de mayor frecuencia. Si se tiene clara una <em><strong>métrica de negocio</strong></em> o un <em><strong>punto de operación</strong></em>, podríamos ajustar el número de prediccion incorrectas a favor de este <em><strong>punto de operación</strong></em>, tal como en el problema de detección de cancer, donde el interés era reducir (<em><strong>FN</strong></em>). Para analizar el comportamiento del clasificador a diferentes umbrales, utilizamos la curva <em><strong>ROC</strong></em> y <em><strong>precision-recall</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fpr_mlp</span><span class="p">,</span> <span class="n">tpr_mlp</span><span class="p">,</span> <span class="n">thresholds_mlp</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">grid</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_mlp</span><span class="p">,</span> <span class="n">tpr_mlp</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;ROC Curve MLP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;FPR&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;TPR (recall)&quot;</span><span class="p">)</span>
<span class="n">close_default_mlp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">thresholds_mlp</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_mlp</span><span class="p">[</span><span class="n">close_default_mlp</span><span class="p">],</span> <span class="n">tpr_mlp</span><span class="p">[</span><span class="n">close_default_mlp</span><span class="p">],</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> 
         <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;threshold 0.5 MLP&quot;</span><span class="p">,</span> <span class="n">fillstyle</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5d32aa09e306c54377c2dad6a66311f83f79a86f386c042211e4db8af122507a.png" src="_images/5d32aa09e306c54377c2dad6a66311f83f79a86f386c042211e4db8af122507a.png" />
</div>
</div>
<ul class="simple">
<li><p>Nótese que la tasa <em><strong>TPR</strong></em> se mantiene aproximadamente contante para valores de <em><strong>FPR&gt;0.2</strong></em>, esto es, para esta tasas de <em><strong>FPR</strong></em> no se está sacrificando la tasa <em><strong>recall</strong></em>. Si deseamos mantener un <em><strong>recall</strong></em> alto, aproximadamente, mayor que 0.8, podemos considerar por ejemplo una tasa <em><strong>FPR</strong></em> en torno a 0.1, con la que no estaríamos sacrificando <em><strong>recall</strong></em> y además, obtendriamos un tasa de falsos positivos <em><strong>FPR</strong></em> relativamente baja.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">precision_mlp</span><span class="p">,</span> <span class="n">recall_mlp</span><span class="p">,</span> <span class="n">thresholds_mlp</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">grid</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">precision_mlp</span><span class="p">,</span> <span class="n">recall_mlp</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;MLP&quot;</span><span class="p">)</span>
<span class="n">close_default_mlp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">thresholds_mlp</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">precision_mlp</span><span class="p">[</span><span class="n">close_default_mlp</span><span class="p">],</span> <span class="n">recall_mlp</span><span class="p">[</span><span class="n">close_default_mlp</span><span class="p">],</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
         <span class="n">markersize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;threshold 0.5 MLP&quot;</span><span class="p">,</span> <span class="n">fillstyle</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">mew</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Precision&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Recall&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/859905d45090955a392bb04e073feb8f84b406d55a534585babfb9cfd7261e0f.png" src="_images/859905d45090955a392bb04e073feb8f84b406d55a534585babfb9cfd7261e0f.png" />
</div>
</div>
<ul class="simple">
<li><p>Nótese que con el umbral por defecto, la curva <em><strong>precision-recall</strong></em> muestra una clasificación con <em><strong>scores bastante buenos para cada una de estas métricas</strong></em>, incluso, si movemos el umbral un poco, para favorecer la clase 0 que es la menos frecuente, vamos a obtener de igual manera un rango de valores altos para <em><strong>precision</strong></em> y <em><strong>recall</strong></em>. Sólo valores de <em><strong>precision</strong></em> muy elevados, esto es, valores cuya distancia 1 tiende a cero, estaríamos sacrificando <em><strong>recall</strong></em>.</p></li>
</ul>
</section>
<section id="redes-neuronales-convolucionales">
<h2><span class="section-number">8.12. </span>Redes Neuronales Convolucionales<a class="headerlink" href="#redes-neuronales-convolucionales" title="Link to this heading">#</a></h2>
<div class="tip admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Hasta ahora, las redes neuronales se han considerado como <em><strong>receptoras de vectores de características en su capa de entrada</strong></em>, similar a otros clasificadores discutidos anteriormente. Estos vectores se derivan de datos brutos para condensar información relevante para la tarea de aprendizaje automático. Sin embargo, a finales de los años 80, surgió un enfoque alternativo: <em><strong>integrar la generación de características en el entrenamiento de la red neuronal</strong></em>.</p></li>
<li><p>Así nacieron las <em><strong>Redes Neuronales Convolucionales (CNN)</strong></em>, que <em><strong>aprenden características directamente de los datos junto con los parámetros de la red</strong></em>. Su éxito inicial fue en el reconocimiento de dígitos OCR <span id="id8">[<a class="reference internal" href="biblio.html#id27" title="Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989.">LeCun <em>et al.</em>, 1989</a>]</span>. El término <em><strong>“convolucional” se refiere a que la primera capa realiza convoluciones en lugar de productos internos</strong></em>, usados en redes totalmente conectadas.</p></li>
</ul>
</div>
<section id="la-necesidad-de-convoluciones">
<h3><span class="section-number">8.12.1. </span>La necesidad de convoluciones<a class="headerlink" href="#la-necesidad-de-convoluciones" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Asegurémonos primero de que entendemos la razón por la que <em><strong>no podemos, en la práctica, alimentar la entrada de una red neuronal directamente con datos brutos</strong></em>, por ejemplo, una <em><strong>matriz de imágenes</strong></em> o las muestras de una <em><strong>versión digitalizada de un segmento de voz</strong></em>, y por qué es necesario un <em><strong>preprocesamiento para generar características</strong></em>. Lo mismo ocurre para cualquier predictor/aprendiz y no sólo para las redes neuronales. En muchas aplicaciones, <em><strong>trabajar directamente con los datos en bruto hace que la tarea sea sencillamente inmanejable</strong></em>.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/hex.jpg"><img alt="_images/hex.jpg" src="_images/hex.jpg" style="width: 540.0px; height: 445.6px;" /></a>
</figure>
<ul class="simple">
<li><p>Tomemos, como ejemplo, el caso de una <em><strong>imagen, matriz de 256 × 256</strong></em>. Al <em><strong>vectorizarla se obtiene un vector de entrada</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\in\mathbb{R}^{l}\)</span>, donde <span class="math notranslate nohighlight">\(l\approx 65000\)</span>. Supongamos que el <em><strong>número de nodos de la primera capa</strong></em> es <span class="math notranslate nohighlight">\(k_{1} = 1000\)</span>. Entonces, el número de los parámetros implicados, <span class="math notranslate nohighlight">\(\theta_{jk}, j = 1, 2,\dots, 65000\)</span>, <span class="math notranslate nohighlight">\(k = 1, 2,\dots,1000\)</span>, que <em><strong>conectan todos los nodos de entrada a todos los nodos de la primera capa en una red totalmente conectada</strong></em>, sería del orden de 65 millones. <em><strong>Este número se dispara aún más si la imagen de entrada es de alta resolución</strong></em> y tiene <em><strong>píxeles del orden de <span class="math notranslate nohighlight">\(1000\times 1000\)</span></strong></em>. Además, este número <em><strong>aumenta si tratamos, por ejemplo, con imágenes en color en las que la dimensionalidad de la entrada se multiplica por tres</strong></em> para un esquema de representación del <em><strong>color <code class="docutils literal notranslate"><span class="pre">RGB</span> <span class="pre">(red-green-blue)</span></code></strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>Además, <em><strong>a medida que se añaden capas ocultas, el número de parámetros sigue aumentando</strong></em>. Asimismo los <em><strong>problemas de carga computacional asociados</strong></em>, sabemos que <em><strong>entrenar redes con un gran número de parámetros pone seriamente a prueba su rendimiento de generalización</strong></em>. Estas redes <em><strong>requerirían una enorme cantidad de datos de entrenamiento para hacer frente a la tendencia al sobreajuste</strong></em>. Además de la explosión del número de parámetros, la <em><strong>vectorización de una matriz de imágenes conlleva una pérdida de información</strong></em>, ya que desechamos información importante sobre <em><strong>cómo se interrelacionan los píxeles en una zona de la imagen</strong></em>. De hecho, el objetivo de los distintos métodos de generación de características, que se han desarrollado a lo largo de los años es exactamente eso. Es decir, <em><strong>extraer información que cuantifica las correlaciones u otras dependencias estadísticas que relacionan los valores de los píxeles dentro de la imagen</strong></em>. En este modo, se puede <em><strong>“codificar”</strong></em> eficazmente la información relacionada con el aprendizaje que reside en los datos brutos.</p></li>
</ul>
<ul class="simple">
<li><p>Al emplear <em><strong>convoluciones</strong></em>, se pueden <em><strong>abordar simultáneamente ambos problemas</strong></em>, es decir, el de la <em><strong>explosión de parámetros</strong></em> y el de la <em><strong>extracción de información estadística útil</strong></em>. Los pasos básicos de cualquier red convolucional son:</p>
<ul>
<li><p><em><strong>la etapa de convolución,</strong></em></p></li>
<li><p><em><strong>el paso de no linealidad,</strong></em></p></li>
<li><p><em><strong>el paso de agrupación.</strong></em></p></li>
</ul>
</li>
</ul>
</section>
<section id="la-etapa-de-convolucion">
<h3><span class="section-number">8.12.2. </span>La etapa de convolución<a class="headerlink" href="#la-etapa-de-convolucion" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Una forma de <em><strong>reducir el número de parámetros</strong></em> es mediante el <em><strong>reparto de pesos</strong></em>, como se ha comentado al final de la sección <a class="reference internal" href="#fully-connected-ann"><span class="std std-ref">Redes Totalmente Conectadas</span></a>. Ahora tomaremos prestada esta idea del <em><strong>reparto de pesos y la utilizaremos de una forma más sofisticada</strong></em>. Para ello, nos centraremos en el caso en que <em><strong>la entrada de la red esté formada por imágenes</strong></em>. La <em><strong>imagen matricial de entrada se denomina</strong></em> <span class="math notranslate nohighlight">\(I\)</span>.</p></li>
</ul>
<figure class="align-center" id="image-input-conv-numref">
<a class="reference internal image-reference" href="_images/image_input_conv.png"><img alt="_images/image_input_conv.png" src="_images/image_input_conv.png" style="width: 587.2px; height: 367.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.22 </span><span class="caption-text">(A) Imagen matricial de entrada de <span class="math notranslate nohighlight">\(3\times 3\)</span>. (B) Nodos de la capa oculta.</span><a class="headerlink" href="#image-input-conv-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Para el caso de la <a class="reference internal" href="#image-input-conv-numref"><span class="std std-numref">Fig. 8.22</span></a>, se trata de una <em><strong>matriz de 3 × 3</strong></em>; nótese que <em><strong>la entrada es no vectorizada</strong></em>. Para <em><strong>enfatizar que nos apartaremos de la lógica de las operaciones de multiplicar-añadir (producto interno) de las redes feed-forward completamente conectadas</strong></em>, utilizaremos un símbolo diferente <span class="math notranslate nohighlight">\(h\)</span> en lugar de <span class="math notranslate nohighlight">\(\theta\)</span>, para denotar los <em><strong>parámetros asociados</strong></em>.</p></li>
<li><p>Introducimos ahora el concepto de <em><strong>reparto de pesos</strong></em>. Recordemos que en una red totalmente conectada, cada nodo está asociado con un vector de parámetros, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{i}\)</span>, para el nodo <span class="math notranslate nohighlight">\(i\)</span>th <em><strong>cuya dimensionalidad es igual al número de nodos de la capa anterior</strong></em>. En cambio, <em><strong>ahora cada nodo estará asociado a un único parámetro</strong></em>. Para ello, dispondremos los <em><strong>nodos en forma de matriz bidimensional</strong></em>, como se muestra en <a class="reference internal" href="#image-input-conv-numref"><span class="std std-numref">Fig. 8.22</span></a> (B). Para el caso de la figura, hemos supuesto <em><strong>cuatro nodos dispuestos en una matriz de 2 × 2</strong></em>, <span class="math notranslate nohighlight">\(H\)</span>.</p></li>
</ul>
<div class="important admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>El <em><strong>primer nodo se caracteriza por</strong></em> <span class="math notranslate nohighlight">\(h(1, 1)\)</span>, <em><strong>el segundo por</strong></em> <span class="math notranslate nohighlight">\(h(1, 2)\)</span>, y así sucesivamente. En otras palabras, <em><strong>cualquier conexión que termine en el primer nodo se multiplicará por el mismo peso</strong></em> <span class="math notranslate nohighlight">\(h(1, 1)\)</span>, y <em><strong>un argumento similar es válido para el resto de los nodos</strong></em>. Este razonamiento reduce drásticamente el número de parámetros. Sin embargo, para que esto tenga sentido, tenemos que <em><strong>alejarnos de la lógica de las operaciones de producto interior de las redes totalmente conectadas</strong></em>.</p></li>
<li><p>Para entender por qué, <em><strong>supongamos que utilizamos un único parámetro por nodo en una red totalmente conectada</strong></em>. Entonces, <em><strong>la salida del combinador lineal asociado al primer nodo sería</strong></em> <span class="math notranslate nohighlight">\(O(1, 1) = h(1, 1)a\)</span>, donde <span class="math notranslate nohighlight">\(a\)</span> es la <em><strong>suma de todas las entradas al nodo recibidas de la capa anterior</strong></em>. La salida respectiva del segundo nodo sería <span class="math notranslate nohighlight">\(O(1, 2) = h(1, 2)a\)</span>, y así sucesivamente. Por lo tanto, <em><strong>todos los nodos proporcionarían básicamente la misma información con respecto a los valores de entrada</strong></em>; la <em><strong>única diferencia</strong></em> serían los <em><strong>distintos pesos que actúan sobre la misma información de entrada de la capa anterior</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Pasemos ahora a introducir un concepto diferente, en el que <em><strong>mantenemos un único parámetro por nodo</strong></em>, aunque <em><strong>cada una de las salidas de una capa oculta transmite información diferente con respecto a las distintas entradas que se reciben de la capa anterior</strong></em>.</p></li>
<li><p>Para ello, <em><strong>introduciremos las convoluciones</strong></em>. En este contexto, los <em><strong>nodos de la capa oculta se interpretan como elementos de una matriz</strong></em> <span class="math notranslate nohighlight">\(H\)</span>, y <em><strong>convolucionamos</strong></em> <span class="math notranslate nohighlight">\(H\)</span> <em><strong>con la matriz de entrada entrada</strong></em> <span class="math notranslate nohighlight">\(I\)</span>. El primer valor de salida de la capa oculta será</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
O(1, 1)=h(1, 1)I(1, 1)+h(1, 2)I(1, 2)+h(2, 1)I(2, 1)+h(2, 2)I(2, 2)
\]</div>
<ul class="simple">
<li><p>El resultado anterior se obtiene si <em><strong>colocamos la matriz</strong></em> <span class="math notranslate nohighlight">\(H\)</span> <em><strong>de</strong></em> <span class="math notranslate nohighlight">\(2\times2\)</span> <em><strong>sobre</strong></em> <span class="math notranslate nohighlight">\(I\)</span>, <em><strong>empezando por la esquina superior izquierda</strong></em> (el cuadrado rojo de <a class="reference internal" href="#image-input-conv-numref"><span class="std std-numref">Fig. 8.22</span></a>(A) indica la posición de la matriz <span class="math notranslate nohighlight">\(H\)</span>). A continuación, <em><strong>multiplicamos los elementos en las partes superpuestas de las dos matrices y los sumamos</strong></em>. Desde un <em><strong>punto de vista físico</strong></em>, el valor <span class="math notranslate nohighlight">\(O(1, 1)\)</span> resultante es una <em><strong>media ponderada sobre un área local dentro de la matriz</strong></em> <span class="math notranslate nohighlight">\(I\)</span>.</p></li>
<li><p>En la operación anterior, el área correspondiente de la imagen está formada por los <em><strong>píxeles de la parte superior izquierda 2 × 2 de la matriz</strong></em> <span class="math notranslate nohighlight">\(I\)</span>. Para obtener el <em><strong>segundo valor de salida,</strong></em> <span class="math notranslate nohighlight">\(O(1, 2)\)</span><em><strong>, deslizamos</strong></em> <span class="math notranslate nohighlight">\(H\)</span> <em><strong>un píxel hacia la derecha</strong></em>, como indica el recuadro rojo punteado, y repetimos las operaciones, es decir</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
O(1, 2) = h(1, 1)I(1, 2) + h(1, 2)I (1, 3) + h(2, 1)I(2, 2) + h(2, 2)I(2, 3).
\]</div>
<ul class="simple">
<li><p>Siguiendo el mismo razonamiento, <em><strong>deslizamos</strong></em> <span class="math notranslate nohighlight">\(H\)</span> <em><strong>para “escanear” toda la imagen matricial; así, se obtienen otros dos valores de salida</strong></em> <span class="math notranslate nohighlight">\(O(2, 1)\)</span> y <span class="math notranslate nohighlight">\(O(2, 2)\)</span>. Las cuatro posiciones posibles de <span class="math notranslate nohighlight">\(H\)</span> encima de <span class="math notranslate nohighlight">\(I\)</span> se indican en la <a class="reference internal" href="#image-input-conv-numref"><span class="std std-numref">Fig. 8.22</span></a>(A) por los cuadros cuadrados de color rojo completo, rojo punteado, gris oscuro y gris punteado. <em><strong>Para cada posición se obtiene un valor de salida</strong></em>. Por lo tanto, bajo el escenario descrito anteriormente, <em><strong>las salidas de la primera capa oculta forman una matriz de</strong></em> <span class="math notranslate nohighlight">\(2\times 2\)</span>, <span class="math notranslate nohighlight">\(O\)</span>. Cada uno de los elementos de la matriz de salida codifica información de un área diferente de la imagen de entrada.</p></li>
</ul>
<div class="admonition-convolucion admonition">
<p class="admonition-title">Convolución</p>
<ul class="simple">
<li><p>En el entorno más general, la <em><strong>operación de convolución entre dos matrices,</strong></em> <span class="math notranslate nohighlight">\(H\in R^{m\times m}\)</span> e <span class="math notranslate nohighlight">\(I\in\mathbb{R}^{l\times l}\)</span>, <em><strong>es otra matriz, definida como</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-conv-oph-eq">
<span class="eqno">(8.11)<a class="headerlink" href="#equation-conv-oph-eq" title="Link to this equation">#</a></span>\[
O(i, j)=\sum_{t=1}^{m}\sum_{r=1}^{m}h(t, r)I(i+t-1, j+r-1),~\text{donde en este caso}~m&lt;l.
\]</div>
</div>
<ul class="simple">
<li><p>En otras palabras, <span class="math notranslate nohighlight">\(O(i, j)\)</span> <em><strong>contiene información en un área de ventana de la matriz de entrada</strong></em>. De acuerdo con la definición de la Eq. <a class="reference internal" href="#equation-conv-oph-eq">(8.11)</a> <em><strong>el elemento</strong></em> <span class="math notranslate nohighlight">\(I(i, j)\)</span> <em><strong>es el elemento superior izquierdo de esta área de la ventana</strong></em>. El tamaño de la ventana depende del valor de <span class="math notranslate nohighlight">\(m\)</span>. El <em><strong>tamaño de la matriz de salida depende de las suposiciones que se adopten sobre cómo tratar los elementos/píxeles en los bordes de</strong></em> <span class="math notranslate nohighlight">\(I\)</span>.</p></li>
<li><p>En sentido estricto, en la <em><strong>jerga del procesamiento de señales</strong></em>, la Eq. <a class="reference internal" href="#equation-conv-oph-eq">(8.11)</a> se conoce como <em><strong>operación de correlación cruzada</strong></em>. Para la <em><strong>operación de convolución</strong></em>, primero hay que invertir los índices. Sin embargo, este es el <em><strong>nombre que ha “sobrevivido” en la comunidad del aprendizaje automático</strong></em> y bien nos adherimos a él. Al fin y al cabo, ambas son <em><strong>operaciones ponderadas sobre los píxeles dentro de un área de ventana de una imagen</strong></em>.</p></li>
</ul>
<div class="proof observation admonition" id="observation_ann2">
<p class="admonition-title"><span class="caption-number">Observation 8.1 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>La discusión anterior nos “obliga” a pensar en una <em><strong>capa oculta</strong></em> como una <em><strong>colección de nodos uno al lado del otro</strong></em>. En cambio, <em><strong>en una CNN, cada capa oculta corresponde a una (o a más de una, como pronto veremos) matriz</strong></em> <span class="math notranslate nohighlight">\(H\)</span>. Además, <span class="math notranslate nohighlight">\(H\)</span> <em><strong>se utiliza para realizar convoluciones</strong></em>.</p></li>
<li><p>Desde el punto de vista del <em><strong>tratamiento de señales, esta matriz es un <code class="docutils literal notranslate"><span class="pre">filtro</span> <span class="pre">que</span> <span class="pre">actúa</span> <span class="pre">sobre</span> <span class="pre">la</span> <span class="pre">entrada</span> <span class="pre">para</span> <span class="pre">proporcionar</span> <span class="pre">la</span> <span class="pre">salida.</span></code></strong></em> En la jerga del aprendizaje de maquinas, también se denomina <em><strong>matriz kernel en lugar de filtro</strong></em>. La <em><strong>matriz de salida suele denominarse la matriz del mapa de características</strong></em>.</p></li>
</ul>
</section>
</div><ul class="simple">
<li><p>En resumen, <em><strong>al realizar convoluciones, en lugar de operaciones de producto interno</strong></em>, hemos conseguido</p>
<ol class="arabic simple">
<li><p>Los <em><strong>parámetros que componen la capa oculta son compartidos por todos los píxeles de entrada</strong></em> y no tenemos un conjunto dedicado de parámetros por elemento de entrada (píxel)</p></li>
<li><p>Las <em><strong>salidas de la capa oculta <code class="docutils literal notranslate"><span class="pre">codifican</span> <span class="pre">la</span> <span class="pre">información</span> <span class="pre">de</span> <span class="pre">correlación</span> <span class="pre">de</span> <span class="pre">vecindad</span> <span class="pre">local</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">distintas</span> <span class="pre">zonas</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">imagen</span> <span class="pre">de</span> <span class="pre">entrada</span></code></strong></em>.</p></li>
<li><p>Además, como la <em><strong>salida de la capa oculta también es una matriz de imágenes</strong></em>, se puede considerar como la <em><strong>entrada a una segunda capa oculta y construir así una red con muchas capas, cada una de las cuales realiza convoluciones</strong></em>.</p></li>
</ol>
</li>
</ul>
<ul class="simple">
<li><p>De hecho, <em><strong>estas operaciones de filtrado se han utilizado tradicionalmente para generar características a partir de imágenes</strong></em>. La diferencia era que los elementos de la matriz de filtrado se preseleccionaban. Tomemos ejemplo, la siguiente matriz:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-h-filter-conv-eq">
<span class="eqno">(8.12)<a class="headerlink" href="#equation-h-filter-conv-eq" title="Link to this equation">#</a></span>\[\begin{split}
H=
\begin{bmatrix}
-1 &amp; -1 &amp; -1\\
-1 &amp; 8 &amp; -1\\
-1 &amp; -1 &amp; -1
\end{bmatrix}
\end{split}\]</div>
<ul class="simple">
<li><p>El filtro anterior se conoce como <em><strong>detector de bordes</strong></em>. <em><strong>Convolucionando una matriz de imagen,</strong></em> <span class="math notranslate nohighlight">\(I\)</span> <em><strong>, con la matriz anterior,</strong></em> <span class="math notranslate nohighlight">\(H\)</span><em><strong>,detecta los bordes de una imagen</strong></em>. La <a class="reference internal" href="#edge-detect-convh-numref"><span class="std std-numref">Fig. 8.23</span></a>A muestra la <em><strong>imagen de un barco</strong></em> y la <a class="reference internal" href="#edge-detect-convh-numref"><span class="std std-numref">Fig. 8.23</span></a>B muestra la <em><strong>salida después del filtrado de la imagen de la izquierda con la matriz de filtrado</strong></em> <span class="math notranslate nohighlight">\(H\)</span> anterior.</p></li>
</ul>
<figure class="align-center" id="edge-detect-convh-numref">
<a class="reference internal image-reference" href="_images/edge_detect_convH.png"><img alt="_images/edge_detect_convH.png" src="_images/edge_detect_convH.png" style="width: 667.2px; height: 360.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.23 </span><span class="caption-text">(A) Imagen original y (B) Bordes de la imagen extraídos tras filtrar la matriz de la imagen original con el filtro <span class="math notranslate nohighlight">\(H\)</span> de la ecuación Eq. <a class="reference internal" href="#equation-h-filter-conv-eq">(8.12)</a>. Fuente <span id="id9">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#edge-detect-convh-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La detección de bordes es de <em><strong>gran importancia en la comprensión de imágenes</strong></em>. Además, <em><strong>cambiando adecuadamente los valores en</strong></em> <span class="math notranslate nohighlight">\(H\)</span><em><strong>, se pueden detectar bordes en diferentes orientaciones</strong></em>, por ejemplo, <em><strong>diagonal, vertical, horizontal</strong></em>; en otras palabras, cambiando los valores de <span class="math notranslate nohighlight">\(H\)</span> se pueden <em><strong>generar diferentes tipos de características</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">skimage.color</span> <span class="kn">import</span> <span class="n">rgb2gray</span>
<span class="kn">from</span> <span class="nn">skimage.io</span> <span class="kn">import</span> <span class="n">imread</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">signal</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">pylab</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">im</span> <span class="o">=</span> <span class="n">rgb2gray</span><span class="p">(</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;imgs/cameraman.jpg&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">im</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">im</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(256, 256) 0.9921568627450982
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">blur_box_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="o">/</span> <span class="mi">9</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">edge_laplace_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">edge_laplace_kernel</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0,  1,  0],
       [ 1, -4,  1],
       [ 0,  1,  0]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">im_blurred</span> <span class="o">=</span> <span class="n">signal</span><span class="o">.</span><span class="n">convolve2d</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">blur_box_kernel</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">im_edges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">convolve2d</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">edge_laplace_kernel</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">pylab</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">pylab</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Original Image&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im_blurred</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">pylab</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Box Blur&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im_edges</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">pylab</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Laplace Edge Detection&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/397bb7041d9e3f106a40d873b19a934a19e75f63d15ff63bb39dfb18545d2a19.png" src="_images/397bb7041d9e3f106a40d873b19a934a19e75f63d15ff63bb39dfb18545d2a19.png" />
</div>
</div>
<ul class="simple">
<li><p>Nos hemos acercado a <em><strong>la idea que subyace a las CNN</strong></em></p>
<ol class="arabic simple">
<li><p><em><strong>En lugar de utilizar una matriz de filtro/núcleo fija</strong></em>, como en el ejemplo del detector de bordes, <em><strong><code class="docutils literal notranslate"><span class="pre">deje</span> <span class="pre">el</span> <span class="pre">cálculo</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">matriz</span> <span class="pre">de</span> <span class="pre">filtro</span></code>,</strong></em> <span class="math notranslate nohighlight">\(H\)</span> <em><strong>, <code class="docutils literal notranslate"><span class="pre">para</span> <span class="pre">la</span> <span class="pre">fase</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code></strong></em>. En otras palabras, hacemos que <span class="math notranslate nohighlight">\(H\)</span> se adapte a los datos y no preseleccionada.</p></li>
<li><p><em><strong>En lugar de utilizar una única matriz de filtros, empleamos más de una</strong></em>. Cada una de ellas generará un tipo diferente de características. Por ejemplo, una puede <em><strong>generar bordes diagonales, la otra horizontales, etc</strong></em>. Por lo tanto, <em><strong>cada capa oculta comprenderá más de una matriz de filtrado</strong></em>. Los valores de los elementos de cada una de <em><strong><code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">matrices</span> <span class="pre">de</span> <span class="pre">filtrado</span> <span class="pre">se</span> <span class="pre">calcularán</span> <span class="pre">durante</span> <span class="pre">la</span> <span class="pre">fase</span> <span class="pre">de</span> <span class="pre">entrenamiento,</span> <span class="pre">optimizando</span> <span class="pre">algún</span> <span class="pre">criterio</span></code></strong></em>. En otras palabras, <em><strong>cada capa oculta de una CNN genera un conjunto de características de forma óptima.</strong></em></p></li>
</ol>
</li>
</ul>
<figure class="align-center" id="feature-maps-inp-img-numref">
<a class="reference internal image-reference" href="_images/feature_maps_inp_img.png"><img alt="_images/feature_maps_inp_img.png" src="_images/feature_maps_inp_img.png" style="width: 666.4000000000001px; height: 362.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.24 </span><span class="caption-text">Ilustración de <em><strong>tres filtros/núcleos</strong></em>. <em><strong>Profundidad de la capa oculta</strong></em> (número de filtro) es <em><strong>tres</strong></em>. Fuente <span id="id10">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#feature-maps-inp-img-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La <a class="reference internal" href="#feature-maps-inp-img-numref"><span class="std std-numref">Fig. 8.24</span></a> ilustra la entrada y la primera capa oculta de una CNN. La entrada comprende una matriz imagen. La <em><strong>capa oculta consta de tres matrices de filtrado, a saber,</strong></em> <span class="math notranslate nohighlight">\(H_{1}, H_{2}, H_{3}\)</span>. Nótese que cada matriz es el resultado de <em><strong>convolucionar una matriz de filtros diferente sobre la imagen de entrada</strong></em>. Cuantos <em><strong>más filtros se empleen, más mapas de características se extraerán y, en principio, mejor será el rendimiento de la red</strong></em>.</p></li>
<li><p>Sin embargo, cuantos más filtros utilicemos, más parámetros habrá que aprender, lo que plantea <em><strong>problemas computacionales y de sobreajuste</strong></em>. Nótese que <em><strong>cada píxel de una matriz de mapeo de características de salida codifica información dentro del área de la ventana</strong></em> que está definida por la <em><strong>posición correspondiente de la respectiva matriz de filtro</strong></em>.</p></li>
</ul>
<div class="proof observation admonition" id="observation_ann3">
<p class="admonition-title"><span class="caption-number">Observation 8.2 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>Una característica importante de una CNN es que la <em><strong>invarianza de traslación (<code class="docutils literal notranslate"><span class="pre">reconoce</span> <span class="pre">patrones</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">imagen</span> <span class="pre">sin</span> <span class="pre">importar</span> <span class="pre">dónde</span> <span class="pre">se</span> <span class="pre">encuentren</span> <span class="pre">esos</span> <span class="pre">patrones</span></code>) está integrada de forma natural en la red y es un subproducto de las circunvoluciones implicadas</strong></em>. De hecho, estas últimas <em><strong>se realizan deslizando la misma matriz de filtros sobre toda la imagen</strong></em>.</p></li>
<li><p>Así, <em><strong>si un objeto presente en una imagen se coloca en otra posición</strong></em>, la única diferencia sería que <em><strong>la contribución de este objeto a la salida también se desplazará la misma cantidad en el número de píxeles</strong></em>.</p></li>
<li><p>Es interesante señalar que <em><strong>existen pruebas sólidas en el campo de la neurociencia visual de que en el ser humano se realizan cálculos similares</strong></em>. (ver <span id="id11">[<a class="reference internal" href="biblio.html#id28" title="David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. The Journal of physiology, 160(1):106, 1962.">Hubel and Wiesel, 1962</a>, <a class="reference internal" href="biblio.html#id29" title="Thomas Serre, Gabriel Kreiman, Minjoon Kouh, Charles Cadieu, Ulf Knoblich, and Tomaso Poggio. A quantitative theory of immediate visual recognition. Progress in brain research, 165:33–56, 2007.">Serre <em>et al.</em>, 2007</a>]</span>). La noción de convoluciones fue usada inicialmente por <span id="id12">[<a class="reference internal" href="biblio.html#id30" title="Kunihiko Fukushima, Sei Miyake, and Takayuki Ito. Neocognitron: a neural network model for a mechanism of visual pattern recognition. IEEE transactions on systems, man, and cybernetics, pages 826–834, 1983.">Fukushima <em>et al.</em>, 1983</a>]</span> en el contexto del aprendizaje no supervisado.</p></li>
</ul>
</section>
</div><ul class="simple">
<li><p>A continuación, presentamos algunos <em><strong>términos de la jerga utilizada en relación con las CNN:</strong></em></p></li>
</ul>
<ol class="arabic simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Profundidad</span></code></strong></em>: La <em><strong>profundidad de una capa</strong></em> es el <em><strong>número de matrices de filtro que se emplean en esta capa</strong></em>. No debe confundirse profundidad de la red, que corresponde al número total de capas ocultas utilizadas. <em><strong>A veces, nos referimos al número de filtros como el número de canales</strong></em>.</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Campo</span> <span class="pre">receptivo</span></code></strong></em>: Cada <em><strong>píxel de una matriz de características de salida</strong></em> resulta como una <em><strong>media ponderada de los píxeles dentro de un área específica de la matriz imagen de entrada</strong></em> (o de la salida de la capa anterior). <em><strong>El área específica que corresponde a un píxel se conoce como su campo receptivo</strong></em> (ver <a class="reference internal" href="#feature-maps-inp-img-numref"><span class="std std-numref">Fig. 8.24</span></a>).</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Deslizamiento</span></code></strong></em>: En la práctica, <em><strong>en lugar de deslizar la matriz de filtros de píxel en píxel, se puede deslizar, por ejemplo,</strong></em> <span class="math notranslate nohighlight">\(s\)</span> <em><strong>píxeles</strong></em>. Este valor se conoce como <em><strong>stride</strong></em>. Para valores de <span class="math notranslate nohighlight">\(s &gt; 1\)</span>, se obtienen matrices de mapas de características de menor tamaño. Esto se ilustra en <a class="reference internal" href="#stride-conv-prop-numref"><span class="std std-numref">Fig. 8.25</span></a>A y B.</p></li>
</ol>
<figure class="align-center" id="stride-conv-prop-numref">
<a class="reference internal image-reference" href="_images/stride_conv_prop.png"><img alt="_images/stride_conv_prop.png" src="_images/stride_conv_prop.png" style="width: 671.2px; height: 415.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.25 </span><span class="caption-text">Matriz de entrada y filtro de tamaños 5 × 5 y 3 × 3 respectivamente. En (A), el paso es igual a <span class="math notranslate nohighlight">\(s = 1\)</span> y en (B) es igual a <span class="math notranslate nohighlight">\(s = 2\)</span>. En (A) la salida es una matriz de tamaño 3 × 3 y en (B) de tamaño 2 × 2. Fuente <span id="id13">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#stride-conv-prop-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ol class="arabic simple" start="4">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Relleno</span> <span class="pre">de</span> <span class="pre">ceros</span></code></strong></em>: A veces, <em><strong>se utilizan ceros para rellenar la matriz de entrada alrededor de los píxeles del borde</strong></em>. De esta forma, <em><strong>la dimensión de la matriz aumenta</strong></em>. Si la matriz original tiene dimensiones <span class="math notranslate nohighlight">\(l\times l\)</span>, <em><strong>después de expandirla con</strong></em> <span class="math notranslate nohighlight">\(p\)</span> <em><strong>columnas y filas, las nuevas dimensiones pasan a ser</strong></em> (<span class="math notranslate nohighlight">\(l + 2p\)</span>). Esto se muestra en la <a class="reference internal" href="#zero-fill-conv-numref"><span class="std std-numref">Fig. 8.26</span></a>.</p></li>
</ol>
<figure class="align-center" id="zero-fill-conv-numref">
<a class="reference internal image-reference" href="_images/zero_fill_conv.png"><img alt="_images/zero_fill_conv.png" src="_images/zero_fill_conv.png" style="width: 182.4px; height: 180.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.26 </span><span class="caption-text">Ejemplo en el que la matriz original es de 5 × 5 y tras rellenarla con <span class="math notranslate nohighlight">\(p = 2\)</span> filas y columnas, su tamaño pasa a ser de
9 × 9. Fuente <span id="id14">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#zero-fill-conv-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ol class="arabic simple" start="5">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Término</span> <span class="pre">de</span> <span class="pre">sesgo</span></code></strong></em>: Después de cada operación de convolución que genera un píxel del mapa de características, <em><strong>se añade un término de sesgo,</strong></em> <span class="math notranslate nohighlight">\(b\)</span>. El valor de este término también <em><strong>se calcula durante el entrenamiento</strong></em>. Nótese que se utiliza un término de sesgo común para todos los píxeles del mismo mapa de características. Esto está en consonancia con la <em><strong>lógica del reparto de pesos</strong></em>. <em><strong>Del mismo modo que todos los parámetros de una matriz de filtro son compartidos por todos los píxeles de la matriz de imágenes de entrada, se utiliza el mismo término de sesgo para todos los píxeles</strong></em>.</p></li>
</ol>
<ul>
<li><p>Se puede <em><strong>ajustar el tamaño de una matriz de mapa de características</strong></em> de salida <em><strong>ajustando el valor del stride,</strong></em> <span class="math notranslate nohighlight">\(s\)</span>, y el
<em><strong>número de columnas y filas cero adicionales</strong></em> en el relleno. En general, se puede comprobar fácilmente que si <span class="math notranslate nohighlight">\(I\in\mathbb{R}^{l\times l}, H\in\mathbb{R}^{m\times m}\)</span>, y <span class="math notranslate nohighlight">\(p\)</span> <em><strong>es el número de filas y columnas adicionales</strong></em> para el relleno, entonces el mapa de características tiene dimensiones <span class="math notranslate nohighlight">\(k\times k\)</span>, donde</p>
<div class="math notranslate nohighlight" id="equation-resulting-matrix-size">
<span class="eqno">(8.13)<a class="headerlink" href="#equation-resulting-matrix-size" title="Link to this equation">#</a></span>\[
    k=\lfloor\frac{l+2p-m}{s}+1\rfloor,
    \]</div>
<p>y <span class="math notranslate nohighlight">\(\lfloor\cdot\rfloor\)</span> es el <em><strong>operador suelo</strong></em>, i.e <span class="math notranslate nohighlight">\(\lfloor2.7\rfloor=2\)</span>.</p>
</li>
</ul>
<ul class="simple">
<li><p>Por ejemplo, si <span class="math notranslate nohighlight">\(l = 5, m = 3, p = 0\)</span> y <span class="math notranslate nohighlight">\(s = 1\)</span>, entonces <span class="math notranslate nohighlight">\(k = 3\)</span>. Por otro lado, si <span class="math notranslate nohighlight">\(l = 5, m = 3, p = 0\)</span> y <span class="math notranslate nohighlight">\(s = 2\)</span>, entonces <span class="math notranslate nohighlight">\(k = 2\)</span> (ver <a class="reference internal" href="#stride-conv-prop-numref"><span class="std std-numref">Fig. 8.25</span></a>A y B). Nótese que si los valores de <span class="math notranslate nohighlight">\(l, m, p\)</span> y <span class="math notranslate nohighlight">\(s\)</span> son tales que <em><strong>la matriz de filtrado, al deslizarse sobre <span class="math notranslate nohighlight">\(I\)</span>, cae fuera de <span class="math notranslate nohighlight">\(I\)</span>, estas operaciones no se realizan. Solo realizamos operaciones mientras la matriz filtro
esté contenida dentro de</strong></em> <span class="math notranslate nohighlight">\(I\)</span>.</p></li>
</ul>
</section>
<section id="el-paso-de-la-no-linealidad">
<h3><span class="section-number">8.12.3. </span>El paso de la no linealidad<a class="headerlink" href="#el-paso-de-la-no-linealidad" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Una vez que <em><strong>se han realizado las convoluciones y se ha añadido el término de sesgo</strong></em> a todos los valores del mapa de características, el siguiente paso es <em><strong><code class="docutils literal notranslate"><span class="pre">aplicar</span> <span class="pre">una</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">(función</span> <span class="pre">de</span> <span class="pre">activación)</span> <span class="pre">a</span> <span class="pre">cada</span> <span class="pre">uno</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">píxeles</span> <span class="pre">de</span> <span class="pre">cada</span> <span class="pre">matriz</span></code></strong></em> de mapas de características.</p></li>
<li><p>Se puede emplear cualquiera de las no linealidades que se han comentado anteriormente. Actualmente, la <em><strong>función de activación lineal rectificada, <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>, parece ser la más popular</strong></em>. La <a class="reference internal" href="#relu-activ-fn-numref"><span class="std std-numref">Fig. 8.27</span></a>A muestra la imagen obtenida después de <em><strong>filtrar la imagen original del barco con el detector de bordes en la</strong></em> <a class="reference internal" href="#equation-h-filter-conv-eq">(8.12)</a> y la <a class="reference internal" href="#relu-activ-fn-numref"><span class="std std-numref">Fig. 8.27</span></a>B muestra el resultado que se obtiene tras la <em><strong>aplicación de la no linealidad en cada píxel individual</strong></em>.</p></li>
</ul>
<figure class="align-center" id="relu-activ-fn-numref">
<a class="reference internal image-reference" href="_images/relu_activ_fn.png"><img alt="_images/relu_activ_fn.png" src="_images/relu_activ_fn.png" style="width: 630.4000000000001px; height: 340.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.27 </span><span class="caption-text">(A) Imagen donde se han extraído los bordes y (B) imagen resultante tras aplicar el en cada uno de los píxeles. Fuente <span id="id15">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#relu-activ-fn-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="la-etapa-de-agrupacion">
<h3><span class="section-number">8.12.4. </span>La etapa de agrupación<a class="headerlink" href="#la-etapa-de-agrupacion" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>El objetivo de este paso es <em><strong>reducir la dimensionalidad de cada matriz de “mapas de características” (representaciones intermedias)</strong></em>. A veces, el paso también se denomina <em><strong><code class="docutils literal notranslate"><span class="pre">pooling</span> <span class="pre">espacial</span></code></strong></em>. Para ello, <em><strong>se define una ventana y se desliza sobre la matriz correspondiente</strong></em>. El deslizamiento puede realizarse <em><strong>adoptando un valor para el respectivo parámetro stride,</strong></em> <span class="math notranslate nohighlight">\(s\)</span>. La operación de pooling consiste en <em><strong>elegir un único valor para representar todos los píxeles que se encuentran dentro de la ventana</strong></em>.</p></li>
<li><p>La operación más utilizada es la <em><strong>agrupación máxima (<code class="docutils literal notranslate"><span class="pre">max</span> <span class="pre">pooling</span></code>); es decir, entre todos los píxeles que se encuentran dentro de la ventana, el que tiene el valor más alto es seleccionado</strong></em>. Otra posibilidad es la <em><strong>agrupación en la que se selecciona el valor medio de todos los píxeles (<code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">pooling</span></code>)</strong></em>; a veces se denomina pooling de suma. La operación de pooling se ilustra en la <a class="reference internal" href="#pooling-step-conv-numref"><span class="std std-numref">Fig. 8.28</span></a>. La matriz de la <em><strong>imagen original es de 6 × 6 y la ventana es de tamaño 2 × 2</strong></em>. Hemos elegido que el stride sea igual a <span class="math notranslate nohighlight">\(s = 2\)</span>.</p></li>
</ul>
<figure class="align-center" id="pooling-step-conv-numref">
<a class="reference internal image-reference" href="_images/pooling_step_conv.png"><img alt="_images/pooling_step_conv.png" src="_images/pooling_step_conv.png" style="width: 333.6px; height: 207.20000000000002px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.28 </span><span class="caption-text">(A) Matriz original de tamaño de 6 × 6. Agrupación mediante una ventana de 2 × 2 y un stride <span class="math notranslate nohighlight">\(s = 2\)</span>. Valor máximo por ubicación de la ventana se indica en negrita. (B) Matriz 3 × 3 resultante tras la agrupación máxima. Fuente <span id="id16">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#pooling-step-conv-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Se puede utilizar la misma fórmula que en la Eq. <a class="reference internal" href="#equation-resulting-matrix-size">(8.13)</a> para <em><strong>calcular el tamaño de la matriz resultante en el caso general</strong></em>. Por lo tanto, el efecto de la agrupación es <em><strong>reducir (a través de la reducción de la muestra) la dimensionalidad y reducir el tamaño de las matrices</strong></em>. Esto es importante porque la <em><strong>salida de cada capa se presenta como la entrada de la siguiente</strong></em>.</p></li>
<li><p>Por lo tanto, <em><strong>controlar el tamaño de las matrices es de vital importancia para controlar el número de parámetros implicados</strong></em>. Por supuesto, la reducción de tamaño debe hacerse de tal manera que la <em><strong>pérdida de información sea la menor posible</strong></em>.</p></li>
</ul>
<figure class="align-center" id="pooling-invariant-conv-numref">
<a class="reference internal image-reference" href="_images/pooling_invariant_conv.png"><img alt="_images/pooling_invariant_conv.png" src="_images/pooling_invariant_conv.png" style="width: 607.2px; height: 375.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.29 </span><span class="caption-text">(A) Bordes de la imagen del barco tras la aplicación de <code class="docutils literal notranslate"><span class="pre">ReLu</span></code> (B) Imagen resultante tras aplicar <code class="docutils literal notranslate"><span class="pre">max-pooling</span></code> utilizando una ventana de 8 × 8. Fuente <span id="id17">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#pooling-invariant-conv-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="important admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p><a class="reference internal" href="#pooling-invariant-conv-numref"><span class="std std-numref">Fig. 8.29</span></a> muestra el efecto de <em><strong>aplicar el pooling a la imagen de la izquierda</strong></em>. Sin duda, <em><strong>los bordes se vuelven más gruesos, pero la información relacionada con los bordes puede extraerse</strong></em>. Nótese que después de la agrupación, el tamaño de la matriz imagen es reducido.</p></li>
<li><p>Desde otro punto de vista, <em><strong>el polling resume las estadísticas dentro del área pooling</strong></em>. El pooling puede considerarse un <em><strong>tipo especial de <code class="docutils literal notranslate"><span class="pre">filtrado,</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">que,</span> <span class="pre">en</span> <span class="pre">lugar</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">convolución,</span> <span class="pre">se</span> <span class="pre">selecciona</span> <span class="pre">el</span> <span class="pre">valor</span> <span class="pre">máximo</span> <span class="pre">(o</span> <span class="pre">medio)</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">imagen</span></code></strong></em>. El pooling ayuda a que la representación sea aproximadamente invariante a pequeñas traslaciones de la entrada.</p></li>
</ul>
</div>
</section>
<section id="convolucion-sobre-volumenes">
<h3><span class="section-number">8.12.5. </span>Convolución sobre volúmenes<a class="headerlink" href="#convolucion-sobre-volumenes" title="Link to this heading">#</a></h3>
<div class="important admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>En la <a class="reference internal" href="#feature-maps-inp-img-numref"><span class="std std-numref">Fig. 8.24</span></a>, <em><strong>la salida de la primera capa oculta comprende tres matrices de imágenes</strong></em>. Éstas constituirán la entrada de la capa siguiente. Tal configuración de entrada que consiste en múltiples imágenes es también el caso cuando la <em><strong>imagen de entrada es en color</strong></em> y su representación se da en términos de una <em><strong>representación RGB</strong></em>; es decir, la entrada consta de <em><strong>tres matrices, una por color</strong></em>.</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Cada</span> <span class="pre">matriz</span> <span class="pre">(o</span> <span class="pre">canal)</span> <span class="pre">representa</span> <span class="pre">la</span> <span class="pre">intensidad</span> <span class="pre">de</span> <span class="pre">luz</span> <span class="pre">de</span> <span class="pre">uno</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">colores</span> <span class="pre">en</span> <span class="pre">cada</span> <span class="pre">punto</span> <span class="pre">(o</span> <span class="pre">píxel)</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">imagen</span></code></strong></em>. Cada uno de estos tres componentes se expresa en *<strong>valores que van de 0 a 255</strong>. Por ejemplo, <em><strong>para generar el color verde</strong></em>: <code class="docutils literal notranslate"><span class="pre">Rojo</span> <span class="pre">(R):</span> <span class="pre">0</span> <span class="pre">Verde</span> <span class="pre">(G):</span> <span class="pre">255</span> <span class="pre">Azul</span> <span class="pre">(B):</span> <span class="pre">0</span></code>. Otro ejemplo pueden ser las diferentes tonalidades de grises</p>
<ul>
<li><p><em><strong>Gris oscuro</strong></em>: (64, 64, 64)</p></li>
<li><p><em><strong>Gris claro</strong></em>: (192, 192, 192)</p></li>
<li><p><em><strong>Blanco</strong></em> (gris más claro posible): (255, 255, 255)</p></li>
<li><p><em><strong>Negro</strong></em> (gris más oscuro posible): (0, 0, 0)</p></li>
</ul>
</li>
<li><p>Así, en general, las entradas no son matrices bidimensionales, sino <em><strong>conjuntos de matrices bidimensionales</strong></em>. En matemáticas, estas entidades se conocen como <em><strong>matrices multilineales, matrices tridimensionales, tensores tridimensionales o volúmenes</strong></em>. Nos ceñiremos a este último término, porque recuerda a la geometría asociada, del mismo modo que pensamos en una imagen como un cuadrado bidimensional. La cuestión ahora es ver <em><strong><code class="docutils literal notranslate"><span class="pre">cómo</span> <span class="pre">se</span> <span class="pre">pueden</span> <span class="pre">realizar</span> <span class="pre">convoluciones</span> <span class="pre">cuando</span> <span class="pre">hay</span> <span class="pre">volúmenes</span> <span class="pre">implicados</span></code></strong></em>.</p></li>
</ul>
</div>
<figure class="align-center" id="matrix-to-volume-conv-numref">
<a class="reference internal image-reference" href="_images/matrix_to_volume_conv.png"><img alt="_images/matrix_to_volume_conv.png" src="_images/matrix_to_volume_conv.png" style="width: 433.6px; height: 172.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.30 </span><span class="caption-text">Se apilan <span class="math notranslate nohighlight">\(d\)</span> matrices de tamaño <span class="math notranslate nohighlight">\(h\times w\)</span> cada una para formar un volumen de tamaño <span class="math notranslate nohighlight">\(h \times w\times d\)</span>. En este caso, <span class="math notranslate nohighlight">\(h = w = 5\)</span> y <span class="math notranslate nohighlight">\(d = 3\)</span>. Fuente <span id="id18">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#matrix-to-volume-conv-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Por convención, las tres dimensiones de un volumen se representarán como <span class="math notranslate nohighlight">\(h\)</span> <em><strong>para la altura,</strong></em> <span class="math notranslate nohighlight">\(w\)</span> <em><strong>para la anchura</strong></em> y <span class="math notranslate nohighlight">\(d\)</span> <em><strong>para la profundidad</strong></em>. Nótese que la profundidad <span class="math notranslate nohighlight">\(d\)</span> <em><strong>corresponde al número de imágenes implicadas</strong></em>. Así pues, si tenemos <em><strong>tres imágenes de 256 × 256</strong></em>, entonces <span class="math notranslate nohighlight">\(h = w = 256\)</span> y <span class="math notranslate nohighlight">\(d = 3\)</span> y diremos que el volumen es de <em><strong>tamaño (dimensión) 256 × 256 × 3</strong></em>. <a class="reference internal" href="#matrix-to-volume-conv-numref"><span class="std std-numref">Fig. 8.30</span></a> ilustra la geometría asociada a las respectivas definiciones.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Convolución de volúmenes</p>
<ul class="simple">
<li><p>Sea la entrada de una capa un volumen <span class="math notranslate nohighlight">\(h\times w\times d\)</span>. Cuando se trata de volúmenes, las <em><strong>capas ocultas están formadas por volúmenes de filtro/núcleo</strong></em>. Sin embargo, aquí hay un punto crucial. El <em><strong>volumen de filtro asociado con la capa oculta</strong></em> debe tener la <em><strong><code class="docutils literal notranslate"><span class="pre">misma</span> <span class="pre">profundidad</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">volumen</span> <span class="pre">de</span> <span class="pre">entrada</span></code></strong></em>. Las dimensiones de <em><strong><code class="docutils literal notranslate"><span class="pre">altura</span> <span class="pre">y</span> <span class="pre">anchura</span> <span class="pre">pueden</span> <span class="pre">ser</span> <span class="pre">(y</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">práctica</span> <span class="pre">suelen</span> <span class="pre">ser)</span> <span class="pre">diferentes</span></code></strong></em>. Utilizaremos letras <em><strong>mayúsculas en negrita para indicar los volúmenes</strong></em>.</p></li>
<li><p>Supongamos que <em><strong>la entrada es un volumen</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span> de <span class="math notranslate nohighlight">\(l\times l\times d\)</span>. Nótese que, éste comprende <span class="math notranslate nohighlight">\(d\)</span> imágenes, digamos, <span class="math notranslate nohighlight">\(I_{r}, r = 1, 2,\dots, d\)</span>, cada una de ellas de <em><strong>dimensiones</strong></em> <span class="math notranslate nohighlight">\(l\times l\)</span>. Sea <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span> el filtro volumen de <span class="math notranslate nohighlight">\(m\times m\times d\)</span>. Este último comprende el conjunto de <span class="math notranslate nohighlight">\(d\)</span> <em><strong>imágenes</strong></em>, <span class="math notranslate nohighlight">\(H_{r}, r = 1, 2,\dots, d\)</span>, cada una de dimensiones <span class="math notranslate nohighlight">\(m\times m\)</span>. A continuación, la <em><strong>operación de convolución se define mediante los siguientes pasos</strong></em>:</p></li>
</ul>
<ol class="arabic simple">
<li><p><em><strong>Convolucionar las correspondientes matrices de imágenes bidimensionales para generar</strong></em> <span class="math notranslate nohighlight">\(d\)</span> <em><strong>matrices bidimensionales de salida</strong></em>, es decir</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
O_{r}=I_{r}\star H_{r},\quad r=1,2,\dots,d.
\]</div>
<ol class="arabic" start="2">
<li><p>La <em><strong>convolución de los dos volúmenes</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span>, se define como</p>
<div class="math notranslate nohighlight">
\[
    O=\sum_{r=1}^{d}O_{r}.
    \]</div>
<p>En otras palabras, la <em><strong><code class="docutils literal notranslate"><span class="pre">convolución</span> <span class="pre">(denotada</span> <span class="pre">por</span></code></strong></em> <span class="math notranslate nohighlight">\(\star\)</span>) <em><strong><code class="docutils literal notranslate"><span class="pre">de</span> <span class="pre">dos</span> <span class="pre">volúmenes</span> <span class="pre">es</span> <span class="pre">una</span> <span class="pre">matriz</span> <span class="pre">bidimensional</span></code></strong></em>, es decir,</p>
<div class="math notranslate nohighlight">
\[
    \text{3D volume}\star\text{3D volume}=\text{2D array}.
    \]</div>
</li>
</ol>
</div>
<figure class="align-center" id="volume-conv-3dto2d-numref">
<a class="reference internal image-reference" href="_images/volume_conv_3Dto2D.png"><img alt="_images/volume_conv_3Dto2D.png" src="_images/volume_conv_3Dto2D.png" style="width: 749.4px; height: 158.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.31 </span><span class="caption-text">Convolución <span class="math notranslate nohighlight">\(\text{3D volume}\star\text{3D volume}=\text{2D array}\)</span>. <span class="math notranslate nohighlight">\(h = w = l, h = w = m\)</span> y <span class="math notranslate nohighlight">\(d = 3\)</span>.</span><a class="headerlink" href="#volume-conv-3dto2d-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La operación se ilustra en la <a class="reference internal" href="#volume-conv-3dto2d-numref"><span class="std std-numref">Fig. 8.31</span></a>. Las <em><strong>matrices correspondientes (mostradas por diferentes colores y tipos de líneas)</strong></em>. Las <em><strong>tres matrices de salida</strong></em> (<span class="math notranslate nohighlight">\(d = 3\)</span>) <em><strong>se suman posteriormente para formar la convolución de los dos volúmenes</strong></em>. La dimensión <span class="math notranslate nohighlight">\(k\)</span> de la salida depende de los valores de <span class="math notranslate nohighlight">\(l\)</span> y <span class="math notranslate nohighlight">\(m\)</span>, del intervalo <span class="math notranslate nohighlight">\(s\)</span> y del relleno <span class="math notranslate nohighlight">\(p\)</span>, si se utiliza, según la Eq. <a class="reference internal" href="#equation-resulting-matrix-size">(8.13)</a>.</p></li>
<li><p>En la práctica, <em><strong>cada capa de una red convolucional comprende varios de estos volúmenes de filtrado</strong></em>. Por ejemplo, si la entrada a una capa es un volumen <span class="math notranslate nohighlight">\(l\times l\times d\)</span>, y hay, digamos, <span class="math notranslate nohighlight">\(c\)</span> volúmenes de núcleo, cada uno de dimensiones <span class="math notranslate nohighlight">\(m\times m\times d\)</span>, la salida de la capa será un volumen <span class="math notranslate nohighlight">\(k\times k\times c\)</span>, donde <span class="math notranslate nohighlight">\(k\)</span> se determina la Eq. <a class="reference internal" href="#equation-resulting-matrix-size">(8.13)</a>.</p></li>
</ul>
</section>
<section id="red-en-red-y-convolucion-1-1">
<h3><span class="section-number">8.12.6. </span>Red en red y convolución 1 × 1<a class="headerlink" href="#red-en-red-y-convolucion-1-1" title="Link to this heading">#</a></h3>
<div class="important admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La convolución 1 × 1 no tiene sentido cuando se trata de matrices bidimensionales. En efecto <em><strong>una matriz de filtro 1 × 1 es un escalar</strong></em>. <em><strong>Convolucionar una matriz</strong></em> <span class="math notranslate nohighlight">\(I\)</span> <em><strong>de</strong></em> <span class="math notranslate nohighlight">\(l\times l\)</span> <em><strong>con un escalar</strong></em> <span class="math notranslate nohighlight">\(a\)</span> <em><strong>equivale a deslizar el valor escalar sobre todos los píxeles y multiplicar cada uno de ellos por</strong></em> <span class="math notranslate nohighlight">\(a\)</span>. El resultado es la trivial <span class="math notranslate nohighlight">\(aI\)</span> .</p></li>
</ul>
</div>
<ul>
<li><p>Sin embargo, <em><strong>cuando se trata de volúmenes, la convolución 1 × 1 tiene sentido</strong></em>. En este caso, el filtro correspondiente, <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span>, es un volumen de tamaño <span class="math notranslate nohighlight">\(1\times 1\times d\)</span>. <em><strong>Geométricamente, se trata de un “tubo”</strong></em>, con <span class="math notranslate nohighlight">\(h = w = 1\)</span> y <span class="math notranslate nohighlight">\(d\)</span> elementos en profundidad, <span class="math notranslate nohighlight">\(h(1, 1, r), r = 1, 2,\dots,d\)</span>. Por lo tanto, <em><strong>el resultado de la convolución de un volumen</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span> de <span class="math notranslate nohighlight">\(l\times l\times d\)</span> con un <span class="math notranslate nohighlight">\(1\times 1\times d\)</span> volumen <span class="math notranslate nohighlight">\(H\)</span> <em><strong>es la media ponderada</strong></em>,</p>
<div class="math notranslate nohighlight">
\[
    O=\boldsymbol{I}\star\boldsymbol{H}=\sum_{r=1}^{d}h(1, 1, r)I_{r}
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(I_{r},~r=1,2,\dots,d,\)</span> son las <span class="math notranslate nohighlight">\(d\)</span> matrices, cada una de dimensiones <span class="math notranslate nohighlight">\(l\times l\)</span>, que comprende <span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span>.</p>
</li>
</ul>
<div class="admonition-convolucion-1-times-1 admonition">
<p class="admonition-title">Convolución <span class="math notranslate nohighlight">\(1\times 1\)</span></p>
<ul>
<li><p>Ahora bien, cabe preguntarse <em><strong>por qué necesitamos una operación de este tipo en la práctica</strong></em>. La respuesta está relacionada con el <em><strong>tamaño de los volúmenes implicados</strong></em>; mediante el uso de convoluciones <span class="math notranslate nohighlight">\(1\times 1\)</span>, <em><strong>se puede <code class="docutils literal notranslate"><span class="pre">controlar</span> <span class="pre">y</span> <span class="pre">cambiar</span> <span class="pre">su</span> <span class="pre">tamaño</span> <span class="pre">para</span> <span class="pre">adaptarlo</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">necesidades</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">red</span></code></strong></em>.</p></li>
<li><p>Supongamos que en una <em><strong>etapa/capa</strong></em> de una red profunda hemos obtenido un volumen <span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span> de dimensiones <span class="math notranslate nohighlight">\(k\times k\times d\)</span>. <em><strong>Para cambiar la profundidad de</strong></em> <span class="math notranslate nohighlight">\(d\)</span> a <span class="math notranslate nohighlight">\(c\)</span><em><strong>, conservando el mismo tamaño</strong></em> <span class="math notranslate nohighlight">\(k\)</span>, para la altura y la anchura, <em><strong>empleamos</strong></em> <span class="math notranslate nohighlight">\(c\)</span> <em><strong>volúmenes,</strong></em> <span class="math notranslate nohighlight">\(H_{t}, t = 1, 2,\dots,c\)</span>, <em><strong>cada uno de dimensiones</strong></em> <span class="math notranslate nohighlight">\(1\times 1\times d\)</span>. Al realizar las <span class="math notranslate nohighlight">\(c\)</span> convoluciones obtenemos</p>
<div class="math notranslate nohighlight">
\[
    O_{t}=\boldsymbol{I}\star\boldsymbol{H}_{t}=\sum_{r=1}^{d}h_{t}(1, 1, r)I_{r},~t=1,2,\dots,c.
    \]</div>
<p>Apilando <span class="math notranslate nohighlight">\(O_{t}, t = 1, 2,\dots,c\)</span>, obtenemos el volumen <span class="math notranslate nohighlight">\(\boldsymbol{O}\)</span> de dimensión <span class="math notranslate nohighlight">\(k\times k\times c\)</span> (ver <a class="reference internal" href="#x1conv-dim-reduction-numref"><span class="std std-numref">Fig. 8.32</span></a>).</p>
</li>
<li><p>La <em><strong>información original se sigue conservando en el nuevo volumen, de forma promediada</strong></em>. A menudo, una vez obtenido el nuevo volumen <span class="math notranslate nohighlight">\(\boldsymbol{O}\)</span>, <em><strong>sus elementos se “empujan” a través de una no linealidad</strong></em>, por ejemplo, <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>. La convolución <span class="math notranslate nohighlight">\(1\times 1\)</span> seguida de la no linealidad se denomina <strong><code class="docutils literal notranslate"><span class="pre">operación</span> <span class="pre">de</span> <span class="pre">red</span> <span class="pre">en</span> <span class="pre">red</span></code></strong> y su finalidad es <em><strong>añadir una etapa de no linealidad adicional en el flujo de operaciones a través de la red</strong></em>. Por lo tanto, en este contexto, si <span class="math notranslate nohighlight">\(c&lt;d\)</span>, <em><strong>la operación de red en red puede considerarse una técnica de reducción de la dimensionalidad no lineal</strong></em>.</p></li>
</ul>
</div>
<figure class="align-center" id="x1conv-dim-reduction-numref">
<a class="reference internal image-reference" href="_images/1x1conv_dim_reduction.png"><img alt="_images/1x1conv_dim_reduction.png" src="_images/1x1conv_dim_reduction.png" style="width: 706.3px; height: 274.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.32 </span><span class="caption-text">Convolución <span class="math notranslate nohighlight">\(1\times 1\)</span>, con <span class="math notranslate nohighlight">\(c\)</span> tubos de dimensión <span class="math notranslate nohighlight">\(1\times 1\times d\)</span>, donde <span class="math notranslate nohighlight">\(d\)</span> es la profundidad del volumen de entrada. Fuente <span id="id19">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#x1conv-dim-reduction-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="important admonition">
<p class="admonition-title">Ejemplo</p>
<ul class="simple">
<li><p>Consideremos que la <em><strong>entrada de una capa es un volumen</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span> de 28 × 28 × 192. El objetivo es <em><strong>producir a la salida de la capa un volumen,</strong></em> <span class="math notranslate nohighlight">\(O\)</span><em><strong>, de dimensión</strong></em> 28 × 28 × 32. Para ello <em><strong>emplear 5 × 5 convoluciones iguales</strong></em> y un volumen intermedio <span class="math notranslate nohighlight">\(\boldsymbol{O}'\)</span> de dimensiones <span class="math notranslate nohighlight">\(28\times 28\times 16\)</span>.</p></li>
<li><p>Observe que, rellenando todas las matrices apiladas en <span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span> con <span class="math notranslate nohighlight">\(p\)</span> cero columnas y filas, se tiene que el <em><strong>número de multiplicaciones y adiciones (MADS)</strong></em> es <span class="math notranslate nohighlight">\(3.7\times 10^{6}\times 32\approx 120~\)</span> millones MADS. Además, <em><strong>usando convoluciones</strong></em> <span class="math notranslate nohighlight">\(1\times 1\)</span><em><strong>, este número se reduce a</strong></em> <span class="math notranslate nohighlight">\(28^{2}\times 25\times 16\times 32\approx 10\)</span> millones MADS (ver <span id="id20">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>).</p></li>
<li><p>A menudo, el volumen intermedio, <span class="math notranslate nohighlight">\(\boldsymbol{O}'\)</span>, se conoce como <em><strong>capa <code class="docutils literal notranslate"><span class="pre">cuello</span> <span class="pre">de</span> <span class="pre">botella</span> <span class="pre">(bottleneck</span> <span class="pre">layer)</span></code></strong></em>; <em><strong>su función es “reducir” primero el tamaño del volumen de entrada, antes de obtener el volumen de salida final</strong></em> (ver <a class="reference internal" href="#bottleneck-layer-1x1conv-numref"><span class="std std-numref">Fig. 8.33</span></a>).</p></li>
</ul>
</div>
<figure class="align-center" id="bottleneck-layer-1x1conv-numref">
<a class="reference internal image-reference" href="_images/bottleneck_layer_1x1conv.png"><img alt="_images/bottleneck_layer_1x1conv.png" src="_images/bottleneck_layer_1x1conv.png" style="width: 665.6px; height: 239.20000000000002px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.33 </span><span class="caption-text">Capa cuello de botella. Fuente <span id="id21">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#bottleneck-layer-1x1conv-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="arquitectura-cnn-completa">
<h3><span class="section-number">8.12.7. </span>Arquitectura CNN completa<a class="headerlink" href="#arquitectura-cnn-completa" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>La forma típica de una red convolucional completa consiste en una <em><strong><code class="docutils literal notranslate"><span class="pre">secuencia</span> <span class="pre">de</span> <span class="pre">capas</span> <span class="pre">convolucionales</span></code>, cada una de las cuales que comprende los tres pasos básicos</strong></em>, a saber, <em><strong><code class="docutils literal notranslate"><span class="pre">convolución</span> <span class="pre">(Conv.),</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">(ReLU)</span> <span class="pre">y</span> <span class="pre">agrupación</span> <span class="pre">(Pooling)</span></code></strong></em>, como se describe al principio de esta sección. <em><strong>Dependiendo de la aplicación, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">pueden</span> <span class="pre">apilar</span> <span class="pre">tantas</span> <span class="pre">capas</span> <span class="pre">como</span> <span class="pre">sea</span> <span class="pre">necesario</span></code></strong></em>, donde la salida de una capa se convierte en la entrada de la siguiente. Las <em><strong>entradas y salidas de cada capa son volúmenes</strong></em>, como se ha descrito anteriormente.</p></li>
</ul>
<figure class="align-center" id="complete-cnnarq-numref">
<a class="reference internal image-reference" href="_images/complete_cnnarq.png"><img alt="_images/complete_cnnarq.png" src="_images/complete_cnnarq.png" style="width: 667.2px; height: 375.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.34 </span><span class="caption-text">Arquitectura CNN completa. Fuente <span id="id22">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#complete-cnnarq-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="important admonition">
<p class="admonition-title">Procedimiento CNN completa</p>
<ul class="simple">
<li><p>En la <em><strong>primera capa se emplea un número de volúmenes de filtro (canales)</strong></em> para realizar <em><strong><code class="docutils literal notranslate"><span class="pre">convoluciones</span></code></strong></em> seguidas de la <em><strong><code class="docutils literal notranslate"><span class="pre">operación</span> <span class="pre">no</span> <span class="pre">lineal</span></code>)</strong></em>. A continuación, <em><strong>la etapa de <code class="docutils literal notranslate"><span class="pre">pooling</span></code> toma el relevo para reducir la altura y la anchura de cada volumen de salida, que se utiliza como entrada de la segunda capa, y así sucesivamente</strong></em>. Por último, <em><strong>el <code class="docutils literal notranslate"><span class="pre">volumen</span> <span class="pre">de</span> <span class="pre">salida</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">última</span> <span class="pre">capa</span> <span class="pre">se</span> <span class="pre">vectoriza</span></code></strong></em>. A veces, esto también se denomina <em><strong>operación de aplanamiento (<code class="docutils literal notranslate"><span class="pre">flattening</span></code>)</strong></em>.</p></li>
<li><p>En otras palabras, <em><strong>todos los <code class="docutils literal notranslate"><span class="pre">elementos</span> <span class="pre">del</span> <span class="pre">volumen</span> <span class="pre">de</span> <span class="pre">salida</span> <span class="pre">se</span> <span class="pre">apilan</span> <span class="pre">uno</span> <span class="pre">debajo</span> <span class="pre">de</span> <span class="pre">otro</span> <span class="pre">para</span> <span class="pre">formar</span> <span class="pre">un</span> <span class="pre">vector</span></code></strong></em>. La vectorización puede realizarse mediante varias estrategias. De hecho, <em><strong>el vector obtenido forma el vector de características que finalmente se ha generado mediante las diversas transformaciones que las convoluciones aplican capa tras capa</strong></em>. Este vector de características <em><strong>se utilizará como <code class="docutils literal notranslate"><span class="pre">entrada</span> <span class="pre">para</span> <span class="pre">un</span> <span class="pre">aprendiz</span></code>, por ejemplo, para una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">neuronal</span> <span class="pre">totalmente</span> <span class="pre">conectada</span></code></strong></em> (ver <a class="reference internal" href="#complete-cnnarq-numref"><span class="std std-numref">Fig. 8.34</span></a>) <em><strong>o a cualquier otro predictor, como una <code class="docutils literal notranslate"><span class="pre">máquina</span> <span class="pre">de</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">soporte</span></code></strong></em>.</p></li>
</ul>
</div>
<div class="warning admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>La estrategia general implica <em><strong>reducir la altura y anchura de los volúmenes mientras se incrementa la profundidad</strong></em>, lo que se traduce en <em><strong>más filtros por etapa y, por ende, más características</strong></em>. Tanto el número de capas convolucionales como de capas totalmente conectadas dependen de la aplicación, y actualmente <em><strong>no hay un método formal para determinar automáticamente estos valores</strong></em>; la selección es una cuestión de evaluación de combinaciones.</p></li>
<li><p>Se recomienda <em><strong>utilizar arquitecturas existentes como punto de partida</strong></em>, y el aprendizaje Bayesiano emerge como una opción para sistematizar la elección del número de nodos o filtros por capa.</p></li>
</ul>
</div>
<div class="seealso admonition">
<p class="admonition-title"><em><strong>¿Qué aprenden las redes neuronales profundas?</strong></em></p>
<ul class="simple">
<li><p>Las redes convolucionales funcionan bien en diversas aplicaciones, pero es crucial entender <em><strong>qué características aprenden y cómo se propaga la información entre capas para mejorar su interpretabilidad, especialmente en áreas como la medicina y las finanzas</strong></em>. Las técnicas de visualización muestran una estructura jerárquica en los rasgos detectados: <em><strong>capas inferiores responden a esquinas y bordes, capas intermedias capturan texturas, y las capas superiores identifican características específicas de clase, como caras de perros o formas de pájaros</strong></em>. Esta comprensión podría ayudar a desarrollar mejores modelos. (ver <a class="reference internal" href="#complete-cnnarq-numref"><span class="std std-numref">Fig. 8.34</span></a>).</p></li>
</ul>
</div>
</section>
<section id="arquitecturas-de-redes-neuronales-convolucionales">
<h3><span class="section-number">8.12.8. </span>Arquitecturas de Redes Neuronales Convolucionales<a class="headerlink" href="#arquitecturas-de-redes-neuronales-convolucionales" title="Link to this heading">#</a></h3>
<div class="information admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Lo que hemos descrito en la última subsección son los <em><strong>pasos básicos que se utilizan para diseñar una CNN</strong></em>. Hay una serie de <em><strong>variantes en torno a la arquitectura dada en la</strong></em> <a class="reference internal" href="#complete-cnnarq-numref"><span class="std std-numref">Fig. 8.34</span></a>. Además, hay diferentes trucos y algoritmos que se pueden utilizar para realizar cálculos, por ejemplo, para el <em><strong>cálculo eficiente de las convoluciones implicadas</strong></em>.  Sin duda, se requiere un gran esfuerzo técnico para lograr que redes tan grandes aprendan los parámetros y funcionen de manera eficaz en aplicaciones prácticas.</p></li>
<li><p>A continuación se describen brevemente <em><strong>algunas redes convolucionales clásicas</strong></em>. Se recomienda al lector que <em><strong>desee familiarizarse y profundizar en el conocimiento de las CNN que se familiarice con ellas</strong></em>. Aunque algunos de los trucos de implementación adoptados allí pueden no estar en uso hoy en día, <em><strong>los papers citados pueden ayudar al lector a obtener una comprensión de las CNN</strong></em>.</p></li>
</ul>
</div>
</section>
<section id="lenet-5">
<h3><span class="section-number">8.12.9. </span>LeNet-5<a class="headerlink" href="#lenet-5" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">LeNet-5</p>
<ul class="simple">
<li><p>Este es un ejemplo típico de la <em><strong>primera generación de CNNs y fue construido para reconocer dígitos de números</strong></em> (ver <span id="id23">[<a class="reference internal" href="biblio.html#id31" title="Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.">LeCun <em>et al.</em>, 1998</a>]</span>). Por razones históricas, comentemos un poco su arquitectura. <em><strong>La entrada de la red consiste en imágenes en escala de grises de tamaño 32 × 32 × 1</strong></em>. La red emplea <em><strong>dos capas de convolución</strong></em>. En la primera capa, el <em><strong>volumen de salida tiene un tamaño de 28 × 28 × 6</strong></em>, que tras la agrupación <em><strong>se convierte en 14 × 14 × 6</strong></em>.</p></li>
<li><p>Las <em><strong>dimensiones del volumen en la segunda capa eran 10 × 10 × 16 y, tras la agrupación, 5 × 5 × 16</strong></em>. La <em><strong>no linealidad utilizada entonces era de tipo <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code></strong></em>. Nótese que la <em><strong><code class="docutils literal notranslate"><span class="pre">altura</span> <span class="pre">y</span> <span class="pre">la</span> <span class="pre">anchura</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">volúmenes</span> <span class="pre">disminuyen</span> <span class="pre">y</span> <span class="pre">la</span> <span class="pre">profundidad</span> <span class="pre">aumenta</span></code></strong></em>, como se ha señalado antes. El <em><strong>número de elementos del último volumen es igual a 400</strong></em>. Estos elementos se apilan en un vector y alimentan los correspondientes <em><strong>nodos de entrada de una red totalmente conectada</strong></em>.</p></li>
<li><p>Esta última consta de <em><strong>dos capas ocultas con 120 nodos en la primera y 84 nodos en la segunda</strong></em>. Hay <em><strong>10 nodos de salida, uno por dígito, que utilizan una no linealidad <code class="docutils literal notranslate"><span class="pre">softmax</span></code></strong></em>. El número total de parámetros implicados es del orden de 60.000.</p></li>
</ul>
</div>
<figure class="align-center" id="letnet5-architecture-numref">
<img alt="_images/letnet5_architecture.jpeg" src="_images/letnet5_architecture.jpeg" />
<figcaption>
<p><span class="caption-number">Fig. 8.35 </span><span class="caption-text">Arquitectura LeNet-5. Fuente <span id="id24">[<a class="reference internal" href="biblio.html#id31" title="Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.">LeCun <em>et al.</em>, 1998</a>]</span>.</span><a class="headerlink" href="#letnet5-architecture-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="alexnet">
<h3><span class="section-number">8.12.10. </span>AlexNet<a class="headerlink" href="#alexnet" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">AlexNet</p>
<ul class="simple">
<li><p>Esta red también es histórica ya que, <em><strong>demostró que el punto crucial para hacer grandes redes es la disponibilidad de grandes conjuntos de entrenamiento</strong></em> <span id="id25">[<a class="reference internal" href="biblio.html#id32" title="Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 2012.">Krizhevsky <em>et al.</em>, 2012</a>]</span>. El artículo relacionado es el que realmente hizo volver a las CNN y <em><strong>actuó como catalizador para su adopción mucho más allá de la tarea de reconocimiento de dígitos</strong></em>. La <em><strong>Alexnet es un desarrollo de la LeNet-5, pero es mucho más grande e implica aproximadamente 60 millones de parámetros</strong></em>.</p></li>
<li><p>Las <em><strong>entradas a la red son imágenes RGB de tamaño 227 × 227 × 3</strong></em>. Comprende <em><strong>cinco capas ocultas</strong></em> y el <em><strong>volumen final consta de 9216 elementos</strong></em> que alimentan una red totalmente conectada con <em><strong>dos capas ocultas de 4096 unidades cada una</strong></em>. La <em><strong>salida consta de 1000 nodos <code class="docutils literal notranslate"><span class="pre">softmax</span></code> (uno por clase) para reconocer imágenes del conjunto de datos ImageNet</strong></em> para el reconocimiento de objetos. <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> se ha utilizado como no linealidad en las capas ocultas.</p></li>
</ul>
</div>
<figure class="align-center" id="alexnet-architecture-numref">
<img alt="_images/alexnet_architecture.png" src="_images/alexnet_architecture.png" />
<figcaption>
<p><span class="caption-number">Fig. 8.36 </span><span class="caption-text">Arquitectura LeNet-5. Fuente <span id="id26">[<a class="reference internal" href="biblio.html#id32" title="Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 2012.">Krizhevsky <em>et al.</em>, 2012</a>]</span>.</span><a class="headerlink" href="#alexnet-architecture-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="vgg-16">
<h3><span class="section-number">8.12.11. </span>VGG-16<a class="headerlink" href="#vgg-16" title="Link to this heading">#</a></h3>
<div class="admonition-vgg-16 admonition">
<p class="admonition-title">VGG-16</p>
<ul class="simple">
<li><p>Esta red <span id="id27">[<a class="reference internal" href="biblio.html#id33" title="Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.">Simonyan and Zisserman, 2014</a>]</span> es <em><strong>mucho mayor que AlexNet</strong></em>. Implica un total de aproximadamente <em><strong>140 millones de parámetros</strong></em>. La principal característica de esta red es su <em><strong>regularidad</strong></em>. Involucra <em><strong>filtros 3 × 3 para realizar las mismas convoluciones utilizando padding y stride</strong></em> <span class="math notranslate nohighlight">\(s = 1\)</span> y <em><strong>ventanas 2 × 2 para maxpooling</strong></em> con stride <span class="math notranslate nohighlight">\(s = 2\)</span>. <em><strong>Cada vez que se realiza un pooling, la altura y la anchura de los volúmenes se reducen a la mitad y la profundidad se multiplica por dos</strong></em>.</p></li>
<li><p>Partiendo de <em><strong>224 × 224 × 3 imagen de entrada</strong></em> y después de <em><strong>13 capas</strong></em> el <em><strong>volumen final tiene un tamaño de 7 × 7 × 512, un total de 7168 elementos</strong></em>, que tras su vectorización <em><strong>alimenta a una red totalmente conectada con 2 capas ocultas de 4096 nodos cada una</strong></em>. Los <em><strong>1000 nodos de salida están construidos en torno a la no linealidad softmax y se ha utilizado ReLU para las unidades ocultas en toda la red</strong></em>.</p></li>
</ul>
</div>
<figure class="align-center" id="vgg16-architecture-numref">
<a class="reference internal image-reference" href="_images/vgg16_architecture.png"><img alt="_images/vgg16_architecture.png" src="_images/vgg16_architecture.png" style="width: 658.0px; height: 386.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.37 </span><span class="caption-text">Arquitectura VGG-16. Fuente <span id="id28">[<a class="reference internal" href="biblio.html#id33" title="Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.">Simonyan and Zisserman, 2014</a>]</span>.</span><a class="headerlink" href="#vgg16-architecture-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>El diseño de <code class="docutils literal notranslate"><span class="pre">VGG-16</span></code> se basó en la hipótesis de que <em><strong>aumentar la profundidad de una red convolucional, mientras se usan pequeños filtros, conduciría a un mejor rendimiento en la tarea de clasificación de imágenes</strong></em>. La naturaleza homogénea de la arquitectura con filtros 3x3 repetidos permitió que la red aprendiera características jerárquicas más profundas, conservando al mismo tiempo una buena eficiencia computacional.</p></li>
<li><p>El uso de <em><strong>filtros pequeños</strong></em> y <em><strong>capas profundas</strong></em> fue una decisión clave porque:</p>
<ul>
<li><p>Filtros más pequeños <em><strong>capturan características locales</strong></em>, pero apilados sucesivamente tienen un campo receptivo mayor.</p></li>
<li><p>Mayor profundidad permite <em><strong>extraer características más abstractas y complejas</strong></em> a medida que las capas avanzan.</p></li>
</ul>
</li>
</ul>
</section>
<section id="googlenet-y-la-red-inception">
<h3><span class="section-number">8.12.12. </span>GoogleNet y la red Inception<a class="headerlink" href="#googlenet-y-la-red-inception" title="Link to this heading">#</a></h3>
<div class="tip admonition">
<p class="admonition-title">GoogleNet y la red Inception</p>
<ul class="simple">
<li><p>La arquitectura utilizada en <em><strong>esta red se desvía del “arquetípico” que se muestra en</strong></em> <a class="reference internal" href="#complete-cnnarq-numref"><span class="std std-numref">Fig. 8.34</span></a>. En el corazón de esta red se encuentra el llamado módulo de inicio <span id="id29">[<a class="reference internal" href="biblio.html#id34" title="Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, 1–9. 2015.">Szegedy <em>et al.</em>, 2015</a>]</span>. Un <em><strong>módulo de inicio consta de filtros de diferentes tamaños y profundidades</strong></em>, así como de una <em><strong>ruta de agrupación diferente</strong></em>.</p></li>
<li><p>La arquitectura <code class="docutils literal notranslate"><span class="pre">Inception</span></code> toma la salida de una capa y la divide en varias rutas: una usa <em><strong>convolución 1x1 para reducir la profundidad</strong></em>, otra hace <em><strong>pooling seguido de convolución 5x5</strong></em>, y dos rutas realizan <em><strong>convoluciones separadas con filtros 3x3 y 5x5</strong></em>. Se usa una capa de <em><strong>cuello de botella (convolución 1x1) para reducir la carga computacional antes de las convoluciones</strong></em>.</p></li>
<li><p>En el módulo de inicio, se <em><strong>concatenan los volúmenes de salida de varias trayectorias</strong></em>, permitiendo que la red, durante el entrenamiento, <em><strong>elija las mejores operaciones para cada capa</strong></em>. A medida que se avanza hacia capas superiores, <em><strong>se incrementa la proporción de convoluciones 3 × 3 y 5 × 5, ya que captan rasgos más abstractos</strong></em>. La red tiene <em><strong>22 capas y 6 millones de parámetros</strong></em>.</p></li>
</ul>
</div>
<figure class="align-center" id="inception-architecture-numref">
<a class="reference internal image-reference" href="_images/inception_architecture.png"><img alt="_images/inception_architecture.png" src="_images/inception_architecture.png" style="width: 583.0999999999999px; height: 389.9px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.38 </span><span class="caption-text">Arquitectura Inception. Fuente <span id="id30">[<a class="reference internal" href="biblio.html#id34" title="Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, 1–9. 2015.">Szegedy <em>et al.</em>, 2015</a>]</span>.</span><a class="headerlink" href="#inception-architecture-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="redes-residuales-resnets">
<h3><span class="section-number">8.12.13. </span>Redes residuales (ResNets)<a class="headerlink" href="#redes-residuales-resnets" title="Link to this heading">#</a></h3>
<div class="seealso admonition">
<p class="admonition-title"><em><strong>Redes residuales (ResNets)</strong></em></p>
<ul class="simple">
<li><p>Ya hemos hablado de las ventajas de diseñar redes profundas. También abordamos la forma de <em><strong>hacer frente al problema de los gradientes que se desvanecen/explotan</strong></em> mediante una combinación de métodos y trucos que permiten que el <em><strong>algoritmo backpropagation converja con suficiente rapidez</strong></em>. Sin embargo, <em><strong>una vez que empezamos a construir redes muy profundas (del orden de decenas o incluso centenares de capas) nos encontramos con el siguiente comportamiento “poco ortodoxo”</strong></em>.</p></li>
<li><p>Cabría esperar que, <em><strong>añadiendo más y más capas, el error de entrenamiento mejore o al menos no aumenta</strong></em>. Sin embargo, lo que se observa en la práctica es que <em><strong><code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">partir</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">cierto</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">capas,</span> <span class="pre">el</span> <span class="pre">error</span> <span class="pre">de</span> <span class="pre">entrenamiento</span> <span class="pre">empieza</span> <span class="pre">a</span> <span class="pre">aumentar</span></code></strong></em>. Esto se ilustra gráficamente en la <a class="reference internal" href="#residual-net-architecture-numref"><span class="std std-numref">Fig. 8.39</span></a>. Este fenómeno <em><strong>no tiene nada que ver con el sobreajuste</strong></em>. Después de todo, estamos hablando del <em><strong>error de entrenamiento y no del de generalización</strong></em>. Parece que esto puede deberse a <em><strong><code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">tarea</span> <span class="pre">de</span> <span class="pre">optimización</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">vuelve</span> <span class="pre">más</span> <span class="pre">y</span> <span class="pre">más</span> <span class="pre">difícil</span> <span class="pre">a</span> <span class="pre">medida</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">añaden</span> <span class="pre">más</span> <span class="pre">y</span> <span class="pre">más</span> <span class="pre">capas</span></code></strong></em>.</p></li>
</ul>
</div>
<figure class="align-center" id="residual-net-architecture-numref">
<a class="reference internal image-reference" href="_images/residual_net_architecture.png"><img alt="_images/residual_net_architecture.png" src="_images/residual_net_architecture.png" style="width: 490.40000000000003px; height: 274.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.39 </span><span class="caption-text"><em><strong>Aumento en lugar de disminución del error</strong></em> cuando el número de capas supera cierto número, en una red muy profunda. <em><strong>(curva roja) como se esperaba a partir de la teoría.</strong></em></span><a class="headerlink" href="#residual-net-architecture-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Matemáticamente, <em><strong>cualquier capa, por ejemplo, la capa</strong></em> <span class="math notranslate nohighlight">\(r\)</span><em><strong>, puede verse como un mapeo que asigna la entrada correspondiente, por ejemplo,</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{y}_{r-1}\)</span><em><strong>, a la salida, por ejemplo,</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{y}_{r}\)</span>. Denotemos el mapeo respectivo como</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y}^{r}=H(\boldsymbol{y}^{r-1}).
\]</div>
<ul class="simple">
<li><p>Desde este punto de vista, <em><strong>es fácil ver por qué, al añadir más capas, el error de entrenamiento no debería aumentar</strong></em>. En el peor de los casos, en el que toda la información se ha extraído hasta una capa <span class="math notranslate nohighlight">\(r\)</span>, <em><strong>esperamos que añadiendo una capa más, esta debería implementar el mapeo de identidad, es decir,</strong></em> <span class="math notranslate nohighlight">\(y^{r}=H(y^{r-1})-y^{r-1}\)</span>. Es decir, <em><strong>la capa extra no añade información y simplemente copia la entrada a la salida</strong></em>.</p></li>
<li><p>Sin embargo, parece que <em><strong><code class="docutils literal notranslate"><span class="pre">una</span> <span class="pre">vez</span> <span class="pre">que</span> <span class="pre">la</span> <span class="pre">red</span> <span class="pre">empieza</span> <span class="pre">a</span> <span class="pre">ser</span> <span class="pre">muy</span> <span class="pre">profunda,</span> <span class="pre">la</span> <span class="pre">precisión</span> <span class="pre">se</span> <span class="pre">“satura”</span></code></strong></em> y <em><strong>las herramientas de optimización tienen dificultades para encontrar una solución lo suficientemente precisa a este mapeo de identidad</strong></em>, al menos en un tiempo factible. En <span id="id31">[<a class="reference internal" href="biblio.html#id35" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778. 2016.">He <em>et al.</em>, 2016</a>]</span> se propone una forma de sortear esta dificultad. La idea es <em><strong>ajustar un mapeo alternativo equivalente</strong></em>, es decir</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
F(\boldsymbol{y}^{r})=H(\boldsymbol{y}^{r-1})-\boldsymbol{y}^{r-1}.
\]</div>
<ul class="simple">
<li><p>Entonces el mapeo original, <span class="math notranslate nohighlight">\(H(\boldsymbol{y}^{r-1})\)</span>, pasa a ser igual a <span class="math notranslate nohighlight">\(F(\boldsymbol{y}^{r-1}) + \boldsymbol{y}^{r-1}\)</span>. En la práctica, la <em><strong>optimización con respecto al mapeo residual,</strong></em> <span class="math notranslate nohighlight">\(F\)</span><em><strong>, es más fácil que la optimización con respecto al mapeo original,</strong></em> <span class="math notranslate nohighlight">\(H\)</span>. En el caso extremo, cuando se debe realizar un mapeo de identidad, parece ser <em><strong>más fácil empujar el residual a cero, que ajustar el mapeo de la identidad</strong></em>.</p></li>
</ul>
<ul class="simple">
<li><p>El uso de la <em><strong>representación residual</strong></em> no es nuevo y ya <em><strong>se ha utilizado anteriormente en el contexto de la cuantificación vectorial</strong></em>. La esencia del aprendizaje residual es <em><strong>introducir el llamado bloque de construcción residual</strong></em>, que se muestra en la <a class="reference internal" href="#residual-architecture-cnn-numref"><span class="std std-numref">Fig. 8.40</span></a>.</p></li>
</ul>
<figure class="align-center" id="residual-architecture-cnn-numref">
<a class="reference internal image-reference" href="_images/residual_architecture_cnn.png"><img alt="_images/residual_architecture_cnn.png" src="_images/residual_architecture_cnn.png" style="width: 363.20000000000005px; height: 398.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.40 </span><span class="caption-text">Bloque de construcción residual. Fuente <span id="id32">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#residual-architecture-cnn-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>De esta manera, un número de capas, digamos, dos, como en el caso de la <a class="reference internal" href="#residual-architecture-cnn-numref"><span class="std std-numref">Fig. 8.40</span></a>, <em><strong>se apilan</strong></em> y dejamos que estas capas se <em><strong>ajusten explícitamente al mapeo residual, a través de las llamadas conexiones de atajo u omisión</strong></em>. Cada capa de pesos realiza una transformación sobre su entrada, por ejemplo, convoluciones. Si <span class="math notranslate nohighlight">\(\boldsymbol{y}^{r}\)</span> e <span class="math notranslate nohighlight">\(\boldsymbol{y}^{r-1}\)</span> son de dimensiones diferentes, el <em><strong>atajo de mapeo de identidad se modifica a</strong></em> <span class="math notranslate nohighlight">\(W\boldsymbol{y}^{r-1}\)</span>, donde <span class="math notranslate nohighlight">\(W\)</span> es una matriz de dimensiones apropiadas.</p></li>
</ul>
</section>
<section id="reconocimiento-facial-de-emociones">
<h3><span class="section-number">8.12.14. </span>Reconocimiento facial de emociones<a class="headerlink" href="#reconocimiento-facial-de-emociones" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Los datos para el siguiente ejemplo pueden ser descargados del siguiente link: <a class="reference external" href="https://github.com/lihkir/Data/tree/main/face_sentiment_detection">Face Sentiment Detection</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">devices</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Available devices:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">gpus</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">gpus</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TensorFlow is using GPU.&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">gpu</span> <span class="ow">in</span> <span class="n">gpus</span><span class="p">:</span>
        <span class="n">gpu_details</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">get_device_details</span><span class="p">(</span><span class="n">gpu</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU details: </span><span class="si">{</span><span class="n">gpu_details</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TensorFlow is not using GPU.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Available devices:
PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;)
PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)
TensorFlow is using GPU.
GPU details: {&#39;compute_capability&#39;: (8, 9), &#39;device_name&#39;: &#39;NVIDIA GeForce RTX 4070&#39;}
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I0000 00:00:1727226009.344075    2151 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">load_img</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">MaxPool2D</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span><span class="p">,</span> <span class="n">RMSprop</span><span class="p">,</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span><span class="n">EarlyStopping</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">regularizers</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_dir</span> <span class="o">=</span> <span class="s1">&#39;/home/lihkir/Data/face_sentiment_detection/train/&#39;</span>
<span class="n">test_dir</span>  <span class="o">=</span> <span class="s1">&#39;/home/lihkir/Data/face_sentiment_detection/test/&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">count_exp</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">set_</span><span class="p">):</span>
    <span class="n">dict_</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">expression</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">dir_</span> <span class="o">=</span> <span class="n">path</span> <span class="o">+</span> <span class="n">expression</span>
        <span class="n">dict_</span><span class="p">[</span><span class="n">expression</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">dir_</span><span class="p">))</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dict_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="n">set_</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_count</span> <span class="o">=</span> <span class="n">count_exp</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">test_count</span> <span class="o">=</span> <span class="n">count_exp</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">train_count</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_count</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>       surprise  disgust  happy  fear  angry  neutral   sad
train      3171      436   7215  4097   3995     4965  4830
      surprise  disgust  happy  fear  angry  neutral   sad
test       831      111   1774  1024    958     1233  1247
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_count</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/488bf0db970f2930c91fda98bdf940aa38c6335e5c243cacc06b4d47c3bc7919.png" src="_images/488bf0db970f2930c91fda98bdf940aa38c6335e5c243cacc06b4d47c3bc7919.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_count</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/95610265aab81ebe715b69703acc2b155153d1469c8d1076f81b73d965441034.png" src="_images/95610265aab81ebe715b69703acc2b155153d1469c8d1076f81b73d965441034.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">22</span><span class="p">))</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">expression</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_dir</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">load_img</span><span class="p">((</span><span class="n">train_dir</span> <span class="o">+</span> <span class="n">expression</span> <span class="o">+</span><span class="s1">&#39;/&#39;</span><span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_dir</span> <span class="o">+</span> <span class="n">expression</span><span class="p">)[</span><span class="mi">5</span><span class="p">]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">expression</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d57a2cc2465f01f8b3ea24d299b8861cb1937942191fb364543279f9062ae749.png" src="_images/d57a2cc2465f01f8b3ea24d299b8861cb1937942191fb364543279f9062ae749.png" />
</div>
</div>
<ul class="simple">
<li><p>Creamos los conjuntos de datos de <em><strong>entrenamiento, prueba y validación</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">rescale</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="mf">255.0</span><span class="p">,</span>
                                   <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                   <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">rescale=1./255</span></code>: Normaliza las imágenes escalando los valores de los píxeles de un rango de [0, 255] a [0, 1]. Esto es una práctica común para mejorar la eficiencia del entrenamiento de redes neuronales.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">horizontal_flip=True</span></code>: Permite aplicar una transformación de “flip” horizontal (volteo) de manera aleatoria a las imágenes, una técnica de <em>data augmentation</em> para hacer que el modelo generalice mejor.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">validation_split=0.2</span></code>: Define una división de los datos, reservando el 20% de ellos para validación, lo que significa que el 80% de los datos será usado para entrenamiento.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">training_set</span> <span class="o">=</span> <span class="n">train_datagen</span><span class="o">.</span><span class="n">flow_from_directory</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span>
                                                <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                                <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span><span class="mi">48</span><span class="p">),</span>
                                                <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                <span class="n">color_mode</span><span class="o">=</span><span class="s1">&#39;grayscale&#39;</span><span class="p">,</span>
                                                <span class="n">class_mode</span><span class="o">=</span><span class="s1">&#39;categorical&#39;</span><span class="p">,</span>
                                                <span class="n">subset</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 22968 images belonging to 7 classes.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">train_dir</span></code>: Es la ruta del directorio que contiene las imágenes de entrenamiento organizadas en subdirectorios según la clase.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size=64</span></code>: Carga 64 imágenes a la vez (en lotes) para ser procesadas en cada paso del entrenamiento.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target_size=(48,48)</span></code>: Redimensiona todas las imágenes al tamaño de 48x48 píxeles. Esto es útil para mantener un tamaño uniforme de las imágenes que ingresan al modelo.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>: Aleatoriza las imágenes en cada epoch, lo que evita que el modelo se adapte a un orden particular de los datos.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">color_mode='grayscale'</span></code>: Convierte las imágenes en escala de grises (1 canal en lugar de 3 para imágenes en color RGB).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">class_mode='categorical'</span></code>: Las etiquetas se asignan como categóricas, es decir, cada imagen pertenece a una clase específica (por ejemplo, clasificación multiclase).
<code class="docutils literal notranslate"><span class="pre">subset='training'</span></code>: Utiliza el 80% de los datos (definidos anteriormente con <code class="docutils literal notranslate"><span class="pre">validation_split=0.2</span></code>) para entrenamiento.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">validation_set</span> <span class="o">=</span> <span class="n">train_datagen</span><span class="o">.</span><span class="n">flow_from_directory</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span>
                                                <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                                <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span><span class="mi">48</span><span class="p">),</span>
                                                <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                <span class="n">color_mode</span><span class="o">=</span><span class="s1">&#39;grayscale&#39;</span><span class="p">,</span>
                                                <span class="n">class_mode</span><span class="o">=</span><span class="s1">&#39;categorical&#39;</span><span class="p">,</span>
                                                <span class="n">subset</span><span class="o">=</span><span class="s1">&#39;validation&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 5741 images belonging to 7 classes.
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">subset='validation'</span></code>: Selecciona el 20% de los datos reservados para la validación (definidos anteriormente con <code class="docutils literal notranslate"><span class="pre">validation_split=0.2</span></code>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">rescale</span><span class="o">=</span><span class="mf">1.0</span><span class="o">/</span><span class="mf">255.0</span><span class="p">,</span>
                                   <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_set</span> <span class="o">=</span> <span class="n">test_datagen</span><span class="o">.</span><span class="n">flow_from_directory</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span>
                                                <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                                <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span><span class="mi">48</span><span class="p">),</span>
                                                <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                <span class="n">color_mode</span><span class="o">=</span><span class="s1">&#39;grayscale&#39;</span><span class="p">,</span>
                                                <span class="n">class_mode</span><span class="o">=</span><span class="s1">&#39;categorical&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 7178 images belonging to 7 classes.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">training_set</span><span class="o">.</span><span class="n">class_indices</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;angry&#39;: 0,
 &#39;disgust&#39;: 1,
 &#39;fear&#39;: 2,
 &#39;happy&#39;: 3,
 &#39;neutral&#39;: 4,
 &#39;sad&#39;: 5,
 &#39;surprise&#39;: 6}
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Arquitectura del modelo</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">7</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">),</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span><span class="mi">48</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;elu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;elu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
 
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;elu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">))</span>
 
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;elu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;elu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">BatchNormalization</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.4</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;elu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.0001</span><span class="p">),</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_7"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ conv2d_32 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │         <span style="color: #00af00; text-decoration-color: #00af00">1,088</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ activation_18 (<span style="color: #0087ff; text-decoration-color: #0087ff">Activation</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_34          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │           <span style="color: #00af00; text-decoration-color: #00af00">256</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_33 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │        <span style="color: #00af00; text-decoration-color: #00af00">65,600</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ activation_19 (<span style="color: #0087ff; text-decoration-color: #0087ff">Activation</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_35          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">48</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │           <span style="color: #00af00; text-decoration-color: #00af00">256</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_21 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_27 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)            │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_34 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │       <span style="color: #00af00; text-decoration-color: #00af00">131,200</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ activation_20 (<span style="color: #0087ff; text-decoration-color: #0087ff">Activation</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_36          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">24</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │           <span style="color: #00af00; text-decoration-color: #00af00">512</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_22 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_28 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)            │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_35 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │       <span style="color: #00af00; text-decoration-color: #00af00">262,272</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ activation_21 (<span style="color: #0087ff; text-decoration-color: #0087ff">Activation</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_37          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │           <span style="color: #00af00; text-decoration-color: #00af00">512</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_36 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)              │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │       <span style="color: #00af00; text-decoration-color: #00af00">262,272</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ activation_22 (<span style="color: #0087ff; text-decoration-color: #0087ff">Activation</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_38          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">12</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)    │           <span style="color: #00af00; text-decoration-color: #00af00">512</span> │
│ (<span style="color: #0087ff; text-decoration-color: #0087ff">BatchNormalization</span>)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_23 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>) │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6</span>, <span style="color: #00af00; text-decoration-color: #00af00">6</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout_29 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)            │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">6</span>, <span style="color: #00af00; text-decoration-color: #00af00">6</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)      │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten_6 (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">4608</span>)           │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_15 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)            │       <span style="color: #00af00; text-decoration-color: #00af00">589,952</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ activation_23 (<span style="color: #0087ff; text-decoration-color: #0087ff">Activation</span>)      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">128</span>)            │             <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_16 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>)              │           <span style="color: #00af00; text-decoration-color: #00af00">903</span> │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,315,335</span> (5.02 MB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,314,311</span> (5.01 MB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,024</span> (4.00 KB)
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">to_file</span><span class="o">=</span><span class="s1">&#39;model_cnn.png&#39;</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">show_layer_names</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2630c8aae06bf565b34fa0f578053b9624587e37ff720ef4caa72bb818782ff1.png" src="_images/2630c8aae06bf565b34fa0f578053b9624587e37ff720ef4caa72bb818782ff1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">120</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpointer</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">ModelCheckpoint</span><span class="p">(</span>
        <span class="n">filepath</span><span class="o">=</span><span class="s1">&#39;model.weights.best.keras&#39;</span><span class="p">,</span> 
        <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;max&#39;</span>
    <span class="p">)</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">dump</span><span class="p">,</span> <span class="n">load</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history_cnn</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;history_cnn.joblib&#39;</span><span class="p">):</span>
    <span class="n">history_cnn</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;history_cnn.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El archivo &#39;history_cnn.joblib&#39; ya existe. Se ha cargado el historial del entrenamiento.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">history_cnn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">training_set</span><span class="p">,</span> 
                            <span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">checkpointer</span><span class="p">,</span> 
                            <span class="n">validation_data</span> <span class="o">=</span> <span class="n">validation_set</span><span class="p">,</span>
                            <span class="n">verbose</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">dump</span><span class="p">(</span><span class="n">history_cnn</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="s1">&#39;history_cnn.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_cnn.joblib&#39;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/120

Epoch 1: val_accuracy improved from -inf to 0.59502, saving model to model.weights.best.keras
359/359 - 5s - 14ms/step - accuracy: 0.6683 - loss: 0.9239 - val_accuracy: 0.5950 - val_loss: 1.1575
Epoch 2/120

Epoch 2: val_accuracy did not improve from 0.59502
359/359 - 5s - 13ms/step - accuracy: 0.6782 - loss: 0.9027 - val_accuracy: 0.5945 - val_loss: 1.1356
Epoch 3/120

Epoch 3: val_accuracy improved from 0.59502 to 0.60321, saving model to model.weights.best.keras
359/359 - 5s - 13ms/step - accuracy: 0.6866 - loss: 0.8813 - val_accuracy: 0.6032 - val_loss: 1.1336
Epoch 4/120

Epoch 4: val_accuracy did not improve from 0.60321
359/359 - 5s - 13ms/step - accuracy: 0.6955 - loss: 0.8648 - val_accuracy: 0.5983 - val_loss: 1.1448
Epoch 5/120

Epoch 5: val_accuracy improved from 0.60321 to 0.60704, saving model to model.weights.best.keras
359/359 - 5s - 13ms/step - accuracy: 0.7010 - loss: 0.8428 - val_accuracy: 0.6070 - val_loss: 1.1362
Epoch 6/120

Epoch 6: val_accuracy improved from 0.60704 to 0.60739, saving model to model.weights.best.keras
359/359 - 5s - 13ms/step - accuracy: 0.7091 - loss: 0.8241 - val_accuracy: 0.6074 - val_loss: 1.1460
Epoch 7/120

Epoch 7: val_accuracy did not improve from 0.60739
359/359 - 5s - 13ms/step - accuracy: 0.7149 - loss: 0.8101 - val_accuracy: 0.6034 - val_loss: 1.1287
Epoch 8/120

Epoch 8: val_accuracy did not improve from 0.60739
359/359 - 5s - 13ms/step - accuracy: 0.7251 - loss: 0.7878 - val_accuracy: 0.6037 - val_loss: 1.1624
Epoch 9/120

Epoch 9: val_accuracy improved from 0.60739 to 0.60756, saving model to model.weights.best.keras
359/359 - 5s - 13ms/step - accuracy: 0.7320 - loss: 0.7694 - val_accuracy: 0.6076 - val_loss: 1.1672
Epoch 10/120

Epoch 10: val_accuracy did not improve from 0.60756
359/359 - 5s - 13ms/step - accuracy: 0.7368 - loss: 0.7529 - val_accuracy: 0.6037 - val_loss: 1.1993
Epoch 11/120

Epoch 11: val_accuracy improved from 0.60756 to 0.61557, saving model to model.weights.best.keras
359/359 - 5s - 13ms/step - accuracy: 0.7456 - loss: 0.7368 - val_accuracy: 0.6156 - val_loss: 1.1559
Epoch 12/120

Epoch 12: val_accuracy improved from 0.61557 to 0.61906, saving model to model.weights.best.keras
359/359 - 5s - 13ms/step - accuracy: 0.7498 - loss: 0.7164 - val_accuracy: 0.6191 - val_loss: 1.1780
Epoch 13/120

Epoch 13: val_accuracy did not improve from 0.61906
359/359 - 5s - 13ms/step - accuracy: 0.7571 - loss: 0.7013 - val_accuracy: 0.6131 - val_loss: 1.1891
Epoch 14/120

Epoch 14: val_accuracy did not improve from 0.61906
359/359 - 5s - 13ms/step - accuracy: 0.7660 - loss: 0.6892 - val_accuracy: 0.6154 - val_loss: 1.1916
Epoch 15/120

Epoch 15: val_accuracy did not improve from 0.61906
359/359 - 5s - 13ms/step - accuracy: 0.7711 - loss: 0.6719 - val_accuracy: 0.6128 - val_loss: 1.2086
Epoch 16/120

Epoch 16: val_accuracy did not improve from 0.61906
359/359 - 5s - 13ms/step - accuracy: 0.7786 - loss: 0.6519 - val_accuracy: 0.6182 - val_loss: 1.1857
Epoch 17/120

Epoch 17: val_accuracy improved from 0.61906 to 0.62062, saving model to model.weights.best.keras
359/359 - 5s - 13ms/step - accuracy: 0.7847 - loss: 0.6326 - val_accuracy: 0.6206 - val_loss: 1.1927
Epoch 18/120

Epoch 18: val_accuracy did not improve from 0.62062
359/359 - 5s - 13ms/step - accuracy: 0.7913 - loss: 0.6176 - val_accuracy: 0.6159 - val_loss: 1.2088
Epoch 19/120

Epoch 19: val_accuracy did not improve from 0.62062
359/359 - 5s - 13ms/step - accuracy: 0.7960 - loss: 0.6013 - val_accuracy: 0.6022 - val_loss: 1.2471
Epoch 20/120

Epoch 20: val_accuracy did not improve from 0.62062
359/359 - 5s - 13ms/step - accuracy: 0.8049 - loss: 0.5880 - val_accuracy: 0.6178 - val_loss: 1.2403
Epoch 21/120

Epoch 21: val_accuracy improved from 0.62062 to 0.62080, saving model to model.weights.best.keras
359/359 - 5s - 13ms/step - accuracy: 0.8053 - loss: 0.5811 - val_accuracy: 0.6208 - val_loss: 1.2441
Epoch 22/120

Epoch 22: val_accuracy did not improve from 0.62080
359/359 - 5s - 13ms/step - accuracy: 0.8128 - loss: 0.5603 - val_accuracy: 0.6194 - val_loss: 1.2741
Epoch 23/120

Epoch 23: val_accuracy did not improve from 0.62080
359/359 - 5s - 13ms/step - accuracy: 0.8172 - loss: 0.5489 - val_accuracy: 0.6081 - val_loss: 1.2811
Epoch 24/120

Epoch 24: val_accuracy did not improve from 0.62080
359/359 - 5s - 13ms/step - accuracy: 0.8239 - loss: 0.5325 - val_accuracy: 0.6149 - val_loss: 1.2809
Epoch 25/120

Epoch 25: val_accuracy did not improve from 0.62080
359/359 - 5s - 13ms/step - accuracy: 0.8273 - loss: 0.5202 - val_accuracy: 0.6095 - val_loss: 1.3033
Epoch 26/120

Epoch 26: val_accuracy improved from 0.62080 to 0.62341, saving model to model.weights.best.keras
359/359 - 5s - 13ms/step - accuracy: 0.8366 - loss: 0.5051 - val_accuracy: 0.6234 - val_loss: 1.2804
Epoch 27/120

Epoch 27: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8400 - loss: 0.4902 - val_accuracy: 0.6192 - val_loss: 1.3059
Epoch 28/120

Epoch 28: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8427 - loss: 0.4788 - val_accuracy: 0.6185 - val_loss: 1.3307
Epoch 29/120

Epoch 29: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8483 - loss: 0.4682 - val_accuracy: 0.6210 - val_loss: 1.3316
Epoch 30/120

Epoch 30: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8526 - loss: 0.4578 - val_accuracy: 0.6170 - val_loss: 1.3451
Epoch 31/120

Epoch 31: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8553 - loss: 0.4512 - val_accuracy: 0.6204 - val_loss: 1.3681
Epoch 32/120

Epoch 32: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8600 - loss: 0.4382 - val_accuracy: 0.6196 - val_loss: 1.3851
Epoch 33/120

Epoch 33: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8678 - loss: 0.4172 - val_accuracy: 0.6185 - val_loss: 1.4119
Epoch 34/120

Epoch 34: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8692 - loss: 0.4122 - val_accuracy: 0.6178 - val_loss: 1.4180
Epoch 35/120

Epoch 35: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8726 - loss: 0.4043 - val_accuracy: 0.6171 - val_loss: 1.4173
Epoch 36/120

Epoch 36: val_accuracy did not improve from 0.62341
359/359 - 5s - 13ms/step - accuracy: 0.8766 - loss: 0.3912 - val_accuracy: 0.6196 - val_loss: 1.4120
Epoch 36: early stopping
Restoring model weights from the end of the best epoch: 26.
El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_cnn.joblib&#39;.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">train_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training Accuracy vs Validation Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Train&#39;</span><span class="p">,</span> <span class="s1">&#39;Validation&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training Loss vs Validation Loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Train&#39;</span><span class="p">,</span> <span class="s1">&#39;Validation&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/814ea01dc9f77d905f314d11d475e508ab48a23b8f031cbb12e597a516f9722d.png" src="_images/814ea01dc9f77d905f314d11d475e508ab48a23b8f031cbb12e597a516f9722d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">training_set</span><span class="p">)</span>
<span class="n">test_loss</span><span class="p">,</span> <span class="n">test_acc</span>   <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">validation_set</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;final train accuracy = </span><span class="si">{:.2f}</span><span class="s2"> , validation accuracy = </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_acc</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="n">test_acc</span><span class="o">*</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">359/359</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">8s</span> 22ms/step - accuracy: 0.9346 - loss: 0.2856
<span class=" -Color -Color-Bold">90/90</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 20ms/step - accuracy: 0.6201 - loss: 1.2962
final train accuracy = 93.66 , validation accuracy = 61.94
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Matriz de confusión del <em><strong>conjunto de entrenamiento</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">training_set</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">class_labels</span> <span class="o">=</span> <span class="n">test_set</span><span class="o">.</span><span class="n">class_indices</span>
<span class="n">class_labels</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">class_labels</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="n">cm_train</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">training_set</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Classification Report&#39;</span><span class="p">)</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">class_labels</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">training_set</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">target_names</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm_train</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">tick_mark</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_names</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_mark</span><span class="p">,</span> <span class="n">target_names</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_mark</span><span class="p">,</span> <span class="n">target_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">359/359</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">8s</span> 21ms/step
Confusion Matrix
[[ 376   34  450  783  644  557  352]
 [  47   12   45   75   71   57   42]
 [ 451   44  456  829  605  556  337]
 [ 766   86  753 1467 1119  957  624]
 [ 543   60  550  997  748  654  420]
 [ 524   68  482  935  698  746  411]
 [ 357   32  324  627  458  415  324]]
Classification Report
              precision    recall  f1-score   support

       angry       0.12      0.12      0.12      3196
     disgust       0.04      0.03      0.04       349
        fear       0.15      0.14      0.14      3278
       happy       0.26      0.25      0.26      5772
     neutral       0.17      0.19      0.18      3972
         sad       0.19      0.19      0.19      3864
    surprise       0.13      0.13      0.13      2537

    accuracy                           0.18     22968
   macro avg       0.15      0.15      0.15     22968
weighted avg       0.18      0.18      0.18     22968
</pre></div>
</div>
<img alt="_images/8d4807ec39530ef3c728be2d95dc38d7cc28ed1ae0d10623027267078be474e4.png" src="_images/8d4807ec39530ef3c728be2d95dc38d7cc28ed1ae0d10623027267078be474e4.png" />
</div>
</div>
<ul class="simple">
<li><p>Matriz de confusión en el conjunto de <em><strong>datos de validación</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">validation_set</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cm_val</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">validation_set</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm_val</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Classification Report&#39;</span><span class="p">)</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">class_labels</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">validation_set</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">target_names</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm_train</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">tick_mark</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_names</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_mark</span><span class="p">,</span> <span class="n">target_names</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_mark</span><span class="p">,</span> <span class="n">target_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">90/90</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 21ms/step
Confusion Matrix
[[116   7  87 202 166 149  72]
 [  7   0   9  22  27   9  13]
 [ 87  14  83 217 185 141  92]
 [190  12 147 348 323 279 144]
 [130  13 120 237 223 159 111]
 [111   7 107 218 236 194  93]
 [ 74   7  78 162 154 102  57]]
Classification Report
              precision    recall  f1-score   support

       angry       0.16      0.15      0.15       799
     disgust       0.00      0.00      0.00        87
        fear       0.13      0.10      0.11       819
       happy       0.25      0.24      0.24      1443
     neutral       0.17      0.22      0.19       993
         sad       0.19      0.20      0.19       966
    surprise       0.10      0.09      0.09       634

    accuracy                           0.18      5741
   macro avg       0.14      0.14      0.14      5741
weighted avg       0.18      0.18      0.18      5741
</pre></div>
</div>
<img alt="_images/8d4807ec39530ef3c728be2d95dc38d7cc28ed1ae0d10623027267078be474e4.png" src="_images/8d4807ec39530ef3c728be2d95dc38d7cc28ed1ae0d10623027267078be474e4.png" />
</div>
</div>
<ul class="simple">
<li><p>Matriz de confusión del conjunto de <em><strong>datos de prueba</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">cm_test</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_set</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Classification Report&#39;</span><span class="p">)</span>
<span class="n">target_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">class_labels</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">test_set</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">target_names</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm_test</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">tick_mark</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">target_names</span><span class="p">))</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">tick_mark</span><span class="p">,</span> <span class="n">target_names</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">tick_mark</span><span class="p">,</span> <span class="n">target_names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">113/113</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 20ms/step
Confusion Matrix
[[113   7 121 234 196 174 113]
 [ 11   0  15  25  25  26   9]
 [138  10 127 247 213 173 116]
 [217  15 200 417 392 342 191]
 [150  12 149 284 279 227 132]
 [144  19 142 311 260 234 137]
 [111   9  97 183 168 173  90]]
Classification Report
              precision    recall  f1-score   support

       angry       0.13      0.12      0.12       958
     disgust       0.00      0.00      0.00       111
        fear       0.15      0.12      0.14      1024
       happy       0.25      0.24      0.24      1774
     neutral       0.18      0.23      0.20      1233
         sad       0.17      0.19      0.18      1247
    surprise       0.11      0.11      0.11       831

    accuracy                           0.18      7178
   macro avg       0.14      0.14      0.14      7178
weighted avg       0.17      0.18      0.17      7178
</pre></div>
</div>
<img alt="_images/01967d9cf20448fd227b1828600231c6570879a5788b52166529aba161fad140.png" src="_images/01967d9cf20448fd227b1828600231c6570879a5788b52166529aba161fad140.png" />
</div>
</div>
<ul class="simple">
<li><p>Grafico de predicciones</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x_test</span><span class="p">[</span><span class="n">index</span><span class="p">]))</span>
    <span class="n">predict_index</span> <span class="o">=</span> <span class="n">class_labels</span><span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predict</span><span class="p">[</span><span class="n">index</span><span class="p">]))]</span>
    <span class="n">true_index</span> <span class="o">=</span> <span class="n">class_labels</span><span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">index</span><span class="p">]))]</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">predict_index</span><span class="p">),</span> 
                                  <span class="p">(</span><span class="n">true_index</span><span class="p">)),</span>
                                  <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;green&quot;</span> <span class="k">if</span> <span class="n">predict_index</span> <span class="o">==</span> <span class="n">true_index</span> <span class="k">else</span> <span class="s2">&quot;red&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0eb0cdecf85eced3a1795c50a6bf5543cecd16fd3b8bb56cb87dbe46f3ca4d6c.png" src="_images/0eb0cdecf85eced3a1795c50a6bf5543cecd16fd3b8bb56cb87dbe46f3ca4d6c.png" />
</div>
</div>
</section>
</section>
<section id="redes-neuronales-recurrentes">
<h2><span class="section-number">8.13. </span>Redes Neuronales Recurrentes<a class="headerlink" href="#redes-neuronales-recurrentes" title="Link to this heading">#</a></h2>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Recordemos de la sección anterior que en el corazón de las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">convolucionales</span></code> se encuentra el concepto de <code class="docutils literal notranslate"><span class="pre">peso</span> <span class="pre">compartido</span></code>. Es decir, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">misma</span> <span class="pre">matriz</span> <span class="pre">de</span> <span class="pre">filtro</span> <span class="pre">se</span> <span class="pre">desliza</span> <span class="pre">sobre</span> <span class="pre">una</span> <span class="pre">matriz</span> <span class="pre">de</span> <span class="pre">imágenes</span> <span class="pre">en</span> <span class="pre">lugar</span> <span class="pre">de</span> <span class="pre">dedicar</span> <span class="pre">un</span> <span class="pre">peso</span> <span class="pre">específico</span> <span class="pre">a</span> <span class="pre">cada</span> <span class="pre">píxel</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">imagen</span></code>. De este modo, una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">neuronal</span></code> puede <code class="docutils literal notranslate"><span class="pre">escalarse</span> <span class="pre">fácilmente</span> <span class="pre">a</span> <span class="pre">imágenes</span> <span class="pre">de</span> <span class="pre">diferentes</span> <span class="pre">dimensiones</span></code>.</p></li>
<li><p>Nuestro interés en esta sección se centra en el caso de los <code class="docutils literal notranslate"><span class="pre">datos</span> <span class="pre">secuenciales</span></code>. Es decir, los <code class="docutils literal notranslate"><span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">no</span> <span class="pre">son</span> <span class="pre">independientes,</span> <span class="pre">sino</span> <span class="pre">que</span> <span class="pre">aparecen</span> <span class="pre">en</span> <span class="pre">secuencia</span></code>. Además, el <code class="docutils literal notranslate"><span class="pre">orden</span> <span class="pre">específico</span> <span class="pre">en</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">producen</span> <span class="pre">encierra</span> <span class="pre">información</span> <span class="pre">importante</span></code>. Por ejemplo, este tipo de secuencias se dan en el <strong><code class="docutils literal notranslate"><span class="pre">reconocimiento</span> <span class="pre">del</span> <span class="pre">habla</span> <span class="pre">y</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">procesamiento</span> <span class="pre">del</span> <span class="pre">lenguaje,</span> <span class="pre">como</span> <span class="pre">la</span> <span class="pre">traducción</span> <span class="pre">automática,</span> <span class="pre">así</span> <span class="pre">como</span> <span class="pre">también</span> <span class="pre">el</span> <span class="pre">pronostico</span> <span class="pre">de</span> <span class="pre">series</span> <span class="pre">de</span> <span class="pre">tiempo</span> <span class="pre">financieras</span></code></strong>. Sin duda, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">secuencia</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">producen</span> <span class="pre">las</span> <span class="pre">palabras</span> <span class="pre">es</span> <span class="pre">de</span> <span class="pre">suma</span> <span class="pre">importancia</span></code>.</p></li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">reparto</span> <span class="pre">de</span> <span class="pre">pesos</span> <span class="pre">mediante</span> <span class="pre">convoluciones</span> <span class="pre">también</span> <span class="pre">podría</span> <span class="pre">ser</span> <span class="pre">y</span> <span class="pre">ha</span> <span class="pre">sido</span> <span class="pre">utilizado</span> <span class="pre">para</span> <span class="pre">estos</span> <span class="pre">casos</span></code> <span id="id33">[<a class="reference internal" href="biblio.html#id36" title="Kevin J Lang, Alex H Waibel, and Geoffrey E Hinton. A time-delay neural network architecture for isolated word recognition. Neural networks, 3(1):23–43, 1990.">Lang <em>et al.</em>, 1990</a>]</span>. Tales redes se conocen como <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">neuronales</span> <span class="pre">de</span> <span class="pre">retardo</span> <span class="pre">temporal</span></code>. Sin embargo, <code class="docutils literal notranslate"><span class="pre">deslizar</span> <span class="pre">un</span> <span class="pre">filtro</span> <span class="pre">a</span> <span class="pre">través</span> <span class="pre">del</span> <span class="pre">tiempo</span></code> para formar convoluciones es una <code class="docutils literal notranslate"><span class="pre">operación</span> <span class="pre">de</span> <span class="pre">naturaleza</span> <span class="pre">local</span></code>. La salida es una función de las muestras de entrada dentro de una ventana temporal que abarca la <code class="docutils literal notranslate"><span class="pre">longitud</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">respuesta</span> <span class="pre">al</span> <span class="pre">impulso</span> <span class="pre">del</span> <span class="pre">filtro</span></code>, que por razones prácticas no puede ser muy larga.</p></li>
</ul>
</div>
<div class="admonition-redes-neuronales-recurrentes admonition">
<p class="admonition-title">Redes Neuronales Recurrentes</p>
<ul>
<li><p>Las variables que intervienen en una <code class="docutils literal notranslate"><span class="pre">RNN</span></code> son:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Vector</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(n\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">denotado</span> <span class="pre">como</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. El símbolo nos recuerda que <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> es un vector de variables ocultas (capa oculta en la jerga de las redes neuronales); <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">vector</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">constituye</span> <span class="pre">la</span> <span class="pre">memoria</span> <span class="pre">del</span> <span class="pre">sistema</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Vector</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">momento</span></code> <span class="math notranslate nohighlight">\(n\)</span>, denominado <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Vector</span> <span class="pre">de</span> <span class="pre">salida</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">momento</span></code> <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>, y el vector de salida objetivo, <span class="math notranslate nohighlight">\(\boldsymbol{y}_{n}\)</span>.</p></li>
</ul>
</li>
<li><p>El modelo se describe mediante un <code class="docutils literal notranslate"><span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">desconocidos</span></code>, a saber, <span class="math notranslate nohighlight">\(U, W, V , \boldsymbol{b}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span>, que <code class="docutils literal notranslate"><span class="pre">deben</span> <span class="pre">aprenderse</span> <span class="pre">durante</span> <span class="pre">el</span> <span class="pre">entrenamiento</span></code>.</p></li>
<li><p>Las <code class="docutils literal notranslate"><span class="pre">ecuaciones</span> <span class="pre">que</span> <span class="pre">describen</span> <span class="pre">un</span> <span class="pre">modelo</span> <span class="pre">RNN</span></code> son</p>
<div class="math notranslate nohighlight" id="equation-rnn-system-eq">
<span class="eqno">(8.14)<a class="headerlink" href="#equation-rnn-system-eq" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \boldsymbol{h}_{n}&amp;=f(U\boldsymbol{x}_{n}+W\boldsymbol{h}_{n-1}+\boldsymbol{b})\\
    \hat{\boldsymbol{y}}_{n}&amp;=g(V\boldsymbol{h}_{n}+\boldsymbol{c}).
    \end{align*}
    \end{split}\]</div>
<p>donde <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">funciones</span> <span class="pre">no</span> <span class="pre">lineales</span></code> <span class="math notranslate nohighlight">\(f\)</span> y <span class="math notranslate nohighlight">\(g\)</span><code class="docutils literal notranslate"> <span class="pre">actúan</span> <span class="pre">elemento</span> <span class="pre">a</span> <span class="pre">elemento</span> <span class="pre">(element-wise)</span></code> y <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">aplican</span> <span class="pre">individualmente</span> <span class="pre">a</span> <span class="pre">cada</span> <span class="pre">elemento</span> <span class="pre">de</span> <span class="pre">sus</span> <span class="pre">argumentos</span> <span class="pre">vectoriales</span></code>.</p>
</li>
<li><p>En otras palabras, <code class="docutils literal notranslate"><span class="pre">una</span> <span class="pre">vez</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">ha</span> <span class="pre">observado</span> <span class="pre">un</span> <span class="pre">nuevo</span> <span class="pre">vector</span> <span class="pre">de</span> <span class="pre">entrada,</span> <span class="pre">se</span> <span class="pre">actualiza</span> <span class="pre">el</span> <span class="pre">vector</span> <span class="pre">de</span> <span class="pre">estado</span></code>. Su nuevo valor <code class="docutils literal notranslate"><span class="pre">depende</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">información</span> <span class="pre">más</span> <span class="pre">reciente,</span> <span class="pre">transmitida</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">entrada</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> así como de la <code class="docutils literal notranslate"><span class="pre">historia</span> <span class="pre">pasada,</span> <span class="pre">ya</span> <span class="pre">que</span> <span class="pre">ésta</span> <span class="pre">se</span> <span class="pre">ha</span> <span class="pre">acumulado</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n-1}\)</span>. La salida depende del <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">actualizado</span></code>, <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. Es decir, <code class="docutils literal notranslate"><span class="pre">depende</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">&quot;historia&quot;</span> <span class="pre">hasta</span> <span class="pre">el</span> <span class="pre">instante</span> <span class="pre">actual</span></code> <span class="math notranslate nohighlight">\(n\)</span>, tal y como se expresa en <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>.</p></li>
<li><p>Las opciones típicas para <span class="math notranslate nohighlight">\(f\)</span> son la <code class="docutils literal notranslate"><span class="pre">tangente</span> <span class="pre">hiperbólica,</span> <span class="pre">tanh,</span> <span class="pre">o</span> <span class="pre">las</span> <span class="pre">no</span> <span class="pre">linealidades</span> <span class="pre">ReLU</span></code>. El valor inicial <span class="math notranslate nohighlight">\(\boldsymbol{h}_{0}\)</span> <code class="docutils literal notranslate"><span class="pre">suele</span> <span class="pre">ser</span> <span class="pre">igual</span> <span class="pre">al</span> <span class="pre">vector</span> <span class="pre">cero</span></code>. La <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">de</span> <span class="pre">salida,</span></code> <span class="math notranslate nohighlight">\(g\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">se</span> <span class="pre">elige</span> <span class="pre">a</span> <span class="pre">menudo</span> <span class="pre">para</span> <span class="pre">ser</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">softmax</span></code>.</p></li>
</ul>
</div>
<figure class="align-center" id="recurrent-neural-network-arch-numref">
<a class="reference internal image-reference" href="_images/recurrent_neural_network_arch.png"><img alt="_images/recurrent_neural_network_arch.png" src="_images/recurrent_neural_network_arch.png" style="width: 552.0px; height: 304.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.41 </span><span class="caption-text">Arquitectura de una <code class="docutils literal notranslate"><span class="pre">Red</span> <span class="pre">Neuronal</span> <span class="pre">Recurrente</span></code>. Fuente <span id="id34">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#recurrent-neural-network-arch-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>De las ecuaciones anteriores se deduce que las <code class="docutils literal notranslate"><span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">se</span> <span class="pre">comparten</span> <span class="pre">en</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">instantes</span> <span class="pre">temporales</span></code>. Durante el entrenamiento, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">inicializan</span> <span class="pre">mediante</span> <span class="pre">números</span> <span class="pre">aleatorios</span></code>. El modelo gráfico
asociado con Eq. <a class="reference internal" href="#equation-rnn-system-eq">(8.14)</a> se muestra en la <a class="reference internal" href="#recurrent-neural-network-arch-numref"><span class="std std-numref">Fig. 8.41</span></a>A. En la <a class="reference internal" href="#recurrent-neural-network-arch-numref"><span class="std std-numref">Fig. 8.41</span></a>B, <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">gráfico</span> <span class="pre">se</span> <span class="pre">despliega</span> <span class="pre">sobre</span> <span class="pre">los</span> <span class="pre">distintos</span> <span class="pre">instantes</span> <span class="pre">de</span> <span class="pre">tiempo</span></code> para los que se dispone de observaciones. Por ejemplo, <code class="docutils literal notranslate"><span class="pre">si</span> <span class="pre">la</span> <span class="pre">secuencia</span> <span class="pre">de</span> <span class="pre">interés</span> <span class="pre">es</span> <span class="pre">una</span> <span class="pre">frase</span> <span class="pre">de</span> <span class="pre">10</span> <span class="pre">palabras</span></code>, entonces <span class="math notranslate nohighlight">\(N\)</span> se establece igual a 10, mientras que <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> es el <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">que</span> <span class="pre">codifica</span> <span class="pre">las</span> <span class="pre">respectivas</span> <span class="pre">palabras</span> <span class="pre">de</span> <span class="pre">entrada</span></code>.</p></li>
</ul>
<section id="backpropagation-en-tiempo">
<h3><span class="section-number">8.13.1. </span>Backpropagation en tiempo<a class="headerlink" href="#backpropagation-en-tiempo" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>El entrenamiento de las <code class="docutils literal notranslate"><span class="pre">RNN</span></code> sigue una <code class="docutils literal notranslate"><span class="pre">lógica</span> <span class="pre">similar</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">del</span> <span class="pre">algoritmo</span> <span class="pre">backpropagation</span></code> para el entrenamiento de redes neuronales de avance. Después de todo, <code class="docutils literal notranslate"><span class="pre">una</span> <span class="pre">RNN</span> <span class="pre">puede</span> <span class="pre">verse</span> <span class="pre">como</span> <span class="pre">una</span> <span class="pre">red</span> <span class="pre">feed-forward</span> <span class="pre">con</span></code> <span class="math notranslate nohighlight">\(N\)</span> <code class="docutils literal notranslate"><span class="pre">capas</span></code>. La <code class="docutils literal notranslate"><span class="pre">capa</span> <span class="pre">superior</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">del</span> <span class="pre">instante</span> <span class="pre">de</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(N\)</span> y la <code class="docutils literal notranslate"><span class="pre">primera</span> <span class="pre">capa</span> <span class="pre">corresponde</span> <span class="pre">al</span> <span class="pre">instante</span> <span class="pre">de</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(n = 1\)</span>. Una diferencia radica en que las <code class="docutils literal notranslate"><span class="pre">capas</span> <span class="pre">ocultas</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">RNN</span> <span class="pre">también</span> <span class="pre">producen</span> <span class="pre">salidas</span></code>, es decir, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>, y se alimentan directamente con entradas. Sin embargo, <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">lo</span> <span class="pre">que</span> <span class="pre">respecta</span> <span class="pre">al</span> <span class="pre">entrenamiento,</span> <span class="pre">estas</span> <span class="pre">diferencias</span> <span class="pre">no</span> <span class="pre">afectan</span> <span class="pre">al</span> <span class="pre">razonamiento</span> <span class="pre">principal</span></code>.</p></li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">aprendizaje</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">desconocidos</span></code> se consigue mediante un esquema de gradiente descendente, de acuerdo con Eq. <a class="reference internal" href="#equation-update-equations-gd">(8.3)</a>. Resulta que los <code class="docutils literal notranslate"><span class="pre">gradientes</span> <span class="pre">requeridos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span></code>, con respecto a los parámetros desconocidos, <code class="docutils literal notranslate"><span class="pre">tienen</span> <span class="pre">lugar</span> <span class="pre">recursivamente,</span> <span class="pre">comenzando</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">último</span> <span class="pre">instante</span> <span class="pre">de</span> <span class="pre">tiempo,</span></code> <span class="math notranslate nohighlight">\(N\)</span> , y retrocediendo en el tiempo, <span class="math notranslate nohighlight">\(n = N-1, N-2,\dots,1\)</span>. Esta es la razón por la que el algoritmo se conoce como <code class="docutils literal notranslate"><span class="pre">bakpropagation</span> <span class="pre">a</span> <span class="pre">traves</span> <span class="pre">del</span> <span class="pre">tiempo</span> <span class="pre">(BPTT)</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">suma</span> <span class="pre">a</span> <span class="pre">lo</span> <span class="pre">largo</span> <span class="pre">del</span> <span class="pre">tiempo,</span></code> <span class="math notranslate nohighlight">\(n\)</span>, de las correspondientes <code class="docutils literal notranslate"><span class="pre">contribuciones</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span></code>, que dependen de los valores respectivos de <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}, \boldsymbol{x}_{n}\)</span>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(U, W, V, \boldsymbol{b}, \boldsymbol{c})=\sum_{n=1}^{N}J_{n}(U, W, V, \boldsymbol{b}, \boldsymbol{c}).
\]</div>
<ul>
<li><p>Por ejemplo, para el caso de la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span> <span class="pre">de</span> <span class="pre">entropía</span> <span class="pre">cruzada</span></code>,</p>
<div class="math notranslate nohighlight">
\[
    J_{n}(U, W, V, \boldsymbol{b}, \boldsymbol{c}):=-\sum_{k}y_{nk}\ln\hat{y}_{nk},
    \]</div>
<p>donde <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">suma</span> <span class="pre">es</span> <span class="pre">sobre</span> <span class="pre">la</span> <span class="pre">dimensionalidad</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, y</p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{y}}_{n}=g(\boldsymbol{h}_{n}, V, \boldsymbol{c})~\text{y}~\boldsymbol{h}_{n}=f(\boldsymbol{x}_{n}, \boldsymbol{h}_{n-1}, U, W, \boldsymbol{b}).
    \]</div>
</li>
</ul>
<ul>
<li><p>En el corazón del <code class="docutils literal notranslate"><span class="pre">cálculo</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span> <span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">diversas</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span></code> se encuentra el cálculo de los <code class="docutils literal notranslate"><span class="pre">gradientes</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(J\)</span> <code class="docutils literal notranslate"><span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">estado,</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. Una vez calculados estos últimos, el resto de los gradientes, con respecto a las <code class="docutils literal notranslate"><span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">desconocidos</span></code>, es una tarea sencilla. Para ello, nótese que cada <span class="math notranslate nohighlight">\(h_{n},~n=1, 2,\dots,N-1\)</span>, afecta a <span class="math notranslate nohighlight">\(J\)</span> de dos maneras:</p>
<ul class="simple">
<li><p>Directamente, <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">traves</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(J_{n}\)</span></p></li>
<li><p>Indirectamente, <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">través</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">cadena</span> <span class="pre">que</span> <span class="pre">impone</span> <span class="pre">la</span> <span class="pre">estructura</span> <span class="pre">RNN</span></code>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{h}_{n}\rightarrow\boldsymbol{h}_{n+1}\rightarrow\cdots\rightarrow\boldsymbol{h}_{N}.
    \]</div>
<p>Es decir, <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>, además de <span class="math notranslate nohighlight">\(J_{n}\)</span>, <code class="docutils literal notranslate"><span class="pre">también</span> <span class="pre">afecta</span> <span class="pre">a</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">de</span> <span class="pre">coste</span> <span class="pre">posteriores</span></code>, <span class="math notranslate nohighlight">\(J_{n+1},\dots, J_{N}\)</span>. Nótese que, a traves de la cadena <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n+1}=f(\boldsymbol{x}_{n+1}, \boldsymbol{h}_{n}, U, W, \boldsymbol{b}).\)</span></p>
</li>
</ul>
<ul>
<li><p>Empleando la <code class="docutils literal notranslate"><span class="pre">regla</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">cadena</span></code> para las derivadas, las <code class="docutils literal notranslate"><span class="pre">dependencias</span> <span class="pre">anteriores</span> <span class="pre">conducen</span> <span class="pre">al</span> <span class="pre">siguiente</span> <span class="pre">cálculo</span> <span class="pre">recursivo</span></code>:</p>
<div class="math notranslate nohighlight" id="equation-indirect-direct-rec-part">
<span class="eqno">(8.15)<a class="headerlink" href="#equation-indirect-direct-rec-part" title="Link to this equation">#</a></span>\[
    \frac{\partial J}{\partial\boldsymbol{h}_{n}}=\underbrace{{\left(\frac{\partial\boldsymbol{h}_{n+1}}{\partial\boldsymbol{h}_{n}}\right)^{T}}\frac{\partial J}{\partial\boldsymbol{h}_{n+1}}}_{\text{parte recursiva indirecta}}+\underbrace{\left(\frac{\partial\hat{\boldsymbol{y}}_{n}}{\partial\boldsymbol{h}_{n}}\right)^{T}\frac{\partial J}{\partial\hat{\boldsymbol{y}}_{n}}}_{\text{parte directa}},
    \]</div>
<p>donde, por definición, la <code class="docutils literal notranslate"><span class="pre">derivada</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">vector</span></code>, digamos, <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, <code class="docutils literal notranslate"><span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">otro</span> <span class="pre">vector</span></code>, digamos, <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, se define como la matriz</p>
<div class="math notranslate nohighlight">
\[
    \left[\frac{\partial\boldsymbol{y}}{\partial\boldsymbol{x}}\right]_{ij}:=\frac{\partial y_{i}}{\partial x_{j}}.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Nótese que el <code class="docutils literal notranslate"><span class="pre">gradiente</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span></code>, con respecto a los <code class="docutils literal notranslate"><span class="pre">parámetros</span> <span class="pre">ocultos</span> <span class="pre">(vector</span> <span class="pre">de</span> <span class="pre">estado)</span></code> en la capa “<span class="math notranslate nohighlight">\(n\)</span>”, se da como una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">del</span> <span class="pre">gradiente</span> <span class="pre">respectivo</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">anterior</span></code>, es decir, con respecto al <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(n + 1\)</span>. Las dos <code class="docutils literal notranslate"><span class="pre">pasadas</span> <span class="pre">requeridas</span> <span class="pre">por</span> <span class="pre">backpropagation</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">tiempo</span></code> se resumen a continuación.</p></li>
</ul>
<div class="admonition-pasadas-de-backpropagation-en-tiempo admonition">
<p class="admonition-title">Pasadas de Backpropagation en Tiempo</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Paso</span> <span class="pre">hacia</span> <span class="pre">adelante</span></code>:</p>
<ul class="simple">
<li><p>Iniciando en <span class="math notranslate nohighlight">\(n=1\)</span> y <code class="docutils literal notranslate"><span class="pre">utilizando</span> <span class="pre">las</span> <span class="pre">estimaciones</span> <span class="pre">actuales</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">implicados</span></code>, calcular en secuencia,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    (\boldsymbol{h}_{1}, \hat{\boldsymbol{y}}_{1})\rightarrow(\boldsymbol{h}_{2}, \hat{\boldsymbol{y}}_{2})\rightarrow\cdots\rightarrow(\boldsymbol{h}_{N}, \hat{\boldsymbol{y}}_{N}).
    \]</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Paso</span> <span class="pre">hacia</span> <span class="pre">atrás</span></code>:</p>
<ul class="simple">
<li><p>Empezando en <span class="math notranslate nohighlight">\(n = N\)</span>, calcular en secuencia,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \frac{\partial J}{\partial\boldsymbol{h}_{N}}\rightarrow\frac{\partial J}{\partial\boldsymbol{h}_{N-1}}\rightarrow\cdots\rightarrow\frac{\partial J}{\partial\boldsymbol{h}_{1}}.
    \]</div>
</li>
</ul>
</div>
<ul class="simple">
<li><p>Nótese que el <code class="docutils literal notranslate"><span class="pre">cálculo</span> <span class="pre">del</span> <span class="pre">gradiente</span></code> <span class="math notranslate nohighlight">\(\partial J/\partial\boldsymbol{h}_{N}\)</span> es sencillo, y <code class="docutils literal notranslate"><span class="pre">solo</span> <span class="pre">involucra</span> <span class="pre">la</span> <span class="pre">parte</span> <span class="pre">directa</span> <span class="pre">en</span></code> Eq. <a class="reference internal" href="#equation-indirect-direct-rec-part">(8.15)</a>.</p></li>
<li><p>Para la implementación de la <code class="docutils literal notranslate"><span class="pre">BPTT</span></code>, se procede a</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">inicializar</span> <span class="pre">aleatoriamente</span> <span class="pre">las</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">desconocidos</span> <span class="pre">implicados,</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">calcular</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">requeridos,</span> <span class="pre">siguiendo</span> <span class="pre">los</span> <span class="pre">pasos</span> <span class="pre">indicados</span> <span class="pre">anteriormente</span></code>, y</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">realizar</span> <span class="pre">las</span> <span class="pre">actualizaciones</span> <span class="pre">según</span> <span class="pre">el</span> <span class="pre">esquema</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">descendente</span></code>.</p></li>
</ol>
</li>
<li><p>Los pasos <code class="docutils literal notranslate"><span class="pre">(2)</span> <span class="pre">y</span> <span class="pre">(3)</span> <span class="pre">se</span> <span class="pre">realizan</span> <span class="pre">de</span> <span class="pre">forma</span> <span class="pre">iterativa</span> <span class="pre">hasta</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">cumple</span> <span class="pre">un</span> <span class="pre">criterio</span> <span class="pre">de</span> <span class="pre">convergencia</span></code>, de forma análoga al <code class="docutils literal notranslate"><span class="pre">algoritmo</span> <span class="pre">estándar</span> <span class="pre">de</span> <span class="pre">backpropagation</span></code>.</p></li>
</ul>
</section>
<section id="desvanecimiento-y-explosion-de-gradientes">
<h3><span class="section-number">8.13.2. </span>Desvanecimiento y explosión de gradientes<a class="headerlink" href="#desvanecimiento-y-explosion-de-gradientes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>La tarea de <code class="docutils literal notranslate"><span class="pre">desvanecimiento</span> <span class="pre">y</span> <span class="pre">explosión</span> <span class="pre">de</span> <span class="pre">gradientes</span></code> se ha introducido y discutido en secciones anteriores, en el contexto del algoritmo de backpropagation. <code class="docutils literal notranslate"><span class="pre">Los</span> <span class="pre">mismos</span> <span class="pre">problemas</span> <span class="pre">se</span> <span class="pre">presentan</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">algoritmo</span> <span class="pre">BPTT</span></code>, dado que, este último es una <code class="docutils literal notranslate"><span class="pre">forma</span> <span class="pre">específica</span> <span class="pre">del</span> <span class="pre">concepto</span> <span class="pre">de</span> <span class="pre">backpropagation</span></code> y, como se ha dicho, una <code class="docutils literal notranslate"><span class="pre">RNN</span></code> puede considerarse como una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">multicapa,</span> <span class="pre">donde</span> <span class="pre">cada</span> <span class="pre">instante</span> <span class="pre">de</span> <span class="pre">tiempo</span> <span class="pre">corresponde</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">capa</span> <span class="pre">diferente</span></code>. De hecho en las <code class="docutils literal notranslate"><span class="pre">RNN</span></code>, el fenómeno de <code class="docutils literal notranslate"><span class="pre">desvanecimiento/explosión</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">aparece</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">forma</span> <span class="pre">bastante</span> <span class="pre">&quot;agresiva&quot;</span></code>, teniendo en cuenta que <span class="math notranslate nohighlight">\(N\)</span> puede alcanzar valores grandes.</p></li>
</ul>
<ul class="simple">
<li><p>La naturaleza multiplicativa de la propagación de gradientes puede verse fácilmente en la Eq. <a class="reference internal" href="#equation-indirect-direct-rec-part">(8.15)</a>. Para ayudar a comprender el concepto principal, simplifiquemos el escenario y <code class="docutils literal notranslate"><span class="pre">supongamos</span> <span class="pre">que</span> <span class="pre">sólo</span> <span class="pre">interviene</span> <span class="pre">una</span> <span class="pre">variante</span> <span class="pre">de</span> <span class="pre">estado</span></code>. Entonces los vectores de estado se convierten en escalares, <span class="math notranslate nohighlight">\(h_{n}\)</span> , y la matriz <span class="math notranslate nohighlight">\(W\)</span> en un escalar <span class="math notranslate nohighlight">\(w\)</span>. Además, <code class="docutils literal notranslate"><span class="pre">supongamos</span> <span class="pre">que</span> <span class="pre">las</span> <span class="pre">salidas</span> <span class="pre">también</span> <span class="pre">son</span> <span class="pre">escalares</span></code>. Entonces la recursión en la Eq. <a class="reference internal" href="#equation-indirect-direct-rec-part">(8.15)</a> se simplifica como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial h_{n}}=\frac{\partial h_{n+1}}{\partial h_{n}}\frac{\partial J}{\partial h_{n+1}}+\frac{\partial\hat{y}_{n}}{\partial h_{n}}\frac{\partial J}{\partial\hat{y}_{n}}.
\]</div>
<ul class="simple">
<li><p>Suponiendo en la Eq. <a class="reference internal" href="#equation-rnn-system-eq">(8.14)</a> que <span class="math notranslate nohighlight">\(f\)</span> <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">la</span> <span class="pre">función</span></code> <span class="math notranslate nohighlight">\(\tanh(\cdot)\)</span> <code class="docutils literal notranslate"><span class="pre">estándar</span></code>, teniendo en cuenta que, <span class="math notranslate nohighlight">\(\text{sech}^2(\cdot)=1-\tanh^2(\cdot)\)</span> y <span class="math notranslate nohighlight">\(|\tanh(\cdot)|&lt;1\)</span>, sobre su dominio, se ve fácilmente que</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-partial-deriv-hn">
<span class="eqno">(8.16)<a class="headerlink" href="#equation-partial-deriv-hn" title="Link to this equation">#</a></span>\[
\frac{\partial h_{n+1}}{\partial h_{n}}=w(1-h_{n+1}^{2}).
\]</div>
<ul class="simple">
<li><p>Escribiendo la <code class="docutils literal notranslate"><span class="pre">recursión</span> <span class="pre">para</span> <span class="pre">dos</span> <span class="pre">pasos</span> <span class="pre">sucesivos</span></code>, repitiendo el proceso en Eq. <a class="reference internal" href="#equation-partial-deriv-hn">(8.16)</a>, por ejemplo, para <span class="math notranslate nohighlight">\(\partial h_{n+2}/\partial h_{n+1}\)</span> obtenemos que,</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-partialj-hn-eq">
<span class="eqno">(8.17)<a class="headerlink" href="#equation-partialj-hn-eq" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\frac{\partial J}{\partial h_{n}}&amp;=\frac{\partial h_{n+1}}{\partial h_{n}}\frac{\partial J}{\partial h_{n+1}}+\frac{\partial\hat{y}_{n}}{\partial h_{n}}\frac{\partial J}{\partial\hat{y}_{n}}\\
&amp;=w^{2}(1-h_{n+1}^{2})(1-h_{n+2}^{2})\frac{\partial J}{\partial h_{n+2}}+\text{otro términos}
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>No es difícil ver que la <code class="docutils literal notranslate"><span class="pre">multiplicación</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">términos</span> <span class="pre">menores</span> <span class="pre">que</span> <span class="pre">uno</span> <span class="pre">puede</span> <span class="pre">llevar</span> <span class="pre">a</span> <span class="pre">valores</span> <span class="pre">de</span> <span class="pre">desvanecimiento</span></code>, sobre todo si tenemos en cuenta que, en la práctica, las <code class="docutils literal notranslate"><span class="pre">secuencias</span> <span class="pre">pueden</span> <span class="pre">ser</span> <span class="pre">bastante</span> <span class="pre">grandes,</span> <span class="pre">por</span> <span class="pre">ejemplo,</span></code> <span class="math notranslate nohighlight">\(N = 100\)</span>. Por lo tanto <code class="docutils literal notranslate"><span class="pre">para</span> <span class="pre">instantes</span> <span class="pre">de</span> <span class="pre">tiempo</span> <span class="pre">cercanos</span> <span class="pre">a</span></code> <span class="math notranslate nohighlight">\(n = 1\)</span>, la <code class="docutils literal notranslate"><span class="pre">contribución</span> <span class="pre">al</span> <span class="pre">gradiente</span> <span class="pre">del</span> <span class="pre">primer</span> <span class="pre">término</span> <span class="pre">del</span> <span class="pre">lado</span> <span class="pre">derecho</span> <span class="pre">en</span> <span class="pre">la</span></code> Eq. <a class="reference internal" href="#equation-partialj-hn-eq">(8.17)</a> implicará un <code class="docutils literal notranslate"><span class="pre">gran</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">productos</span> <span class="pre">de</span> <span class="pre">números</span> <span class="pre">menores</span> <span class="pre">que</span> <span class="pre">uno</span> <span class="pre">en</span> <span class="pre">magnitud</span></code>. Por otra parte, el valor de <span class="math notranslate nohighlight">\(w\)</span> estará contribuyendo en <span class="math notranslate nohighlight">\(w^{n}\)</span> potencia. Por lo tanto, <code class="docutils literal notranslate"><span class="pre">si</span> <span class="pre">su</span> <span class="pre">valor</span> <span class="pre">es</span> <span class="pre">mayor</span> <span class="pre">que</span> <span class="pre">uno,</span> <span class="pre">puede</span> <span class="pre">conducir</span> <span class="pre">a</span> <span class="pre">valores</span> <span class="pre">explosivos</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">respectivos</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>En varios casos, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">puede</span> <span class="pre">truncar</span> <span class="pre">el</span> <span class="pre">algoritmo</span> <span class="pre">de</span> <span class="pre">backpropagation</span> <span class="pre">a</span> <span class="pre">unos</span> <span class="pre">pocos</span> <span class="pre">pasos</span> <span class="pre">de</span> <span class="pre">tiempo</span></code>. Otra forma, es <code class="docutils literal notranslate"><span class="pre">sustituir</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">tanh</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">ReLU</span></code>. Para el caso del <code class="docutils literal notranslate"><span class="pre">valor</span> <span class="pre">explosivo</span></code>, se puede <code class="docutils literal notranslate"><span class="pre">introducir</span> <span class="pre">una</span> <span class="pre">técnica</span> <span class="pre">que</span> <span class="pre">recorte</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">umbral</span> <span class="pre">predeterminado</span></code>, una vez que los <code class="docutils literal notranslate"><span class="pre">valores</span> <span class="pre">superen</span> <span class="pre">ese</span> <span class="pre">umbral</span></code>. Sin embargo, otra técnica que suele emplearse en la práctica es <code class="docutils literal notranslate"><span class="pre">sustituir</span> <span class="pre">la</span> <span class="pre">formulación</span> <span class="pre">RNN</span></code> estándar descrita anteriormente por una <code class="docutils literal notranslate"><span class="pre">estructura</span> <span class="pre">alternativa,</span> <span class="pre">que</span> <span class="pre">puede</span> <span class="pre">hacer</span> <span class="pre">mejor</span> <span class="pre">frente</span> <span class="pre">a</span> <span class="pre">estos</span> <span class="pre">fenómenos</span> <span class="pre">causados</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">dependencia</span> <span class="pre">a</span> <span class="pre">largo</span> <span class="pre">plazo</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">RNN</span></code>.</p></li>
</ul>
<div class="proof observation admonition" id="observation_ann9">
<p class="admonition-title"><span class="caption-number">Observation 8.3 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">profundas</span></code></strong>: Además de la red <code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">básica</span> <span class="pre">que</span> <span class="pre">comprende</span> <span class="pre">una</span> <span class="pre">sola</span> <span class="pre">capa</span> <span class="pre">de</span> <span class="pre">estados</span></code>, se han propuesto <code class="docutils literal notranslate"><span class="pre">extensiones</span> <span class="pre">que</span> <span class="pre">implican</span> <span class="pre">múltiples</span> <span class="pre">capas</span> <span class="pre">de</span> <span class="pre">estados</span></code>, una encima de otra (ver <span id="id35">[<a class="reference internal" href="biblio.html#id37" title="Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026, 2013.">Pascanu <em>et al.</em>, 2013</a>]</span>).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">bidireccionales</span></code></strong>: Como su nombre indica, en las <code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">bidireccionales</span> <span class="pre">hay</span> <span class="pre">dos</span> <span class="pre">variables</span> <span class="pre">de</span> <span class="pre">estado</span></code>, es decir, una denotada como <span class="math notranslate nohighlight">\(\overset{\rightarrow}{\boldsymbol{h}}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">propaga</span> <span class="pre">hacia</span> <span class="pre">adelante</span></code>, y otra, <span class="math notranslate nohighlight">\(\overset{\leftarrow}{\boldsymbol{h}}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">propaga</span> <span class="pre">hacia</span> <span class="pre">atrás</span></code>. De este modo, <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">salidas</span> <span class="pre">dependen</span> <span class="pre">tanto</span> <span class="pre">del</span> <span class="pre">pasado</span> <span class="pre">como</span> <span class="pre">del</span> <span class="pre">futuro</span></code> (ver <span id="id36">[<a class="reference internal" href="biblio.html#id38" title="Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing, 6645–6649. Ieee, 2013.">Graves <em>et al.</em>, 2013</a>]</span>).</p></li>
</ul>
</section>
</div></section>
</section>
<section id="red-de-memoria-a-largo-plazo-lstm">
<h2><span class="section-number">8.14. </span>Red de memoria a largo plazo (LSTM)<a class="headerlink" href="#red-de-memoria-a-largo-plazo-lstm" title="Link to this heading">#</a></h2>
<div class="proof observation admonition" id="observation_ann10">
<p class="admonition-title"><span class="caption-number">Observation 8.4 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">idea</span> <span class="pre">clave</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">red</span> <span class="pre">LSTM</span></code>, propuesta en el artículo seminal <span id="id37">[<a class="reference internal" href="biblio.html#id39" title="Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.">Hochreiter and Schmidhuber, 1997</a>]</span>, es el llamado <code class="docutils literal notranslate"><span class="pre">estado</span> <span class="pre">de</span> <span class="pre">celda</span></code>, que ayuda a <code class="docutils literal notranslate"><span class="pre">superar</span> <span class="pre">los</span> <span class="pre">problemas</span> <span class="pre">asociados</span> <span class="pre">con</span> <span class="pre">los</span> <span class="pre">fenómenos</span> <span class="pre">de</span> <span class="pre">desvanecimiento/explosión</span></code> que son causados por las <code class="docutils literal notranslate"><span class="pre">dependencias</span> <span class="pre">largo</span> <span class="pre">plazo</span></code> dentro de la red. Las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">LSTM</span></code> tienen la capacidad incorporada de <code class="docutils literal notranslate"><span class="pre">controlar</span> <span class="pre">el</span> <span class="pre">flujo</span> <span class="pre">de</span> <span class="pre">información</span> <span class="pre">que</span> <span class="pre">entra</span> <span class="pre">y</span> <span class="pre">sale</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">memoria</span> <span class="pre">del</span> <span class="pre">sistema</span> <span class="pre">mediante</span> <span class="pre">algoritmos</span> <span class="pre">no</span> <span class="pre">lineales</span></code>. Estas puertas se implementan <code class="docutils literal notranslate"><span class="pre">mediante</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">sigmoidea</span> <span class="pre">y</span> <span class="pre">un</span> <span class="pre">multiplicador</span></code>. Desde un punto de vista algorítmico, las puertas equivalen a <code class="docutils literal notranslate"><span class="pre">aplicar</span> <span class="pre">una</span> <span class="pre">ponderación</span> <span class="pre">al</span> <span class="pre">flujo</span> <span class="pre">de</span> <span class="pre">información</span> <span class="pre">correspondiente</span></code>. Los <code class="docutils literal notranslate"><span class="pre">pesos</span></code> se sitúan en el intervalo <span class="math notranslate nohighlight">\([0,1]\)</span> y <code class="docutils literal notranslate"><span class="pre">dependen</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">variables</span> <span class="pre">implicadas</span> <span class="pre">que</span> <span class="pre">activan</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">sigmoidea</span></code>.</p></li>
</ul>
</section>
</div><figure class="align-center" id="lstm-arch-rnn-numref">
<a class="reference internal image-reference" href="_images/lstm_arch_rnn.png"><img alt="_images/lstm_arch_rnn.png" src="_images/lstm_arch_rnn.png" style="width: 664.8000000000001px; height: 332.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.42 </span><span class="caption-text">Arquitectura de unidad LSTM. Fuente <span id="id38">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#lstm-arch-rnn-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En otras palabras, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">ponderación</span> <span class="pre">(control)</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">información</span></code> tiene lugar en el contexto. Según este razonamiento, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">red</span> <span class="pre">tiene</span> <span class="pre">la</span> <span class="pre">agilidad</span> <span class="pre">de</span> <span class="pre">olvidar</span> <span class="pre">la</span> <span class="pre">información</span> <span class="pre">que</span> <span class="pre">ya</span> <span class="pre">ha</span> <span class="pre">sido</span> <span class="pre">utilizada</span> <span class="pre">y</span> <span class="pre">ya</span> <span class="pre">no</span> <span class="pre">es</span> <span class="pre">necesaria</span></code>. La <code class="docutils literal notranslate"><span class="pre">célula/unidad</span> <span class="pre">LSTM</span> <span class="pre">básica</span></code> se
se muestra en la <a class="reference internal" href="#lstm-arch-rnn-numref"><span class="std std-numref">Fig. 8.42</span></a>. Se construye en torno a <code class="docutils literal notranslate"><span class="pre">dos</span> <span class="pre">conjuntos</span> <span class="pre">de</span> <span class="pre">variables,</span> <span class="pre">apiladas</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">vector</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{s}\)</span>, que se conoce como el <code class="docutils literal notranslate"><span class="pre">estado</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">célula</span> <span class="pre">o</span> <span class="pre">unidad</span></code>, y el vector <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span>, que se conoce como el <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">de</span> <span class="pre">variables</span> <span class="pre">ocultas</span></code>. Una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">LSTM</span></code> se construye a partir de la <code class="docutils literal notranslate"><span class="pre">concatenación</span> <span class="pre">sucesiva</span> <span class="pre">de</span> <span class="pre">esta</span> <span class="pre">unidad</span> <span class="pre">básica</span></code>. La unidad correspondiente al tiempo <span class="math notranslate nohighlight">\(n\)</span>, además del vector de entrada, <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span>, recibe <span class="math notranslate nohighlight">\(\boldsymbol{s}_{n-1}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n-1}\)</span> de la etapa anterior y pasa <span class="math notranslate nohighlight">\(\boldsymbol{s}_{n}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span> a la siguiente.</p></li>
</ul>
<ul>
<li><p>A continuación se resumen las <code class="docutils literal notranslate"><span class="pre">ecuaciones</span> <span class="pre">de</span> <span class="pre">actualización</span> <span class="pre">asociadas</span></code>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \boldsymbol{f}&amp;=\sigma(U^{f}\boldsymbol{x}_{n}+W^{f}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{f}),\\
    \boldsymbol{i}&amp;=\sigma(U^{i}\boldsymbol{x}_{n}+W^{i}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{i}),\\
    \tilde{\boldsymbol{s}}&amp;=\tanh(U^{s}\boldsymbol{x}_{n}+W^{s}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{s}),\\
    \boldsymbol{o}&amp;=\sigma(U^{o}\boldsymbol{x}_{n}+W^{o}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{o}),\\
    \boldsymbol{s}_{n}&amp;=\boldsymbol{s}_{n-1}\circ f+\boldsymbol{i}\circ\tilde{\boldsymbol{s}},\\
    \boldsymbol{h}_{n}&amp;=\boldsymbol{o}\circ\tanh(\boldsymbol{s}_{n}),
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\circ\)</span> denota el <code class="docutils literal notranslate"><span class="pre">producto</span> <span class="pre">elemento</span> <span class="pre">a</span> <span class="pre">elemento</span> <span class="pre">entre</span> <span class="pre">vectores</span> <span class="pre">o</span> <span class="pre">matrices</span></code> (producto de <code class="docutils literal notranslate"><span class="pre">Hadamard</span></code>), es decir, <span class="math notranslate nohighlight">\((s\circ f)_{i} = s_{i}f_{i}\)</span>, y <span class="math notranslate nohighlight">\(\sigma\)</span> denota la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">sigmoidea</span> <span class="pre">logística</span></code>.</p>
</li>
</ul>
<div class="proof observation admonition" id="observation_ann11">
<p class="admonition-title"><span class="caption-number">Observation 8.5 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>Nótese que el <code class="docutils literal notranslate"><span class="pre">estado</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">célula</span></code>, <span class="math notranslate nohighlight">\(\boldsymbol{s}\)</span>, <code class="docutils literal notranslate"><span class="pre">pasa</span> <span class="pre">información</span> <span class="pre">directa</span> <span class="pre">del</span> <span class="pre">instante</span> <span class="pre">anterior</span> <span class="pre">al</span> <span class="pre">siguiente</span></code>. Esta información es <code class="docutils literal notranslate"><span class="pre">controlada</span> <span class="pre">primero</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">primera</span> <span class="pre">puerta</span></code>, según los elementos en <span class="math notranslate nohighlight">\(f\)</span>, que toman valores en el rango <span class="math notranslate nohighlight">\([0, 1]\)</span>, <code class="docutils literal notranslate"><span class="pre">dependiendo</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">entrada</span> <span class="pre">actual</span> <span class="pre">y</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">variables</span> <span class="pre">ocultas</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">reciben</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">etapa</span> <span class="pre">anterior</span></code>. Esto es lo que decíamos antes, es decir, que <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">ponderación</span> <span class="pre">se</span> <span class="pre">ajusta</span> <span class="pre">en</span> <span class="pre">&quot;contexto&quot;</span></code>. A continuación, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">añade</span> <span class="pre">nueva</span> <span class="pre">información</span></code>, es decir, <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{s}}\)</span>, a <span class="math notranslate nohighlight">\(\boldsymbol{s}_{n-1}\)</span>, que también está <code class="docutils literal notranslate"><span class="pre">controlada</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">segunda</span> <span class="pre">red</span> <span class="pre">de</span> <span class="pre">puertas</span> <span class="pre">sigmoidales</span></code> (es decir, <span class="math notranslate nohighlight">\(\boldsymbol{i}\)</span>). Así se garantiza que la <code class="docutils literal notranslate"><span class="pre">información</span> <span class="pre">del</span> <span class="pre">pasado</span> <span class="pre">se</span> <span class="pre">transmita</span> <span class="pre">al</span> <span class="pre">futuro</span> <span class="pre">de</span> <span class="pre">forma</span> <span class="pre">directa,</span> <span class="pre">lo</span> <span class="pre">cual,</span> <span class="pre">ayuda</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">red</span> <span class="pre">a</span> <span class="pre">memorizar</span> <span class="pre">información.</span></code>.</p></li>
<li><p>Resulta que este tipo de memoria <code class="docutils literal notranslate"><span class="pre">explota</span> <span class="pre">mejor</span> <span class="pre">las</span> <span class="pre">dependencias</span> <span class="pre">de</span> <span class="pre">largo</span> <span class="pre">alcance</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">datos</span></code>, en comparación con la estructura <code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">básica</span></code>. El vector de variables ocultas <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> <code class="docutils literal notranslate"><span class="pre">está</span> <span class="pre">controlado</span> <span class="pre">tanto</span> <span class="pre">por</span> <span class="pre">el</span> <span class="pre">estado</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">célula</span> <span class="pre">como</span> <span class="pre">por</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">actuales</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">variables</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">y</span> <span class="pre">de</span> <span class="pre">estados</span> <span class="pre">anteriores</span></code>. Todas las <code class="docutils literal notranslate"><span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">implicados</span> <span class="pre">se</span> <span class="pre">aprenden</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">fase</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>. Nótese que hay dos líneas asociadas a <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. La de <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">derecha</span> <span class="pre">conduce</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">siguiente</span> <span class="pre">etapa</span></code> y la de <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">parte</span> <span class="pre">superior</span> <span class="pre">se</span> <span class="pre">utiliza</span> <span class="pre">para</span> <span class="pre">proporcionar</span> <span class="pre">la</span> <span class="pre">salida</span></code>, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>, en el tiempo <span class="math notranslate nohighlight">\(n\)</span>, a <code class="docutils literal notranslate"><span class="pre">través</span> <span class="pre">de,</span> <span class="pre">digamos,</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">softmax</span></code>, como en las RNN estándar en Eq. <a class="reference internal" href="#equation-rnn-system-eq">(8.14)</a>.</p></li>
</ul>
</section>
</div><div class="admonition-variantes-y-aplicaciones admonition">
<p class="admonition-title">Variantes y Aplicaciones</p>
<ul class="simple">
<li><p>Además de la estructura LSTM ya comentada, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">han</span> <span class="pre">propuesto</span> <span class="pre">diversas</span> <span class="pre">variantes</span></code>. Un extenso <code class="docutils literal notranslate"><span class="pre">estudio</span> <span class="pre">comparativo</span> <span class="pre">entre</span> <span class="pre">diferentes</span> <span class="pre">arquitecturas</span> <span class="pre">LSTM</span> <span class="pre">y</span> <span class="pre">RNN</span></code> se puede encontrar en <span id="id39">[<a class="reference internal" href="biblio.html#id40" title="Klaus Greff, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber. Lstm: a search space odyssey. IEEE transactions on neural networks and learning systems, 28(10):2222–2232, 2016.">Greff <em>et al.</em>, 2016</a>, <a class="reference internal" href="biblio.html#id41" title="Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent network architectures. In International conference on machine learning, 2342–2350. PMLR, 2015.">Jozefowicz <em>et al.</em>, 2015</a>]</span>. Las <code class="docutils literal notranslate"><span class="pre">RNNs</span> <span class="pre">y</span> <span class="pre">las</span> <span class="pre">LSTMs</span></code> se han utilizado con éxito en una <code class="docutils literal notranslate"><span class="pre">amplia</span> <span class="pre">gama</span> <span class="pre">de</span> <span class="pre">aplicaciones</span></code>, tales como:</p>
<ul>
<li><p>el <code class="docutils literal notranslate"><span class="pre">modelado</span> <span class="pre">del</span> <span class="pre">lenguaje</span></code> <span id="id40">[<a class="reference internal" href="biblio.html#id42" title="Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), 1017–1024. 2011.">Sutskever <em>et al.</em>, 2011</a>]</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">traducción</span> <span class="pre">de</span> <span class="pre">máquinas</span></code> <span id="id41">[<a class="reference internal" href="biblio.html#id43" title="Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. A recursive recurrent neural network for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1491–1500. 2014.">Liu <em>et al.</em>, 2014</a>]</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reconocimiento</span> <span class="pre">del</span> <span class="pre">habla</span></code> <span id="id42">[<a class="reference internal" href="biblio.html#id44" title="Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In International conference on machine learning, 1764–1772. PMLR, 2014.">Graves and Jaitly, 2014</a>]</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">visión</span> <span class="pre">artificial</span></code> para la generación de descriptores de imágenes <span id="id43">[<a class="reference internal" href="biblio.html#id45" title="Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, 3128–3137. 2015.">Karpathy and Fei-Fei, 2015</a>]</span>,</p></li>
<li><p>análisis de datos de fMRI para comprender la <code class="docutils literal notranslate"><span class="pre">dinámica</span> <span class="pre">temporal</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">redes</span> <span class="pre">cerebrales</span></code> asociadas <span id="id44">[<a class="reference internal" href="biblio.html#id46" title="Youngjoo Seo, Manuel Morante, Yannis Kopsinis, and Sergios Theodoridis. Unsupervised pre-training of the brain connectivity dynamic using residual d-net. In Neural Information Processing: 26th International Conference, ICONIP 2019, Sydney, NSW, Australia, December 12–15, 2019, Proceedings, Part III 26, 608–620. Springer, 2019.">Seo <em>et al.</em>, 2019</a>]</span>.</p></li>
<li><p>pronostico de <code class="docutils literal notranslate"><span class="pre">volatilidad</span> <span class="pre">de</span> <span class="pre">Bitcoin</span></code> <span id="id45">[<a class="reference internal" href="biblio.html#id47" title="Tiago E Pratas, Filipe R Ramos, and Lihki Rubio. Forecasting bitcoin volatility: exploring the potential of deep learning. Eurasian Economic Review, pages 1–21, 2023.">Pratas <em>et al.</em>, 2023</a>]</span></p></li>
</ul>
</li>
<li><p>Por ejemplo, en el <code class="docutils literal notranslate"><span class="pre">procesamiento</span> <span class="pre">del</span> <span class="pre">lenguaje</span></code> la <code class="docutils literal notranslate"><span class="pre">entrada</span> <span class="pre">suele</span> <span class="pre">ser</span> <span class="pre">una</span> <span class="pre">secuencia</span> <span class="pre">de</span> <span class="pre">palabras,</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">codifican</span> <span class="pre">como</span> <span class="pre">números</span></code> (son punteros al diccionario disponible). La <code class="docutils literal notranslate"><span class="pre">salida</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">secuencia</span> <span class="pre">de</span> <span class="pre">palabras</span> <span class="pre">que</span> <span class="pre">hay</span> <span class="pre">que</span> <span class="pre">predecir</span></code>. Durante el entrenamiento, se establece <span class="math notranslate nohighlight">\(\boldsymbol{y}_{n} = \boldsymbol{x}_{n+1}\)</span>. Es decir, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">red</span> <span class="pre">se</span> <span class="pre">entrena</span> <span class="pre">como</span> <span class="pre">un</span> <span class="pre">predictor</span> <span class="pre">no</span> <span class="pre">lineal</span></code>.</p></li>
</ul>
</div>
</section>
<section id="atencion-y-memoria">
<h2><span class="section-number">8.15. </span>Atención y Memoria<a class="headerlink" href="#atencion-y-memoria" title="Link to this heading">#</a></h2>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>El uso histórico de <code class="docutils literal notranslate"><span class="pre">esquemas</span> <span class="pre">de</span> <span class="pre">atención</span> <span class="pre">en</span> <span class="pre">redes</span> <span class="pre">neuronales</span></code> se inspira en el <code class="docutils literal notranslate"><span class="pre">mecanismo</span> <span class="pre">de</span> <span class="pre">atención</span> <span class="pre">humana</span></code>. En la visión humana, <code class="docutils literal notranslate"><span class="pre">nos</span> <span class="pre">centramos</span> <span class="pre">en</span> <span class="pre">información</span> <span class="pre">contextual</span> <span class="pre">importante</span></code> cuando observamos escenas. En el aprendizaje automático, los <code class="docutils literal notranslate"><span class="pre">modelos</span> <span class="pre">de</span> <span class="pre">atención</span> <span class="pre">implementan</span> <span class="pre">este</span> <span class="pre">concepto</span> <span class="pre">asignando</span> <span class="pre">pesos</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">variables</span> <span class="pre">que</span> <span class="pre">influyen</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">salida</span></code>. Por ejemplo, en una <code class="docutils literal notranslate"><span class="pre">RNN</span></code>, la salida <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span> depende del vector de estado <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>, pero <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">siempre</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">información</span> <span class="pre">más</span> <span class="pre">importante</span> <span class="pre">para</span> <span class="pre">tareas</span></code> como la traducción de idiomas (aunque el estado codifica la memoria hasta el tiempo más reciente, <span class="math notranslate nohighlight">\(n\)</span>.).</p></li>
</ul>
</div>
<div class="proof observation admonition" id="observation_ann12">
<p class="admonition-title"><span class="caption-number">Observation 8.6 </span> (Caso típico)</p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>En la <code class="docutils literal notranslate"><span class="pre">traducción</span> <span class="pre">del</span> <span class="pre">japonés</span> <span class="pre">al</span> <span class="pre">inglés,</span> <span class="pre">la</span> <span class="pre">última</span> <span class="pre">palabra</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">frase</span> <span class="pre">japonesa</span> <span class="pre">puede</span> <span class="pre">influir</span> <span class="pre">mucho</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">primera</span> <span class="pre">palabra</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">traducción</span> <span class="pre">inglesa</span></code>. Del mismo modo, en la <code class="docutils literal notranslate"><span class="pre">toma</span> <span class="pre">de</span> <span class="pre">decisiones,</span> <span class="pre">nuestras</span> <span class="pre">acciones</span> <span class="pre">dependen</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">experiencias</span> <span class="pre">pasadas</span> <span class="pre">acumuladas</span></code>, y algunas <code class="docutils literal notranslate"><span class="pre">experiencias</span> <span class="pre">pasadas</span> <span class="pre">específicas</span> <span class="pre">ejercen</span> <span class="pre">más</span> <span class="pre">influencia</span> <span class="pre">que</span> <span class="pre">las</span> <span class="pre">recientes</span></code>.</p></li>
</ul>
</section>
</div><ul class="simple">
<li><p>Para solucionar este problema, <code class="docutils literal notranslate"><span class="pre">los</span> <span class="pre">mecanismos</span> <span class="pre">de</span> <span class="pre">atención</span> <span class="pre">asignan</span> <span class="pre">pesos</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">anteriores</span></code>, lo que permite que <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">salida</span> <span class="pre">dependa</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">combinación</span> <span class="pre">ponderada</span> <span class="pre">de</span> <span class="pre">información</span> <span class="pre">previa</span></code>. Estas ponderaciones se aprenden durante el entrenamiento, lo que <code class="docutils literal notranslate"><span class="pre">permite</span> <span class="pre">al</span> <span class="pre">sistema</span> <span class="pre">dar</span> <span class="pre">prioridad</span> <span class="pre">a</span> <span class="pre">detalles</span> <span class="pre">contextuales</span> <span class="pre">significativos</span></code> a la hora de tomar decisiones o generar resultados.</p></li>
</ul>
<ul>
<li><p>Por ejemplo, el <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">de</span> <span class="pre">salida</span></code> puede modificarse para que <code class="docutils literal notranslate"><span class="pre">dependa</span> <span class="pre">de</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">calculados</span> <span class="pre">anteriormente</span></code></p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{y}}_{n}=f\left(\sum_{i=1}^{n}\alpha_{ni}\boldsymbol{h}_{i}+\boldsymbol{c}\right),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\alpha_{ni}\)</span> son los <code class="docutils literal notranslate"><span class="pre">correspondientes</span> <span class="pre">pesos</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(n\)</span>.</p>
</li>
<li><p>La idea anterior de <code class="docutils literal notranslate"><span class="pre">combinar</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">estado</span></code> ha sido empleada, en una formulación algo diferente, en el <code class="docutils literal notranslate"><span class="pre">sistema</span> <span class="pre">de</span> <span class="pre">traducción</span> <span class="pre">automática</span></code> (ver <span id="id46">[<a class="reference internal" href="biblio.html#id48" title="Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.">Bahdanau <em>et al.</em>, 2014</a>]</span>)</p></li>
</ul>
<figure class="align-center" id="attention-weights-grayscale-numref">
<a class="reference internal image-reference" href="_images/attention_weights_grayscale.png"><img alt="_images/attention_weights_grayscale.png" src="_images/attention_weights_grayscale.png" style="width: 381.6px; height: 434.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.43 </span><span class="caption-text">Valores de los pesos de atención en escala de grises. <a class="reference internal" href="#attention_example_grayscale">Example 8.1</a></span><a class="headerlink" href="#attention-weights-grayscale-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof example admonition" id="attention_example_grayscale">
<p class="admonition-title"><span class="caption-number">Example 8.1 </span></p>
<section class="example-content" id="proof-content">
<p>La <a class="reference internal" href="#attention-weights-grayscale-numref"><span class="std std-numref">Fig. 8.43</span></a> demuestra el razonamiento que subyace a la <code class="docutils literal notranslate"><span class="pre">utilización</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">mecanismo</span> <span class="pre">de</span> <span class="pre">ponderación</span></code>. La <code class="docutils literal notranslate"><span class="pre">entrada</span></code> está representada por las <code class="docutils literal notranslate"><span class="pre">palabras</span> <span class="pre">en</span> <span class="pre">francés</span></code>, mientras que las secuencias de <code class="docutils literal notranslate"><span class="pre">salida</span></code> correspondientes están representadas por las <code class="docutils literal notranslate"><span class="pre">palabras</span> <span class="pre">en</span> <span class="pre">inglés</span></code>. <code class="docutils literal notranslate"><span class="pre">La</span> <span class="pre">visualización</span> <span class="pre">representa</span> <span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">de</span> <span class="pre">atención</span> <span class="pre">como</span> <span class="pre">píxeles</span></code>, donde los <code class="docutils literal notranslate"><span class="pre">pesos</span> <span class="pre">más</span> <span class="pre">altos</span> <span class="pre">se</span> <span class="pre">representan</span> <span class="pre">como</span> <span class="pre">píxeles</span> <span class="pre">más</span> <span class="pre">blancos</span></code>. En particular, el <code class="docutils literal notranslate"><span class="pre">término</span> <span class="pre">&quot;produce&quot;</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">salida</span></code> es consecuencia de <code class="docutils literal notranslate"><span class="pre">asignar</span> <span class="pre">peso</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">información</span> <span class="pre">de</span> <span class="pre">tres</span> <span class="pre">puntos</span> <span class="pre">temporales</span> <span class="pre">consecutivos</span></code>, vinculados con las palabras <code class="docutils literal notranslate"><span class="pre">&quot;peut</span> <span class="pre">plus</span> <span class="pre">produire&quot;</span></code>, mientras que el término <code class="docutils literal notranslate"><span class="pre">&quot;destruction&quot;</span></code> está <code class="docutils literal notranslate"><span class="pre">asociado</span> <span class="pre">con</span> <span class="pre">dos</span> <span class="pre">palabras</span></code>, a saber, <code class="docutils literal notranslate"><span class="pre">&quot;la</span> <span class="pre">destruction&quot;</span></code>.</p>
</section>
</div><div class="proof observation admonition" id="observation_ann13">
<p class="admonition-title"><span class="caption-number">Observation 8.7 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>Una ventaja notable de <code class="docutils literal notranslate"><span class="pre">incorporar</span> <span class="pre">un</span> <span class="pre">mecanismo</span> <span class="pre">de</span> <span class="pre">atención</span></code> al modelo es la <code class="docutils literal notranslate"><span class="pre">transparencia</span> <span class="pre">que</span> <span class="pre">aporta</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">acciones</span> <span class="pre">del</span> <span class="pre">modelo</span></code> y a la formación de la información de salida. Esto resulta especialmente valioso cuando la <code class="docutils literal notranslate"><span class="pre">interpretabilidad</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">red</span> <span class="pre">es</span> <span class="pre">una</span> <span class="pre">preocupación</span></code>, ya que permite comprender el <code class="docutils literal notranslate"><span class="pre">&quot;por</span> <span class="pre">qué&quot;</span> <span class="pre">y</span> <span class="pre">el</span> <span class="pre">&quot;cómo&quot;</span> <span class="pre">del</span> <span class="pre">proceso</span> <span class="pre">de</span> <span class="pre">toma</span> <span class="pre">de</span> <span class="pre">decisiones</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">red</span></code>.</p></li>
</ul>
</section>
</div></section>
<section id="entrenamiento-adversario">
<h2><span class="section-number">8.16. </span>Entrenamiento adversario<a class="headerlink" href="#entrenamiento-adversario" title="Link to this heading">#</a></h2>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">neuronales</span> <span class="pre">profundas</span></code> lideran en la obtención de un <code class="docutils literal notranslate"><span class="pre">rendimiento</span> <span class="pre">y</span> <span class="pre">precisión</span> <span class="pre">sobresalientes</span></code>, frecuentemente comparables e incluso en <code class="docutils literal notranslate"><span class="pre">algunos</span> <span class="pre">casos</span> <span class="pre">superiores</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">habilidades</span> <span class="pre">humanas</span></code>. Sin embargo, persiste el <code class="docutils literal notranslate"><span class="pre">desafío</span> <span class="pre">de</span> <span class="pre">afirmar</span> <span class="pre">que</span> <span class="pre">estos</span> <span class="pre">modelos</span> <span class="pre">realmente</span> <span class="pre">&quot;comprenden&quot;</span> <span class="pre">las</span> <span class="pre">tareas</span> <span class="pre">que</span> <span class="pre">han</span> <span class="pre">&quot;aprendido&quot;</span> <span class="pre">a</span> <span class="pre">realizar</span></code>, a pesar de su habilidad para predecir etiquetas precisas en tareas de clasificación con una confianza considerablemente alta.</p></li>
<li><p>Es <code class="docutils literal notranslate"><span class="pre">viable</span> <span class="pre">generar</span> <span class="pre">ejemplos</span> <span class="pre">adversos</span> <span class="pre">que</span> <span class="pre">sistemáticamente</span> <span class="pre">engañan</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">modelos</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">automático</span></code>. Aquí, <code class="docutils literal notranslate"><span class="pre">&quot;adversario&quot;</span> <span class="pre">denota</span> <span class="pre">la</span> <span class="pre">introducción</span> <span class="pre">intencional</span> <span class="pre">de</span> <span class="pre">pequeñas</span> <span class="pre">perturbaciones</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">patrones</span> <span class="pre">dentro</span> <span class="pre">del</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">datos</span> <span class="pre">de</span> <span class="pre">entrada</span></code>, lo que resulta en una <code class="docutils literal notranslate"><span class="pre">alta</span> <span class="pre">probabilidad</span> <span class="pre">de</span> <span class="pre">clasificación</span> <span class="pre">incorrecta</span></code>. Notablemente, estas sutiles perturbaciones de ruido son <code class="docutils literal notranslate"><span class="pre">apenas</span> <span class="pre">detectables</span> <span class="pre">para</span> <span class="pre">los</span> <span class="pre">sentidos</span> <span class="pre">humanos</span></code>, ya sea en forma de imágenes o música.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>La <a class="reference internal" href="#wrong-image-classification-numref"><span class="std std-numref">Fig. 8.44</span></a> muestra una serie de <code class="docutils literal notranslate"><span class="pre">nueve</span> <span class="pre">imágenes</span></code>. Se ha entrenado una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">neuronal</span> <span class="pre">(AlexNet)</span></code> para <code class="docutils literal notranslate"><span class="pre">discernir</span> <span class="pre">los</span> <span class="pre">contenidos</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">imágenes</span></code>. El trío <code class="docutils literal notranslate"><span class="pre">izquierdo</span> <span class="pre">de</span> <span class="pre">imágenes</span></code>, tomadas del respectivo conjunto de pruebas, fueron <code class="docutils literal notranslate"><span class="pre">identificadas</span> <span class="pre">con</span> <span class="pre">precisión</span></code>. Las imágenes en el <code class="docutils literal notranslate"><span class="pre">centro</span> <span class="pre">son</span> <span class="pre">versiones</span> <span class="pre">ruidosas</span></code> añadidas a las originales de la izquierda. Las composiciones <code class="docutils literal notranslate"><span class="pre">resultantes</span> <span class="pre">se</span> <span class="pre">presentan</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">derecha</span></code>. Mientras los seres <code class="docutils literal notranslate"><span class="pre">humanos</span> <span class="pre">anticipan</span> <span class="pre">sin</span> <span class="pre">dificultad</span> <span class="pre">las</span> <span class="pre">etiquetas</span> <span class="pre">correctas</span></code>, <code class="docutils literal notranslate"><span class="pre">AlexNet</span> <span class="pre">catalogó</span> <span class="pre">de</span> <span class="pre">forma</span> <span class="pre">errónea</span> <span class="pre">las</span> <span class="pre">tres</span> <span class="pre">imágenes</span> <span class="pre">como</span> <span class="pre">&quot;avestruz,</span> <span class="pre">struthio</span> <span class="pre">camelus&quot;</span></code>.</p></li>
</ul>
<figure class="align-center" id="wrong-image-classification-numref">
<a class="reference internal image-reference" href="_images/wrong_image_classification.png"><img alt="_images/wrong_image_classification.png" src="_images/wrong_image_classification.png" style="width: 672.0px; height: 661.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.44 </span><span class="caption-text">Imágenes de la <code class="docutils literal notranslate"><span class="pre">izquierda</span></code> se han <code class="docutils literal notranslate"><span class="pre">clasificado</span> <span class="pre">correctamente</span></code>. Todas las <code class="docutils literal notranslate"><span class="pre">imágenes</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">derecha</span></code> han sido <code class="docutils literal notranslate"><span class="pre">clasificadas</span> <span class="pre">como</span> <span class="pre">&quot;avestruz,</span> <span class="pre">Struthio</span> <span class="pre">camelus&quot;</span></code>. Fuente <span id="id47">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#wrong-image-classification-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>La causa de este <code class="docutils literal notranslate"><span class="pre">comportamiento</span> <span class="pre">aparentemente</span> <span class="pre">&quot;peculiar&quot;</span></code> parece estar vinculada a la <code class="docutils literal notranslate"><span class="pre">naturaleza</span> <span class="pre">altamente</span> <span class="pre">dimensional</span> <span class="pre">del</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">entrada</span></code>. Por lo general, en un contexto de aprendizaje, se cumple el <code class="docutils literal notranslate"><span class="pre">supuesto</span> <span class="pre">de</span> <span class="pre">suavidad</span></code>.  Esto implica que <code class="docutils literal notranslate"><span class="pre">para</span> <span class="pre">un</span> <span class="pre">valor</span> <span class="pre">positivo</span> <span class="pre">suficientemente</span> <span class="pre">pequeño</span></code> <span class="math notranslate nohighlight">\(\epsilon\)</span> y un <code class="docutils literal notranslate"><span class="pre">patrón</span> <span class="pre">de</span> <span class="pre">entrada</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, se espera que el patrón</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{x}':= \boldsymbol{x} + \boldsymbol{v},~\text{donde}~\boldsymbol{v} : \|\boldsymbol{v}\|\leq\epsilon,
    \]</div>
<p>se <code class="docutils literal notranslate"><span class="pre">clasifique</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">misma</span> <span class="pre">categoría</span> <span class="pre">que</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> <code class="docutils literal notranslate"><span class="pre">con</span> <span class="pre">una</span> <span class="pre">alta</span> <span class="pre">probabilidad</span></code>.</p>
</li>
<li><p>El impacto de la <code class="docutils literal notranslate"><span class="pre">alta</span> <span class="pre">dimensionalidad</span> <span class="pre">(número</span> <span class="pre">de</span> <span class="pre">características</span> <span class="pre">igual</span> <span class="pre">o</span> <span class="pre">superior</span> <span class="pre">al</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">observaciones)</span></code> en la condición de suavidad se hace evidente, sobre todo en el contexto de un <code class="docutils literal notranslate"><span class="pre">clasificador</span> <span class="pre">lineal</span></code>. Consideremos un <code class="docutils literal notranslate"><span class="pre">clasificador</span></code> entrenado <code class="docutils literal notranslate"><span class="pre">descrito</span> <span class="pre">por</span> <span class="pre">sus</span> <span class="pre">parámetros</span></code>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. Dado un <code class="docutils literal notranslate"><span class="pre">patrón</span> <span class="pre">de</span> <span class="pre">entrada</span></code>, <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, la <code class="docutils literal notranslate"><span class="pre">etiqueta</span> <span class="pre">se</span> <span class="pre">determina</span> <span class="pre">basándose</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">signo</span> <span class="pre">del</span> <span class="pre">producto</span> <span class="pre">interior</span></code>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x}\)</span>. Para el escenario en el que interviene <span class="math notranslate nohighlight">\(\boldsymbol{x}'\)</span>, el producto interior se expresa como</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{T}\boldsymbol{x}' = \boldsymbol{\theta}^{T}\boldsymbol{x} + \boldsymbol{\theta}^{T}\boldsymbol{v}.
\]</div>
<ul class="simple">
<li><p>Si establecemos intencionadamente <span class="math notranslate nohighlight">\(\boldsymbol{v} = \pm\epsilon\cdot\text{sgn}(\boldsymbol{\theta})\)</span>, donde la <code class="docutils literal notranslate"><span class="pre">operación</span> <span class="pre">signo</span> <span class="pre">se</span> <span class="pre">aplica</span> <span class="pre">elemento</span> <span class="pre">a</span> <span class="pre">elemento</span></code>, se obtiene un resultado digno de mención. Entonces</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{T}\boldsymbol{x}'=\boldsymbol{\theta}^{T}\boldsymbol{x}+\boldsymbol{\theta}^{T}\boldsymbol{v}=\boldsymbol{\theta}^{T}\pm\epsilon\sum_{i=1}^{l}|\theta_{i}|.
\]</div>
<ul class="simple">
<li><p>Por lo tanto, cuando la <code class="docutils literal notranslate"><span class="pre">dimensionalidad</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">entrada</span></code> <span class="math notranslate nohighlight">\(l\)</span> <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">alta</span></code>, es probable que se produzcan <code class="docutils literal notranslate"><span class="pre">diferencias</span> <span class="pre">significativas</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">del</span> <span class="pre">producto</span> <span class="pre">interno</span></code>, lo que podría causar etiquetas de <code class="docutils literal notranslate"><span class="pre">predicción</span> <span class="pre">distintas</span></code> tanto para <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> como para <span class="math notranslate nohighlight">\(\boldsymbol{x}'\)</span>. En términos sencillos, la <code class="docutils literal notranslate"><span class="pre">interacción</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">linealidad</span> <span class="pre">y</span> <span class="pre">la</span> <span class="pre">alta</span> <span class="pre">dimensionalidad</span> <span class="pre">rompe</span> <span class="pre">el</span> <span class="pre">supuesto</span> <span class="pre">de</span> <span class="pre">suavidad</span></code>.</p></li>
</ul>
<div class="proof observation admonition" id="observation_ann14">
<p class="admonition-title"><span class="caption-number">Observation 8.8 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>A pesar de que los <code class="docutils literal notranslate"><span class="pre">ejemplos</span> <span class="pre">adversos</span> <span class="pre">son</span> <span class="pre">poco</span> <span class="pre">frecuentes</span></code> en los datos de entrada (tanto en los conjuntos de <code class="docutils literal notranslate"><span class="pre">entrenamiento</span></code> como en los de <code class="docutils literal notranslate"><span class="pre">prueba</span></code>), <code class="docutils literal notranslate"><span class="pre">su</span> <span class="pre">existencia</span> <span class="pre">sigue</span> <span class="pre">siendo</span> <span class="pre">desconcertante</span></code>. Además, pueden aprovecharse para <code class="docutils literal notranslate"><span class="pre">engañar</span> <span class="pre">deliberadamente</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">redes</span></code>. En consecuencia, han surgido varias <code class="docutils literal notranslate"><span class="pre">técnicas</span> <span class="pre">para</span> <span class="pre">reforzar</span> <span class="pre">la</span> <span class="pre">resistencia</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">redes</span> <span class="pre">frente</span> <span class="pre">a</span> <span class="pre">adversarios</span></code>.</p></li>
</ul>
</section>
</div><ul class="simple">
<li><p>Uno estudio descrito en <span id="id48">[<a class="reference internal" href="biblio.html#id49" title="Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.">Szegedy <em>et al.</em>, 2013</a>]</span>, consiste en <code class="docutils literal notranslate"><span class="pre">generar</span> <span class="pre">ejemplos</span> <span class="pre">adversos</span> <span class="pre">e</span> <span class="pre">incorporarlos</span> <span class="pre">de</span> <span class="pre">nuevo</span> <span class="pre">al</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>. Esta estrategia funciona como una forma de <code class="docutils literal notranslate"><span class="pre">regularización</span> <span class="pre">a</span> <span class="pre">través</span> <span class="pre">del</span> <span class="pre">aumento</span> <span class="pre">de</span> <span class="pre">datos</span></code>. Sin embargo, una perspectiva alternativa presentada en <span id="id49">[<a class="reference internal" href="biblio.html#id50" title="Corey Kereliuk, Bob L Sturm, and Jan Larsen. Deep learning and music adversaries. IEEE Transactions on Multimedia, 17(11):2059–2071, 2015.">Kereliuk <em>et al.</em>, 2015</a>]</span> argumenta que este método <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">mejoró</span> <span class="pre">notablemente</span> <span class="pre">el</span> <span class="pre">rendimiento</span> <span class="pre">cuando</span> <span class="pre">se</span> <span class="pre">aplicó</span> <span class="pre">a</span> <span class="pre">datos</span> <span class="pre">musicales</span></code>.</p></li>
</ul>
<ul>
<li><p>En <span id="id50">[<a class="reference internal" href="biblio.html#id51" title="Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.">Goodfellow <em>et al.</em>, 2014</a>]</span>, la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span></code> <span class="math notranslate nohighlight">\(J\)</span> <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">modifica</span> <span class="pre">adecuadamente</span></code> como</p>
<div class="math notranslate nohighlight">
\[
    J'(\boldsymbol{\theta}, \boldsymbol{x}, \boldsymbol{y})=\alpha J(\boldsymbol{\theta}, \boldsymbol{x}, \boldsymbol{y})+(1-\alpha)J(\boldsymbol{\theta}, \boldsymbol{x}+\Delta\boldsymbol{x}, \boldsymbol{y}),~0&lt;\alpha&lt;1,
    \]</div>
<p>donde</p>
<div class="math notranslate nohighlight">
\[
    \Delta\boldsymbol{x}=\epsilon\cdot\text{sgn}\left(\frac{\partial}{\partial\boldsymbol{x}}J(\boldsymbol{\theta}, \boldsymbol{x}, \boldsymbol{y})\right),~\epsilon&gt;0,
    \]</div>
<p>es una <code class="docutils literal notranslate"><span class="pre">dirección</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">perturbación</span> <span class="pre">adversarial</span></code>.</p>
</li>
<li><p>Es importante señalar que los <code class="docutils literal notranslate"><span class="pre">ejemplos</span> <span class="pre">adversarios</span></code> siguen siendo un foco de investigación en evolución dinámica. Se subrayan los <code class="docutils literal notranslate"><span class="pre">peligros</span> <span class="pre">potenciales,</span> <span class="pre">como</span> <span class="pre">la</span> <span class="pre">manipulación</span> <span class="pre">de</span> <span class="pre">vehículos</span> <span class="pre">autónomos</span> <span class="pre">utilizando</span> <span class="pre">muestras</span> <span class="pre">adversarios</span></code>, entre otros (ver <span id="id51">[<a class="reference internal" href="biblio.html#id53" title="Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In Artificial intelligence safety and security, pages 99–112. Chapman and Hall/CRC, 2018.">Kurakin <em>et al.</em>, 2018</a>, <a class="reference internal" href="biblio.html#id52" title="Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security, 506–519. 2017.">Papernot <em>et al.</em>, 2017</a>, <a class="reference internal" href="biblio.html#id54" title="Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of security and privacy in machine learning. arXiv preprint arXiv:1611.03814, 2016.">Papernot <em>et al.</em>, 2016</a>]</span>).</p></li>
</ul>
</section>
<section id="modelos-generativos-profundos">
<h2><span class="section-number">8.17. </span>Modelos Generativos Profundos<a class="headerlink" href="#modelos-generativos-profundos" title="Link to this heading">#</a></h2>
<section id="maquina-de-boltzmann-restringida">
<h3><span class="section-number">8.17.1. </span>Máquina de Boltzmann Restringida<a class="headerlink" href="#maquina-de-boltzmann-restringida" title="Link to this heading">#</a></h3>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Una <code class="docutils literal notranslate"><span class="pre">máquina</span> <span class="pre">de</span> <span class="pre">Boltzmann</span> <span class="pre">restringida</span> <span class="pre">(RBM)</span></code> es una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">neuronal</span> <span class="pre">artificial</span> <span class="pre">estocástica</span> <span class="pre">generativa</span></code> que puede <code class="docutils literal notranslate"><span class="pre">aprender</span> <span class="pre">una</span> <span class="pre">distribución</span> <span class="pre">de</span> <span class="pre">probabilidad</span> <span class="pre">sobre</span> <span class="pre">su</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">entradas</span></code> bajo ajuste de parámetros. Las RBM fueron estudiadas inicialmente con el nombre de <code class="docutils literal notranslate"><span class="pre">Harmonium</span> <span class="pre">por</span> <span class="pre">Paul</span> <span class="pre">Smolensky</span> <span class="pre">en</span> <span class="pre">1986</span></code>, y adquirieron prominencia después de que <code class="docutils literal notranslate"><span class="pre">Geoffrey</span> <span class="pre">Hinton</span></code> y sus colaboradores propusieron para estas, algoritmos de aprendizaje rápido a mediados de 2000.</p></li>
<li><p>Las <code class="docutils literal notranslate"><span class="pre">RBM</span></code> han encontrado <code class="docutils literal notranslate"><span class="pre">aplicaciones</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">reducción</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">dimensionalidad,</span> <span class="pre">clasificación,</span> <span class="pre">filtrado,</span> <span class="pre">aprendizaje</span> <span class="pre">de</span> <span class="pre">funciones,</span> <span class="pre">modelado</span> <span class="pre">de</span> <span class="pre">temas</span> <span class="pre">e</span> <span class="pre">incluso</span> <span class="pre">en</span> <span class="pre">muchas</span> <span class="pre">mecánicas</span> <span class="pre">cuánticas</span> <span class="pre">corporales</span></code>. Pueden ser entrenados en <code class="docutils literal notranslate"><span class="pre">forma</span> <span class="pre">supervisada</span> <span class="pre">o</span> <span class="pre">no</span> <span class="pre">supervisada</span></code>, dependiendo de la tarea.</p></li>
</ul>
</div>
<ul>
<li><p>El modelo gráfico de la <a class="reference internal" href="#rboltzmann-model-numref"><span class="std std-numref">Fig. 8.45</span></a> ilustra una <code class="docutils literal notranslate"><span class="pre">máquina</span> <span class="pre">de</span> <span class="pre">Boltzmann</span> <span class="pre">restringida</span> <span class="pre">(RBM)</span></code>. La máquina de Boltzmann utiliza la siguiente <code class="docutils literal notranslate"><span class="pre">distribución</span> <span class="pre">de</span> <span class="pre">probabilidad</span> <span class="pre">conjunta</span> <span class="pre">de</span> <span class="pre">Boltzmann</span></code> definida como</p>
<div class="math notranslate nohighlight">
\[
    p(x_{1}, \dots, x_{l}):=p(\boldsymbol{x})=\frac{1}{Z}\exp\left(-\sum_{i}\left(\sum_{j&gt;i}\theta_{ij}x_{i}x_{j}+\theta_{i0}x_{i}\right)\right),
    \]</div>
<p>donde cada <code class="docutils literal notranslate"><span class="pre">variable</span> <span class="pre">aleatoria</span> <span class="pre">toma</span> <span class="pre">valores</span> <span class="pre">binarios</span></code> en <span class="math notranslate nohighlight">\(\{-1, 1\}\)</span>, <span class="math notranslate nohighlight">\(\theta_{ij}=0\)</span> si los <code class="docutils literal notranslate"><span class="pre">nodos</span> <span class="pre">respectivos</span> <span class="pre">no</span> <span class="pre">están</span> <span class="pre">conectados</span></code> y la constante <span class="math notranslate nohighlight">\(Z\)</span> se conoce como <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">partición</span></code> y es la <code class="docutils literal notranslate"><span class="pre">constante</span> <span class="pre">normalizadora</span></code> para <code class="docutils literal notranslate"><span class="pre">garantizar</span> <span class="pre">que</span></code> <span class="math notranslate nohighlight">\(p(x_{1}, \dots, x_{l})\)</span> <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">una</span> <span class="pre">distribución</span> <span class="pre">de</span> <span class="pre">probabilidad</span></code>.</p>
</li>
<li><p>Es importante destacar que los <code class="docutils literal notranslate"><span class="pre">nodos</span> <span class="pre">dentro</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">misma</span> <span class="pre">capa</span> <span class="pre">carecen</span> <span class="pre">de</span> <span class="pre">interconexiones</span></code>. Esta arquitectura abarca <code class="docutils literal notranslate"><span class="pre">nodos</span> <span class="pre">visibles</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">inferior</span></code> que reciben observaciones, mientras que la <code class="docutils literal notranslate"><span class="pre">capa</span> <span class="pre">superior</span> <span class="pre">contiene</span> <span class="pre">nodos</span> <span class="pre">vinculados</span> <span class="pre">a</span> <span class="pre">variables</span> <span class="pre">ocultas</span></code>. Cabe destacar que <code class="docutils literal notranslate"><span class="pre">sólo</span> <span class="pre">los</span> <span class="pre">nodos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">inferior</span> <span class="pre">reciben</span> <span class="pre">observaciones</span></code>. Es posible construir <code class="docutils literal notranslate"><span class="pre">RBM</span> <span class="pre">profundos</span></code> apilando múltiples <code class="docutils literal notranslate"><span class="pre">RBM</span></code> unos sobre otros. De acuerdo con la <code class="docutils literal notranslate"><span class="pre">definición</span> <span class="pre">general</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">máquina</span> <span class="pre">de</span> <span class="pre">Boltzmann</span></code>, la distribución conjunta de las variables aleatorias implicadas puede expresarse como:</p>
<div class="math notranslate nohighlight">
\[
    P(v_{1}, \dots, v_{J}, h_{1}, \dots, h_{I})=\frac{1}{Z}\exp(-E(\boldsymbol{v}, \boldsymbol{h})),
    \]</div>
<p>donde hemos utilizado símbolos diferentes para las <span class="math notranslate nohighlight">\(J\)</span> <code class="docutils literal notranslate"><span class="pre">variables</span> <span class="pre">visibles</span></code> (<span class="math notranslate nohighlight">\(v_{j},~j = 1, 2,\dots, J\)</span>) y las <span class="math notranslate nohighlight">\(I\)</span> <code class="docutils literal notranslate"><span class="pre">variables</span> <span class="pre">ocultas</span></code> (<span class="math notranslate nohighlight">\(h_{i},~i = 1, 2,\dots, I\)</span>).</p>
</li>
</ul>
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">energía</span> <span class="pre">no</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">salida</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">RBM</span></code>, por lo contrario, es una métrica para medir la calidad de un modelo. Produce un <code class="docutils literal notranslate"><span class="pre">valor</span> <span class="pre">escalar</span> <span class="pre">que</span> <span class="pre">corresponde</span> <span class="pre">básicamente</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">configuración</span> <span class="pre">del</span> <span class="pre">modelo</span></code> y es un <code class="docutils literal notranslate"><span class="pre">indicador</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">probabilidad</span> <span class="pre">de</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">modelo</span> <span class="pre">esté</span> <span class="pre">en</span> <span class="pre">esa</span> <span class="pre">configuración</span></code>. Si el modelo está configurado para favorecer una energía baja, las <code class="docutils literal notranslate"><span class="pre">configuraciones</span> <span class="pre">que</span> <span class="pre">conduzcan</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">energía</span> <span class="pre">baja</span> <span class="pre">tendrán</span> <span class="pre">una</span> <span class="pre">probabilidad</span> <span class="pre">más</span> <span class="pre">alta</span></code>.</p></li>
</ul>
<figure class="align-center" id="rboltzmann-model-numref">
<a class="reference internal image-reference" href="_images/rboltzmann_model.png"><img alt="_images/rboltzmann_model.png" src="_images/rboltzmann_model.png" style="width: 274.4px; height: 121.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.45 </span><span class="caption-text">Arquitectura de <code class="docutils literal notranslate"><span class="pre">modelo</span> <span class="pre">(RBM)</span></code>. Fuente <span id="id52">[<a class="reference internal" href="biblio.html#id17" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#rboltzmann-model-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">energía</span></code> se define en <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">desconocidos</span></code>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
E(\boldsymbol{v}, \boldsymbol{h})=-\sum_{i=1}^{I}\sum_{j=1}^{J}\theta_{ij}h_{i}v_{j}-\sum_{i=1}^{I}b_{i}h_{i}-\sum_{j=1}^{J}c_{j}v_{j}.                                                                                                                                                                              
\]</div>
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">constante</span> <span class="pre">de</span> <span class="pre">normalización</span></code> se obtiene como</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
Z=\sum_{v}\sum_{h}\exp(-E(\boldsymbol{v}, \boldsymbol{h})).
\]</div>
<ul class="simple">
<li><p>Nuestro enfoque será en <code class="docutils literal notranslate"><span class="pre">variables</span> <span class="pre">discretas</span></code>, lo que implica que las <code class="docutils literal notranslate"><span class="pre">distribuciones</span> <span class="pre">relacionadas</span> <span class="pre">son</span> <span class="pre">de</span> <span class="pre">carácter</span> <span class="pre">probabilístico</span></code>. Específicamente, nos centraremos en <code class="docutils literal notranslate"><span class="pre">variables</span> <span class="pre">de</span> <span class="pre">naturaleza</span> <span class="pre">binaria</span></code>, es decir, aquellas que poseen únicamente dos posibles valores, <span class="math notranslate nohighlight">\(v_{j}, h_{i}\in\{0, 1\},\)</span> <span class="math notranslate nohighlight">\(j=1,\dots,J\)</span>, <span class="math notranslate nohighlight">\(i=1,\dots,I\)</span>.</p></li>
</ul>
<ul>
<li><p>El objetivo al <code class="docutils literal notranslate"><span class="pre">entrenar</span> <span class="pre">una</span> <span class="pre">Máquina</span> <span class="pre">de</span> <span class="pre">Boltzmann</span> <span class="pre">Restringida</span> <span class="pre">(RBM)</span></code> consiste en <code class="docutils literal notranslate"><span class="pre">adquirir</span> <span class="pre">el</span> <span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">desconocidos</span></code> <span class="math notranslate nohighlight">\(\theta_{ij}, b_{i}, c_{j}\)</span>, los cuales serán designados colectivamente como, <span class="math notranslate nohighlight">\(\boldsymbol{\Theta},\boldsymbol{b}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span> respectivamente. Un enfoque primordial para lograr esto es <code class="docutils literal notranslate"><span class="pre">maximizar</span> <span class="pre">la</span> <span class="pre">log-verosimilitud</span></code>, utilizando <span class="math notranslate nohighlight">\(N\)</span> observaciones de las <code class="docutils literal notranslate"><span class="pre">variables</span> <span class="pre">visibles</span></code>, representadas como <span class="math notranslate nohighlight">\(v_{n},~n=1,\dots, N\)</span>, donde</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{v}_{n}=[v_{1n}, \dots, v_{Jn}]^{T},
    \]</div>
<p>es el vector que contiene las <code class="docutils literal notranslate"><span class="pre">observaciones</span></code> correspondientes en el <code class="docutils literal notranslate"><span class="pre">momento</span></code> <span class="math notranslate nohighlight">\(n\)</span>.</p>
</li>
</ul>
<ul>
<li><p>Diremos que los <code class="docutils literal notranslate"><span class="pre">nodos</span> <span class="pre">visibles</span> <span class="pre">están</span> <span class="pre">fijados</span> <span class="pre">en</span> <span class="pre">las</span> <span class="pre">observaciones</span> <span class="pre">respectivas</span></code>. La correspondiente <code class="docutils literal notranslate"><span class="pre">log-verosimilitud</span> <span class="pre">(promedio)</span> </code> se expresa como sigue:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    L(\boldsymbol{\Theta}, \boldsymbol{b}, \boldsymbol{c})&amp;=\frac{1}{N}\sum_{n=1}^{N}\ln P(\boldsymbol{v}_{n};\boldsymbol{\Theta}, \boldsymbol{b}, \boldsymbol{c})\\
    &amp;=\frac{1}{N}\sum_{n=1}^{N}\ln\left(\frac{1}{Z}\sum_{\boldsymbol{h}}\exp(-E(\boldsymbol{v}_{n}, \boldsymbol{h}; \boldsymbol{\Theta}, \boldsymbol{b}, \boldsymbol{c}))\right)\\
    &amp;=\frac{1}{N}\sum_{n=1}^{N}\ln\left(\sum_{\boldsymbol{h}}\exp(-E(\boldsymbol{v}_{n}, \boldsymbol{h}; \boldsymbol{\Theta}, \boldsymbol{b}, \boldsymbol{c}))\right)-\ln\sum_{\boldsymbol{v}}\sum_{\boldsymbol{h}}\exp\left(-E(\boldsymbol{v}, \boldsymbol{h})\right),
    \end{align*}
    \end{split}\]</div>
<p>donde el índice “<span class="math notranslate nohighlight">\(n\)</span>” en la energía se refiere a las <code class="docutils literal notranslate"><span class="pre">observaciones</span> <span class="pre">respectivas</span> <span class="pre">en</span> <span class="pre">las</span> <span class="pre">cuales</span> <span class="pre">los</span> <span class="pre">nodos</span> <span class="pre">visibles</span> <span class="pre">han</span> <span class="pre">sido</span> <span class="pre">fijados</span></code>, y el símbolo “<span class="math notranslate nohighlight">\(\Theta\)</span>” ha sido <code class="docutils literal notranslate"><span class="pre">incluido</span> <span class="pre">explícitamente</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">notación</span></code>.</p>
</li>
</ul>
<ul>
<li><p>Tomando la derivada de <span class="math notranslate nohighlight">\(L(\boldsymbol{\Theta}, \boldsymbol{b}, \boldsymbol{c})\)</span> respecto a <span class="math notranslate nohighlight">\(\theta_{ij}\)</span> (similar es el caso de las derivadas respecto a respecto a <span class="math notranslate nohighlight">\(b_{i}\)</span> y <span class="math notranslate nohighlight">\(c_{j}\)</span>) y aplicando las <code class="docutils literal notranslate"><span class="pre">propiedades</span> <span class="pre">estándar</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">derivadas</span></code>, se tiene que (<code class="docutils literal notranslate"><span class="pre">verifíquelo</span></code>)</p>
<div class="math notranslate nohighlight" id="equation-gradient-boltzmann-eq">
<span class="eqno">(8.18)<a class="headerlink" href="#equation-gradient-boltzmann-eq" title="Link to this equation">#</a></span>\[
    \frac{\partial L(\boldsymbol{\Theta}, \boldsymbol{b}, \boldsymbol{c})}{\partial\theta_{ij}}=\frac{1}{N}\sum_{n=1}^{N}\left(\sum_{\boldsymbol{h}}P(\boldsymbol{h}|\boldsymbol{v}_{n})h_{i}v_{jn}\right)-\sum_{\boldsymbol{v}}\sum_{\boldsymbol{h}}P(\boldsymbol{v}, \boldsymbol{h})h_{i}v_{j},
    \]</div>
<p>donde</p>
<div class="math notranslate nohighlight">
\[
    P(\boldsymbol{h}|\boldsymbol{v})=\frac{P(\boldsymbol{v}|\boldsymbol{h})}{\sum_{\boldsymbol{h}'}P(\boldsymbol{v}, \boldsymbol{h}')}.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>El gradiente en la Eq. <a class="reference internal" href="#equation-gradient-boltzmann-eq">(8.18)</a> consta de <code class="docutils literal notranslate"><span class="pre">dos</span> <span class="pre">términos</span></code>. El primero puede ser <code class="docutils literal notranslate"><span class="pre">calculado</span> <span class="pre">una</span> <span class="pre">vez</span> <span class="pre">que</span></code> <span class="math notranslate nohighlight">\(P(\boldsymbol{h}|\boldsymbol{v})\)</span> <code class="docutils literal notranslate"><span class="pre">está</span> <span class="pre">disponible</span></code>. Básicamente, este término es la <code class="docutils literal notranslate"><span class="pre">tasa</span> <span class="pre">media</span> <span class="pre">de</span> <span class="pre">disparo</span> <span class="pre">o</span> <span class="pre">correlación</span></code> cuando la Máquina de Boltzmann Restringida <code class="docutils literal notranslate"><span class="pre">(RBM)</span> <span class="pre">está</span> <span class="pre">operando</span> <span class="pre">en</span> <span class="pre">su</span> <span class="pre">fase</span> <span class="pre">fijada</span></code>; a menudo, nos referimos a esto como la <code class="docutils literal notranslate"><span class="pre">fase</span> <span class="pre">positiva</span></code> (<code class="docutils literal notranslate"><span class="pre">regla</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">de</span> <span class="pre">Hebb</span></code> <span id="id53">[<a class="reference internal" href="biblio.html#id55" title="Donald Olding Hebb. The organization of behavior: A neuropsychological theory. Psychology press, 2005.">Hebb, 2005</a>]</span>), y el término se denota como <span class="math notranslate nohighlight">\(\langle h{i}v_{j}\rangle^{+}\)</span>. El segundo término es la <code class="docutils literal notranslate"><span class="pre">correlación</span> <span class="pre">correspondiente</span> <span class="pre">cuando</span> <span class="pre">la</span> <span class="pre">RBM</span> <span class="pre">está</span> <span class="pre">funcionando</span> <span class="pre">en</span> <span class="pre">su</span> <span class="pre">llamada</span> <span class="pre">fase</span> <span class="pre">libre</span> <span class="pre">o</span> <span class="pre">negativa</span></code>, y se denota como <span class="math notranslate nohighlight">\(\langle h_{i}v_{j}\rangle^{-}\)</span>. Por lo tanto, un <code class="docutils literal notranslate"><span class="pre">esquema</span> <span class="pre">de</span> <span class="pre">aumento</span> <span class="pre">de</span> <span class="pre">gradiente</span></code> para maximizar la log-verosimilitud tendrá la forma:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\theta_{ij}(\text{new})=\theta_{ij}(old)+\mu(\langle h{i}v_{j}\rangle^{+}-\langle h_{i}v_{j}\rangle^{-}).
\]</div>
</section>
</section>
<section id="autoencoders">
<h2><span class="section-number">8.18. </span>Autoencoders<a class="headerlink" href="#autoencoders" title="Link to this heading">#</a></h2>
<ul>
<li><p>Los <code class="docutils literal notranslate"><span class="pre">autoencoders</span></code> han sido propuestos como <code class="docutils literal notranslate"><span class="pre">métodos</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">reducción</span> <span class="pre">de</span> <span class="pre">dimensionalidad</span></code>. Un <code class="docutils literal notranslate"><span class="pre">autoencoder</span></code> consta de dos partes, el <code class="docutils literal notranslate"><span class="pre">codificador</span></code> y el <code class="docutils literal notranslate"><span class="pre">decodificador</span></code>. La <code class="docutils literal notranslate"><span class="pre">salida</span> <span class="pre">del</span> <span class="pre">codificador</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">representación</span> <span class="pre">reducida</span> <span class="pre">del</span> <span class="pre">patrón</span> <span class="pre">de</span> <span class="pre">entrada</span></code>, y se define en términos de una función vectorial</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{f}:~\boldsymbol{x}\in\mathbb{R}^{l}\longmapsto\boldsymbol{h}\in\mathbb{R}^{m},
    \]</div>
<p>donde</p>
<div class="math notranslate nohighlight">
\[
    h_{i}:=f_{i}(\boldsymbol{x})=\phi_{e}(\boldsymbol{\theta}_{i}^{T}\boldsymbol{x}+b_{i}),\quad i=1,2,\dots,m,
    \]</div>
<p>con <span class="math notranslate nohighlight">\(\phi_{e}\)</span> función de <code class="docutils literal notranslate"><span class="pre">activación</span></code>; esta última suele tomarse como la función <code class="docutils literal notranslate"><span class="pre">sigmoidea</span> <span class="pre">logística</span></code> <span class="math notranslate nohighlight">\(\phi_{e}(\cdot)=\sigma(\cdot)\)</span>. En otras palabras, el <code class="docutils literal notranslate"><span class="pre">codificador</span> <span class="pre">es</span> <span class="pre">una</span> <span class="pre">red</span> <span class="pre">neuronal</span> <span class="pre">feed-forward</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">sola</span> <span class="pre">capa</span> <span class="pre">oculta</span></code></p>
</li>
</ul>
<ul>
<li><p>El <code class="docutils literal notranslate"><span class="pre">decodificador</span></code> es otra función <span class="math notranslate nohighlight">\(\boldsymbol{g}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{g}:~\boldsymbol{h}\in\mathbb{R}^{m}\longmapsto\hat{\boldsymbol{x}}\in\mathbb{R}^{l},
    \]</div>
<p>donde</p>
<div class="math notranslate nohighlight">
\[
    \hat{x}_{j}=g_{j}(\boldsymbol{h})=\phi_{d}(\boldsymbol{\theta}'^{T}\boldsymbol{h}+b_{j}'),\quad j=1,\dots,l.
    \]</div>
<p>La función de <code class="docutils literal notranslate"><span class="pre">activación</span></code> <span class="math notranslate nohighlight">\(\phi_{d}\)</span> suele ser la identidad (<code class="docutils literal notranslate"><span class="pre">reconstrucción</span> <span class="pre">lineal</span></code>) o la <code class="docutils literal notranslate"><span class="pre">sigmoidea</span> <span class="pre">logística</span></code>.</p>
</li>
</ul>
<ul class="simple">
<li><p>La tarea de entrenamiento consiste en <code class="docutils literal notranslate"><span class="pre">estimar</span> <span class="pre">los</span> <span class="pre">parámetros</span></code></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\Theta:=[\boldsymbol{\theta}_{1},\dots,\boldsymbol{\theta}_{m}],~\boldsymbol{b},~\Theta':=[\boldsymbol{\theta}_{1}',\dots,\boldsymbol{\theta}_{l}'],~\boldsymbol{b}'.
\]</div>
<ul class="simple">
<li><p>Es habitual suponer que <span class="math notranslate nohighlight">\(\Theta'=\Theta^{T}\)</span>. Los parámetros se estiman para que el <code class="docutils literal notranslate"><span class="pre">error</span> <span class="pre">de</span> <span class="pre">reconstrucción</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{e}=\boldsymbol{x}-\hat{\boldsymbol{x}}\)</span>, sobre las muestras de entrada disponibles sea <code class="docutils literal notranslate"><span class="pre">mínimo</span></code>. Normalmente, se utiliza el <code class="docutils literal notranslate"><span class="pre">coste</span> <span class="pre">por</span> <span class="pre">mínimos</span> <span class="pre">cuadrados</span></code> pero también son posibles otras opciones. Las <code class="docutils literal notranslate"><span class="pre">versiones</span> <span class="pre">regularizadas</span></code>, que implican una norma de los parámetros, también es una posibilidad. Si se elige que la activación <span class="math notranslate nohighlight">\(\phi_{e}\)</span> sea la identidad (representación lineal) y <span class="math notranslate nohighlight">\(m &lt; l\)</span> (para evitar la trivialidad), el <code class="docutils literal notranslate"><span class="pre">autoencoder</span> <span class="pre">es</span> <span class="pre">equivalente</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">técnica</span> <span class="pre">PCA</span></code>.</p></li>
</ul>
</section>
<section id="aplicacion-procesamiento-del-lenguaje-natural">
<h2><span class="section-number">8.19. </span>Aplicación: Procesamiento del Lenguaje Natural<a class="headerlink" href="#aplicacion-procesamiento-del-lenguaje-natural" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Iniciamos verificando que <code class="docutils literal notranslate"><span class="pre">Tensorflow</span></code> está correctamente instalado, y que, además, <code class="docutils literal notranslate"><span class="pre">puede</span> <span class="pre">utilizar</span> <span class="pre">la</span> <span class="pre">GPU</span> <span class="pre">disponible</span> <span class="pre">en</span> <span class="pre">su</span> <span class="pre">computadora</span></code>. En este caso corresponde a una tarjeta de video dedicada <code class="docutils literal notranslate"><span class="pre">GTX</span> <span class="pre">4070</span> <span class="pre">Maxq</span> <span class="pre">12G</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tensor Flow Version: </span><span class="si">{</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">);</span>
<span class="nb">print</span><span class="p">();</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Python </span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">);</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pandas </span><span class="si">{</span><span class="n">pd</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">);</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scikit-Learn </span><span class="si">{</span><span class="n">sk</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">);</span>
<span class="n">gpu</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">list_physical_devices</span><span class="p">(</span><span class="s1">&#39;GPU&#39;</span><span class="p">))</span><span class="o">&gt;</span><span class="mi">0</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPU is&quot;</span><span class="p">,</span> <span class="s2">&quot;available&quot;</span> <span class="k">if</span> <span class="n">gpu</span> <span class="k">else</span> <span class="s2">&quot;NOT AVAILABLE&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tensor Flow Version: 2.17.0

Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Pandas 2.2.3
Scikit-Learn 1.5.2
GPU is available
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>La librería <code class="docutils literal notranslate"><span class="pre">gensim</span></code> utilizada en la presente implementación, debe ser instalada en su <code class="docutils literal notranslate"><span class="pre">versión</span> <span class="pre">3.8.3</span></code> usando la orden. Además, debe instalar <code class="docutils literal notranslate"><span class="pre">graphviz</span></code> del siguiente sitio web (ver <a class="reference external" href="https://graphviz.org/download/">GraphViz</a>)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">gensim</span><span class="o">==</span><span class="mf">3.8.3</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">(</span><span class="n">desc</span><span class="o">=</span><span class="s2">&quot;progress-bar&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Doc2Vec</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">keras_preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">gensim.models.doc2vec</span> <span class="kn">import</span> <span class="n">TaggedDocument</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;https://raw.githubusercontent.com/lihkir/Data/main/spam_text_class.csv&#39;</span><span class="p">,</span><span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span><span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;latin-1&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;Category&#39;</span><span class="p">,</span><span class="s1">&#39;Message&#39;</span><span class="p">]]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">notnull</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">])]</span>
<span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Message&#39;</span><span class="p">:</span><span class="s1">&#39;Message&#39;</span><span class="p">},</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Category</th>
      <th>Message</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>ham</td>
      <td>Go until jurong point, crazy.. Available only ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ham</td>
      <td>Ok lar... Joking wif u oni...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>spam</td>
      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>ham</td>
      <td>U dun say so early hor... U c already then say...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>ham</td>
      <td>Nah I don't think he goes to usf, he lives aro...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5572, 2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5572</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>87265
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc_file_defaults</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cnt_pro</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Category&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">cnt_pro</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">cnt_pro</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of Occurrences&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Category&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2bdbac57e597036293ac9088d17c738b25c90c68dad1965d59ca195ccb909ef3.png" src="_images/2bdbac57e597036293ac9088d17c738b25c90c68dad1965d59ca195ccb909ef3.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_message</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
    <span class="n">example</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">index</span> <span class="o">==</span> <span class="n">index</span><span class="p">][[</span><span class="s1">&#39;Message&#39;</span><span class="p">,</span> <span class="s1">&#39;Category&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">example</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Message:&#39;</span><span class="p">,</span> <span class="n">example</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">print_message</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>URGENT! You have won a 1 week FREE membership in our Â£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&amp;C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18
Message: spam
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_message</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...
Message: ham
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Preprocesamiento</span> <span class="pre">de</span> <span class="pre">Texto</span></code>: A continuación, establecemos una función para <code class="docutils literal notranslate"><span class="pre">transformar</span> <span class="pre">el</span> <span class="pre">texto</span> <span class="pre">a</span> <span class="pre">minúsculas</span> <span class="pre">y</span> <span class="pre">eliminar</span> <span class="pre">signos</span> <span class="pre">de</span> <span class="pre">puntuación/símbolos</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">palabras</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">lxml</span>
<span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="nn">re</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="k">def</span> <span class="nf">cleanText</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="s2">&quot;html.parser&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\|\|\|&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span> 
    <span class="n">text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;http\S+&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;&lt;URL&gt;&#39;</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">cleanText</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">cleanText</span><span class="p">)</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.000001</span> <span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;/home/lihkir/nltk_data&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt_tab&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package punkt_tab to /home/lihkir/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">tokens</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">tokens</span>

<span class="n">test_message</span> <span class="o">=</span> <span class="s2">&quot;¡Hola mundo! Este es un test.&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenize_text</span><span class="p">(</span><span class="n">test_message</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;¡hola&#39;, &#39;mundo&#39;, &#39;!&#39;, &#39;este&#39;, &#39;es&#39;, &#39;un&#39;, &#39;test&#39;, &#39;.&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_tagged</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">TaggedDocument</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">tokenize_text</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]),</span> <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">Category</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_tagged</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="n">TaggedDocument</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">tokenize_text</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]),</span> <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">Category</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Número máximo de palabras a utilizar (<code class="docutils literal notranslate"><span class="pre">más</span> <span class="pre">frecuentes</span></code>)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_fatures</span> <span class="o">=</span> <span class="mi">500000</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Número máximo de palabras en cada reclamación.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">max_fatures</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="s1">&#39;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Found </span><span class="si">%s</span><span class="s1"> unique tokens.&#39;</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 5572 unique tokens.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Message&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of data tensor:&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of data tensor: (5572, 50)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_tagged</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([TaggedDocument(words=[&#39;and&#39;, &#39;also&#39;, &#39;i&#39;, &quot;&#39;ve&quot;, &#39;sorta&#39;, &#39;blown&#39;, &#39;him&#39;, &#39;off&#39;, &#39;a&#39;, &#39;couple&#39;, &#39;times&#39;, &#39;recently&#39;, &#39;so&#39;, &#39;id&#39;, &#39;rather&#39;, &#39;not&#39;, &#39;tet&#39;, &#39;him&#39;, &#39;out&#39;, &#39;of&#39;, &#39;the&#39;, &#39;blue&#39;, &#39;looking&#39;, &#39;for&#39;, &#39;weed&#39;], tags=[&#39;ham&#39;]),
       TaggedDocument(words=[&#39;mmm&#39;, &#39;thats&#39;, &#39;better&#39;, &#39;now&#39;, &#39;i&#39;, &#39;got&#39;, &#39;a&#39;, &#39;roast&#39;, &#39;down&#39;, &#39;me&#39;, &#39;!&#39;, &#39;iâ\x92d&#39;, &#39;b&#39;, &#39;better&#39;, &#39;if&#39;, &#39;i&#39;, &#39;had&#39;, &#39;a&#39;, &#39;few&#39;, &#39;drinks&#39;, &#39;down&#39;, &#39;me&#39;, &#39;2&#39;, &#39;!&#39;, &#39;good&#39;, &#39;indian&#39;, &#39;?&#39;], tags=[&#39;ham&#39;]),
       TaggedDocument(words=[&#39;mm&#39;, &#39;have&#39;, &#39;some&#39;, &#39;kanji&#39;, &#39;dont&#39;, &#39;eat&#39;, &#39;anything&#39;, &#39;heavy&#39;, &#39;ok&#39;], tags=[&#39;ham&#39;]),
       ...,
       TaggedDocument(words=[&#39;prabha&#39;, &#39;..&#39;, &#39;i&#39;, &quot;&#39;m&quot;, &#39;soryda&#39;, &#39;..&#39;, &#39;realy&#39;, &#39;..&#39;, &#39;frm&#39;, &#39;heart&#39;, &#39;i&#39;, &quot;&#39;m&quot;, &#39;sory&#39;], tags=[&#39;ham&#39;]),
       TaggedDocument(words=[&#39;nt&#39;, &#39;joking&#39;, &#39;seriously&#39;, &#39;i&#39;, &#39;told&#39;], tags=[&#39;ham&#39;]),
       TaggedDocument(words=[&#39;did&#39;, &#39;he&#39;, &#39;just&#39;, &#39;say&#39;, &#39;somebody&#39;, &#39;is&#39;, &#39;named&#39;, &#39;tampa&#39;], tags=[&#39;ham&#39;])],
      dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2v_model</span> <span class="o">=</span> <span class="n">Doc2Vec</span><span class="p">(</span><span class="n">dm</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dm_mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.065</span><span class="p">,</span> <span class="n">min_alpha</span><span class="o">=</span><span class="mf">0.065</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2v_model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_tagged</span><span class="o">.</span><span class="n">values</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8032474.25it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">d2v_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">shuffle</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_tagged</span><span class="o">.</span><span class="n">values</span><span class="p">)]),</span> <span class="n">total_examples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">train_tagged</span><span class="o">.</span><span class="n">values</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2v_model</span><span class="o">.</span><span class="n">alpha</span> <span class="o">-=</span> <span class="mf">0.002</span>
    <span class="n">d2v_model</span><span class="o">.</span><span class="n">min_alpha</span> <span class="o">=</span> <span class="n">d2v_model</span><span class="o">.</span><span class="n">alpha</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 7119581.84it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 7012745.37it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 7324911.47it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8884588.43it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8867729.63it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 7183051.82it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8857645.03it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8593772.56it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8631868.34it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8945814.54it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8797615.81it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8035236.45it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8804245.51it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8478398.98it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8830864.54it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8459981.02it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8834203.25it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 6958447.76it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8438594.29it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8466111.44it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8861004.01it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8921904.38it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8767905.28it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8390114.03it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8751485.99it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8844234.51it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8683191.22it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8568561.64it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 8973297.84it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 5571/5571 [00:00&lt;00:00, 5655001.84it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 2.83 s, sys: 50 ms, total: 2.88 s
Wall time: 2.94 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">d2v_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Doc2Vec&lt;dm/m,d20,n5,w8,s0.001&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">num_words</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>9362
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index_to_key</span>
<span class="nb">print</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;frmcloud&#39;, &#39;2mwen&#39;, &#39;limited&#39;, &#39;counts&#39;, &#39;asda&#39;, &#39;slowing&#39;, &#39;particularly&#39;, &#39;torrents&#39;, &#39;breadstick&#39;, &#39;thepub&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span><span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">dv</span><span class="p">)):</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">d2v_model</span><span class="o">.</span><span class="n">dv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">vec</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1000</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">dv</span><span class="p">)</span>
        <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">vec</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">vec</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>  
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0
KeyedVectors&lt;vector_size=20, 2 keys&gt;
[-1.181241   -3.710024   -1.5540404   1.7452742   0.3889721  -0.5239634
 -2.4200559  -0.16418658 -3.6092436   0.85619104  0.46824312 -0.5189654
  0.04126288  0.744131   -3.1146233  -1.9534556   0.8371728   2.528787
 -3.689852   -2.0174308 ]
-1.181241
1
KeyedVectors&lt;vector_size=20, 2 keys&gt;
[ -4.3021917    4.3163033   -3.0488737    1.4984138    7.108087
 -10.087342    -3.94172     -6.9372716    2.3337421   -7.018936
   9.643784     7.381326    -5.2318425   -0.19244012  -0.61319727
   2.9983945    1.8317717   -9.758069    -4.7036347    3.3743062 ]
4.3163033
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;urgent&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;anythingtomorrow&#39;, 0.7989494800567627),
 (&#39;cantdo&#39;, 0.7467942237854004),
 (&#39;myparents&#39;, 0.7182999849319458),
 (&#39;missed&#39;, 0.717624843120575),
 (&#39;cherish&#39;, 0.714693546295166),
 (&#39;colleagues&#39;, 0.705409586429596),
 (&#39;holder&#39;, 0.7042951583862305),
 (&#39;dear&#39;, 0.702792763710022),
 (&#39;0906346330.&#39;, 0.7017916440963745),
 (&#39;09061743811&#39;, 0.6933843493461609)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cherish&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;missed&#39;, 0.8055727481842041),
 (&#39;intrepid&#39;, 0.7639503479003906),
 (&#39;urgent&#39;, 0.7146934866905212),
 (&#39;duo&#39;, 0.7129536271095276),
 (&#39;thank&#39;, 0.7003694772720337),
 (&#39;wish&#39;, 0.6905673742294312),
 (&#39;appreciate&#39;, 0.6860354542732239),
 (&#39;enjoyed&#39;, 0.6818588376045227),
 (&#39;hope&#39;, 0.6803081631660461),
 (&#39;wishin&#39;, 0.6780060529708862)]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Modelo <code class="docutils literal notranslate"><span class="pre">LSTM</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers.legacy</span> <span class="kn">import</span> <span class="n">Adam</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Layer inicial</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Vectores de palabras emmbed</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">d2v_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">embedding_matrix</span><span class="p">],</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Aprender las correlaciones</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">split_input</span><span class="p">(</span><span class="n">sequence</span><span class="p">):</span>
     <span class="k">return</span> <span class="n">sequence</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sequence</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Esqueleto del modelo de salida</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential_1"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                    </span>┃<span style="font-weight: bold"> Output Shape           </span>┃<span style="font-weight: bold">       Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ embedding (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)           │ ?                      │       <span style="color: #00af00; text-decoration-color: #00af00">187,260</span> │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ lstm (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                     │ ?                      │   <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                 │ ?                      │   <span style="color: #00af00; text-decoration-color: #00af00">0</span> (unbuilt) │
└─────────────────────────────────┴────────────────────────┴───────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">187,260</span> (731.48 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">187,260</span> (731.48 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(),</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Category&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">Y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">Y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4736, 50) (4736, 2)
(836, 50) (836, 2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">dump</span><span class="p">,</span> <span class="n">load</span>

<span class="n">history_lstm</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;history_lstm.joblib&#39;</span><span class="p">):</span>
    <span class="n">history_lstm</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="s1">&#39;history_lstm.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El archivo &#39;history_lstm.joblib&#39; ya existe. Se ha cargado el historial del entrenamiento.&quot;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">history_lstm</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">dump</span><span class="p">(</span><span class="n">history_lstm</span><span class="o">.</span><span class="n">history</span><span class="p">,</span> <span class="s1">&#39;history_lstm.joblib&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_lstm.joblib&#39;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/50
148/148 - 2s - 14ms/step - accuracy: 0.9267 - loss: 0.2315
Epoch 2/50
148/148 - 1s - 7ms/step - accuracy: 0.9850 - loss: 0.0600
Epoch 3/50
148/148 - 1s - 7ms/step - accuracy: 0.9930 - loss: 0.0303
Epoch 4/50
148/148 - 1s - 7ms/step - accuracy: 0.9966 - loss: 0.0157
Epoch 5/50
148/148 - 1s - 8ms/step - accuracy: 0.9979 - loss: 0.0087
Epoch 6/50
148/148 - 1s - 7ms/step - accuracy: 0.9987 - loss: 0.0047
Epoch 7/50
148/148 - 1s - 7ms/step - accuracy: 0.9985 - loss: 0.0055
Epoch 8/50
148/148 - 1s - 7ms/step - accuracy: 0.9994 - loss: 0.0017
Epoch 9/50
148/148 - 1s - 6ms/step - accuracy: 0.9996 - loss: 0.0012
Epoch 10/50
148/148 - 1s - 7ms/step - accuracy: 0.9998 - loss: 8.0327e-04
Epoch 11/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 5.5067e-04
Epoch 12/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 4.1406e-04
Epoch 13/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 3.2322e-04
Epoch 14/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 2.5748e-04
Epoch 15/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 2.1433e-04
Epoch 16/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 1.7403e-04
Epoch 17/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 1.5136e-04
Epoch 18/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 1.2444e-04
Epoch 19/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 1.0496e-04
Epoch 20/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 9.1037e-05
Epoch 21/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 8.0365e-05
Epoch 22/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 6.9303e-05
Epoch 23/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 6.1919e-05
Epoch 24/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 5.5886e-05
Epoch 25/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 4.9211e-05
Epoch 26/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 4.3048e-05
Epoch 27/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 3.8843e-05
Epoch 28/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 3.4705e-05
Epoch 29/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 3.1055e-05
Epoch 30/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 2.8125e-05
Epoch 31/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 2.5803e-05
Epoch 32/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 2.3313e-05
Epoch 33/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 2.1367e-05
Epoch 34/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 1.9564e-05
Epoch 35/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 1.8149e-05
Epoch 36/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 1.6110e-05
Epoch 37/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 1.4891e-05
Epoch 38/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 1.3603e-05
Epoch 39/50
148/148 - 1s - 6ms/step - accuracy: 1.0000 - loss: 1.2878e-05
Epoch 40/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 1.1417e-05
Epoch 41/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 1.0457e-05
Epoch 42/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 9.6314e-06
Epoch 43/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 8.8906e-06
Epoch 44/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 8.4654e-06
Epoch 45/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 7.5165e-06
Epoch 46/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 6.9618e-06
Epoch 47/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 6.4309e-06
Epoch 48/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 5.9360e-06
Epoch 49/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 5.4518e-06
Epoch 50/50
148/148 - 1s - 7ms/step - accuracy: 1.0000 - loss: 5.1116e-06
El entrenamiento se ha completado y el historial ha sido guardado en &#39;history_lstm.joblib&#39;.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_lstm</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;model accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;acc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5ba7d9d76e0137edec6b2ac9c4ddfb54697d45a06ea36bda9c1751eaf2c27fe4.png" src="_images/5ba7d9d76e0137edec6b2ac9c4ddfb54697d45a06ea36bda9c1751eaf2c27fe4.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history_lstm</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;model loss&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;epochs&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/53da0a62929b8ec9e06819961af543855103281cef00522baafdf3d48dda379f.png" src="_images/53da0a62929b8ec9e06819961af543855103281cef00522baafdf3d48dda379f.png" />
</div>
</div>
<ul class="simple">
<li><p>Evaluar el modelo</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train: </span><span class="si">%.3f</span><span class="s1">, Test: </span><span class="si">%.4f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_acc</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>148/148 - 0s - 3ms/step - accuracy: 1.0000 - loss: 4.7276e-06
27/27 - 0s - 7ms/step - accuracy: 0.9868 - loss: 0.1246
Train: 1.000, Test: 0.9868
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Predecir probabilidades para el conjunto de prueba</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">yhat_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1.0000000e+00 4.1286904e-13]
 [1.0000000e+00 3.4418371e-12]
 [1.0000000e+00 4.1827555e-13]
 ...
 [1.0000000e+00 3.7423889e-13]
 [1.0000000e+00 2.5833727e-13]
 [1.0000000e+00 5.0445867e-13]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Predecir clases crisp para el conjunto de prueba</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">yhat_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">yhat_classes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1
 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0
 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0
 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0
 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0
 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0
 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0
 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0
 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Reducir a matriz 1d</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat_probs</span> <span class="o">=</span> <span class="n">yhat_probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rounded_labels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rounded_labels</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">rounded_labels</span><span class="p">,</span> <span class="n">yhat_classes</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[727,   2],
       [  9,  98]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lstm_val</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">rounded_labels</span><span class="p">,</span> <span class="n">yhat_classes</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">lstm_val</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">linecolor</span><span class="o">=</span><span class="s1">&#39;cyan&#39;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;BuPu&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;LSTM Classification Confusion Matrix&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Y predict&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y test&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/26350a999f0ca6ca69b210a75147b9a48a621b677627a6bbd529e8e57bfb89c1.png" src="_images/26350a999f0ca6ca69b210a75147b9a48a621b677627a6bbd529e8e57bfb89c1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">validation_size</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">X_validate</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="o">-</span><span class="n">validation_size</span><span class="p">:]</span>
<span class="n">Y_validate</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="o">-</span><span class="n">validation_size</span><span class="p">:]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[:</span><span class="o">-</span><span class="n">validation_size</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[:</span><span class="o">-</span><span class="n">validation_size</span><span class="p">]</span>
<span class="n">score</span><span class="p">,</span><span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;score: </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">score</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;acc: </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">acc</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">20/20</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 4ms/step - accuracy: 0.9835 - loss: 0.1280   
score: 0.16
acc: 0.98
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;Mymodel.keras&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Pruebas con conjuntos de datos nuevos y diferentes de los datos para construir el modelo.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Congratulations! you have won a $1,000 Walmart gift card. Go to http://bit.ly/123456 to claim now.&#39;</span><span class="p">]</span>
<span class="n">seq</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

<span class="n">padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">padded</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ham&#39;</span><span class="p">,</span><span class="s1">&#39;spam&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">1/1</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 13ms/step
[[3.8133455e-11 1.0000000e+00]] spam
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">message</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;thanks for accepting my request to connect&#39;</span><span class="p">]</span>
<span class="n">seq</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

<span class="n">padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">padded</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ham&#39;</span><span class="p">,</span><span class="s1">&#39;spam&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">1/1</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 14ms/step
[[1.000000e+00 8.241917e-13]] ham
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "tf"
        },
        kernelOptions: {
            name: "tf",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'tf'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="svm_model.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Máquinas de vectores de soporte</p>
      </div>
    </a>
    <a class="right-next"
       href="dl_computer_vision.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>Deep Learning para Visión por Computadora</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradiente-descendente">8.1. Gradiente descendente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales">8.2. Redes neuronales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-perceptron">8.3. El perceptrón</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-multicapa-feed-forward">8.4. Redes Neuronales Multicapa Feed-Forward</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-totalmente-conectadas">8.5. Redes Totalmente Conectadas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-algoritmo-de-backpropagation">8.6. El Algoritmo De Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-esquema-de-backpropagation-para-gradiente-descendente">8.7. El Esquema De Backpropagation Para Gradiente Descendente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-gradientes">8.8. Cálculo de gradientes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-delta-nj-r">8.9. Cálculo de <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-de-redes-neuronales">8.10. Tuning de Redes Neuronales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analisis-de-malware-por-api-calls">8.11. Análisis de Malware por API calls</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-convolucionales">8.12. Redes Neuronales Convolucionales</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-necesidad-de-convoluciones">8.12.1. La necesidad de convoluciones</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-etapa-de-convolucion">8.12.2. La etapa de convolución</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#el-paso-de-la-no-linealidad">8.12.3. El paso de la no linealidad</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#la-etapa-de-agrupacion">8.12.4. La etapa de agrupación</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolucion-sobre-volumenes">8.12.5. Convolución sobre volúmenes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#red-en-red-y-convolucion-1-1">8.12.6. Red en red y convolución 1 × 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arquitectura-cnn-completa">8.12.7. Arquitectura CNN completa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arquitecturas-de-redes-neuronales-convolucionales">8.12.8. Arquitecturas de Redes Neuronales Convolucionales</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lenet-5">8.12.9. LeNet-5</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet">8.12.10. AlexNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vgg-16">8.12.11. VGG-16</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#googlenet-y-la-red-inception">8.12.12. GoogleNet y la red Inception</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-residuales-resnets">8.12.13. Redes residuales (ResNets)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reconocimiento-facial-de-emociones">8.12.14. Reconocimiento facial de emociones</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-recurrentes">8.13. Redes Neuronales Recurrentes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-en-tiempo">8.13.1. Backpropagation en tiempo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#desvanecimiento-y-explosion-de-gradientes">8.13.2. Desvanecimiento y explosión de gradientes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#red-de-memoria-a-largo-plazo-lstm">8.14. Red de memoria a largo plazo (LSTM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#atencion-y-memoria">8.15. Atención y Memoria</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entrenamiento-adversario">8.16. Entrenamiento adversario</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-generativos-profundos">8.17. Modelos Generativos Profundos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maquina-de-boltzmann-restringida">8.17.1. Máquina de Boltzmann Restringida</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoencoders">8.18. Autoencoders</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-procesamiento-del-lenguaje-natural">8.19. Aplicación: Procesamiento del Lenguaje Natural</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lihki Rubio, Ph.D.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p>Lihki Rubio, Ph.D. All rights reserved.</p>
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>